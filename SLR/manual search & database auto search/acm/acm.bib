@inproceedings{10.1145/3638530.3664174,
author = {Bin Murtaza, Sardar and Mccoy, Aidan and Ren, Zhiyuan and Murphy, Aidan and Banzhaf, Wolfgang},
title = {LLM Fault Localisation within Evolutionary Computation Based Automated Program Repair},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664174},
doi = {10.1145/3638530.3664174},
abstract = {Repairing bugs can be a daunting task for even a human experienced in debugging, so naturally, attempting to automatically repair programs with a computer system is quite challenging. The existing methods of automated program repair leave a lot of room for improvement. Fault localization, which aims to find lines of code that are potentially buggy, minimises the search space of an automated program repair system. Recent work has shown improvement in these fault localization methods, with the use of Large Language Models. Here, we propose a system where a LLM-based fault localization tool, which we call SemiAutoFL, is used within a fully automatic program repair program, ARJA-e. We show that utilising LLM-based fault localization with ARJA-e can significantly improve its performance on real world bugs. ARJA-e with SemiAutoFL can repair 10 bugs that ARJA-e was previously unable to so do. This finding adds to our understanding of how to improve fault localization and automated program repair, highlighting the potential for more efficient and accurate fault localisation methods being applied to automated program repair.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1824–1829},
numpages = {6},
keywords = {genetic improvement, fault localisation, large language models},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3650212.3680359,
author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
title = {ThinkRepair: Self-Directed Automated Program Repair},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680359},
doi = {10.1145/3650212.3680359},
abstract = {Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.    To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.    Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%∼344.4% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12∼65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1274–1286},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3643795.3648390,
author = {Jiang, Shengbei and Zhang, Jiabao and Chen, Wei and Wang, Bo and Zhou, Jianyi and Zhang, Jie},
title = {Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648390},
doi = {10.1145/3643795.3648390},
abstract = {Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {75–78},
numpages = {4},
keywords = {large language model, fault localization, program repair, software debugging},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.1145/3715004,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715004},
doi = {10.1145/3715004},
abstract = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this paper introduces an innovative APR approach called GiantRepair. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78% on Defects4J v1.2 and 23.40% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Program Repair, Large Language Model, Program Synthesis}
}

@article{10.1145/3736411,
author = {Ahmad, Baleegh and Ah-kiow, Joey and Tan, Benjamin and Karri, Ramesh and Pearce, Hammond},
title = {FLAG: Finding Line Anomalies (in RTL code) with Generative AI},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3736411},
doi = {10.1145/3736411},
abstract = {Bug detection in Hardware Design Languages (HDLs) is an important problem in the System-on-Chip (SoC) development cycle. It is crucial to find defects at the earliest stage possible. While most fault localization requires the use of ‘tests’ (e.g. test benches, fuzzing and assertions) and a simulation or emulation framework, the advent of Large Language Models (LLMs) provides an opportunity for a test-free fault localization approach. This paper proposes such a tool, called FLAG, which can identify functional and security defects in Register Transfer Level (RTL) code without synthesis or simulation. FLAG combines syntactic and generative AI techniques to implement fault localization in RTL code. It takes an RTL design as an input and outputs a set of line(s) that likely contain defects. It targets elements of RTL code most likely to contain bugs through static analysis means and then implements token-level and line-level analysis to obtain differences in original code and code generated by LLM to identify a line as buggy or not. The token-level approach evaluates each generated token (one at a time) and the line level approach evaluates the entire line generated by the LLM. We evaluate our approach on a corpus of synthetic and real-world bugs, of both functional and security related issues, in Verilog and SystemVerilog. Using line-level analysis, FLAG can identify 38 out of 120 real-world bugs and using token-level analysis, FLAG can identify 32 out of 81 synthetic bugs through the top-5 most likely bug locations identified without tests.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {Fault Localization, RTL, Transformers, Verilog, GPT, LLM}
}

@inproceedings{10.1109/ICSE48619.2023.00128,
author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
title = {Automated Repair of Programs from Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00128},
doi = {10.1109/ICSE48619.2023.00128},
abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1469–1481},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3733599,
author = {Luo, Wenqiang and Keung, Jacky and Yang, Boyang and Ye, He and Le Goues, Claire and Bissyand\'{e}, Tegawend\'{e} F. and Tian, Haoye and Le, Xuan Bach D.},
title = {When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3733599},
doi = {10.1145/3733599},
abstract = {Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair.To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67% for Top@10 and 18.44% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Program Repair, Federated Learning, Large Language Models}
}

@inproceedings{10.1145/3643788.3648020,
author = {Jiang, Nan and Wu, Yi},
title = {RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648020},
doi = {10.1145/3643788.3648020},
abstract = {Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {58–60},
numpages = {3},
keywords = {automated program repair, large language model},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1145/3660773,
author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660773},
doi = {10.1145/3660773},
abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks,                                including automated program repair (APR). In this study, we take a deep dive into automated bug localization                                and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug                                locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach                                uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug                                fixing. This methodological separation of bug localization and fixing using different LLMs enables effective                                integration of diverse contextual information and improved incorporation of inductive biases. We introduce                                Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework                                that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a                                bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We                                investigate various styles of prompting to the bug fixing model to identify the most effective prompts that                                better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art                                (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable                                performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our                                approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50,                                and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness                                of various prompts, we also investigate the impact of additional contextual information such as buggy lines                                and code comments on bug localization, and explore the importance of the adjustment model. Our extensive                                experiments offer valuable insights and answers to critical research questions.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {66},
numpages = {23},
keywords = {Automated Bug Localization and Reapir, Large Language Models}
}

@article{10.1145/3728981,
author = {Ma, Yingwei and Cao, Rongyu and Cao, Yongchang and Zhang, Yue and Chen, Jue and Liu, Yibo and Liu, Yuchen and Li, Binhua and Huang, Fei and Li, Yongbin},
title = {SWE-GPT: A Process-Centric Language Model for Automated Software Improvement},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728981},
doi = {10.1145/3728981},
abstract = {Large language models (LLMs) have demonstrated remarkable performance in code generation, significantly enhancing the coding efficiency of developers. Recent advancements in LLM-based agents have led to significant progress in end-to-end automatic software engineering (ASE), particularly in software maintenance (e.g., fixing software issues) and evolution (e.g., adding new features). Despite these encouraging advances, current research faces two major challenges. First, state-of-the-art performance primarily depends on closed-source models like GPT-4, which significantly limits the technology’s accessibility, and potential for customization in diverse software engineering tasks. This dependence also raises concerns about data privacy, particularly when handling sensitive codebases. Second, these models are predominantly trained on static code data, lacking a deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary characteristics inherent in software development. Consequently, they may face challenges in navigating complex project structures and generating contextually relevant solutions, which can affect their practical utility in real-world scenarios. To address these challenges, our study adopts a software engineering perspective. We recognize that real-world software maintenance and evolution processes encompass not only static code data but also developers’ thought processes, utilization of external tools, and the interaction between different functional personnel. Our objective is to develop an open-source large language model specifically optimized for software improvement, aiming to match the performance of closed-source alternatives while offering greater accessibility and customization potential. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT 7B and Lingma SWE-GPT 72B. By learning from and simulating real-world code submission activities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-solving inherent in software development process—such as repository understanding, fault localization, and patch generation—thereby achieving a more comprehensive understanding of software improvement processes. We conducted experimental evaluations using SWE-bench-Verified benchmark (comprising 500 real GitHub issues), recently proposed by OpenAI. The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20% of the GitHub issues, marking a significant improvement in automatic issue resolution (22.76% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source models (31.80% issues of GPT-4o resolved). Notably, Lingma SWE-GPT 7B resolves 18.20% of the issues, surpassing the 17.20% resolution rate of Llama 3.1 70B, highlighting the potential for applying smaller models to ASE tasks.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA104},
numpages = {22},
keywords = {Automated Program Repair, Automatic Software Engineering (ASE), Fault Localization, Large Language Models (LLMs), Software Engineering Agents}
}

@inproceedings{10.1109/SCW63240.2024.00179,
author = {Subramanian, Shashank and Rrapaj, Ermal and Harrington, Peter and Chheda, Smeet and Farrell, Steven and Austin, Brian and Williams, Samuel and Wright, Nicholas and Bhimji, Wahid},
title = {Comprehensive Performance Modeling and System Design Insights for Foundation Models},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00179},
doi = {10.1109/SCW63240.2024.00179},
abstract = {Generative AI, in particular large transformer models, are increasingly driving HPC system design in science and industry. We analyze performance characteristics of such transformer models and discuss their sensitivity to the transformer type, parallelization strategy, and HPC system features (accelerators and interconnects). We utilize a performance model that allows us to explore this complex design space and highlight its key components. We find that different transformer types demand different parallelism and system characteristics at different training regimes. Large Language Models are performant with 3D parallelism and amplify network needs only at pretraining scales with reduced dependence on accelerator capacity and bandwidth. On the other hand, long-sequence transformers, representative of scientific foundation models, place a more uniform dependence on network and capacity with necessary 4D parallelism. Our analysis emphasizes the need for closer performance modeling of different transformer types keeping system features in mind and demonstrates a path towards this.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1380–1397},
numpages = {18},
keywords = {parallelism, performance modeling, transformers},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1145/3722229,
author = {AlOmar, Eman Abdullah},
title = {Nurturing Code Quality: Leveraging Static Analysis and Large Language Models for Software Quality in Education},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
url = {https://doi.org/10.1145/3722229},
doi = {10.1145/3722229},
abstract = {Large Language Models (LLMs), such as ChatGPT, have become widely popular for various software engineering tasks, including programming, testing, code review, and program comprehension. However, their impact on improving software quality in educational settings remains uncertain. This article explores our experience teaching the use of Programming Mistake Detector (PMD) to foster a culture of bug fixing and leverage LLM to improve software quality in the classroom. This article discusses the results of an experiment involving 155 submissions that carried out a code review activity of 1,658 rules. Our quantitative and qualitative analyses reveal that a set of PMD quality issues influences the acceptance or rejection of the issues, and design-related categories that take longer to resolve. Although students acknowledge the potential of using ChatGPT during code review, some skepticism persists. Further, constructing prompts for ChatGPT that possess clarity, complexity, and context nurtures vital learning outcomes, such as enhanced critical thinking, and among the 1,658 issues analyzed, 93% of students indicated that ChatGPT did not identify any additional issues beyond those detected by PMD. Conversations between students and ChatGPT encompass five categories, including ChatGPT’s use of affirmation phrases like “certainly” regarding bug fixing decisions, and apology phrases such as “apologize” when resolving challenges. Through this experiment, we demonstrate that code review can become an integral part of the educational computing curriculum. We envision our findings to enable educators to support students with effective code review strategies, increasing awareness of LLMs, and promoting software quality in education.},
journal = {ACM Trans. Comput. Educ.},
month = may,
articleno = {16},
numpages = {36},
keywords = {large language models, education, bugfix, static analysis, code quality}
}

@inproceedings{10.1145/3641399.3644113,
author = {Roychoudhury, Abhik},
title = {Program Repair and Trusted Automatic Programming},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3644113},
doi = {10.1145/3641399.3644113},
abstract = {Automated program repair can be seen as automated code generation at a micro-scale. The research done in automated program repair is thus particularly relevant today with the movement towards automatic programming using tools like Github Copilot. Since automatically generated code from natural language descriptions lack understanding of program semantics, using semantic analysis techniques to auto-correct or rectify the code is of value. In our work we have proposed the use of semantic or symbolic program analysis techniques to automatically rectify code. Effectively symbolic analysis is used to generalize tests into specifications of intent. These techniques can be employed on manually written code as well as automatically generated code. The techniques have been used for security vulnerability repair in software (thereby achieving autonomous cybersecurity) as well as for supporting intelligent tutoring systems. Apart from the practical value of such techniques, conceptually this gave a new direction to use symbolic reasoning. We use symbolic reasoning to derive a logical constraint which would capture what it means for the program to be "correct" thereby inferring specification about intended program behavior. We will conclude with a forward looking perspective on last mile repair of code generated from large language models, as well as acceptable evidences of correctness for such automatically generated code.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {1},
keywords = {Program Analysis, Programming Language},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/3744367.3744402,
author = {Zhou, Yi and Shan, Zhenyu and Dai, Zeyao and Zhang, Zhao},
title = {CodeEduBench: A Multidimensional Benchmark for Evaluating AI-Driven Code Generation Models in C Programming Education},
year = {2025},
isbn = {9798400715068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744367.3744402},
doi = {10.1145/3744367.3744402},
abstract = {Large language models (LLMs) are a core technology that can facilitate change in programming education. However, the educational applications and impact of LLMs have not yet been thoroughly evaluated. In this research, we develop the CodeEduBench three-dimensional evaluation framework, which assesses the applicability of LLMs for C programming instruction across three aspects: test case creation, code synthesis, and bug fixing. Five widely-used open-source models are chosen for comparative study. The results of this study show that Qwen2.5-coder-32b has excellent overall performance, although its shortcomings in code generation accuracy and error correction. In addition, this study proposes model selection strategies and optimization pathways to provide methodological support for the scientific application of LLM in programming education.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems},
pages = {217–222},
numpages = {6},
keywords = {C language, large language models, multidimensional benchmark, programming education evaluation},
location = {
},
series = {ICAIES '25}
}

@inproceedings{10.1145/3658644.3691412,
author = {Zhang, Elisa and Sun, Shiyu and Xing, Yunlong and Sun, Kun},
title = {Poster: Repairing Bugs with the Introduction of New Variables: A Multi-Agent Large Language Model},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3691412},
doi = {10.1145/3658644.3691412},
abstract = {Trained on billions of tokens, large language models (LLMs) have a broad range of empirical knowledge which enables them to generate software patches with complex repair patterns. We leverage the powerful code-fixing capabilities of LLMs and propose VarPatch, a multi-agent conversational automated program repair (APR) technique that iteratively queries the LLM to generate software patches by providing various prompts and context information. VarPatch focuses on the variable addition repair pattern, as previous APR tools struggle to introduce and use new variables to fix buggy code. Additionally, we summarize commonly used APIs and identify four repair patterns involving new variable addition. Our evaluation on the Defects4J 1.2 dataset shows that VarPatch can repair 69% more bugs than baseline tools and over 8 times more bugs than GPT-4.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {4961–4963},
numpages = {3},
keywords = {automated program repair, large language model, multiple agents},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3641399.3641434,
author = {Agarwal, Shivali and Chimalakonda, Sridhar and Krishnan, Saravanan and Kanvar, Vini and Shah, Samveg},
title = {Tutorial Report on Legacy Software Modernization: A Journey From Non-AI to Generative AI Approaches},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641434},
doi = {10.1145/3641399.3641434},
abstract = {Dealing with ageing software is a reality of the industry, and even open source software systems. This is a great opportunity for the software engineering researchers to apply the traditional techniques of program analysis to solve problems of refactoring and modernization. The generative AI advancements have opened up a whole new world of possibilities for software engineering tasks such as code generation, code translation, bug fixing among others. Industry is keen on exploring scalable solutions for refactoring, automated testing and now automatic code generation. In this tutorial, we aim to (i) provide a background and overview of legacy software modernization and its importance amidst the emergence of AI-Assisted software and Generative AI (ii) discuss the challenges being faced by industry due to monolithic legacy code and systems (iii) introduce architectural and technological paradigms to modernize this legacy or ageing software (iv) highlight the research and engineering problems that remain to be solved in this space discussing the opportunities for the software engineering research community.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {19},
numpages = {3},
keywords = {Code LLMs, Legacy Software Modernization, Program Analysis, Refactoring},
location = {Bangalore, India},
series = {ISEC '24}
}

@article{10.1145/3715703,
author = {Ceurstemont, Sandrine},
title = {Automating Tools for Prompt Engineering},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3715703},
doi = {10.1145/3715703},
abstract = {Prompt engineering can help users obtain more relevant and accurate results from generative AI models.},
journal = {Commun. ACM},
month = apr,
pages = {15–17},
numpages = {3}
}

@inproceedings{10.1145/3597503.3623342,
author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
title = {Large Language Models for Test-Free Fault Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623342},
doi = {10.1145/3597503.3623342},
abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3%--54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {17},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643651.3659892,
author = {Zhang, Lan and Zou, Qingtian and Singhal, Anoop and Sun, Xiaoyan and Liu, Peng},
title = {Evaluating Large Language Models for Real-World Vulnerability Repair in C/C++ Code},
year = {2024},
isbn = {9798400705564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643651.3659892},
doi = {10.1145/3643651.3659892},
abstract = {The advent of Large Language Models (LLMs) has enabled advancement in automated code generation, translation, and summarization. Despite their promise, evaluating the use of LLMs in repairing real-world code vulnerabilities remains underexplored. In this study, we address this gap by evaluating the capability of advanced LLMs, such as ChatGPT-4 and Claude, in fixing memory corruption vulnerabilities in real-world C/C++ code. We meticulously curated 223 real-world C/C++ code snippets encompassing a spectrum of memory corruption vulnerabilities, ranging from straightforward memory leaks to intricate buffer errors. Our findings demonstrate the proficiency of LLMs in rectifying simple memor errors like leaks, where fixes are confined to localized code segments. However, their effectiveness diminishes when addressing complicated vulnerabilities necessitating reasoning about cross-cutting concerns and deeper program semantics. Furthermore, we explore techniques for augmenting LLM performance by incorporating additional knowledge. Our results shed light on both the strengths and limitations of LLMs in automated program repair on genuine code, underscoring the need for advancements in reasoning abilities for handling complex code repair tasks.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {49–58},
numpages = {10},
keywords = {deep learning, large language models, program repair},
location = {Porto, Portugal},
series = {IWSPA '24}
}

@inproceedings{10.1145/3691620.3695066,
author = {Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang},
title = {Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695066},
doi = {10.1145/3691620.3695066},
abstract = {Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {719–731},
numpages = {13},
keywords = {automated program repair, parameter-effective fine-tuning, large language model, execution-based evaluation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3663548.3675660,
author = {Seo, JooYoung and Kamath, Sanchita S. and Zeidieh, Aziz and Venkatesh, Saairam and McCurry, Sean},
title = {MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675660},
doi = {10.1145/3663548.3675660},
abstract = {This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users’ visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users’ decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {57},
numpages = {31},
keywords = {Accessibility, Blind, Data Visualization, Generative AI, Large Language Models, Low Vision, Multimodality, Screen Readers},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3663529.3663815,
author = {Wu, Yonghao and Li, Zheng and Zhang, Jie M. and Liu, Yong},
title = {ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663815},
doi = {10.1145/3663529.3663815},
abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.  To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {642–646},
numpages = {5},
keywords = {Dataset, Fault Localization, Large Language Model, Program Repair},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3733102.3733138,
author = {He, Yiran and Cao, Yun and Yang, Bowen and Zhang, Zeyu},
title = {Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics},
year = {2025},
isbn = {9798400718878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733102.3733138},
doi = {10.1145/3733102.3733138},
abstract = {The rapid development of generative AI facilitates content creation and makes image manipulation easier and more difficult to detect. While multimodal Large Language Models (LLMs) have encoded rich world knowledge, they are not inherently tailored for combating AI-generated Content (AIGC) and struggle to comprehend local forgery details. In this work, we investigate the application of multimodal LLMs in forgery detection. We propose a framework capable of evaluating image authenticity, localizing tampered regions, providing evidence, and tracing generation methods based on semantic tampering clues. Our method demonstrates that the potential of LLMs in forgery analysis can be effectively unlocked through meticulous prompt engineering and the application of few-shot learning techniques. We conduct qualitative and quantitative experiments and show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in LaMa, which is competitive with state-of-the-art AIGC detection methods. We further discuss the limitations of multimodal LLMs in such tasks and propose potential improvements.},
booktitle = {Proceedings of the 2025 ACM Workshop on Information Hiding and Multimedia Security},
pages = {24–34},
numpages = {11},
keywords = {forensics, DeepFake, Large Language models},
location = {
},
series = {IH&amp;MMSEC '25}
}

@inproceedings{10.1145/3706599.3720226,
author = {Kruk, Francesco and Herath, Savindu and Choudhury, Prithwiraj},
title = {BanglAssist: A Bengali-English Generative AI Chatbot for Code-Switching and Dialect-Handling in Customer Service},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720226},
doi = {10.1145/3706599.3720226},
abstract = {In recent years, large language models (LLMs) have demonstrated exponential improvements that promise transformative opportunities across various industries. Their ability to generate human-like text and ensure continuous availability facilitates the creation of interactive service chatbots aimed at enhancing customer experience and streamlining enterprise operations. Despite their potential, LLMs face critical challenges, such as a susceptibility to hallucinations and difficulties handling complex linguistic scenarios, notably code switching and dialectal variations. To address these challenges, this paper describes the design of a multilingual chatbot for Bengali-English customer service interactions utilizing retrieval-augmented generation (RAG) and targeted prompt engineering. This research provides valuable insights for the human-computer interaction (HCI) community, emphasizing the importance of designing systems that accommodate linguistic diversity to benefit both customers and businesses. By addressing the intersection of generative AI and cultural heterogeneity, this late-breaking work inspires future innovations in multilingual and multicultural HCI.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {93},
numpages = {9},
keywords = {Generative AI, Chatbot, Code Switching, Dialect Handling, Multilingual Retrieval, Cross-Lingual Retrieval, Customer Service},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3726302.3730364,
author = {Zhou, Yujia and Ji, Wei and Ge, Xuri and Ai, Qingyao and Jose, Joemon M. and Liu, Yiqun},
title = {The 1st NIP@IR Workshop on New Interaction Paradigms for Information Retrieval in the Era of Generative AI},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730364},
doi = {10.1145/3726302.3730364},
abstract = {The advent of generative artificial intelligence (AI), driven by advancements in large language models (LLMs), has unlocked transformative possibilities for information retrieval (IR), giving rise to a new wave of interactive and conversational paradigms. This workshop, titled New Interaction Paradigms for Information Retrieval in the Era of Generative AI, aims to serve as a collaborative platform for researchers and practitioners to explore the challenges and opportunities of integrating generative AI into IR systems. By focusing on tasks such as multi-turn conversational search, adaptive retrieval interfaces, and context-aware response generation, this workshop will address key areas including system design, user engagement, and evaluation methodologies. The workshop will also delve into broader concerns such as trust, transparency, and fairness, emphasizing the ethical implications of deploying generative AI in IR systems. Through panel discussions, poster sessions, and interactive roundtables, this workshop will foster critical dialogue and innovation, paving the way for a new era of user-centric, generative AI-powered IR systems.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4176–4179},
numpages = {4},
keywords = {generative ai, human-computer interaction, interaction paradigms, retrieval-augmented generation, {information retrieval},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3712716.3712718,
author = {Wickramasekara, Akila and Densmore, Alanna and Breitinger, Frank and Studiawan, Hudan and Scanlon, Mark},
title = {AutoDFBench: A Framework for AI Generated Digital Forensic Code and Tool Testing and Evaluation},
year = {2025},
isbn = {9798400710766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712716.3712718},
doi = {10.1145/3712716.3712718},
abstract = {Generative AI (GenAI) and Large Language Models (LLMs) show great potential in various domains, including digital forensics. A notable use case of these technologies is automatic code generation, which can reasonably be expected to include digital forensic applications in the not-too-distant future. As with any digital forensic tool, these systems must undergo extensive testing and validation. However, manually evaluating outputs, including generated DF code, remains a challenge. AutoDFBench is an automated framework designed to address this by validating AI-generated code and tools against NIST’s Computer Forensics Tool Testing Program (CFTT) procedures and subsequently calculating an AutoDFBench benchmarking score. The framework operates in four phases: data preparation, API handling, code execution, and result recording with score calculation. It benchmarks generative AI systems, such as LLMs and automated code generation agents, for DF applications. This benchmark can support iterative development or serve as a comparison metric between GenAI DF systems. As a proof of concept, NIST’s forensic string search tests were used, involving more than 24,200 tests with five top-performing code generation LLMs. These tests validated the output of 121 cases, considering two levels of user expertise, two programming languages, and ten iterations per case with varying prompts. The results also highlight the significant limitations of the DF-specific solutions generated by generic LLMs.},
booktitle = {Proceedings of the Digital Forensics Doctoral Symposium},
articleno = {1},
numpages = {7},
keywords = {Digital Forensics, Large Language Models, Investigative Process, Automation, Challenges},
location = {
},
series = {DFDS '25}
}

@inproceedings{10.1145/3639478.3639815,
author = {Velasco, Alejandro},
title = {Beyond Accuracy: Evaluating Source Code Capabilities in Large Language Models for Software Engineering},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639815},
doi = {10.1145/3639478.3639815},
abstract = {This dissertation aims to introduce interpretability techniques to comprehensively evaluate the performance of Large Language Models (LLMs) in software engineering tasks, beyond canonical metrics. In software engineering, Deep Learning techniques are widely employed across various domains, automating tasks such as code comprehension, bug fixing, code summarization, machine translation, and code generation. However, the prevalent use of accuracy-based metrics for evaluating Language Models trained on code often leads to an overestimation of their performance. Our work seeks to propose novel and comprehensive interpretability techniques to evaluate source code capabilities and provide a more nuanced understanding of LLMs performance across downstream tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {162–164},
numpages = {3},
keywords = {large language models, interpretability, DL4SE, category theory, causal inference},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3718739,
author = {Zhang, Yuwei and Jin, Zhi and Xing, Ying and Li, Ge and Liu, Fang and Zhu, Jiaxin and Dou, Wensheng and Wei, Jun},
title = {PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3718739},
doi = {10.1145/3718739},
abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Bug Fixing, Large Language Model, Bug Management, Multi-Agent Collaboration}
}

@inproceedings{10.1145/3698038.3698525,
author = {Shetty, Manish and Chen, Yinfang and Somashekar, Gagan and Ma, Minghua and Simmhan, Yogesh and Zhang, Xuchao and Mace, Jonathan and Vandevoorde, Dax and Las-Casas, Pedro and Gupta, Shachee Mishra and Nath, Suman and Bansal, Chetan and Rajmohan, Saravan},
title = {Building AI Agents for Autonomous Clouds: Challenges and Design Principles},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698038.3698525},
doi = {10.1145/3698038.3698525},
abstract = {The rapid growth in the use of Large Language Models (LLMs) and AI Agents as part of software development and deployment is revolutionizing the information technology landscape. While code generation receives significant attention, a higher-impact application lies in using agents for the operational resilience of cloud services, which currently require significant human effort and domain knowledge. There is a growing interest in AI for IT Operations (AIOps), which aims to automate complex operational tasks, like fault localization and root cause analysis, reducing human intervention and customer impact. However, achieving the vision of autonomous and self-healing clouds through AIOps is hampered by the lack of standardized frameworks for building, evaluating, and improving AIOps agents. This vision paper lays the groundwork for such a framework by framing the requirements and then discussing design decisions that satisfy them. We also propose AIOpsLab, a prototype implementation leveraging agent-cloud-interface that orchestrates an application, injects real-time faults using chaos engineering, and interfaces with an agent to localize and resolve the faults. We report promising results and lay the groundwork to build a modular and robust framework for building, evaluating, and improving agents for autonomous clouds.},
booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing},
pages = {99–110},
numpages = {12},
keywords = {Autonomous Clouds, Large Language Models, Reliability},
location = {Redmond, WA, USA},
series = {SoCC '24}
}

@article{10.1145/3744900,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {The Sustainability Face of Automated Program Repair Tools},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744900},
doi = {10.1145/3744900},
abstract = {Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of ten traditional APR tools for Java and eleven fine-tuned Large-Language Models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing trade-off between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-Large, provide a good trade-off between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Automated Program Repair, Software Sustainability, Energy Consumption of Software Tools, Green Computing}
}

@inproceedings{10.1145/3641555.3705061,
author = {Liu, Rongxin and Xu, Benjamin and Perez, Christopher and Zhao, Julianna and Zhukovets, Yuliia and Malan, David J.},
title = {Assessment in CS50 with AI: Leveraging Generative Artificial Intelligence for Personalized Student Evaluation},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705061},
doi = {10.1145/3641555.3705061},
abstract = {The scalability challenges of code review and pair-programming assessments in large computer science courses, such as CS50 at Harvard University, have opened up opportunities for the application of Generative AI. Leveraging large language models (LLMs), CS50.ai offers a suite of AI-based tools that assist both students and instructors in mastering course material while overcoming the limitations posed by human resource constraints. This demo highlights how generative AI can be employed to conduct code reviews and pair-programming simulations, providing real-time feedback, code explanations, and collaborative programming insights. By integrating these AI tools into students' learning journeys, we aim to mimic the 1:1 interaction between instructor and student, improving both formative and summative assessments. We will showcase how these tools are implemented to scale personalized feedback, ensure academic integrity, and maintain pedagogical efficacy. Our presentation will also reflect on lessons learned from deploying these AI-driven tools in recent course offerings.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1735},
numpages = {1},
keywords = {AI, LLMs, artificial intelligence, generative AI, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3704997,
author = {Renzullo, Joseph and Reiter, Pemma and Weimer, Westley and Forrest, Stephanie},
title = {Automated Program Repair: Emerging Trends Pose and Expose Problems for Benchmarks},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704997},
doi = {10.1145/3704997},
abstract = {Machine learning (ML) pervades the field of Automated Program Repair (APR). Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks. But, there are important differences between these applications of ML and earlier work, which complicates the task of ensuring that results are valid and likely to generalize. A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind. This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated.This article reviews work in APR published in the field’s top five venues since 2018, emphasizing emerging trends in the field, including the dramatic rise of ML models, including LLMs. ML-based articles are categorized along structural and functional dimensions, and a variety of issues are identified that these new methods raise. Importantly, data leakage and contamination concerns arise from the challenge of validating ML-based APR using existing benchmarks, which were designed before these techniques were popular. We discuss inconsistencies in evaluation design and performance reporting and offer pointers to solutions where they are available. Finally, we highlight promising new directions that the field is already taking.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {208},
numpages = {18},
keywords = {automated program repair, machine learning, benchmarks, patch quality}
}

@inproceedings{10.1145/3650212.3680384,
author = {Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik},
title = {AutoCodeRover: Autonomous Program Improvement},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680384},
doi = {10.1145/3650212.3680384},
abstract = {Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM’s understanding of the issue’s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1592–1604},
numpages = {13},
keywords = {automatic program repair, autonomous software engineering, autonomous software improvement, large language model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3701716.3716888,
author = {Goswami, Kanika and Mathur, Puneet and Rossi, Ryan and Dernoncourt, Franck},
title = {PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Retrieval Feedback},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3716888},
doi = {10.1145/3701716.3716888},
abstract = {Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including: (1) a Query Planning Agent that breaks down complex user requests into executable steps; (2) a Code Generation Agent that converts pseudocode into executable Python code; and three retrieval feedback agents-(3) a Numeric Feedback Agent, (4) a Lexical Feedback Agent, and (5) a Visual Feedback Agent-that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to reduced debugging time needed for plot errors.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1672–1676},
numpages = {5},
keywords = {agentic generation, llm agents, multimodal retrieval feedback},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3643795.3648380,
author = {S Kumar, Smitha and Adam Lones, Michael and Maarek, Manuel and Zantout, Hind},
title = {Investigating the Proficiency of Large Language Models in Formative Feedback Generation for Student Programmers},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648380},
doi = {10.1145/3643795.3648380},
abstract = {Generative AI has considerably altered traditional workplace practice across numerous industries. Ever since the emergence of large language models (LLMs), their potential to generate formative feedback for introductory programming courses has been extensively researched. However, most of these studies have focused on Python. In this work, we examine the bug-fixing and feedback-generation abilities of Code Llama and ChatGPT for Java programming assignments using our new Java benchmark called CodeWBugs. The results indicate that ChatGPT performs reasonably well, and was able to fix 94.33% programs. By comparison, we observed high variability in the results from Code Llama. We further analyzed the impact of different types of prompts and observed that prompts that included task descriptions and test inputs yielded better results. In most cases, the LLMs precisely localized the bugs and also offered guidance on how to proceed. Nevertheless, we also noticed incorrect responses generated by the LLMs, emphasizing the need to validate responses before disseminating feedback to learners.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {88–93},
numpages = {6},
keywords = {large language models (LLM), GPT-4, feedback, java programming, program repair},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.1145/3690391,
author = {Liu, Jiangfeng and Ma, Xueliang and Wang, Lanyu and Pei, Lei},
title = {How Can Generative Artificial Intelligence Techniques Facilitate Intelligent Research into Ancient Books?},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3690391},
doi = {10.1145/3690391},
abstract = {Generative AI changes the paradigm of natural language processing research, sets off a new trend of research in computational humanities and computational social sciences, and provides unique perspectives on digital intelligence-enabled ancient book revitalization and intelligent applications. The article explores the role of multimodal large models in image processing and OCR of ancient books. We discuss and exemplify how to use Large Language Models for intelligent information processing of ancient texts and explore combining prompt engineering, retrieval augmented generation (RAG), supervised fine-tuning, LangChain, and other techniques to improve performance in ancient text mining and applications. This article also looks forward to the broad prospect of intelligent agent technology combined with the Large Language Model in the innovative application of ancient book revitalization. The research focuses on digitizing ancient books, intelligent processing of ancient texts, and intelligent application of ancient book revitalization. It demonstrates the feasibility, advancement, and creativity of the application of generative AI and its derivative technologies in the field of computational humanities, especially in the field of ancient book preservation, to provide intelligent solutions for the dissemination of traditional thought and culture, from the perspective of the whole process of the technology of digital humanities and computational humanities research. The article also gives examples of the intelligent application of AI in the restoration of ancient books and the annotation of ancient texts. Although Large Language Models demonstrate transformative potential in advancing the field of ancient text research toward intelligent analysis, there remain certain limitations. This article points out their shortcomings in areas such as knowledge completion for ancient texts, understanding emotions and cultural nuances, as well as ethical and accountability issues. It emphasizes the need for a more balanced perspective on the role that generative AI plays in the exploration and utilization of cultural heritage.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {57},
numpages = {20},
keywords = {Computational Humanities, Ancient Book Revitalization, Intelligent Information Processing of Ancient Texts, ChatGPT, Generative AI, AIGC}
}

@inproceedings{10.1145/3696673.3723053,
author = {Vasudevan, Poonkuzhali and Reddivari, Sandeep},
title = {The Role of Generative AI Models in Requirements Engineering: A Systematic Literature Review},
year = {2025},
isbn = {9798400712777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696673.3723053},
doi = {10.1145/3696673.3723053},
abstract = {The software engineering field is experiencing rapid growth, driven by recent advancements in Artificial Intelligence (AI), particularly in Generative AI (GenAI) and Large Language Models (LLMs). Requirements Engineering (RE), a critical phase in software development, involves gathering and defining software requirements. However, research on the impact of GenAI and LLMs within RE remains limited. This paper examines the adoption of GenAI in RE, with the aim of exploring its practical implications, identifying current research trends, and highlighting areas for future development. To achieve this, a systematic literature review was conducted, addressing three research questions and analyzing 44 studies published over the past decade. The findings reveal that GenAI models, especially LLMs, are extensively employed in a variety of RE tasks, underscoring the versatility and potential of LLMs in enhancing the RE process.},
booktitle = {Proceedings of the 2025 ACM Southeast Conference},
pages = {188–194},
numpages = {7},
keywords = {requirements engineering, generative AI, large language models},
location = {Southeast Missouri State University, Cape Girardeau, MO, USA},
series = {ACMSE 2025}
}

@inproceedings{10.1145/3714393.3726486,
author = {Braconaro, Elisa and Losiouk, Eleonora},
title = {A Dataset for Evaluating LLMs Vulnerability Repair Performance in Android Applications: Data/Toolset paper},
year = {2025},
isbn = {9798400714764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3714393.3726486},
doi = {10.1145/3714393.3726486},
abstract = {Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Data and Application Security and Privacy},
pages = {353–358},
numpages = {6},
keywords = {android vulnerabilities, automated program repair, large language models},
location = {Pittsburgh, PA, USA},
series = {CODASPY '25}
}

@article{10.1145/3685680,
author = {Annapureddy, Ravinithesh and Fornaroli, Alessandro and Gatica-Perez, Daniel},
title = {Generative AI Literacy: Twelve Defining Competencies},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3685680},
doi = {10.1145/3685680},
abstract = {This article introduces a competency-based model for generative artificial intelligence (AI) literacy covering essential skills and knowledge areas necessary to interact with generative AI. The competencies range from foundational AI literacy to prompt engineering and programming skills, including ethical and legal considerations. These 12 competencies offer a framework for individuals, policymakers, government officials, and educators looking to navigate and take advantage of the potential of generative AI responsibly. Embedding these competencies into educational programs and professional training initiatives can equip individuals to become responsible and informed users and creators of generative AI. The competencies follow a logical progression and serve as a roadmap for individuals seeking to become familiar with generative AI and for researchers and policymakers to develop assessments, educational programs, guidelines, and regulations.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {13},
numpages = {21},
keywords = {Generative AI literacy, AI literacy, data literacy, generative AI, prompt engineering, AI competencies, AI skills}
}

@inproceedings{10.1145/3613904.3642212,
author = {Feng, Li and Yen, Ryan and You, Yuzhe and Fan, Mingming and Zhao, Jian and Lu, Zhicong},
title = {CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642212},
doi = {10.1145/3613904.3642212},
abstract = {Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators’ progress and intents. In this paper, we aim to investigate ways to assist programmers’ prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators’ prompts and building on their collaborators’ work, reducing repetitive updates and communication costs.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {934},
numpages = {21},
keywords = {collaborative programming, large language model, natural language interface, natural language programming, prompt engineering},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3670474.3685953,
author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
title = {Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685953},
doi = {10.1145/3670474.3685953},
abstract = {In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular C/C++ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {15},
numpages = {9},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@article{10.1145/3719345,
author = {Kong, Jiaolong and Xie, Xiaofei and Cheng, Mingfei and Liu, Shangqing and Du, Xiaoning and Guo, Qi},
title = {ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3719345},
doi = {10.1145/3719345},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this paper, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Program Repair, Large Language Model}
}

@article{10.1145/3696450,
author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
title = {Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696450},
doi = {10.1145/3696450},
abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {43},
keywords = {Automated program repair}
}

@inproceedings{10.1145/3721238.3730762,
author = {Wampfler, Rafael and Yang, Chen and Elste, Dillon and Kovacevic, Nikola and Witzig, Philine and Gross, Markus},
title = {A Platform for Interactive AI Character Experiences},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730762},
doi = {10.1145/3721238.3730762},
abstract = {From movie characters to modern science fiction — bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
articleno = {15},
numpages = {11},
keywords = {Conversational AI, Digital Characters, Embodied Conversational Agents, Large Language Models, Interactive Storytelling, Character Consistency, Personality Modeling, Memory Systems, Speech‑Driven Animation, Speech Synthesis, Multimodal Interaction},
location = {
},
series = {SIGGRAPH Conference Papers '25}
}

@inproceedings{10.1145/3702336.3702347,
author = {Bendel, Oliver and Zbinden, Nick},
title = {The Animal Whisperer Project},
year = {2024},
isbn = {9798400711756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702336.3702347},
doi = {10.1145/3702336.3702347},
abstract = {Generative AI has become widespread since 2022. Technical advancements have resulted in multimodal large language models and other AI models that generate, analyze, and evaluate texts, images, and sounds. Such capabilities can be helpful in encounters between humans and animals. For example, apps with generative AI on a smartphone can be used to assess the body language and behavior of animals – e.g., during a walk or hike – and provide a recommendation for human behavior. It is often useful to take into account the animal's environment and situation. The apps can help people to avert approaches and attacks, and thus also protect animals. In “The Animal Whisperer Project”, three apps were developed as prototypes based on the multimodal large language model GPT-4 from OpenAI from the beginning to mid-2024. Three specific GPTs resulted: the Cow Whisperer, the Horse Whisperer, and the Dog Whisperer. All three showed impressive capabilities after the first prompt engineering. These were improved by implementing information from expert interviews and adding labeled images of animals and other materials. AI-based apps for interpreting body language, behavior, and the overall situation can apparently be created today, without much effort, in a low-budget project. However, turning them into products would certainly raise questions, such as liability in the event of accidents.},
booktitle = {Proceedings of the International Conference on Animal-Computer Interaction},
articleno = {11},
numpages = {9},
keywords = {Animal Body Language, Animal-Computer Interaction, Artificial Intelligence, GPT, Generative AI, Multimodal Large Language Model},
location = {
},
series = {ACI '24}
}

@inbook{10.1145/3715668.3735617,
author = {Guriundefined\u{a}, Alexandra-Elena},
title = {Exploring the Intersection of UI Accessibility and Generative AI: A Framework for Human-AI Collaboration},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715668.3735617},
abstract = {Despite decades of accessibility guidelines, digital interfaces remain largely inaccessible, with over 95% of websites failing basic WCAG standards. My doctoral research examines how generative AI transforms accessibility implementation by investigating how Al-driven tools comply with accessibility standards, what patterns emerge in human-AI collaboration for accessible design, and how we can develop frameworks to guide AI systems toward more accessible outcomes. Through mixed-method studies-evaluating Al- generated interfaces, challenging Al's understanding of accessibility principles, and interviewing professional designers I've identified key compliance patterns, discovered AI limitations, and developed novel interactive prompt engineering methods. This work contributes both theoretical understanding of human-AI collaboration and practical frameworks for integrating generative AI into accessible interface design workflows.},
booktitle = {Companion Publication of the 2025 ACM Designing Interactive Systems Conference},
pages = {116–120},
numpages = {5}
}

@inproceedings{10.1145/3589335.3651463,
author = {Le, Tan Khang and Alimadadi, Saba and Ko, Steven Y.},
title = {A Study of Vulnerability Repair in JavaScript Programs with Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651463},
doi = {10.1145/3589335.3651463},
abstract = {In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {666–669},
numpages = {4},
keywords = {automatic program repair, cwe, javascript, large language models, prompt engineering},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.5555/3729857.3729868,
author = {Bandi, Ajay},
title = {Pedagogical Evaluation of Generative AI Course for Technologists},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {6},
issn = {1937-4771},
abstract = {Generative AI is a transformative technology that impacts various fields, including software development, data analytics, and cybersecurity. To address this, we have designed and developed a Generative AI course for technologists, integrating foundational knowledge of various Gen AI architecture models with hands-on practical experience using Python libraries, including HuggingFace. This paper discusses the detailed course structure and assessments. A pedagogical evaluation approach is followed to identify the challenges encountered in the course and how to overcome them. The results demonstrate that the Generative AI Course for Technologists effectively equips students with technical expertise and critical thinking skills through a balanced combination of theoretical concepts and practical exercises, such as chatbot development and prompt engineering. The course addresses challenges like hardware limitations and API integration by proposing future improvements, including a dedicated Python module and access to cloud-based GPU tools, ensuring learners are well-prepared to navigate and ethically apply Generative AI in real-world contexts.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {99–110},
numpages = {12}
}

@inproceedings{10.1145/3715275.3732118,
author = {Djeffal, Christian},
title = {Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and AI Interaction Design},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732118},
doi = {10.1145/3715275.3732118},
abstract = {Responsible prompt engineering has emerged as a critical pracitce for ensuring that generative artificial intelligence (AI) systems are aligned with ethical, legal, and social principles. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. It is, therefore, necessary to examine how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes “Reflexive Prompt Engineering”, a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of emerging practices, this article illustrates how responsible prompt engineering serves as a crucial connection between AI development and deployment, enabling organizations to align AI outputs without modifying underlying model architectures. This approach links with broader “Responsibility by Design” principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering as an essential component of AI literacy.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1757–1768},
numpages = {12},
keywords = {AI Ethics, AI Governance, AI alignment, Accountability, Human-AI Interaction, Prompt Engineering, Responsible AI},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3689050.3704424,
author = {Lindley, Joseph and Whitham, Roger},
title = {From Prompt Engineering to Prompt Craft},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689050.3704424},
doi = {10.1145/3689050.3704424},
abstract = {This pictorial presents an ongoing research programme comprising three practice-based Design Research projects conducted through 2024, exploring the affordances of diffusion-based AI image generation systems, specifically Stable Diffusion. The research employs tangible and embodied interactions to investigate emerging qualitative aspects of generative AI, including uncertainty and materiality. Our approach leverages the flexibility and adaptability of Design Research to navigate the rapidly evolving field of generative AI. The pictorial proposes the notion of prompt craft as a productive reframing of prompt engineering. This is comprised of two contributions: (1) reflections on the notion of materiality for diffusion-based generative AI and a proposed method for a craft-like navigation of the latent space within generative AI models and (2) discussing interaction design strategies for designing user interfaces informed by these affordances. The outcomes are presented as strong concepts or intermediate knowledge, applicable to various situations and domains.},
booktitle = {Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {54},
numpages = {12},
location = {
},
series = {TEI '25}
}

@inproceedings{10.1145/3701716.3717574,
author = {Jin, Can and Peng, Hongwu and Zhao, Shiyu and Wang, Zhenting and Xu, Wujiang and Han, Ligong and Zhao, Jiahui and Zhong, Kai and Rajasekaran, Sanguthevar and Metaxas, Dimitris N.},
title = {APEER : &lt;u&gt;A&lt;/u&gt;utomatic &lt;u&gt;P&lt;/u&gt;rompt &lt;u&gt;E&lt;/u&gt;ngineering &lt;u&gt;E&lt;/u&gt;nhances Large Language Model &lt;u&gt;R&lt;/u&gt;eranking},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717574},
doi = {10.1145/3701716.3717574},
abstract = {Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2494–2502},
numpages = {9},
keywords = {information retrieval, large language model, prompt engineering, reranking},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3637528.3671501,
author = {Jiang, Zhe and Zhao, Liang and Zhou, Xun and Zhang, Junbo and Shekhar, Shashi and Ye, Jieping},
title = {The 4th KDD Workshop on Deep Learning for Spatiotemporal Data, Applications, and Systems (DeepSpatial'24)},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671501},
doi = {10.1145/3637528.3671501},
abstract = {Over the last decades, a rapidly growing volume of spatiotemporal data has been collected from smartphones and GPS, terrestrial, seaborne, airborne, and spaceborne sensors, as well as computational simulations. Meanwhile, advances in deep learning technologies, especially the recent breakthroughs of generative AI and foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs), have achieved tremendous success in natural language processing and computer vision applications. There is growing anticipation of the same level of accomplishment of AI on spatiotemporal data in tackling grand societal challenges, such as national water resource management, monitoring coastal hazards, energy and food security, as well as mitigation and adaptation to climate change. When deep learning, especially emerging foundation models, intersects spatiotemporal data in scientific domains, it opens up new opportunities and challenges. The workshop aims to bring together academic researchers in both AI and scientific domains, government program managers, leaders from non-profit organizations, as well as industry executives to brainstorm and debate on the emerging opportunities and novel challenges of deep learning (foundation models) for spatiotemporal data inspired by real-world scientific applications.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6722–6723},
numpages = {2},
keywords = {deep learning, foundation models, spatiotemporal data},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3706598.3713166,
author = {Subramonyam, Hari and Thakkar, Divy and Ku, Andrew and Dieber, Juergen and Sinha, Anoop K.},
title = {Prototyping with Prompts: Emerging Approaches and Challenges in Generative AI Design for Collaborative Software Teams},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713166},
doi = {10.1145/3706598.3713166},
abstract = {Generative AI models are increasingly being integrated into human task workflows, enabling the production of expressive content across a wide range of contexts. Unlike traditional human-AI design methods, the new approach to designing generative capabilities focuses heavily on prompt engineering strategies. This shift requires a deeper understanding of how collaborative software teams establish and apply design guidelines, iteratively prototype prompts, and evaluate them to achieve specific outcomes. To explore these dynamics, we conducted design studies with 39 industry professionals, including UX designers, AI engineers, and product managers. Our findings highlight emerging practices and role shifts in AI system prototyping among multistakeholder teams. We observe various prompting and prototyping strategies, highlighting the pivotal role of to-be-generated content characteristics in enabling rapid, iterative prototyping with generative AI. By identifying associated challenges, such as the limited model interpretability and overfitting the design to specific example content, we outline considerations for generative AI prototyping.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {882},
numpages = {22},
keywords = {Generative AI, Prompt Engineering, Human-Centered AI},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3650212.3680323,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680323},
doi = {10.1145/3650212.3680323},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {819–831},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3734921.3734925,
author = {Li, Yanhong and Zhang, Min and Zhang, Shaofan and Zheng, Maoran and Gu, Yayun and Tang, Wuchen and Gao, Ming},
title = {Research on Quality Assessment and Report Generation of Power Grid Fault Datasets Based on Prompt Engineering},
year = {2025},
isbn = {9798400713460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734921.3734925},
doi = {10.1145/3734921.3734925},
abstract = {This study proposes an automated quality assessment method for power grid fault datasets based on large language models (LLMs) and prompt engineering, along with the automatic generation of quality assessment reports. By leveraging the semantic understanding capabilities of LLMs and the flexibility of prompt engineering, the method efficiently analyzes and accurately detects multidimensional issues such as missing values, duplicate entries, outliers, and irregular time intervals within the datasets. Experimental results demonstrate that the prompt-engineering-based approach achieves an overall accuracy of 86% in data quality assessment and 100% accuracy in outlier detection. Furthermore, the method can automatically generate detailed assessment reports in Word format, including visual analytical charts. This approach enhances the automation and accuracy of dataset quality assessments, providing reliable data support for intelligent operation and maintenance of power systems.},
booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence and Digital Media},
pages = {19–25},
numpages = {7},
keywords = {Automated report generation, ChatGPT, Data quality evaluation, Large language models, Prompt engineering},
location = {
},
series = {GADM '25}
}

@inproceedings{10.1145/3661167.3661273,
author = {Mezzaro, Simone and Gambi, Alessio and Fraser, Gordon},
title = {An Empirical Study on How Large Language Models Impact Software Testing Learning},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661273},
doi = {10.1145/3661167.3661273},
abstract = {Software testing is a challenging topic in software engineering education and requires creative approaches to engage learners. For example, the Code Defenders game has students compete over a Java class under test by writing effective tests and mutants. While such gamified approaches deal with problems of motivation and engagement, students may nevertheless require help to put testing concepts into practice. The recent widespread diffusion of Generative AI and Large Language Models raises the question of whether and how these disruptive technologies could address this problem, for example, by providing explanations of unclear topics and guidance for writing tests. However, such technologies might also be misused or produce inaccurate answers, which would negatively impact learning. To shed more light on this situation, we conducted the first empirical study investigating how students learn and practice new software testing concepts in the context of the Code Defenders testing game, supported by a smart assistant based on a widely known, commercial Large Language Model. Our study shows that students had unrealistic expectations about the smart assistant, “blindly” trusting any output it generated, and often trying to use it to obtain solutions for testing exercises directly. Consequently, students who resorted to the smart assistant more often were less effective and efficient than those who did not. For instance, they wrote 8.6% fewer tests, and their tests were not useful in 78.0% of the cases. We conclude that giving unrestricted and unguided access to Large Language Models might generally impair learning. Thus, we believe our study helps to raise awareness about the implications of using Generative AI and Large Language Models in Computer Science Education and provides guidance towards developing better and smarter learning tools.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {555–564},
numpages = {10},
keywords = {ChatGPT, Computer Science Education, Generative AI, Smart Learning Assistant},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3658617.3697625,
author = {Abdelatty, Manar and Ma, Jingxiao and Reda, Sherief},
title = {MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697625},
doi = {10.1145/3658617.3697625},
abstract = {Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {995–1001},
numpages = {7},
keywords = {LLM, verilog, metrics, post-synthesis, reasoning, chain-of-thought},
location = {Tokyo, Japan},
series = {ASPDAC '25}
}

@inproceedings{10.1145/3701716.3717810,
author = {Syah, Riza Alaudin and Haryanto, Christoforus Yoga and Lomempow, Emily and Malik, Krishna and Putra, Irvan},
title = {EdgePrompt: Engineering Guardrail Techniques for Offline LLMs in K-12 Educational Settings},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717810},
doi = {10.1145/3701716.3717810},
abstract = {EdgePrompt is a prompt engineering framework that implements pragmatic guardrails for Large Language Models (LLMs) in the K-12 educational settings through structured prompting inspired by neural-symbolic principles. The system addresses educational disparities in Indonesia's Frontier, Outermost, Underdeveloped (3T) regions by enabling offline-capable content safety controls. It combines: (1) content generation with structured constraint templates, (2) assessment processing with layered validation, and (3) lightweight storage for content and result management. The framework implements a multi-stage verification workflow that maintains safety boundaries while preserving model capabilities in connectivity-constrained environments. Initial deployment targets Grade 5 language instruction, demonstrating effective guardrails through structured prompt engineering without formal symbolic reasoning components.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1635–1638},
numpages = {4},
keywords = {ai safety, content filtering, edge computing, educational technology, guardrails, k-12 education, large language models, offline ai, prompt engineering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3659677.3659749,
author = {Boukhlif, Mohamed and Kharmoum, Nassim and Hanine, Mohamed},
title = {LLMs for Intelligent Software Testing: A Comparative Study},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659749},
doi = {10.1145/3659677.3659749},
abstract = {The need for effective and timely testing processes has become critical in the constantly changing field of software development. Large Language Models (LLMs) have demonstrated promise in automating test case creation, defect detection, and other software testing tasks through the use of the capabilities of machine/deep learning and natural language processing. This work explores the field of intelligent software testing, with a focus on the use of LLMs in this context. The purpose of this comparative study is to assess the corpus of research in the field in terms of used LLMs, how to interact with them, the use of fine-tuning, and prompt engineering, and explore the different technologies and testing types automated using LLMs. The findings of this study not only contribute to the growing body of knowledge on intelligent software testing but also guide fellow researchers and industry engineers in selecting the most suitable LLM for their specific testing needs.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {42},
numpages = {8},
keywords = {Comparative Study, Large Language Models, Natural Language Processing, Software Testing, Test Case Generation},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3691620.3695062,
author = {Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi},
title = {FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695062},
doi = {10.1145/3691620.3695062},
abstract = {Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\texttimes{} compared to the autoregressive decoding algorithm.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {669–680},
numpages = {12},
keywords = {automated program repair, large language models, programming education, inference acceleration},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3657604.3664694,
author = {Calo, Tommaso and Maclellan, Christopher},
title = {Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664694},
doi = {10.1145/3657604.3664694},
abstract = {Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {305–309},
numpages = {5},
keywords = {human-centered computing, intelligent tutoring systems, intelligent-user-interfaces, ui/ux},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3649476.3660393,
author = {Paria, Sudipta and Dasgupta, Aritra and Bhunia, Swarup},
title = {Navigating SoC Security Landscape on LLM-Guided Paths},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649476.3660393},
doi = {10.1145/3649476.3660393},
abstract = {The increasing prominence of Large Language Models (LLMs) is being acknowledged for their exceptional abilities in comprehending natural language, conducting advanced reasoning, and generating contextual responses. LLMs expedite code generation, verification, and bug-fixing tasks across software and hardware domains. Development of hardware designs typically involves translating natural language specifications into Hardware Description Languages (HDLs) like Verilog or SystemVerilog, followed by circuit synthesis, physical layout, and fabrication, with the potential for human errors in the process. In the current industry practice, HDL verification tasks typically rely on manual expertise from security professionals to detect and address vulnerabilities. Modern System-on-Chip (SoC) designs integrate several Intellectual Property (IP) blocks implemented using HDL and communicate through a common bus to perform intended functions. Ensuring security throughout the SoC design process requires innovative solutions due to the complex nature of SoC designs and the distribution of assets across multiple IP blocks. Popular conversation LLMs such as Open AI’s ChatGPT and Google’s GEMINI (formerly BARD) offer the potential to automate HDL code generation and verification tasks by interpreting user prompts represented in natural language descriptions, thereby minimizing manual effort and enhancing hardware design quality. This paper explores recent research works on HDL generation, verification, and bug fix leveraging LLMs while addressing prevailing challenges and presenting potential opportunities for improvement.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024},
pages = {252–257},
numpages = {6},
keywords = {Bug Fix, CWEs., Formal Verification, HDL Code, LLMs, SVA, SoC Security, Verilog},
location = {Clearwater, FL, USA},
series = {GLSVLSI '24}
}

@inproceedings{10.1145/3696630.3728568,
author = {Wynn-Williams, Stephen and Tyrrell, Ryan and Pantelic, Vera and Lawford, Mark and Menghi, Claudio and Nalla, Phaneendra and Artail, Hassan},
title = {Can Generative AI Produce Test Cases? An Experience from the Automotive Domain},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728568},
doi = {10.1145/3696630.3728568},
abstract = {Engineers need automated support for software testing. Generative AI is a novel technology for generating new content; however, its applicability for test case generation is still unclear. This work considers the following question: Can generative AI produce test cases in industrial software applications? We framed our question in the automotive domain. We performed our evaluation in collaboration with a large automotive manufacturer to assess to what extent generative AI can produce test cases (a.k.a. test scripts) from informal test case specifications. We considered 1) informal test case specifications defined in Rational Quality Manager, an industrial test management tool from IBM, and 2) executable test scripts specified as ecu.test packages supported by the ecu.test tool from Tracetronic. We used generative AI to produce the test scripts from the informal test case descriptions. Our results show that generative AI can produce correct or near-correct test scripts in a reasonable number of cases. We also analyzed the effects of prompt design, choice of generative AI model, and context accuracy on the effectiveness of our solution and reflected on our results.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {456–467},
numpages = {12},
keywords = {software testing, LLM, generative AI, automotive software},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3718088,
author = {Pinckney, Nathaniel and Batten, Christopher and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
title = {Revisiting VerilogEval: A Year of Improvements in Large-Language Models for Hardware Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3718088},
doi = {10.1145/3718088},
abstract = {The application of large-language models (LLMs) to digital hardware code generation is an emerging field, with most LLMs primarily trained on natural language and software code. Hardware code like Verilog constitutes a small portion of training data, and few hardware benchmarks exist. The open-source VerilogEval benchmark, released in November 2023, provided a consistent evaluation framework for LLMs on code completion tasks. Since then, both commercial and open models have seen significant development. In this work, we evaluate new commercial and open models since VerilogEval’s original release—including GPT-4o, GPT-4 Turbo, Llama3.1 (8B/70B/405B), Llama3 70B, Mistral Large, DeepSeek Coder (33B and 6.7B), CodeGemma 7B, and RTL-Coder—against an improved VerilogEval benchmark suite. We find measurable improvements in state-of-the-art models: GPT-4o achieves a 63% pass rate on specification-to-RTL tasks. The recently released and open Llama3.1 405B achieves a 58% pass rate, almost matching GPT-4o, while the smaller domain-specific RTL-Coder 6.7B models achieve an impressive 34% pass rate. Additionally, we enhance VerilogEval’s infrastructure by automatically classifying failures, introducing in-context learning support, and extending the tasks to specification-to-RTL translation. We find that prompt engineering remains crucial for achieving good pass rates and varies widely with model and task. A benchmark infrastructure that allows for prompt engineering and failure analysis is essential for continued model development and deployment.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = feb,
keywords = {large language models, RTL code generation, benchmarks}
}

@article{10.1145/3736408,
author = {Koyuncu, Anil},
title = {Exploring Fine-Grained Bug Report Categorization with Large Language Models and Prompt Engineering: An Empirical Study},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3736408},
doi = {10.1145/3736408},
abstract = {Accurate classification of issues is essential for effective project management and timely responses, as the volume of issue reports continues to grow. Manual classification is labor-intensive and error-prone, necessitating automated solutions. While large language models (LLMs) show promise in automated issue labeling, most research focuses on broad categorization (e.g., bugs, feature requests), with limited attention to fine-grained categorization. Understanding specific bug types is crucial, as different bugs require tailored resolution strategies.This study addresses this gap by evaluating LLMs and prompt engineering strategies for fine-grained bug report categorization. We analyze 221,184 fine-grained bug report category labels generated by selected LLMs using various prompt engineering strategies for 1,024 bug reports. We examine how LLMs and prompt engineering influence output characteristics, control over outputs, and categorization performance. Our findings highlight that LLMs and prompt engineering significantly impact output consistency and classification capability, with some yielding consistent results and others introducing variability. Based on these findings, we analyze the agreements and disagreements between LLM-generated labels and human annotations to assess category correctness. Our results suggest that examining label consistency and discrepancies can serve as a complementary method for validating bug report categories, identifying unclear reports, and detecting misclassifications in human annotations.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Prompt Engineering, Large Language Models, Automatic Bug Report Classification, Label correctness}
}

@article{10.1145/3728885,
author = {Tang, Lingxiao and Liu, Jiakun and Liu, Zhongxin and Yang, Xiaohu and Bao, Lingfeng},
title = {LLM4SZZ: Enhancing SZZ Algorithm with Context-Enhanced Assessment on Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728885},
doi = {10.1145/3728885},
abstract = {The SZZ algorithm is the dominant technique for identifying bug-inducing commits and serves as a foundation for many software engineering studies, such as bug prediction and static code analysis, thereby enhancing software quality and facilitating better maintenance practices. Researchers have proposed many variants to enhance the SZZalgorithm’s performance since its introduction. The majority of them rely on static techniques or heuristic assumptions, making them easy to implement, but their performance improvements are often limited. Recently, a deep learning-based SZZ algorithm has been introduced to enhance the original SZZ algorithm. However, it requires complex preprocessing and is restricted to a single programming language. Additionally, while it enhances precision, it sacrifices recall. Furthermore, most of variants overlook crucial information, such as commit messages and patch context, and are limited to bug-fixing commits involving deleted lines.    The emergence of large language models (LLMs) offers an opportunity to address these drawbacks. In   this study, we investigate the strengths and limitations of LLMs and propose LLM4SZZ, which employs two approaches (i.e., rank-based identification and context-enhanced identification) to handle different types of bug-fixing commits. We determine which approach to adopt based on the LLM’s   ability to comprehend the bug and identify whether the bug is present in a commit. The context-enhanced identification provides the LLM with more context and requires it to find the bug-inducing commit among a set of candidate commits. In rank-based identification, we ask the LLM to select buggy statements from the bug-fixing commit and rank them based on their relevance to the root cause. Experimental results show that LLM4SZZ outperforms all baselines across three datasets, improving F1-score by 6.9% to 16.0% without significantly sacrificing recall. Additionally, LLM4SZZ can identify many bug-inducing commits that the baselines fail to detect, accounting for 7.8%, 7.4% and 2.5% of the total bug-inducing commits across three datasets, respectively.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA016},
numpages = {23},
keywords = {SZZ Algorithm, large language model}
}

@inproceedings{10.1145/3696630.3728603,
author = {Zhao, Sebastian and Zhu, Alan and Mozannar, Hussein and Sontag, David and Talwalkar, Ameet and Chen, Valerie},
title = {CodingGenie: A Proactive LLM-Powered Programming Assistant},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728603},
doi = {10.1145/3696630.3728603},
abstract = {While developers increasingly adopt tools powered by large language models (LLMs) in day-to-day workflows, these tools still require explicit user invocation. To seamlessly integrate LLM capabilities to a developer's workflow, we introduce CodingGenie, a proactive assistant integrated into the code editor. CodingGenie autonomously provides suggestions, ranging from bug fixing to unit testing, based on the current code context and allows users to customize suggestions by providing a task description and selecting what suggestions are shown. We demonstrate multiple use cases to show how proactive suggestions from CodingGenie can improve developer experience, and also analyze the cost of adding proactivity. We believe this open-source tool will enable further research into proactive assistants. CodingGenie is open-sourced at https://github.com/sebzhao/CodingGenie/ and video demos are available at https://sebzhao.github.io/CodingGenie/.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1168–1172},
numpages = {5},
keywords = {coding assistants, proactive chat assistants, large language models},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3742430,
author = {Gomes Lopes, Samuel and Zhu, Shien and Alonso, Gustavo},
title = {Exploring Large Language Models for Hierarchical Hardware Circuit and Testbench Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3742430},
doi = {10.1145/3742430},
abstract = {Designing and verifying hardware circuits using a Hardware Description Language (HDL) is an essential but time-consuming part of hardware design. Generating the desired correct circuit and testbench code usually requires a significant engineering effort. Recently, Large Language Models (LLMs) have claimed to have strong code generation capabilities to reduce such engineering costs. Existing work has provided quantitative evaluations using LLMs for single-module, simple circuit generation. However, it is still unclear whether modern LLMs are useful in production workflows, e.g., generating correct hierarchical circuits with testbenches. And if they are capable, what are the best prompt engineering practices for hardware design? In this paper, we evaluate LLMs for HDL generation by exploring a 3-dimensional design space: commercial and open-source language models, single-module and hierarchical circuits, and prompting methods with varying complexity. We propose a 3-step design space exploration methodology to answer the two aforementioned questions. First, we explore the best prompt engineering practices across generating simple, middle, and hard single-module circuits with testbenches on CodeLLama-34B. We also define two fine-grained checklists to evaluate the circuit and testbench quality from a user’s perspective. Second, we benchmark 11 LLMs with prompt adaptation on 4 single-module circuits that CodeLLama-34B has trouble with to further find models that may be useful in a production workflow. Third, we apply the learned prompt practices on four top-level models to generate simple 2 to 4-module and more complex multi-module hierarchical circuits and testbenches. As a result, we find that some of the latest LLMs can generate correct simple hierarchical circuits and testbenches with given proper prompts, but still struggle with complex hierarchical circuits. We further provide useful guidelines from an end-user’s perspective on leveraging LLMs for hardware design.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
keywords = {Large Language Models (LLMs), Hardware Description Language (HDL), Electronic Design Automation (EDA)}
}

@inproceedings{10.1145/3706468.3706564,
author = {Ramanathan, Sriram and Lim, Lisa-Angelique and Mottaghi, Nazanin Rezazadeh and Buckingham Shum, Simon},
title = {When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706564},
doi = {10.1145/3706468.3706564},
abstract = {With the emergence of generative AI, the field of Learning Analytics (LA) has increasingly embraced the use of Large Language Models (LLMs) to automate qualitative analysis. Deductive analysis requires theoretical or other conceptual grounding to inform coding. However, few studies detail the process of translating the literature into a codebook, and then into an effective LLM prompt. In this paper, we introduce Grounded Prompt Engineering (GROPROE) as a systematic process to develop a literature-grounded prompt for deductive analysis. We demonstrate our GROPROE process on a dataset of 860 written reflections, coding for students’ affective engagement and sense of belonging. To evaluate the quality of the coding we demonstrate substantial human/LLM Inter-Annotator Reliability (IAR). To evaluate the consistency of LLM coding, a subset of the data was analysed 60 times using the LLM Quotient showing how this stabilized for most codes. We discuss the dynamics of human-AI interaction when following GROPROE, foregrounding how the prompt took over as the iteratively revised codebook, and how the LLM provoked codebook revision. The contributions to the LA field are threefold: (i) GROPROE as a systematic prompt-design process for deductive coding grounded in literature, (ii) a detailed worked example showing its application to Belonging Analytics, and (iii) implications for human-AI interaction in automated deductive analysis.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {713–725},
numpages = {13},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3637528.3671452,
author = {Ding, Hao and Fan, Ziwei and Guehring, Ingo and Gupta, Gaurav and Ha, Wooseok and Huan, Jun and Liu, Linbo and Omidvar-Tehrani, Behrooz and Wang, Shiqi and Zhou, Hao},
title = {Reasoning and Planning with Large Language Models in Code Development},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671452},
doi = {10.1145/3637528.3671452},
abstract = {Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6480–6490},
numpages = {11},
keywords = {application modernization, code development, code generation, code migration, large language model},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3707640.3731928,
author = {Romero Lauro, Quentin and Gautam, Aakash and Kotturi, Yasmine},
title = {BizChat: Scaffolding AI-Powered Business Planning for Small Business Owners Across Digital Skill Levels},
year = {2025},
isbn = {9798400713972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707640.3731928},
doi = {10.1145/3707640.3731928},
abstract = {Generative AI can help small business owners automate tasks, increase efficiency, and improve their bottom line. However, despite the seemingly intuitive design of systems like ChatGPT, significant barriers remain for those less comfortable with technology. To address these disparities, prior work highlights accessory skills—beyond prompt engineering—users must master to successfully adopt generative AI including keyboard shortcuts, editing skills, file conversions, and browser literacy. Building on a design workshop series and 15 interviews with small businesses, we introduce BizChat, a large language model (LLM)-powered web application that helps business owners across digital skills levels write their business plan—an essential but often neglected document. To do so, BizChat’s interface embodies three design considerations inspired by learning sciences: ensuring accessibility to users with less digital skills while maintaining extensibility to power users (“low-floor-high-ceiling”), providing in situ micro-learning to support entrepreneurial education (“just-in-time learning”), and framing interaction around business activities (“contextualized technology introduction”). We conclude with plans for a future BizChat deployment.},
booktitle = {Adjunct Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
articleno = {22},
numpages = {4},
keywords = {Small Business, Entrepreneurship, Business Planning, Generative AI},
location = {
},
series = {CHIWORK '25 Adjunct}
}

@inproceedings{10.1145/3652620.3687803,
author = {Buchmann, Thomas and Peinl, Ren\'{e} and Schw\"{a}gerl, Felix},
title = {White-box LLM-supported Low-code Engineering: A Vision and First Insights},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687803},
doi = {10.1145/3652620.3687803},
abstract = {Low-code development (LCD) platforms promise to empower citizen developers to define core domain models and rules for business applications. However, as domain rules grow complex, LCD platforms may fail to do so effectively. Generative AI, driven by large language models (LLMs), offers source code generation from natural language but suffers from its non-deterministic black-box nature and limited explainability. Therefore, rather than having LLMs generate entire applications from single prompts, we advocate for a white-box approach allowing citizen developers to specify domain models semi-formally, attaching constraints and operations as natural language annotations. These annotations are fed incrementally into an LLM contextualized with the generated application stub. This results in deterministic and better explainable generation of static application components, while offering citizen developers an appropriate level of abstraction. We report on a case study in manufacturing execution systems, where the implementation of the approach provides first insights.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {556–560},
numpages = {5},
keywords = {model-driven engineering, large language models, low-code, semiformal, artificial intelligence},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3687810,
author = {Tabassum, Mirza Rehenuma and Ritchie, Matthew J. and Mustafiz, Sadaf and Kienzle, J\"{o}rg},
title = {Using LLMs for Use Case Modelling of IoT Systems: An Experience Report},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687810},
doi = {10.1145/3652620.3687810},
abstract = {Requirements engineering (RE) plays an essential role in the success of system and software development. Textual use case models are valuable for capturing diverse scenarios describing the interactions between the system and its actors, but their development, particularly for the Internet of Things (IoT), can be tedious and error-prone due to the added complexities and heterogeneous nature of such systems. Automating requirements elicitation and specification tasks with the use of generative AI is highly desirable. This paper explores the potential of large language models (LLMs) for generating interaction models for IoT systems from informal problem descriptions. We investigate the capabilities of OpenAI's GPT-4 and Google's Gemini for generating standard and UCM4IoT textual use cases by carrying out a comparative study using four IoT applications. While both of these LLMs show promise as supporting tools, our findings indicate a need for further refinement and domain-specific training to enhance their precision and reliability in requirements development for the IoT domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {611–619},
numpages = {9},
keywords = {requirements engineering, use case modelling, large language model, LLM, model-based development},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3706599.3720079,
author = {Shin, Subin and Oh, Jeesun and Lee, Sangwon},
title = {Can LLMs See What I See? A Study on Five Prompt Engineering Techniques for Evaluating UX on a Shopping Site},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720079},
doi = {10.1145/3706599.3720079},
abstract = {Usability testing is essential for improving digital user experiences but has practical limitations in terms of cost-effectiveness. Recent advancements in multimodal Large Language Models (LLMs), like ChatGPT-4, offer new possibilities for UX evaluations. This study investigated the most effective prompt engineering techniques for identifying UX issues in digital interfaces. To achieve this, five prompt engineering techniques were carefully selected based on previous research, and the outputs generated using these techniques were analyzed based on severity assessment criteria. We discovered that Role Prompting and (Zero-Shot) Chain of Thought Prompting were highly effective. Further investigation revealed that a hybrid approach combining both techniques produced the best results. Our findings shed light on the possibility of using multimodal LLM as a UX evaluator, offering meaningful value for future advancements in LLM-based UX evaluations.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {125},
numpages = {7},
keywords = {UX Evaluation, Large Language Model (LLM), Multimodal LLM, Prompt Engineering, Role Prompting, (Zero-Shot) Chain of Thought Prompting},
location = {
},
series = {CHI EA '25}
}

@inbook{10.1145/3724504.3724574,
author = {Guo, Hui and Zhang, Long and Feng, Xilong and Zheng, Qiusheng},
title = {A Review of the Application of Prompt Engineering in the Safety of Large Language Models},
year = {2025},
isbn = {9798400711732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724504.3724574},
abstract = {With the rapid development of large language models, their popular application in dialogue systems has achieved certain remarkable generative performances. However, this also brings several possible security risks, such as poisoned content generation and its exploitation via adversarial prompts. Prompt engineering is one of the key technologies that guide models in generating safe and reasonable outputs by designing and optimizing prompts for both offense and defense. In this paper, LLM safety with regard to prompt engineering is systematically investigated from both attack and defense viewpoints, including common attack methods like red teaming, template-based attacks, and neural prompt attacks, and the use of system prompts, deletion/filtering mechanisms, and contrastive decoding as defense strategies. The goal is to present new perspectives on LLM safety research while predicting the possible applications and challenges facing prompt engineering in high-risk domains.},
booktitle = {Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence},
pages = {424–430},
numpages = {7}
}

@inproceedings{10.1109/ICSE48619.2023.00129,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3701716.3717808,
author = {Mondal, Chayan and Pham, Duc-Son and Gupta, Ashu and Tan, Tele and Gedeon, Tom},
title = {Leveraging Prompt Engineering with Lightweight Large Language Models to Label and Extract Clinical Information from Radiology Reports},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717808},
doi = {10.1145/3701716.3717808},
abstract = {Chest X-ray imaging plays a critical role in diagnosing chest diseases, making it a cornerstone in clinical and research domains. Automating disease diagnosis and extracting relevant clinical information from chest X-ray reports have become essential for developing AI-driven healthcare systems. While effective, deep learning models require extensive labelled datasets, making the labelling of diseases from radiology reports crucial. Traditionally, rule-based labelling approaches have been employed, but the emergence of large language models (LLMs) has introduced new possibilities through instruction-based prompt engineering. In this study, we explore various prompt engineering techniques, including in-context learning and prompt chaining, to label multilabel disease reports and extract key clinical findings from radiology reports. We conducted ablation studies on both proprietary LLMs (e.g., GPT-4 Turbo, GPT-3.5 Turbo) and publicly available LLMs (e.g., Llama2-7B, Llama2-13B, Llama3-8B, Llama2-70B), comparing their performance in terms of clinical accuracy, privacy, and computational cost. Our findings demonstrate that well-crafted prompts on publicly available and lightweight LLMs can achieve competitive results compared to larger and/or proprietary models, offering a cost-effective and privacy-preserving solution for clinical applications. These results highlight the potential of leveraging advanced prompt engineering to streamline disease labelling and enhance the quality of automated report generation in radiology.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1616–1625},
numpages = {10},
keywords = {chest x-ray report., generative ai, in-context learning, llm, prompt engineering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3716815.3729018,
author = {Tuck, Bryan E.},
title = {LLMs Under Attack: Understanding the Adversarial Mindset},
year = {2025},
isbn = {9798400715013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716815.3729018},
doi = {10.1145/3716815.3729018},
abstract = {With Large Language Models (LLMs) powering critical applications, adversarial threats present urgent challenges to their safety and reliability. This tutorial explores adversarial threats against LLMs by covering foundational concepts, identifying key security implications, examining specific attack vectors (such as data poisoning, evasion techniques, and prompt-engineering vulnerabilities), and highlighting LLMs' dual roles as both targets and enablers of malicious activity. We critically assess current defensive approaches, discuss recent criticisms regarding detection reliability and ethical considerations, and outline key open research challenges. Attendees will gain practical insights into anticipating and mitigating adversarial threats to secure the deployment and application of LLM systems.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {34–35},
numpages = {2},
keywords = {large language models, adversarial machine learning, nlp security, prompt engineering, content detection, tutorial},
location = {Pittsburgh, PA, USA},
series = {IWSPA '25}
}

@inproceedings{10.1145/3724389.3730794,
author = {Dorard, Antoine and K\"{u}ppers, Bastian and Sai, Ashish Rajendra and Schnitzler, Theodor},
title = {Adding Context to Automated Vulnerability Detection for Teaching Software Security},
year = {2025},
isbn = {9798400715693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724389.3730794},
doi = {10.1145/3724389.3730794},
abstract = {Considering the recent developments in the fiel of generative AI, large language models (LLMs) can be leveraged to enhance static application security testing (SAST) tools and teaching about this topic. These models provide contextual information about identified vulnerabilities which can help students to differentiate genuine issues from false alarms while learning about software security. The approach includes analyzing vulnerabilities in android applications using SAST tools, clustering related code functionalities, and generating multi-level summaries for detected vulnerabilities. The process employs advanced clustering techniques and consensus-building methods to ensure accuracy.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {790},
numpages = {1},
keywords = {automated vulnerability detection, computer security education, sast, static application security testing},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@inproceedings{10.1145/3597503.3608137,
author = {Feng, Sidong and Chen, Chunyang},
title = {Prompting Is All You Need: Automated Android Bug Replay with Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608137},
doi = {10.1145/3597503.3608137},
abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {67},
numpages = {13},
keywords = {automated bug replay, large language model, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3611643.3616253,
author = {Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
title = {Grace: Language Models Meet Code Edits},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616253},
doi = {10.1145/3611643.3616253},
abstract = {Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29% and 54% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1483–1495},
numpages = {13},
keywords = {Associated edits, Code editing, Large language models, Pre-trained model, Programming language processing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3643788.3648021,
author = {Lajko, Mark and Csuvik, Viktor and Gyimothy, Tibor and Vidacs, Laszlo},
title = {Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648021},
doi = {10.1145/3643788.3648021},
abstract = {Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {34–41},
numpages = {8},
keywords = {automated program repair, transformers, GPT-3, codex, JavaScript},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3597503.3639086,
author = {Prenner, Julian Aron and Robbes, Romain},
title = {Out of Context: How important is Local Context in Neural Program Repair?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639086},
doi = {10.1145/3597503.3639086},
abstract = {Deep learning source code models have been applied very successfully to the problem of automated program repair. One of the standing issues is the small input window of current models which often cannot fully fit the context code required for a bug fix (e.g., method or class declarations of a project). Instead, input is often restricted to the local context, that is, the lines below and above the bug location. In this work we study the importance of this local context on repair success: how much local context is needed?; is context before or after the bug location more important? how is local context tied to the bug type? To answer these questions we train and evaluate Transformer models in many different local context configurations on three datasets and two programming languages. Our results indicate that overall repair success increases with the size of the local context (albeit not for all bug types) and confirm the common practice that roughly 50--60% of the input window should be used for context leading the bug. Our results are not only relevant for researchers working on Transformer-based APR tools but also for benchmark and dataset creators who must decide what and how much context to include in their datasets.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {83},
numpages = {13},
keywords = {automated program repair, data-driven software engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.5555/3709347.3743976,
author = {Mohammed, Hamza and Yin, Hang and Boyapati, Sai Chand},
title = {Context Adaptive Memory-Efficient LLM Inference for Edge Multi-Agent Systems},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Large Language Models (LLMs) excel at multi-document QA, summarization, code generation, and other language-intensive tasks, yet they demand substantial memory resources for storing key-value (KV) caches and processing attention in long-context scenarios. These requirements often prohibit on-device or edge deployments in multi-agent systems (MAS), where multiple agents share or update contextual information and need efficient inference pipelines. We present CASK (Context-Adaptive Sparse Key-value), an inference-time strategy that reduces memory usage while preserving strong performance on extended contexts. CASK addresses this challenge with two complementary mechanisms: a dynamic sparse attention module-a lightweight, meta-learned component-that identifies the most relevant context tokens, and an adaptive KV-cache compression technique that dynamically quantizes and prunes less critical key-value pairs based on usage frequency and recency. These innovations enable near-lossless performance on long-context tasks while cutting memory usage by up to 40% and boosting inference speed by as much as 20%. Evaluations on LongBench [2] and multi-agent benchmarks show that CASK maintains over 95% of baseline accuracy while allowing more agents or extended histories under tight GPU budgets. Integration into a vision-language agent for collaborative, multimodal contexts underscores its practicality for resource-constrained LLM deployments in MAS.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2678–2680},
numpages = {3},
keywords = {active learning, caching and paging algorithms, computer vision, dimensionality reduction and manifold learning, heuristic function construction, multi-agent learning, multi-agent reinforcement learning, multi-agent systems, natural language generation, neural networks, online learning algorithms, reinforcement learning, supervised learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@article{10.1145/3737873,
author = {Shi, Liang and Tang, Zhengju and Zhang, Nan and Zhang, Xiaotong and Yang, Zhi},
title = {A Survey on Employing Large Language Models for Text-to-SQL Tasks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3737873},
doi = {10.1145/3737873},
abstract = {With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jun,
keywords = {Large Language Models, Text-to-SQL, Prompt Engineering, Fine-tuning}
}

@inproceedings{10.1145/3706598.3714319,
author = {He, Zeyu and Naphade, Saniya and Huang, Ting-Hao Kenneth},
title = {Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714319},
doi = {10.1145/3706598.3714319},
abstract = {Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, “prompting in the dark,” where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable—only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1195},
numpages = {33},
keywords = {Data Annotation; Data Labeling; Large Language Model; Iterative labeling; End-User Programming},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3580305.3599557,
author = {Kenthapadi, Krishnaram and Lakkaraju, Himabindu and Rajani, Nazneen},
title = {Generative AI meets Responsible AI: Practical Challenges and Opportunities},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599557},
doi = {10.1145/3580305.3599557},
abstract = {Generative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models.In this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5805–5806},
numpages = {2},
keywords = {case studies from industry, ethics in ai, generative ai models and applications, large language models, responsible ai},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1109/ASE56229.2023.00181,
author = {Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing},
title = {An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00181},
doi = {10.1109/ASE56229.2023.00181},
abstract = {The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1174},
numpages = {13},
keywords = {automated program repair, large language models of code, neural machine translation, fine-tuning},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3652620.3688336,
author = {Bucchiarone, Antonio and Panciera, Marco and Cicchetti, Antonio and Mana, Nadia and Castelluccio, Carlotta and Stott, Lee},
title = {PromptDeck: A No-Code Platform for Modular Prompt Engineering},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688336},
doi = {10.1145/3652620.3688336},
abstract = {This paper introduces a no-code platform for modular prompt engineering, designed to democratize access to generative AI for nondevelopers. By integrating advanced technologies such as Node.js, Express, MongoDB, and Azure OpenAI services, the platform provides a robust and flexible environment for creating and managing AI-driven tasks. The intuitive frontend, built with React and TypeScript, enables users with minimal coding expertise to design, execute, and evaluate complex AI workflows. A key feature of the platform is its extensible plugin system, which allows users to easily incorporate additional functionalities to meet their specific needs. This no-code approach empowers a broader audience to harness the power of generative AI, fostering innovation and enabling diverse applications across various fields. By lowering the technical barriers, the platform paves the way for widespread adoption of AI technologies, driving the future of AI-enhanced solutions.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {895–904},
numpages = {10},
keywords = {low-code development platforms, no-code, generative AI, prompt engineering, modularization},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3695993,
author = {Yu, Yongda and Rong, Guoping and Shen, Haifeng and Zhang, He and Shao, Dong and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong},
title = {Fine-Tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695993},
doi = {10.1145/3695993},
abstract = {As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans, thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this article, we contribute Comprehensibility of Automated Code Review using Large Language Models (Carllm), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {14},
numpages = {26},
keywords = {Automated Code Review, Human-machine Collaboration, LLM, LORA}
}

@inproceedings{10.1145/3639478.3639792,
author = {Rodriguez-Cardenas, Daniel},
title = {Beyond Accuracy and Robustness Metrics for Large Language Models for Code},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639792},
doi = {10.1145/3639478.3639792},
abstract = {In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot [31] and Google's CodeBot [21] exemplify how LLMc contributes to substantial time and effort savings in software development. However, despite their widespread use, there is a growing need to thoroughly assess LLMc, as current evaluation processes heavily rely on accuracy and robustness metrics, lacking consensus on additional influential factors in code generation. This gap hinders a holistic understanding of LLMc performance, impacting interpretability, efficiency, bias, fairness, and robustness. The challenges in benchmarking and data maintenance compound this issue, underscoring the necessity for a comprehensive evaluation approach. To address these issues, this dissertation proposes the development of a benchmarking infrastructure, named HolBench, aimed at overcoming gaps in evaluating LLMc quality. The goal is to standardize testing scenarios, facilitate meaningful comparisons across LLMc, and provide multi-metric measurements beyond a sole focus on accuracy. This approach aims to decrease the costs associated with advancing LLMc research, enhancing their reliability for adoption in academia and industry.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {159–161},
numpages = {3},
keywords = {deep learning, code generation, interpretability, transformers},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3696630.3728618,
author = {Chen, Yujia},
title = {AutoReview: An LLM-based Multi-Agent System for Security Issue-Oriented Code Review},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728618},
doi = {10.1145/3696630.3728618},
abstract = {Software vulnerabilities can lead to severe security issues such as data breaches, financial losses, and service disruptions, making security issue-oriented code review a crucial part of the development process. Traditional approaches struggle with analyzing complex code and providing explanations, while large language models (LLMs) show promise in code review but do not focus on security-related issues. To address these limitations, we propose AutoReview, an LLM-based multi-agent system for security code review. It integrates three agents: (1) Issue Detector identifying potential vulnerabilities using knowledge-level retrieval-augmented generation, (2) Issue Locator pinpoints the vulnerability positions through graph-based code slicing, and (3) Issue Repairer generating context-aware fixes via iterative verification. Evaluated on ReposVul with three code LLMs, AutoReview greatly demonstrates its effectiveness in security code reviews, improving F1-score for detection by 18.72%, precision for location by 27.75%, and BLEU for repair by 14.82% over baselines.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1022–1024},
numpages = {3},
keywords = {large language model, code review, code repair},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3706599.3720283,
author = {Wang, Nicole C.},
title = {Scaffolding Creativity: Integrating Generative AI Tools and Real-World Experiences in Business Education},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720283},
doi = {10.1145/3706599.3720283},
abstract = {This exploratory study investigates the intersection of Generative AI tools and experiential learning in business education. Through a case study of an innovative undergraduate course, we examine how students interact with and adapt to various AI modalities—from text-based tools to image generation—alongside real-world experiences. Our findings reveal how this integrated approach enables novice users to overcome creative barriers, accelerates skill acquisition, and creates a dynamic interplay between AI-generated insights and real-world validation. We identify critical interaction challenges, including prompt engineering patterns and the need for more intuitive AI interfaces in educational contexts. These insights inform the design of future AI tools for creative learning and contribute to broader HCI discussions about human-AI collaboration in educational settings.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {468},
numpages = {9},
keywords = {AI-assisted learning, Artificial Intelligence in Education (AIEd), Higher Education, Experiential learning, ChatGPT, Midjourney},
location = {
},
series = {CHI EA '25}
}

@article{10.1145/3729366,
author = {Fruntke, Lukas and Krinke, Jens},
title = {Automatically Fixing Dependency Breaking Changes},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729366},
doi = {10.1145/3729366},
abstract = {Breaking changes in dependencies are a common challenge in software development, requiring manual intervention to resolve. This study examines how well LLM automate the repair of breaking changes caused by dependency updates in Java projects. Although earlier methods have mostly concentrated on detecting breaking changes or investigating their impact, they have not been able to completely automate the repair process. We introduce and compare two new approaches: an agentic system that combines automated tool usage with LLM, and a recursive zero-shot approach, employing iterative prompt refinement. Our experimental framework assesses the repair success of both approaches, using the BUMP dataset of curated breaking changes. We also investigate the impact of variables such as dependency popularity and prompt configuration on repair outcomes. Our results demonstrate a substantial difference in test suite success rates, with the agentic approach achieving a repair success rate of up to 23%, while the zero-shot prompting approach achieved a repair success rate of up to 19%. We show that automated program repair of breaking dependencies with LLMs is feasible and can be optimised to achieve better repair outcomes.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE096},
numpages = {23},
keywords = {Automated program repair, dependency management}
}

@inproceedings{10.1145/3637528.3671467,
author = {Kenthapadi, Krishnaram and Sameki, Mehrnoosh and Taly, Ankur},
title = {Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671467},
doi = {10.1145/3637528.3671467},
abstract = {With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our &lt;u&gt;tutorial&lt;/u&gt;, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6523–6533},
numpages = {11},
keywords = {bias and fairness, model robustness and security, calibration and confidence, copyright infringement, evaluations, generative ai, grounding, large language models, model disgorgement and unlearning, privacy, responsible ai, safety and alignment, transparency and causal interventions, truthfulness},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3701716.3717817,
author = {Jadhav, Suramya and Perumal, Suki and Tadavi, Yasmin and Dash, Bikshita and Parthiban, Srinivasan},
title = {Leveraging Large Language Models for Biomedical Knowledge Graph Construction and Querying: An Advanced NLP Approach},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717817},
doi = {10.1145/3701716.3717817},
abstract = {This paper introduces a novel methodology for constructing a comprehensive biomedical knowledge graph by applying advanced Natural Language Processing (NLP) techniques. By leveraging Large Language Models (LLMs) and a multifaceted prompt engineering approach, we effectively perform Named Entity Recognition (NER) and Relation Extraction (RE) on biomedical literature, targeting entities such as diseases, drugs, proteins, procedures, and symptoms. Our methodology incorporates eight distinct prompt engineering strategies for NER and a standardized approach for RE, facilitating the extraction of intricate inter-entity relationships. The resulting knowledge graph amalgamates diverse data sources into a unified framework, enabling efficient querying, visualization, and analysis of biomedical information. Furthermore, we present an innovative query processing pipeline that integrates GPT-3.5 turbo with the knowledge graph, allowing users to interact with the graph through natural language. This integrated system empowers the discovery of novel correlations, accelerating scientific research and fostering interdisciplinary collaboration. This represents a substantial contribution to the field of biomedical knowledge graph construction, offering a robust platform for accelerating scientific discovery and informing clinical decision-making.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2560–2566},
numpages = {7},
keywords = {knowledge graphs, large language models, named entity recognition, prompt engineering, query processing, relationship extraction},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3644815.3644969,
author = {Bur\'{e}gio, Vanilson and Pereira, Iverson and Cabral, Henrique},
title = {Innovating Translation: Lessons Learned from BWX Generative Language Engine},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644969},
doi = {10.1145/3644815.3644969},
abstract = {The integration of Translation Management Systems (TMS) and Large Language Models (LLMs) has revolutionized the translation landscape, offering nuanced and culturally sensitive translations. This paper explores the lessons learned from developing the BWX Generative Language Engine, an award-winning Generative AI tool for translation, which exemplifies the application of generative AI in translation management. Lessons include the transformative impact of AI, the accelerated delivery of beta features with LLMs, and the strategic integration of enabling technologies. Additionally, insights are drawn from strategic testing for optimal model routing, caching mechanisms, fallbacks, and the importance of security and data policy awareness.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {98–99},
numpages = {2},
keywords = {generative AI, translation management systems, large language models, software engineering},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/3701716.3717818,
author = {Yang, Can and Pereira Nunes, Bernardo and Rodr\'{\i}guez M\'{e}ndez, Sergio},
title = {LLM as Auto-Prompt Engineer: Automated NER Prompt Optimisation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717818},
doi = {10.1145/3701716.3717818},
abstract = {The emergence of Large Language Models (LLMs) has revolutionised natural language processing capabilities. However, despite these advances, effectively optimising prompts for knowledge extraction tasks like Named Entity Recognition (NER) remains challenging. This paper presents a zero-shot automated prompt engineering approach that decomposes the NER task into two phases: entity boundary detection and entity classification. Our method incorporates structured task analysis, automated prompt generation, test case generation, and iterative optimisation, requiring no labelled training examples. This decomposition allows for more precise entity recognition while maintaining the efficiency. Through experimentation on the CoNLL-2003 dataset using standard exact-match evaluation metrics, our approach demonstrates improvements over unified methods, achieving a 75.39% F1 score compared to baseline approaches (72.90%). The key contributions include: (1) A structured pipeline for zero-shot automated prompt engineering in NER tasks that addresses the challenges of prompt design and optimisation; (2) A two-phase approach to NER tasks that separates boundary detection from entity classification; and (3) Experimental results demonstrating the effectiveness of our approach compared to existing zero-shot approaches in NER tasks.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2574–2578},
numpages = {5},
keywords = {automated prompt engineering, large language models, named entity recognition, prompt optimisation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3715109,
author = {Wu, Jie Jw and Fard, Fatemeh H.},
title = {HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715109},
doi = {10.1145/3715109},
abstract = {Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. The most recent trend is using LLM-based agents to iterate the code generation process. Based on the observation that top-level software engineers often ask clarifying questions to reduce Ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. For this purpose, we define the communication skills of LLMs as “being able to ask clarifying questions when the description of the code generation problem has issues”. In this study, we restrict these issues to three matters from the software requirement engineering field: inconsistent requirements, ambiguous requirements, and incomplete requirements. By asking probing questions about the requirements of problem descriptions before generating the final code, the challenges of programming with LLMs, such as unclear intent specification may be alleviated, resulting to a correct code in the initial iterations.In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues mentioned above, Inconsistency, Ambiguity, Incompleteness. We then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Code Clarification and Generation Agent (Okanagan), to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. In the evaluation, we introduced an LLM-based evaluator and created Communication Rate and Good Question Rate as the evaluation metrics to represent the ratio of questions asked and questions with good quality in responses. We found that more than 60% of responses from Code LLMs still generate code rather than ask questions when the problem descriptions are manually modified according to different clarification categories. The Pass@1 and Test Pass Rate of most Code LLMs drop by 35%  (sim)  52% and by 17%  (sim)  35% respectively, with statistical significance in each category for over 75% numbers. Okanagan, as an LLM agent approach that uses LLM such as ChatGPT 3.5, effectively increases the Communication Rate and Good Question Rate by an absolute 58% and 38%, respectively. Thus, Okanagan boosts Pass@1 and Test Pass Rate by an absolute 8% and 7%, respectively, when the problem descriptions are modified based on given clarification categories. This result indicates the potential for achieving more effective communication capability using LLM agent. Our benchmark and full code are publicly available at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan
}

@inproceedings{10.1145/3691620.3695019,
author = {Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian, Xiaoli and Yang, Donghao and Tan, Xin},
title = {DRMiner: Extracting Latent Design Rationale from Jira Issue Logs},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695019},
doi = {10.1145/3691620.3695019},
abstract = {Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, there may be a lack of motivation for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions) when they will not gain immediate benefit, resulting in a lack of standard capture of these rationales. With the turnover of developers, the architecture inevitably becomes eroded. This issue has motivated a number of studies to extract design knowledge from open-source communities in recent years. Unfortunately, none of the existing research has successfully extracted solutions alone with their corresponding arguments due to challenges such as the intricate semantics of online discussions and the lack of benchmarks for design rationale extraction.In this paper, we propose a novel approach, named DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and their relevant arguments, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of large language models (LLMs) and specific heuristic features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira and form a dataset for design rationale mining. Experimental results show that DRMiner outperforms all baselines and achieves F1 improvements of 24%, 22%, and 20% for mining design rationales, solutions, and arguments, respectively, compared to the best baseline. Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that advanced LLMs, when prompted with these extracted rationales, generate 10\texttimes{}-18\texttimes{} more full-match patches and achieve a 10%-13% gain in CodeBLEU scores.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {468–480},
numpages = {13},
keywords = {design rationale, issue logs, design discussion, design recovery, program maintenance},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inbook{10.1145/3696630.3731464,
author = {Rodriguez-Cardenas, Daniel},
title = {Towards More Interpretable Large Language Models for Code},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731464},
abstract = {Large Language Models (LLMc) have shown significant potential in automating software engineering tasks, particularly in code generation. This research bridges the gap between rigorous causal analysis and human interpretability in evaluating LLMc within software engineering (SE) contexts. The study introduces a novel approach InterpretSE that combines a taxonomy of code syntax concepts with a token clustering method to enhance the interpretability of LLMc outputs, facilitate debugging, and advance the field of interpretable AI for SE. A dynamic benchmark is proposed, comprising a testbed, a metric, and a protocol for applying the metric. To bridge the gap between statistical rigor and human interpretability, we propose to introduce a taxonomy of code syntax concepts. This taxonomy maps low-level, non-interpretable tokens to higher-level, human-understandable concepts, enabling practitioners to debug LLMc outputs effectively. The research aims to address two key challenges in LLMc evaluation: the lack of formal, transparent, and interpretable benchmarking methods, and the difficulty in interpreting LLMc outputs due to the complexity of token-level representations. By providing a dynamic, causal analysis-driven benchmark and a human-interpretable taxonomy, this work offers actionable insights for improving LLMc behavior, supporting prompt engineering, and advancing the reliability and effectiveness of LLMc in SE tasks. The expected contribution is a significant advancement in the field of interpretable AI for software engineering, enabling more rigorous and reliable evaluation of LLMc capabilities and limitations.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1270–1272},
numpages = {3}
}

@inproceedings{10.1145/3613904.3642706,
author = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q},
title = {How Beginning Programmers and Code LLMs (Mis)read Each Other},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642706},
doi = {10.1145/3613904.3642706},
abstract = {Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {651},
numpages = {26},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642166,
author = {Mack, Kelly Avery and Qadri, Rida and Denton, Remi and Kane, Shaun K. and Bennett, Cynthia L.},
title = {“They only care to show us the wheelchair”: disability representation in text-to-image AI models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642166},
doi = {10.1145/3613904.3642166},
abstract = {This paper reports on disability representation in images output from text-to-image (T2I) generative AI systems. Through eight focus groups with 25 people with disabilities, we found that models repeatedly presented reductive archetypes for different disabilities. Often these representations reflected broader societal stereotypes and biases, which our participants were concerned to see reproduced through T2I. Our participants discussed further challenges with using these models including the current reliance on prompt engineering to reach satisfactorily diverse results. Finally, they offered suggestions for how to improve disability representation with solutions like showing multiple, heterogeneous images for a single prompt and including the prompt with images generated. Our discussion reflects on tensions and tradeoffs we found among the diverse perspectives shared to inform future research on representation-oriented generative AI system evaluation metrics and development processes.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {288},
numpages = {23},
keywords = {AI harms, algorithmic harms, disability representation, generative AI, human-centered AI, text-to-image models},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inbook{10.1145/3696630.3731466,
author = {Melo, Rui},
title = {Securing Language Models Against Vulnerability Encoding},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731466},
abstract = {The integration of Large Language Models (LLMs) into software development workflows has transformed automated programming but introduced significant security challenges. LLMs often generate vulnerable code due to the insecure patterns present in training data, leading to the generation of code vulnerable to threats such as SQL injection, cross-site scripting, and buffer overflows. Existing mitigation strategies, including static and dynamic analysis tools and prompt engineering, are reactive rather than preventive. Recent advances in model training, such as fine-tuning and adversarial training, offer promising avenues for enhancing the security of LLM-generated code. This paper explores different methodologies and proposes an evaluation framework to embed security directly into AI-assisted programming. By integrating security into model training and assessment, we aim to establish a robust standard for secure AI-driven programming.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1277–1278},
numpages = {2}
}

@inproceedings{10.1145/3661167.3661171,
author = {Vallecillos Ruiz, Fernando},
title = {Agent-Driven Automatic Software Improvement},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661171},
doi = {10.1145/3661167.3661171},
abstract = {With software maintenance accounting for 50% of the cost of developing software, enhancing code quality and reliability has become more critical than ever. In response to this challenge, this doctoral research proposal aims to explore innovative solutions by focusing on the deployment of agents powered by Large Language Models (LLMs) to perform software maintenance tasks. The iterative nature of agents, which allows for continuous learning and adaptation, can help surpass common challenges in code generation. One distinct challenge is the last-mile problems, errors at the final stage of producing functionally and contextually relevant code. Furthermore, this project aims to surpass the inherent limitations of current LLMs in source code through a collaborative framework where agents can correct and learn from each other’s errors. We aim to use the iterative feedback in these systems to further fine-tune the LLMs underlying the agents, becoming better aligned to the task of automated software improvement. Our main goal is to achieve a leap forward in the field of automatic software improvement by developing new tools and frameworks that can enhance the efficiency and reliability of software development.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {470–475},
numpages = {6},
keywords = {Automatic Maintenance, Automatic Software Improvement, LLM-based Agents, ML4Code, Multi-Agent Systems},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3627673.3679071,
author = {Xu, Han and Wang, Xingyuan and Chen, Haipeng},
title = {Towards Real-Time and Personalized Code Generation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679071},
doi = {10.1145/3627673.3679071},
abstract = {Large language models (LLMs) have transformed automated code generation. However, their high computational demands often lead to server overload and increased latency in SaaS deployments. To address this, we present SpeCoder, a framework that accelerates server-side code generation using speculative sampling (SpS) and supervised fine-tuning (SFT). SpS allows lower latency in the code generation, whereas SFT enables more personalized code generation tailored to the user's needs.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5568–5569},
numpages = {2},
keywords = {code generation, llm, natural language processing, speculative sampling, supervised fine-tuning},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3691620.3694997,
author = {Lu, Jiawei and Wang, Haoye and Liu, Zhongxin and Liang, Keyu and Bao, Lingfeng and Yang, Xiaohu},
title = {Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3694997},
doi = {10.1145/3691620.3694997},
abstract = {Recent studies proposed to leverage large language models (LLMs) with In-Context Learning (ICL) to handle code intelligence tasks without fine-tuning. ICL employs task instructions and a set of examples as demonstrations to guide the model in generating accurate answers without updating its parameters. While ICL has proven effective for code intelligence tasks, its performance heavily relies on the selected examples. Previous work has achieved some success in using BM25 to retrieve examples for code intelligence tasks. However, existing approaches lack the ability to understand the semantic and structural information of queries, resulting in less helpful demonstrations. Moreover, they do not adapt well to the complex and dynamic nature of user queries in diverse domains. In this paper, we introduce a novel approach named Instructive Code Retriever (ICR), which is designed to retrieve examples that enhance model inference across various code intelligence tasks and datasets. We enable ICR to learn the semantic and structural information of the corpus by a tree-based loss function. To better understand the correlation between queries and examples, we incorporate the feedback from LLMs to guide the training of the retriever. Experimental results demonstrate that our retriever significantly outperforms state-of-the-art approaches. We evaluate our model's effectiveness on various tasks, i.e., code summarization, program synthesis, and bug fixing. Compared to previous state-of-the-art algorithms, our method achieved improvements of 50.0% and 90.0% in terms of BLEU-4 for two code summarization datasets, 74.6% CodeBLEU on program synthesis dataset, and increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {191–203},
numpages = {13},
keywords = {software engineering, large language models, in-context learning},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3611643.3616271,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
abstract = {During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
keywords = {Completion Engine, Large Language Model, Program Repair},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3728951,
author = {Shang, Ye and Zhang, Quanjun and Fang, Chunrong and Gu, Siqi and Zhou, Jianyi and Chen, Zhenyu},
title = {A Large-Scale Empirical Study on Fine-Tuning Large Language Models for Unit Testing},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728951},
doi = {10.1145/3728951},
abstract = {Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research.   Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs.    To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing.   Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours.   We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering.   Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks.   Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale.   Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks.   We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons.   Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.   Overall, our work demonstrates the promising future of fine-tuning LLMs on unit testing tasks and reduces the manual efforts of unit testing experts in practical scenarios.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA074},
numpages = {23},
keywords = {AI for SE, Large Language Model, Software Testing, Unit Testing}
}

@inproceedings{10.1145/3639856.3639872,
author = {Khatun, Rabina and Sinhababu, Nilanjan},
title = {Improved Sequence Predictions using Knowledge Graph Embedding for Large Language Models},
year = {2024},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639856.3639872},
doi = {10.1145/3639856.3639872},
abstract = {Large Language Models (LLM) have gained huge popularity recently due to their problem-solving capability in multiple domains. Technically LLMs can be considered a critical mixture of huge amounts of training data, smart and exhaustive prompt engineering, and word prediction models along with Reinforcement and Supervised learning mechanisms. Word prediction models are at the core of any Large Language Model. The latest word prediction techniques are sequential and transformer models. Transformers have overcome most of the drawbacks of sequential models with similar embedding knowledge. The literature survey shows little to no improvement in the embedding techniques. In this paper, we examined the existing word prediction models by replacing embedding models with an auto-engineered Knowledge Graph Embedding. This auto-engineered data representation shows drastic improvements in prediction quality. This mechanism also accelerates the prediction by providing more context information to the models with respect to the general embedding mechanism. Standard evaluation strategies are used to compare the model behavior.},
booktitle = {Proceedings of the Third International Conference on AI-ML Systems},
articleno = {16},
numpages = {5},
keywords = {attention mechanisms, generative models, neural networks, text prediction},
location = {Bangalore, India},
series = {AIMLSystems '23}
}

@inproceedings{10.1145/3715336.3735684,
author = {Wang, Zixuan and Fernandez, Nichole and Vines, John},
title = {How Does AI Represent Social Concepts? Examining the Visual Representation of Care in Text-to-Image Tools},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735684},
doi = {10.1145/3715336.3735684},
abstract = {Text-to-image (T2I) generative AI tools like Midjourney are growing in capability and popularity, promising a wide range of applications. However, concerns are rising over the biases in how they represent social concepts like care and the lack of guidance for designers and users to address these in practice. This paper first presents an analysis of 140 “photos of care” generated by Midjourney, and then explores how prompting might influence the results. The findings reveal that AI-generated images reproduce stereotypical and reductive representations of care by default, neglecting the broad spectrums of care practices in everyday life. Furthermore, we find that while prompt engineering might mitigate certain biases, it requires specialised skills, knowledge, and an ongoing reflexive approach to generate meaningful outputs. We conclude by proposing a reflexive prompting framework, and discussing the implications for future T2I evaluation and its responsible use and design.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {2770–2786},
numpages = {17},
keywords = {Bias, Care, Visual Representation, Generative AI, Text-to-image Models, Prompt Engineering, Responsible AI},
location = {
},
series = {DIS '25}
}

@article{10.1145/3712005,
author = {Gao, Cuiyun and Hu, Xing and Gao, Shan and Xia, Xin and Jin, Zhi},
title = {The Current Challenges of Software Engineering in the Era of Large Language Models},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712005},
doi = {10.1145/3712005},
abstract = {With the advent of large language models (LLMs) in the AI area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities.The article aims at revisiting the software development lifecycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The article first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as SE and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement and design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {127},
numpages = {30},
keywords = {Large Language Models, Challenges, LLM4SE}
}

@inbook{10.1145/3696630.3731471,
author = {Cipollone, Daniele},
title = {Enhancing Large Language Model Integration in Integrated Development Environments},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731471},
abstract = {Large language models have emerged as one of the most impactful applications in software engineering. However, their integration into the software development ecosystem remains fragmented and suboptimal. This study aims to bridge this gap by adapting and fine-tuning existing large language models for real-world software development scenarios. Rather than building new models from scratch, our approach leverages advanced techniques that allow Large Language Models to interact seamlessly with existing development tools and Integrated Development Environments (IDEs). This research, conducted in collaboration with JetBrains, focuses on personalizing code generation by adjusting the model to the unique features of a project and the individual coding style of each developer.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1291–1292},
numpages = {2}
}

@inproceedings{10.1145/3706468.3706481,
author = {Ferreira Mello, Rafael and Pereira Junior, Cleon and Rodrigues, Luiz and Pereira, Filipe Dwan and Cabral, Luciano and Costa, Newarney and Ramalho, Geber and Gasevic, Dragan},
title = {Automatic Short Answer Grading in the LLM Era: Does GPT-4 with Prompt Engineering beat Traditional Models?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706481},
doi = {10.1145/3706468.3706481},
abstract = {Assessing short answers in educational settings is challenging due to the need for scalability and accuracy, which led to the field of Automatic Short Answer Grading (ASAG). Traditional machine learning models, such as ensemble and embeddings, have been widely researched in ASAG, but they often suffer from generalizability issues. Recently, Large Language Models (LLMs) emerged as an alternative to optimize ASAG systems. However, previous research has failed to present a comprehensive analysis of LLMs’ performance powered by prompt engineering strategies and compare its capabilities to traditional models. This study presents a comparative analysis between traditional machine learning models and GPT-4 in the context of ASAG. We investigated the effectiveness of different models and text representation techniques and explored prompt engineering strategies for LLMs. The results indicate that traditional machine learning models outperform LLMs. However, GPT-4 showed promising capabilities, especially when configured with optimized prompt components, such as few-shot examples and clear instructions. This study contributes to the literature by providing a detailed evaluation of LLM performance compared to traditional machine learning models in a multilingual ASAG context, offering insights for developing more efficient automatic grading systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {93–103},
numpages = {11},
keywords = {Automatic short answer grading, Natural Language Processing, Assessment, LLM, GPT},
location = {
},
series = {LAK '25}
}

@article{10.1145/3726871,
author = {Xu, Lanling and Zhang, Junjie and Li, Bingqian and Wang, Jinpeng and Chen, Sheng and Zhao, Wayne Xin and Wen, Ji-Rong},
title = {Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3726871},
doi = {10.1145/3726871},
abstract = {Recently, Large Language Models (LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompt engineering. We propose a general framework for leveraging LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task descriptions, user interest modeling, candidate items construction, and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on recommendation performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {105},
numpages = {51},
keywords = {Large Language Models, Recommender Systems, Empirical Study}
}

@inproceedings{10.1145/3701716.3717809,
author = {Pellegrino, Maria Angela and Tuozzo, Gabriele},
title = {From Quality Reports to Knowledge Graphs: a Case Study on CSV-to-KG Transformation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717809},
doi = {10.1145/3701716.3717809},
abstract = {The construction of Knowledge Graphs (KGs) often demands substantial manual effort and domain expertise, especially when converting structured data formats like CSV files into KGs. Recent advancements in Large Language Models (LLMs) offer promising avenues to simplify this process through prompt engineering.This study investigates various prompting strategies-zero-shot, one-shot, prompt chaining, and a hybrid approach-to enable LLMs to automate the creation of KGs from CSV files. Using a dataset containing quality metrics for 2,026 KGs generated by KGHeartBeat, the paper assesses the performance of GPT-4o, GPT-o1 mini, Claude 3.5 Sonnet, and Gemini 1.5 pro, across different prompt configurations. The findings reveal that the hybrid approach consistently produces the most accurate and complete KGs, effectively addressing challenges related to scalability and complexity.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1626–1632},
numpages = {7},
keywords = {comparison, csv converter, data quality, empirical investigation, knowledge graph, llms, prompt engineering, quality assessment},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3696630.3730562,
author = {Klimek, Radoslaw},
title = {RE-oriented Model Development with LLM Support and Deduction-based Verification},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730562},
doi = {10.1145/3696630.3730562},
abstract = {The requirements engineering (RE) phase is pivotal in developing high-quality software. Integrating advanced modelling techniques with large language models (LLMs) and formal verification in a logical style can significantly enhance this process. We propose a comprehensive framework that focuses on specific Unified Modelling Language (UML) diagrams for preliminary system development. This framework offers visualisations at various modelling stages and seamlessly integrates large language models and logical reasoning engines. The behavioural models generated with the assistance of LLMs are automatically translated into formal logical specifications. Deductive formal verification ensures that logical requirements and interrelations between software artefacts are thoroughly addressed. Ultimately, the framework facilitates the automatic generation of program skeletons, streamlining the transition from design to implementation.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1297–1304},
numpages = {8},
keywords = {requirements engineering, formal IDE, UML modelling, large language models, automated logical reasoning, temporal logic},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3701716.3717539,
author = {Kanakaris, Nikos and Ping, Heng and Xiao, Xiongye and Ahmed, Nesreen K. and Luceri, Luca and Ferrara, Emilio and Bogdan, Paul},
title = {Network-informed Prompt Engineering against Organized Astroturf Campaigns under Extreme Class Imbalance},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717539},
doi = {10.1145/3701716.3717539},
abstract = {Detecting organized political campaigns, commonly known as astroturf campaigns, is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on 𝕏 (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2\texttimes{}-3\texttimes{} improvements in terms of precision, recall and F1 scores.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2651–2660},
numpages = {10},
keywords = {class imbalance, disinformation spread, fake news detection, graph classification, graph-aware prompt engineering, large language models, organized disinformation campaign detection, prompt engineering, retrieval-augmented generation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3711006,
author = {He, Zhiting and Su, Jiayi and Chen, Li and Wang, Tianqi and Lc, Ray},
title = { 'I Recall the Past': Exploring How People Collaborate with Generative AI to Create Cultural Heritage Narratives},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711006},
doi = {10.1145/3711006},
abstract = {Visitors to cultural heritage sites often encounter official information, while local people's unofficial stories remain invisible. To explore expression of local narratives, we conducted a workshop with 20 participants utilizing Generative AI (GenAI) to support visual narratives, asking them to use Stable Diffusion to create images of familiar cultural heritage sites, as well as images of unfamiliar ones for comparison. The results revealed three narrative strategies and highlighted GenAI's strengths in illuminating, amplifying, and reinterpreting personal narratives. However, GenAI showed limitations in meeting detailed requirements, portraying cultural features, and avoiding bias, which were particularly pronounced with unfamiliar sites due to participants' lack of local knowledge. To address these challenges, we recommend providing detailed explanations, prompt engineering, and fine-tuning AI models to reduce uncertainties, using objective references to mitigate inaccuracies from participants' inability to recognize errors or misconceptions, and curating datasets to train AI models capable of accurately portraying cultural features.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW108},
numpages = {30},
keywords = {cultural heritage, familiarity, generative ai, narrative}
}

@article{10.1145/3728894,
author = {Zhang, Ziyao and Wang, Chong and Wang, Yanlin and Shi, Ensheng and Ma, Yuchi and Zhong, Wanjun and Chen, Jiachi and Mao, Mingzhi and Zheng, Zibin},
title = {LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728894},
doi = {10.1145/3728894},
abstract = {Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA022},
numpages = {23},
keywords = {Hallucination, Large Language Models, Repository-Level Code Generation}
}

@inproceedings{10.1145/3664646.3664758,
author = {Correia, Jo\~{a}o and Nicholson, Morgan C. and Coutinho, Daniel and Barbosa, Caio and Castelluccio, Marco and Gerosa, Marco and Garcia, Alessandro and Steinmacher, Igor},
title = {Unveiling the Potential of a Conversational Agent in Developer Support: Insights from Mozilla’s PDF.js Project},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664758},
doi = {10.1145/3664646.3664758},
abstract = {Large language models and other foundation models (FMs) boost productivity by automating code generation, supporting bug fixes, and generating documentation. We propose that FMs can further support Open Source Software (OSS) projects by assisting developers and guiding the community. Currently, core developers and maintainers answer queries about processes, architecture, and source code, but their time is limited, often leading to delays. To address this, we introduce DevMentorAI, a tool that enhances developer-project interactions by leveraging source code and technical documentation. DevMentorAI uses the Retrieval Augmented Generation (RAG) architecture to identify and retrieve relevant content for queries. We evaluated DevMentorAI with a case study on PDF.js project, using real questions from a development chat room and comparing the answers provided by DevMentorAI to those from humans. A Mozilla expert rated the answers, finding DevMentorAI's responses more satisfactory in 8/14 of cases, equally satisfactory in 3/14, and less satisfactory in 3/14. These results demonstrate the potential of using foundation models and the RAG approach to support developers and reduce the burden on core developers.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {10–18},
numpages = {9},
keywords = {Conversational Agents, Developer Assistance, Large Language Models, Software Development, Software Engineering},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3715775,
author = {Imtiaz, Sayem Mohammad and Singh, Astha and Batole, Fraol and Rajan, Hridesh},
title = {IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715775},
doi = {10.1145/3715775},
abstract = {Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity, harmful responses, and factual inaccuracies. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, drawing inspiration from fault localization via program slicing, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model’s most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model’s overall versatility by altering a smaller portion of the model. Furthermore, dynamic selection allows for a more nuanced and precise model repair compared to a fixed selection strategy. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE056},
numpages = {23},
keywords = {Detoxification, Dynamic Program Slicing, Fault Localization, Large Language Model, Program Repair, SE4AI}
}

@inproceedings{10.1145/3677389.3702591,
author = {Pandi, Saran Pandian and Park, Seoyeon and Rikka, Praneeth and Phillips, Mark Edward and Caragea, Cornelia},
title = {Can LLMs categorize the specialized documents from web archives in a better way?},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702591},
doi = {10.1145/3677389.3702591},
abstract = {The explosive growth of web archives presents a significant challenge: manually curating specialized document collections from this vast data. Existing approaches rely on supervised techniques, but recent advancements in Large Language Models (LLMs) offer new possibilities for automating collection creation. Large Language Models (LLMs) are demonstrating impressive performance on various tasks even without fine-tuning. This paper investigates the effectiveness of prompt design in achieving results comparable to fine-tuned models. We explore different prompting techniques for collecting specialized documents from web archives like UNT.edu, Michigan.gov, and Texas.gov. We then analyze the performance of LLMs under various prompt configurations. Our findings highlight the significant impact of incorporating task descriptions within prompts. Additionally, including the document type as justification for the search scope leads to demonstrably better results. This research suggests that well-crafted prompts can unlock the potential of LLMs for specialized tasks, potentially reducing reliance on resource-intensive fine-tuning. This research paves the way for automating specialized collection creation using LLMs and prompt engineering.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {46},
numpages = {11},
keywords = {large language models, web archiving, specialized collection, K-shot prompting, chain-of-thoughts prompting},
location = {Hong Kong, China},
series = {JCDL '24}
}

@inproceedings{10.1145/3671127.3698698,
author = {Deng, Yang and Xie, Donghua and Liang, Rui and Zeng, Jingyun and Tai, Samson and Wang, Dan},
title = {BuildProg: Program Generation for Testing ML-based Building Load Forecasting models via LLM and Prompt Engineering},
year = {2024},
isbn = {9798400707063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671127.3698698},
doi = {10.1145/3671127.3698698},
abstract = {Machine learning-based building load forecasting (BLF) is crucial for the building automation community, and numerous ML models have been developed for this purpose. However, a significant challenge arises when promoting these models for deployment in real buildings: building practitioners often struggle with ML-related programming. To address this issue, we propose BuildProg, a program generation tool that leverages prompt engineering to decompose user requirements and guide large language models (LLMs) in generating the necessary Python code. In its current version, BuildProg supports four tasks related to the testing of BLF models.},
booktitle = {Proceedings of the 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {248–249},
numpages = {2},
keywords = {LLM, Model testing, program generation, prompting},
location = {Hangzhou, China},
series = {BuildSys '24}
}

@inproceedings{10.1145/3640471.3680444,
author = {Deldari, Shohreh and Goudarzi, Mohammad and Joshi, Aditya and Shaghaghi, Arash and Finn, Simon and Salim, Flora D. and Jha, Sanjay},
title = {AuditNet: Conversational AI Security Assistant},
year = {2024},
isbn = {9798400705069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640471.3680444},
doi = {10.1145/3640471.3680444},
abstract = {In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate compliance checking on the go, in diverse domains, including but not limited to network infrastructure, legal contracts, educational standards, environmental regulations, and government policies. By leveraging retrieval-augmented generation using large language models, our framework automates the review, indexing, and retrieval of relevant, context-aware information, streamlining the process of verifying adherence to established guidelines and requirements. This AI assistant not only reduces the manual effort involved in compliance checks but also enhances accuracy and efficiency, supporting professionals in maintaining high standards of practice and ensuring regulatory compliance in their respective fields. We propose and demonstrate AuditNet, the first conversational AI security assistant designed to assist IoT network security experts by providing instant access to security standards, policies, and regulations.},
booktitle = {Adjunct Proceedings of the 26th International Conference on Mobile Human-Computer Interaction},
articleno = {22},
numpages = {4},
keywords = {Prompt Engineering, Question Answering, Retrieval-Augmented Generation},
location = {Melbourne, VIC, Australia},
series = {MobileHCI '24 Adjunct}
}

@inproceedings{10.1145/3613905.3650947,
author = {Mahdavi Goloujeh, Atefeh and Sullivan, Anne and Magerko, Brian},
title = {The Social Construction of Generative AI Prompts},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650947},
doi = {10.1145/3613905.3650947},
abstract = {As text-to-image AI tools grow in capability and widespread use, research has focused on studying individualistic user prompt crafting strategies. Recognizing that technologies are socially constructed, this paper examines prompt engineering through a social lens. We propose reframing prompt engineering as a socio-cultural practice shaped by collective knowledge building. Through qualitative analysis of 19 semi-structured interviews with members of the MidJourney community, a text-to-image generative AI tool, we identify four socio-engagement themes: proprietary/solitary, derivative, collaborative, and provocative prompting. These themes reveal a space of social engagement modes based on personal values and motivations from individual exploration to influencing the prompt community and highlight a fine line between being inspired by others’ prompts and maintaining creative ownership. We argue that understanding distinct social engagement preferences can inform the design of AI tools to facilitate transparent prompt reuse mechanisms, integrate collaborative features, or preserve ethical concerns about prompt sharing.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {320},
numpages = {7},
keywords = {Communities of Practice, Generative AI, Prompt Engineering, Text-to-Image},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3644815.3644981,
author = {Rahman, Md Tajmilur and Singh, Rahul and Sultan, Mir Yousuf},
title = {Automating Patch Set Generation from Code Reviews Using Large Language Models},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644981},
doi = {10.1145/3644815.3644981},
abstract = {The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments. Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {273–274},
numpages = {2},
keywords = {large language models, automated code review, software engineering, pull requests, code quality},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/3586182.3616660,
author = {Arawjo, Ian and Vaithilingam, Priyan and Wattenberg, Martin and Glassman, Elena},
title = {ChainForge: An open-source visual programming environment for prompt engineering},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616660},
doi = {10.1145/3586182.3616660},
abstract = {Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model’s strengths and limitations for one’s specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {4},
numpages = {3},
keywords = {language models, prompt engineering, visual programming},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3696630.3728552,
author = {Sun, Tao and Xu, Jian and Li, Yuanpeng and Yan, Zhao and Zhang, Ge and Xie, Lintao and Geng, Lu and Wang, Zheng and Chen, Yueyan and Lin, Qin and Duan, Wenbo and Sui, Kaixin and Zhu, Yuanshuo},
title = {BitsAI-CR: Automated Code Review via LLM in Practice},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728552},
doi = {10.1145/3696630.3728552},
abstract = {Code review remains a critical yet resource-intensive process in software development, particularly challenging in large-scale industrial environments. While Large Language Models (LLMs) show promise for automating code review, existing solutions face significant limitations in precision and practicality. This paper presents BitsAI-CR, an innovative framework that enhances code review through a two-stage approach combining RuleChecker for initial issue detection and ReviewFilter for precision verification. The system is built upon a comprehensive taxonomy of review rules and implements a data flywheel mechanism that enables continuous performance improvement through structured feedback and evaluation metrics. Our approach introduces an Outdated Rate metric that can reflect developers' actual adoption of review comments, enabling automated evaluation and systematic optimization at scale. Empirical evaluation demonstrates BitsAI-CR's effectiveness, achieving 75.0% precision in review comment generation. For the Go language which has predominant usage at ByteDance, we maintain an Outdated Rate of 26.7%. The system has been successfully deployed at ByteDance, serving over 12,000 Weekly Active Users (WAU). Our work provides valuable insights into the practical application of automated code review and offers a blueprint for organizations seeking to implement automated code reviews at scale.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {274–285},
numpages = {12},
keywords = {code review, large language model, data flywheel},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3677333.3678160,
author = {Alsofyani, May and Wang, Liqiang},
title = {Detecting Data Races in OpenMP with Deep Learning and Large Language Models},
year = {2024},
isbn = {9798400718021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677333.3678160},
doi = {10.1145/3677333.3678160},
abstract = {Transformer-based neural network models are increasingly employed to handle software engineering issues, such as bug localization and program repair. These models, equipped with a self-attention mechanism, excel at understanding source code context and semantics. Recently, large language models (LLMs) have emerged as a promising alternative for analyzing and understanding code structure. In this paper, we propose two novel methods for detecting data race bugs in OpenMP programs. The first method is based on a transformer encoder trained from scratch. The second method leverages LLMs, specifically extending GPT-4 Turbo through the use of prompt engineering and fine-tuning techniques. For training and testing our approach, we utilized two datasets comprising different OpenMP directives. Our experiments show that the transformer encoder achieves competitive accuracy compared to LLMs, whether through fine-tuning or prompt engineering techniques. This performance may be attributed to the complexity of many OpenMP directives and the limited availability of labeled datasets.},
booktitle = {Workshop Proceedings of the 53rd International Conference on Parallel Processing},
pages = {96–103},
numpages = {8},
keywords = {CodeBERTa, GPT-4 Turbo, OpenMP, bug detection, data race, large language model, race condition, transformer encoder},
location = {Gotland, Sweden},
series = {ICPP Workshops '24}
}

@inproceedings{10.1145/3626252.3630874,
author = {Shen, Yiyin and Ai, Xinyi and Soosai Raj, Adalbert Gerald and Leo John, Rogers Jeffrey and Syamkumar, Meenakshi},
title = {Implications of ChatGPT for Data Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630874},
doi = {10.1145/3626252.3630874},
abstract = {ChatGPT is a conversational AI platform that can produce code to solve problems when provided with a natural language prompt. Prior work on similar AI models has shown that they perform well on typical intro-level Computer Science problems. However, little is known about the performance of such tools on Data Science (DS) problems. In this work, we assess the performance of ChatGPT on assignments from three DS courses with varying difficulty levels. First, we apply the raw assignment prompts provided to the students and find that ChatGPT performs well on assignments with dataset(s) descriptions and progressive question prompts, which divide the programming requirements into sub-problems. Then, we perform prompt engineering on the assignments for which ChatGPT had low performance. We find that the following prompt engineering techniques significantly increased ChatGPT's performance: breaking down abstract questions into steps, breaking down steps into multiple prompts, providing descriptions of the dataset(s), including algorithmic details, adding specific instructions to entice specific actions, and removing extraneous information. Finally, we discuss how our findings suggest potential changes to curriculum design of DS courses.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1230–1236},
numpages = {7},
keywords = {data science education, large language models, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3691620.3695330,
author = {Ouedraogo, Wendkuuni C. and Kabore, Kader and Tian, Haoye and Song, Yewei and Koyuncu, Anil and Klein, Jacques and Lo, David and Bissyande, Tegawende F.},
title = {LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695330},
doi = {10.1145/3691620.3695330},
abstract = {Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2464–2465},
numpages = {2},
keywords = {automatic test generation, unit tests, large language models, prompt engineering, empirical evaluation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3696630.3728609,
author = {He, Xuan and Li, Dong and Wen, Hao and Zhu, Yueheng and Liu, Chao and Yan, Meng and Zhang, Hongyu},
title = {CoSEFA: An LLM-Based Programming Assistant for Secure Code Generation via Supervised Co-Decoding},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728609},
doi = {10.1145/3696630.3728609},
abstract = {Programming assistants based on Large Language Models (LLMs) assist developers in code generation and improve efficiency. These LLM-based programming assistants are prone to generate code with vulnerabilities. To mitigate security risk, existing approaches retrain an LLM with crafted data without vulnerabilities. However, retraining requires enormous computational costs and the accessible LLMs' internal representation, often unavailable for LLMs deployed as distributed online services. Our previous work, CoSec, proposed a supervised co-decoding approach using base LLMs alongside a small-scale security-focused LLM fine-tuned with code changes before and after vulnerability fixing.In this paper, we present CoSEFA, a programming assistant with CoSec as its core component for secure code hardening and error fixing. The front-end is a Visual Studio Code IDE extension with a dialogue interaction interface. The back-end handles users' request for code generation, test case generation, etc., providing secure code generation and addressing three types of code generation errors. Experimental results showed that CoSEFA improved the generation security by 9.34% and functionality correctness by 11.2%, compared with the base LLM. Our user study showed that experienced developers rated CoSEFA as more useful than Copilot for security-related programming tasks, highlighting the importance of secure code generation.Demo Source Code: https://github.com/TPA115K31/CoSEFADemo Video: https://youtu.be/LO_JH1eQBbY},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1198–1202},
numpages = {5},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3640471.3680446,
author = {Sasaki, Iori and Arikawa, Masatoshi and Lu, Min and Utsumi, Tomihiro and Sato, Ryo},
title = {Geofence-to-Conversation: Hierarchical Geofencing for Augmenting City Walks with Large Language Models},
year = {2024},
isbn = {9798400705069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640471.3680446},
doi = {10.1145/3640471.3680446},
abstract = {This study presents a geofence-based service architecture for city-wide audio augmented reality, tailored for the era of large language models. Traditional geofencing mechanisms, which monitor user entry to geofences, struggle to provide continuous storytelling in areas with few points of interest, degrading the audio tour experiences for pedestrians. Our proposed geofencing architecture consistently incorporates complex and multilayered city features, enabling seamless audio tour experiences. Furthermore, this paper introduces prompt engineering for generating entertaining guide scripts for large language models, that is, the geofence-to-conversation technique. The mobile application developed for the actual field demonstrates the feasibility of our proposed architecture and highlights future challenges in enhancing users’ interaction with a city.},
booktitle = {Adjunct Proceedings of the 26th International Conference on Mobile Human-Computer Interaction},
articleno = {24},
numpages = {5},
location = {Melbourne, VIC, Australia},
series = {MobileHCI '24 Adjunct}
}

@article{10.1145/3672456,
author = {Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin},
title = {Self-Planning Code Generation with Large Language Models},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672456},
doi = {10.1145/3672456},
abstract = {Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4% in Pass@1 compared to direct code generation, and up to 11.9% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {182},
numpages = {30},
keywords = {Code Generation, Large language models, Planning}
}

@inproceedings{10.1145/3726302.3730366,
author = {Zhu, Fengbin and Ma, Yunshan and Feng, Fuli and Wang, Chao and Luan, Huanbo and Ye, Guangnan and Zhang, Shuo and Mehta, Dhagash and Chen, Pingping and Xiang, Bing and Chua, Tat-Seng},
title = {FinIR: The 2nd Workshop on Financial Information Retrieval in the Era of Generative AI},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730366},
doi = {10.1145/3726302.3730366},
abstract = {Recent advancements in Generative AI, such as Large Language Models (LLMs), have demonstrated remarkable success across various general tasks. Extensive studies have explored leveraging generative models in finance, but significant challenges persist. This half-day workshop explores potential approaches and research directions to address these challenges by equipping generative models with advanced Information Retrieval (IR) models. Specifically, this workshop seeks to provide a platform for discussing innovative ideas that facilitate the advancement of IR technology to enrich generative models in finance from four key perspectives: (i) financial IR techniques (ii) financial IR benchmarking and evaluation (iii) financial systems and agents/assistants (iv) and trustworthiness, privacy and security when applying financial IR and generative models. This workshop aims to deepen understanding, accelerate progress, and support the advancement of IR technology to enhance generative models to address financial challenges.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4184–4187},
numpages = {4},
keywords = {bing xiang, chao wang, dhagash mehta, fengbin zhu, fuli feng, guangnan ye, huanbo luan, pingping chen, shuo zhang, tat-seng chua, yunshan ma},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3599957.3606244,
author = {Lee, Eun-young and il, Ngagaba Gogo Dae and An, Gi-hong and Lee, Sungchul and Lim, Kiho},
title = {ChatGPT-Based Debate Game Application Utilizing Prompt Engineering},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606244},
doi = {10.1145/3599957.3606244},
abstract = {This paper1 focuses on the implementation of a debate game using ChatGPT, aiming to investigate the feasibility of incorporating large language models into the educational domain through prompt engineering. The study explores strategies to elicit desired outputs from the GPT model by employing the prompt engineering methodology, as provided by Microsoft.Specifically, the game implementation involves the customization of ChatGPT's responses to facilitate a natural progression of debates, varying levels of difficulty, and an evaluation system for assessing the quality of discourse. By leveraging the prompt engineering methodology, we demonstrate that providing specific instructions or case-based prompts improves the accuracy and relevance of ChatGPT's answers. The developed application targets teenagers, enabling them to engage in real-time debates with ChatGPT and enhance their literacy skills. Furthermore, the game fosters the development of logical reasoning, persuasive abilities, effective expression, active participation, and attentive listening while expressing personal opinions, ultimately fostering a sense of accomplishment. Moreover, through debate evaluation and personalized advice, ChatGPT is expected to recognize and address its shortcomings, thereby continuously improving its conversational capabilities.Overall, this research contributes to the understanding of how large language models can be harnessed in educational settings and underscores the potential benefits of prompt engineering techniques in optimizing the outputs of such models.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {29},
numpages = {6},
keywords = {ChatGPT, Large Language Model, Prompt Engineering},
location = {Gdansk, Poland},
series = {RACS '23}
}

@inproceedings{10.1145/3641555.3705080,
author = {Morales, Jamie and Raman, Preeti},
title = {Prompt-Engineering Strategies for Minimizing Bias in Large Language Model Outputs: Applications in Computing Education},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705080},
doi = {10.1145/3641555.3705080},
abstract = {As large language models (LLMs) increasingly permeate educational applications, concerns about the perpetuation of bias persist. We present our preliminary work on developing prompt-engineering strategies to mitigate bias in content generated by LLMs in computer science (CS) education. This work investigates both empirical insights into fairness-aware prompt formulation and actionable takeaways for educators. We focus on an initial list of prompting strategies for mitigating bias and explore their impact on educational content generation. Recent research has shown the efficacy of prompt-base debiasing [1] as well as the potential disadvantages of using prompts that have not been mitigated for bias, from user dissatisfaction [2] to unsafe outputs [5, 6]. Additionally, a growing body of empirical work points to the idea that certain properties of in-context examples such as flow [7], illustration [3], and order [4] could either improve or derail LLM performance. Our study leverages these findings in the context of generating educational content. The goal is to promote fairness-aware approaches which can be applied to the automated generation of learning materials and the development of LLM-based educational tools. This work also contributes practical insights on prompt-engineering to the evolving curriculum of Ethics in Artificial Intelligence (AI).},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1743},
numpages = {1},
keywords = {bias, education, ethics, generative ai, in-context examples, language model, language technology, llm, nlp, prompt-engineering},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701910,
author = {Gonzalez-Maldonado, David and Liu, Jonathan and Franklin, Diana},
title = {Evaluating GPT for use in K-12 Block Based CS Instruction Using a Transpiler and Prompt Engineering},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701910},
doi = {10.1145/3641554.3701910},
abstract = {Though the increased availability of Large Language Models (LLMs) presents significant potential for change in the way students learn to program, the text-based nature of the available tools currently preclude block-based languages from much of that innovation. In an attempt to remedy this, we identify the strengths and weaknesses of using a transpiler to leverage the existing learning in commercially available LLMs and Scratch, a visual block-based programming language.Using only prompt engineering, we evaluate an LLM's performance on two common classroom tasks in a Scratch curriculum. We evaluate the LLM's ability to: 1) Create project solutions that compile and satisfy project requirements and 2) Analyze student projects' completion of project requirements using natural language. In both cases, we find results indicating that prompt-engineering alone is insufficient to reliably produce high-quality results. For projects of medium complexity, the LLM-generated solutions consistently failed to follow correct syntax or, in the few instances with correct syntax, produce correct solutions. When used for auto-grading, we found a correlation between scores assigned by the official Scratch Encore autograder and those generated by the LLM, nevertheless the discrepancies between the 'real' scores and the scores assigned by the LLM remained too great for the tool to be reliable in a classroom setting.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {388–394},
numpages = {7},
keywords = {block based programming, generative ai, k-12, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3652620.3687778,
author = {Charles, Joel and Michael, Judith and Netz, Lukas and Rumpe, Bernhard},
title = {Teaching Model-Driven Low-Code Development Platforms},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687778},
doi = {10.1145/3652620.3687778},
abstract = {We present and evaluate a method called grammar masking, which is used to guide large language models (LLMs) toward producing syntactically correct models for a given context-free grammar. Prompt engineering methods such as few-shot learning or priming can be used to improve the chances of an LLM producing correct syntax, but the more complex the grammar, the more time-consuming and less promising these methods become. Previous work is focused primarily on the usage of either language model training or prompt engineering. In this work, a method is presented that restricts the output to a given grammar using constrained decoding to ensure the output adheres to a valid syntax. We use several domain-specific languages (DSLs) built with MontiCore and task multiple LLMs to produce models with and without constrained decoding. A corresponding parser is used to confirm the syntactic correctness of each model. We show that grammar masking can dramatically improve the modeling capabilities of several LLMs, reducing the need for well-refined prompting while increasing the chance of producing correct models.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {570–577},
numpages = {8},
keywords = {LLM, MDSE, guidance, CFG, constrained decoding},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3644815.3644956,
author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Liu, Yue and Xing, Zhenchang and Whittle, Jon},
title = {A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644956},
doi = {10.1145/3644815.3644956},
abstract = {Large language model (LLM) based chatbots, such as ChatGPT, have attracted huge interest in foundation models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. However, the architecture design of foundation model based systems has not yet been systematically explored. There is limited understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and system design options. Our taxonomy comprises three categories: the pretraining and adaptation of foundation models, the architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy can serve as concrete guidance for designing foundation model based systems.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {1–6},
numpages = {6},
keywords = {software architecture, foundation model, responsible AI, large language model, LLM, taxonomy, generative AI},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@article{10.1145/3660791,
author = {Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.},
title = {Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660791},
doi = {10.1145/3660791},
abstract = {Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program’s intent. However, there is typically no guarantee that a program’s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The “emergent abilities” of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {84},
numpages = {24},
keywords = {Formal Specifications, Large Language Models, Postconditions}
}

@inproceedings{10.1145/3724363.3729106,
author = {Alpizar-Chacon, Isaac and Keuning, Hieke},
title = {Student's Use of Generative AI as a Support Tool in an Advanced Web Development Course},
year = {2025},
isbn = {9798400715679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724363.3729106},
doi = {10.1145/3724363.3729106},
abstract = {Various studies have studied the impact of Generative AI on Computing Education. However, they have focused on the implications for novice programmers. In this experience report, we analyze the use of GenAI as a support tool for learning, creativity, and productivity in a web development course for undergraduate students with extensive programming experience. We collected diverse data (assignments, reflections, logs, and a survey) and found that students used GenAI on different tasks (code generation, idea generation, etc.) with a reported increase in learning and productivity. However, they are concerned about over-reliance and incorrect solutions and want more training in prompting strategies.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {312–318},
numpages = {7},
keywords = {generative ai, llms, programming education, web development},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@inproceedings{10.1145/3707640.3731929,
author = {Sekwenz, Marie-Therese and Gsenger, Rita and Stocker, Volker and G\"{o}rnemann, Esther and Talypova, Dinara and Parkin, Simon and Greminger, Lea and Smaragdakis, Georgios},
title = {Can't LLMs do that? Supporting Third-Party Audits under the DSA: Exploring Large Language Models for Systemic Risk Evaluation of the Digital Services Act in an Interdisciplinary Setting},
year = {2025},
isbn = {9798400713972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707640.3731929},
doi = {10.1145/3707640.3731929},
abstract = {This paper investigates the feasibility and potential role of using Large Language Models (LLMs) to support systemic risk audits under the European Union’s Digital Services Act (DSA). It examines how automated tools can enhance the work of DSA auditors and other ecosystem actors by enabling scalable, explainable, and legally grounded content analysis. An interdisciplinary expert workshop with twelve participants from legal, technical, and social science backgrounds explored prompting strategies for LLM-assisted auditing. Thematic analysis of the sessions identified key challenges and design considerations, including prompt engineering, model interpretability, legal alignment, and user empowerment. Findings highlight the potential of LLMs to improve annotation workflows and expand audit scale, while underscoring the continued importance of human oversight, iterative testing, and cross-disciplinary collaboration. This study offers practical insights for integrating AI tools into auditing processes and contributes to emerging methodologies for operationalizing systemic risk evaluations under the DSA.},
booktitle = {Adjunct Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
articleno = {15},
numpages = {12},
keywords = {Large Language Models, Digital Services Act, Online Platform Auditing, Systemic Risk, Content Moderation, Human-AI Collaboration.},
location = {
},
series = {CHIWORK '25 Adjunct}
}

@inproceedings{10.1145/3681763.3698478,
author = {Vatsavai, Ranga Raju},
title = {Geospatial Foundation Models: Recent Advances and Applications},
year = {2024},
isbn = {9798400711435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681763.3698478},
doi = {10.1145/3681763.3698478},
abstract = {Foundation models are deep learning models trained on massive datasets. Recent advancements have made them capable of performing a wide range of general tasks, including language processing, summarization, code generation, problem-solving, and reasoning. Geospatial foundation models are specifically trained on large-scale geospatial and temporal data. While the capabilities of generalpurpose foundation models have been demonstrated through numerous popular applications, such as natural language generation, question answering, and text summarization, the applications of geospatial foundation models are still in the exploratory stage. In this article, we summarize recent advancements in geospatial foundation models and describe various applications.},
booktitle = {Proceedings of the 12th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {30–33},
numpages = {4},
keywords = {Foundation Models, Geospatial AI, Geospatial Applications},
location = {Atlanta, GA, USA},
series = {BigSpatial '24}
}

@inproceedings{10.1145/3649217.3653594,
author = {Azaiz, Imen and Kiesler, Natalie and Strickroth, Sven},
title = {Feedback-Generation for Programming Exercises With GPT-4},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653594},
doi = {10.1145/3649217.3653594},
abstract = {Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {31–37},
numpages = {7},
keywords = {GPT-4 turbo, LLMs, assessment, benchmarking, formative feedback, introductory programming, large language models, personalized feedback},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3723178.3723244,
author = {Mridul Provakar, Mondol and Hashi, Emrana Kabir},
title = {Exploring the Effectiveness of Large Language Models in Financial Question Answering},
year = {2025},
isbn = {9798400713828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723178.3723244},
doi = {10.1145/3723178.3723244},
abstract = {The financial sector is undergoing a profound transformation as AI technologies, particularly large language models (LLMs), revolutionize financial analysis through efficient and accurate natural language processing (NLP). This study investigates the efficacy of LLMs in addressing financial question-answering tasks, focusing specifically on two state-of-the-art models: Llama2-7b by Meta and Gemma-7b by Google. Despite their established prowess in general NLP tasks, their suitability for domain-specific applications, such as financial question answering, necessitates further exploration. Employing a comprehensive evaluation approach encompassing zero-shot prompt engineering, few-shot prompt engineering, and supervised fine-tuning methodologies, this study assesses the performance of Llama2 and Gemma using key metrics, including ROUGE-L, cosine similarity, and human evaluation. The preliminary findings reveal significant distinctions between the two models. Llama2 demonstrates a higher frequency of correct answers, but it is prone to hallucinations, often producing incorrect or incomplete information. In contrast, Gemma’s performance is notably inferior, struggling to respond accurately to most queries. These observations highlight the need for continued research to enhance LLMs’ ability to answer financial questions, while this study offers key insights into their strengths and weaknesses in managing financial inquiries.},
booktitle = {Proceedings of the 3rd International Conference on Computing Advancements},
pages = {498–505},
numpages = {8},
keywords = {Financial Question Answering, Large Language Models, Llama-2, Gemma, Prompt Engineering, Fine-Tuning, Low-Rank-Adaptation},
location = {
},
series = {ICCA '24}
}

@inproceedings{10.1145/3726302.3730346,
author = {Joho, Hideo and Jose, Joemon M},
title = {An Instruction-Response Perspective on Large Language Models in Information Retrieval Tasks},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730346},
doi = {10.1145/3726302.3730346},
abstract = {The increasing use of retrieval-augmented applications, where large language models (LLMs) are instructed to generate queries, assess relevance, and synthesise responses, has introduced new challenges in Information Retrieval (IR). The lack of transparency in LLMs means that even subtle variations in instructions can significantly impact the quality, consistency, and reliability of their responses. To address this issue, we propose Instruction-Response Study, an experimental framework for systematically analysing how task instructions influence LLM-generated responses in IR tasks. This paper presents the core components of the framework and demonstrates its utility through four case studies, examining 1) the effect of IR tasks on query formulation, 2) the impact of topic information size on retrieval effectiveness, 3) the reproducibility of LLM-generated queries, and 4) the role of meta-instructions in diversifying instruction design. The findings highlight how the proposed framework enables controlled experimentation on instruction design and its effects, offering a foundation for optimising prompt engineering and enhancing retrieval-augmented applications.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3843–3852},
numpages = {10},
keywords = {explainable ai, information retrieval tasks, instruction-response study, large language models},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3715336.3735823,
author = {Naik, Suchismita and Toombs, Austin L. and Snellinger, Amanda, Ph.D. and Saponas, Scott and Hall, Amanda K},
title = {Designing with Multi-Agent Generative AI: Insights from Industry Early Adopters},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735823},
doi = {10.1145/3715336.3735823},
abstract = {In this paper we present the results of our investigation into how employees at Microsoft, as early adopters of multi-agent generative AI systems, navigate the complexities of designing, testing, and deploying these technologies to extend the organization’s product ecosystem. Through interviews with thirteen developers, we uncover the challenges, use cases, and lessons when designing with and for multi-agent AI frameworks. Our analysis reveals how participants leveraged this advanced emerging technology to enhance collaboration, productivity, customer support, creative processes, and security. Key design strategies include managing agent complexity, fostering transparency, and balancing agent autonomy with human oversight, essential considerations for human-agent interaction design. We provide empirical insights into the capabilities and limitations of multi-agent systems in real-world contexts, informing the design of future AI systems that align AI capabilities with human-centered design. By emphasizing first-person experiences and strategies, our research bridges human needs and AI potentials, advancing both the practice and theory of designing with and for AI systems.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {1961–1972},
numpages = {12},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3722212.3725124,
author = {Cao, Jeffery and Flokas, Lampros and Xu, Yujian and Wu, Eugene and Chu, Xu and Yu, Cong},
title = {Prompt Editor: A Taxonomy-driven System for Guided LLM Prompt Development in Enterprise Settings},
year = {2025},
isbn = {9798400715648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722212.3725124},
doi = {10.1145/3722212.3725124},
abstract = {Large Language Models (LLMs) are increasingly integrated into enterprise platforms, requiring business users to write prompts despite the lack of prompt engineering expertise. Although these users possess domain knowledge, they struggle to translate their intentions into effective prompts and often overlook established practices such as including few-shot examples. We present Prompt Editor, a human-in-the-loop system that assists users in writing high-quality prompts by leveraging an organization's existing corpus of prompts to both initialize and refine user-created prompts. At its core, Prompt Editor leverages a taxonomy of prompt segment types learned from an organization's existing collection of prompts. This supports the segmentation of prompts and feedback generation process. We demonstrate Prompt Editor with a hypothetical LLM-based enterprise platform that extracts structured data from unstructured text. Attendees can interact with the demo to experience first-hand how Prompt Editor supports prompt writing.},
booktitle = {Companion of the 2025 International Conference on Management of Data},
pages = {59–62},
numpages = {4},
keywords = {large language models, prompt engineering, taxonomy},
location = {Berlin, Germany},
series = {SIGMOD/PODS '25}
}

@article{10.1145/3722108,
author = {Tony, Catherine and D\'{\i}az Ferreyra, Nicol\'{a}s E. and Mutas, Markus and Dhif, Salem and Scandariato, Riccardo},
title = {Prompting Techniques for Secure Code Generation: A Systematic Investigation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3722108},
doi = {10.1145/3722108},
abstract = {Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. Method: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {LLMs, secure code generation, prompt engineering}
}

@inproceedings{10.1145/3701716.3717816,
author = {Graux, Damien and Montella, Sebastien and Jabeen, Hajira and Gardent, Claire and Pan, Jeff Z.},
title = {[PromptEng] Second International Workshop on Prompt Engineering for Pre-Trained Language Models},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717816},
doi = {10.1145/3701716.3717816},
abstract = {The recent achievements and availability of Large Language Models have paved the road to a new range of applications and use-cases. Pre-trained language models are now being involved at-scale in many fields where they were until now absent from. More specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. Unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. Based on the success of the past edition, this second international workshop on prompt engineering gathers practitioners (both from Academia and Industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient and safe prompts.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1589–1590},
numpages = {2},
keywords = {best practices, collective task, llm, prompt engineering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3643787.3648036,
author = {Ingemann Tuffveson Jensen, Rasmus and Tawosi, Vali and Alamir, Salwa},
title = {Software Vulnerability and Functionality Assessment using Large Language Models},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648036},
doi = {10.1145/3643787.3648036},
abstract = {While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final "approve or reject" recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {25–28},
numpages = {4},
keywords = {software security, functional validation, large language models},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3689092.3689403,
author = {Xu, Yaoxun and Zhou, Yixuan and Cai, Yunrui and Xie, Jingran and Ye, Runchuan and Wu, Zhiyong},
title = {Multimodal Emotion Captioning Using Large Language Model with Prompt Engineering},
year = {2024},
isbn = {9798400712036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689092.3689403},
doi = {10.1145/3689092.3689403},
abstract = {This paper addresses the challenges in MER 2024 by focusing on the Open Vocabulary (OV) task, which extends beyond traditional fixed label space for multimodal emotion recognition. The study emphasizes the use of Large Language Models (LLMs) to interpret and extract emotional information from multimodal inputs, complemented by speech transcription, speech emotion description, and video clues. The paper explores the integration of these features into a prompt fed into a pre-trained LLaMA3-8B model, utilizing prompt engineering to achieve satisfactory results without fine-tuning. This approach bridges the gap between speech, video and text data, leveraging the full potential of LLMs for open-ended emotion recognition tasks and introducing a solution to the field.},
booktitle = {Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing},
pages = {104–109},
numpages = {6},
keywords = {emotion caption, large language model, multimodal emotion recognition, prompt engineering},
location = {Melbourne VIC, Australia},
series = {MRAC '24}
}

@inproceedings{10.1145/3613904.3642016,
author = {Arawjo, Ian and Swoopes, Chelse and Vaithilingam, Priyan and Wattenberg, Martin and Glassman, Elena L.},
title = {ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642016},
doi = {10.1145/3613904.3642016},
abstract = {Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {304},
numpages = {18},
keywords = {auditing, language models, prompt engineering, toolkits, visual programming environments},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3735636,
author = {Yang, Guang and Zhou, Yu and Cheng, Wei and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry Yue and Liu, Ke and Zhou, Xin and Lo, David and Chen, Taolue},
title = {Less is More: DocString Compression in Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3735636},
doi = {10.1145/3735636},
abstract = {The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25–40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {DocString Compression, Code Generation, Large Language Model}
}

@inproceedings{10.1145/3639478.3641226,
author = {Ibrahimzada, Ali Reza},
title = {Program Decomposition and Translation with Static Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641226},
doi = {10.1145/3639478.3641226},
abstract = {The rising popularity of Large Language Models (LLMs) has motivated exploring their use in code-related tasks. Code LLMs with more than millions of parameters are trained on a massive amount of code in different Programming Languages (PLs). Such models are used for automating various Software Engineering (SE) tasks using prompt engineering. However, given the very large size of industry-scale project files, a major issue of these LLMs is their limited context window size, motivating the question of "Can these LLMs process very large files and can we effectively perform prompt engineering?". Code translation aims to convert source code from one PL to another. In this work, we assess the effect of method-level program decomposition on context window of LLMs and investigate how this approach can enable translation of very large files which originally could not be done due to out-of-context issue. Our observations from 20 well-known java projects and approximately 60K methods suggest that method-level program decomposition significantly improves the limited context window problem of LLMs by 99.5%. Furthermore, our empirical analysis indicate that with method-level decomposition, each input fragment on average only consumes 5% of the context window, leaving more context space for prompt engineering and the output. Finally, we investigate the effectiveness of a Call Graph (CG) approach for translating very large files when doing method-level program decomposition.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {453–455},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3717867.3717913,
author = {Wu, Patrick Y.},
title = {Using Semantically Unrelated and Opposite Terms for In-Context Learning: A Case Study in Identifying Political Aversion in Tweets},
year = {2025},
isbn = {9798400714832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717867.3717913},
doi = {10.1145/3717867.3717913},
abstract = {We investigate how semantic priors embedded in generative large language models (LLMs) interact with concept definitions in prompts, using political aversion detection as a case study. Through systematic variations in the wording of the definition of political aversion—replacing key words with opposite terms, semantically unrelated terms, or deliberately nonsensical strings—we examine how models process and apply these manipulated definitions in classification tasks. Our results show that certain LLMs maintain consistent performance across different prompt configurations, regardless of which terms are used or whether examples are included. Strong classification performances even with nonsensical definitions suggest these models may sometimes rely more on patterns in target content than definitions given in prompts. These findings challenge conventional assumptions about prompt engineering and raise important questions about how LLMs utilize information in prompts for classification decisions, while underscoring the need for careful validation when applying these methods to social and political science measurement tasks.},
booktitle = {Proceedings of the 17th ACM Web Science Conference 2025},
pages = {528–533},
numpages = {6},
keywords = {large language models, in-context learning, social media analysis},
location = {
},
series = {Websci '25}
}

@inproceedings{10.1145/3678717.3695760,
author = {Zhang, Qianheng and Gao, Song},
title = {Automating Geospatial Analysis Workflows Using ChatGPT-4},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678717.3695760},
doi = {10.1145/3678717.3695760},
abstract = {The field of Geospatial Artificial Intelligence (GeoAI) has significantly impacted domain applications such as urban analytics, environmental monitoring, and disaster management. While powerful geoprocessing tools in geographic information systems (GIS) like ArcGIS Pro are available, automating these workflows with Python scripting using AI chatbots remains a challenge, especially for non-expert users. This study investigates whether ChatGPT-4 can automate GIS workflows by generating ArcPy functions based on structured instructions. We tested prompt engineering's ability on helping large language models (LLMs) understand spatial data and GIS workflows. The overall task success rate reaches 80.5%. It is a valid and easy to implement approach for domain scientists who want to use ArcPy to automate their workflows.},
booktitle = {Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
pages = {715–716},
numpages = {2},
keywords = {GIS, GeoAI, LLM, Prompt engineering, automate workflow},
location = {Atlanta, GA, USA},
series = {SIGSPATIAL '24}
}

@inproceedings{10.1145/3639701.3656308,
author = {Alonso del Barrio, David and Tiel, Max and Gatica-Perez, Daniel},
title = {Human Interest or Conflict? Leveraging LLMs for Automated Framing Analysis in TV Shows},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3656308},
doi = {10.1145/3639701.3656308},
abstract = {In the current media landscape, understanding the framing of information is crucial for critical consumption and informed decision making. Framing analysis is a valuable tool for identifying the underlying perspectives used to present information, and has been applied to a variety of media formats, including television programs. However, manual analysis of framing can be time-consuming and labor-intensive. This is where large language models (LLMs) can play a key role. In this paper, we propose a novel approach to use prompt-engineering to identify the framing of spoken content in television programs. Our findings indicate that prompt-engineering LLMs can be used as a support tool to identify frames, with agreement rates between human and machine reaching up to 43%. As LLMs are still under development, we believe that our approach has the potential to be refined and further improved. The potential of this technology for interactive media applications is vast, including the development of support tools for journalists, educational resources for students of journalism learning about framing and related concepts, and interactive media experiences for audiences.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {157–167},
numpages = {11},
keywords = {LLMs, TV, framing analysis, media, prompt-engineering},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@article{10.1145/3697012,
author = {Gu, Xiaodong and Chen, Meng and Lin, Yalan and Hu, Yuhan and Zhang, Hongyu and Wan, Chengcheng and Wei, Zhao and Xu, Yong and Wang, Juhong},
title = {On the Effectiveness of Large Language Models in Domain-Specific Code Generation},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697012},
doi = {10.1145/3697012},
abstract = {Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., Web, game, and math). In this article, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {78},
numpages = {22},
keywords = {large language models, code generation, domain-specific program generation}
}

@inproceedings{10.1145/3586182.3616663,
author = {Zou, Ruishi and Ye, Zi and Ye, Chen},
title = {iTutor: A Generative Tutorial System for Teaching the Elders to Use Smartphone Applications},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616663},
doi = {10.1145/3586182.3616663},
abstract = {We present iTutor, a generative tutorial system for promoting smartphone use proficiency among elders. iTutor is unique because it can dynamically generate tutorials based on current operation goals and UI context, which we achieved through leveraging prompt engineering to large language models (LLMs). Our evaluations showed potential for this approach, as we yielded 78.6% accuracy in the instruction generation process. We conclude by providing the roadmap for further development.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {7},
numpages = {3},
keywords = {Accessibility, Prompt Engineering, Tutorial Interface},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3698205.3733924,
author = {Wang, Deliang and Yang, Chao and Chen, Gaowei},
title = {Using LoRA to Fine-tune Large Language Models for Analyzing Collaborative Argumentation in Classrooms},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3733924},
doi = {10.1145/3698205.3733924},
abstract = {Artificial intelligence (AI) has been employed to provide automated analysis of collaborative argumentation due to its importance. However, traditional deep learning models face challenges with generalizability to other dimensions and contexts. Existing studies on large language models (LLMs) for classroom dialogue primarily rely on prompt engineering techniques because of the high costs associated with fully fine-tuning LLMs. This approach results in limited performance, indicating a need for improvement. To address these issues, this study proposes the use of parameter-efficient fine-tuning (PEFT) techniques to optimize the performance of LLMs in analyzing classroom collaborative argumentation. Specifically, we utilized Low-Rank Adaptation (LoRA), a well-known PEFT technique, to fine-tune two state-of-the-art LLMs, Llama-3.2-3B and Gemma-2-9B. The results demonstrate that, compared to fully fine-tuning BERT and RoBERTa, using LoRA for PEFT of Llama-3.2-3B and Gemma-2-9B achieves superior performance in analyzing argument moves within collaborative argumentation. We conclude that PEFT techniques provide a promising direction for classroom dialogue analysis.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {207–211},
numpages = {5},
keywords = {classroom collaborative argumentation, large language models, parameter-efficient fine-tuning},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3674399.3674482,
author = {Hu, Yuchen and Xu, Ke and Sun, Jialin and Fang, Xinwei and Shan, Weiwei and Wang, Xi and Jiang, Zhe},
title = {Make Each Iteration Count},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674399.3674482},
doi = {10.1145/3674399.3674482},
abstract = {Large Language Models (LLMs) is widely used for code debugging (e.g., C and Python) but face limitations in debugging Register Transfer Level (RTL) code largely due to data scarcity. This paper presents Make Each Iteration Count (MEIC), a novel framework for RTL debugging. Unlike traditional approaches heavily relying on prompt engineering, model tuning, and model training, MEIC employs an iterative process with LLM to address syntax and function errors efficiently. We also introduce an open-source dataset with 178 RTL errors for evaluation. Results demonstrate a 93% fix rate for syntax errors and a 78% fix rate for function errors.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024},
pages = {236–238},
numpages = {3},
location = {Changsha, China},
series = {ACM-TURC '24}
}

@inproceedings{10.1145/3643916.3644409,
author = {Fagadau, Ionut Daniel and Mariani, Leonardo and Micucci, Daniela and Riganelli, Oliviero},
title = {Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644409},
doi = {10.1145/3643916.3644409},
abstract = {Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering.In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {24–34},
numpages = {11},
keywords = {prompt engineering, code generation, copilot},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3706598.3713888,
author = {Kim, Minseo and Kim, Taemin and Vo, Thu Hoang Anh and Jung, Yugyeong and Lee, Uichin},
title = {Exploring Modular Prompt Design for Emotion and Mental Health Recognition},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713888},
doi = {10.1145/3706598.3713888},
abstract = {Recent advances in large language models (LLM) offered human-like capabilities for comprehending emotion and mental states. Prior studies explored diverse prompt engineering techniques for improving classification performance, but there is a lack of analysis of prompt design space and the impact of each component. To bridge this gap, we conduct a qualitative thematic analysis of existing prompts for emotion and mental health classification tasks to define the key components for prompt design space. We then evaluate the impact of major prompt components, such as persona and task instruction, on classification performance by using four LLM models and five datasets. Modular prompt design offers new insights into examining performance variability as well as promoting transparency and reproducibility in LLM-based tasks within health and well-being intervention systems.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1181},
numpages = {18},
keywords = {Large language model, prompt engineering, emotion, mental health},
location = {
},
series = {CHI '25}
}

@inproceedings{10.5555/3721488.3721641,
author = {Abbo, Giulio Antonio and Belpaeme, Tony},
title = {I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots},
year = {2025},
publisher = {IEEE Press},
abstract = {In the rapidly evolving landscape of human-robot interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents a ready-to-use implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4o mini) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, ensures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. The system can be customised and is available as a stand-alone application, a Furhat robot implementation, and a ROS2 package.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1176–1180},
numpages = {5},
keywords = {conversation, dialogue, hri, large language model, prompt engineering, ros, vision language model},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3698061.3726916,
author = {Carrera, Dashiel and Zhao, Zixin and Ajin Thomas, Ashish and Wigdor, Daniel},
title = {Nabokov's Cards: An AI Assisted Prewriting System to Support Bottom-Up Creative Writing},
year = {2025},
isbn = {9798400712890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698061.3726916},
doi = {10.1145/3698061.3726916},
abstract = {We introduce Nabokov’s Cards, a creativity support tool that uses Large Language Models (LLMs) to support prewriting. Inspired by the writing process of Vladimir Nabokov, Nabokov’s Cards enables prewriting ideation by providing users with an interface to write idea fragments on notecards and combine them into new sentences or concepts using an LLM. We evaluated Nabokov’s Cards through a one-week user study with professional creative writers (n=13) to explore writers’ prewriting processes and learn about their usage of the system. Through our interviews, we found that writers characterized prewriting as a long, amorphous process that involved observations of the real world and the accumulation of idea fragments. Writers in our study found that Nabokov’s Cards facilitated prewriting through nonlinear interactions, divergent thinking, play, improvisation, and reflection. It also encouraged innovative approaches among writers that surpassed the clich\'{e}s and redundancy often found within AI generated text today. We note how future AI co-writing systems may benefit from designs that facilitate prompt engineering and modular thinking.},
booktitle = {Proceedings of the 2025 Conference on Creativity and Cognition},
pages = {546–559},
numpages = {14},
keywords = {LLMs, writing assistance, interface metaphors, design, AI writing},
location = {
},
series = {C&amp;C '25}
}

@article{10.1145/3747588,
author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
title = {A Survey on Large Language Models for Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3747588},
doi = {10.1145/3747588},
abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and  provide a quantitative and qualitative comparative analysis of experimental results of code LLMs, sourced from their original papers to ensure a fair comparison on the HumanEval, MBPP, and BigCodeBench benchmarks, across various levels of difficulty and types of programming tasks, to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page () to continuously document and disseminate the most recent advances in the field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {Large Language Models, Code Large Language Models, Code Generation}
}

@article{10.1145/3715749,
author = {Wang, Yanlin and Jiang, Tianyue and Liu, Mingwei and Chen, Jiachi and Mao, Mingzhi and Liu, Xilin and Ma, Yuchi and Zheng, Zibin},
title = {Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715749},
doi = {10.1145/3715749},
abstract = {Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy. Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers exhibit differences in coding style. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE032},
numpages = {23},
keywords = {Code generation, Coding style inconsistency, Large language models}
}

@inproceedings{10.1145/3613905.3651096,
author = {Lo, Priscilla Y.},
title = {An Autoethnographic Reflection of Prompting a Custom GPT Based on Oneself},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651096},
doi = {10.1145/3613905.3651096},
abstract = {What if you could have a chat with yourself? OpenAI’s introduction of custom GPTs in November 2023 provides an opportunity for non-technical users to create specialized generative artificial intelligence chatbots. Users can write prompts in plain language rather than code to instruct how the system should behave. What can one learn from using non-technical methods to develop a specific chatbot persona? To explore this, I conducted an autoethnography of my experience developing and interacting with a custom GPT based on myself. My findings include a discussion of my experiences throughout the process, and its impact on my personal introspection and understanding of prompt engineering. I summarize first-hand challenges and insights intended to inspire further discussion on the topic of generative AI and chatbots.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {40},
numpages = {9},
keywords = {autoethnography, chatbot, generative artificial intelligence, large language model, persona},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3641554.3701867,
author = {Yeh, Thomas Y. and Tran, Karena and Gao, Ge and Yu, Tyler and Fong, Wai On and Chen, Tzu-Yi},
title = {Bridging Novice Programmers and LLMs with Interactivity},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701867},
doi = {10.1145/3641554.3701867},
abstract = {While Large Language Models (LLMs) enable experienced programmers to increase their productivity, LLMs' impact on learning and productivity for novices is currently unclear. Recent work showed novice programmers struggle with prompting LLMs for code generation and suggested that the use of LLMs in CS education could exacerbate existing equity issues. Educators are now faced with the difficult question of whether and when to incorporate the use of LLMs into the CS curriculum without adversely impacting student learning and equity. To address these concerns, we study the effects of using an interactive LLM on code generation with novice programmers. We find that using our interactive LLM improves the accuracy of code generation over the baseline LLM. Additionally, after using the interactive LLM, novices write improved prompts even when using the baseline LLM. Based on our findings, we plan to create iGPTs, a set of customized, interactive LLMs spanning CS education learning goals as templates to facilitate LLM integration for improving student learning and retention.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1295–1301},
numpages = {7},
keywords = {cs1, generative ai, llms, novice programmers},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3716167,
author = {Le-Cong, Thanh and Nguyen, Thanh-Dat and Le, Bach and Murray, Toby},
title = {Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716167},
doi = {10.1145/3716167},
abstract = {Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20%-35% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact.In this paper, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations, and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are considered natural, while 20% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing.Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using Large Language Models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Automated Program Repair, Natural Robustness, Code Naturalness, Code Transformations}
}

@inproceedings{10.1145/3701716.3717813,
author = {Atzberger, Daniel and Jobst, Adrian and Tytarenko, Mariia and Scheibel, Willy and D\"{o}llner, J\"{u}rgen and Schreck, Tobias},
title = {Analyzing the Sensitivity of Prompt Engineering Techniques in Natural Language Interfaces for 2.5D Software Visualization},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717813},
doi = {10.1145/3701716.3717813},
abstract = {Natural Language Interfaces (NLIs) backed by Large Language Models (LLMs) are used to interact with visualizations through natural language queries. Using the specific example of 2.5D treemaps, the Delphi tool was recently presented, introducing an interactive 2.5D visualization with an accompanying chat interface, where the LLM can react to user input and adapt the visualization at its own discretion. While Delphi has demonstrated effectiveness, the authors have not included an evaluation of the LLM's performance with respect to its prompt and specific task types. In this study, we systematically evaluate the impact of prompt engineering on Delphi's ability to answer factual questions related to data and visualization. Specifically, we investigate the effect of the Chain-of-Thought prompting technique by employing a questionnaire comprising 40 questions across ten low-level analytic tasks. Our findings aim to refine prompt design methodologies and enhance the usability and effectiveness of NLIs in advanced visualization systems.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1591–1595},
numpages = {5},
keywords = {chain-of-thought technique, chart question answering, natural language interfaces, prompt sensitivity},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3696630.3727236,
author = {Grandel, Skyler and Schmidt, Douglas and Leach, Kevin},
title = {Applying Large Language Models to Enhance the Assessment of Java Programming Assignments},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3727236},
doi = {10.1145/3696630.3727236},
abstract = {The assessment of programming assignments in computer science (CS) education traditionally relies on manual grading, which strives to provide comprehensive feedback on correctness, style, efficiency, and other software quality attributes. As class sizes increase, however, it is hard to provide detailed feedback consistently, especially when multiple assessors are required to handle a larger number of assignment submissions. Large Language Models (LLMs), such as ChatGPT, Claude, and Gemini, offer a promising alternative to help automate this assessment process in a consistent, scalable, and fair manner.This paper explores the efficacy of ChatGPT-4 and other popular LLMs in automating programming assignment assessment. We conduct a series of studies within multiple Java-based CS courses at Vanderbilt University, comparing LLM-generated assessments to those produced by human graders. The analysis focuses on key metrics, such as accuracy, precision, recall, efficiency, and consistency, to identify programming mistakes based on predefined rubrics. Our findings demonstrate that LLMs improve grading objectivity and efficiency with appropriate prompt engineering and feature selection, serving as a valuable complementary tool to human graders in undergraduate and graduate CS education.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {789–799},
numpages = {11},
keywords = {ChatGPT, education, generative AI, large language models, prompt engineering, automated grading},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3652628.3652746,
author = {Wang, Hao and Zeng, Siliang and Xue, Zhenwei and Zhu, Xinglin and Wu, Fei and Wang, Ning},
title = {ProfLLM: A framework for adapting offline large language models to few-shot expert knowledge},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652746},
doi = {10.1145/3652628.3652746},
abstract = {Large language models perform well at common field, but are much less effective in niche academic fields .The cause of this problem is that large language models lack the ability to handle few-shot expert knowledge. To address this issue, we purpose ProfLLM, a framework for adapting offline large language models to few-shot expert. ProfLLM provides expert knowledge processing capabilities for large language models through vector databases and prompt engineering, serializes the Q-A pairs of unstructured specialized knowledge through associated databases and completes parameter persistence by embedding fine-tuning, large language model fine-tuning and transfer learning. Experiments show that while ProfLLM performs on par with current mainstream fine-tuning schemes in handling specialized tasks with few-shot expert knowledge, it significantly outperforms fine-tuning schemes in handling general tasks. ProfLLM can handling few-shot expert knowledge while preserving the general capability of the large language model.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {708–714},
numpages = {7},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.1145/3664646.3664770,
author = {Kulsum, Ummay and Zhu, Haotian and Xu, Bowen and d'Amorim, Marcelo},
title = {A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664770},
doi = {10.1145/3664646.3664770},
abstract = {Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored.                In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches.                 To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {103–111},
numpages = {9},
keywords = {Automated Vulnerability Repair, Large Language Models},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3715727,
author = {Peng, Yun and Wan, Jun and Li, Yichen and Ren, Xiaoxue},
title = {COFFE: A Code Efficiency Benchmark for Code Generation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715727},
doi = {10.1145/3715727},
abstract = {Code generation has largely improved development efficiency in the era of large language models (LLMs). With the ability to follow instructions, current LLMs can be prompted to generate code solutions given detailed descriptions in natural language. Many research efforts are being devoted to improving the correctness of LLM-generated code, and many benchmarks are proposed to evaluate the correctness comprehensively. Despite the focus on correctness, the time efficiency of LLM-generated code solutions is under-explored. Current correctness benchmarks are not suitable for time efficiency evaluation since their test cases cannot well distinguish the time efficiency of different code solutions. Besides, the current execution time measurement is not stable and comprehensive, threatening the validity of the time efficiency evaluation.                To address the challenges in the time efficiency evaluation of code generation, we propose COFFE, a code generation benchmark for evaluating the time efficiency of LLM-generated code solutions. COFFE contains 398 and 358 problems for function-level and file-level code generation, respectively. To improve the distinguishability, we design a novel stressful test case generation approach with contracts and two new formats of test cases to improve the accuracy of generation. For the time evaluation metric, we propose efficienct@k based on CPU instruction count to ensure a stable and solid comparison between different solutions. We evaluate 14 popular LLMs on COFFE and identify four findings. Based on the findings, we draw some implications for LLM researchers and software practitioners to facilitate future research and usage of LLMs in code generation.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE012},
numpages = {24},
keywords = {Benchmark, Code Efficiency, Code Generation, Time}
}

@inproceedings{10.1145/3644815.3644946,
author = {Li, Ziyu and Shin, Donghwan},
title = {Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644946},
doi = {10.1145/3644815.3644946},
abstract = {Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {150–159},
numpages = {10},
keywords = {large language models, software engineering, mutation analysis},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inbook{10.5555/3716662.3716722,
author = {Kay, Jackie and Kasirzadeh, Atoosa and Mohamed, Shakir},
title = {Epistemic Injustice in Generative AI},
year = {2025},
publisher = {AAAI Press},
abstract = {This paper investigates how generative AI can potentially undermine the integrity of collective knowledge and the processes we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinformation, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlighting these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable information ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {684–697},
numpages = {14}
}

@article{10.1145/3672359.3672364,
author = {Schmidt, Douglas C. and Spencer-Smith, Jesse and Fu, Quchen and White, Jules},
title = {Towards a Catalog of Prompt Patterns to Enhance the Discipline of Prompt Engineering},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3672359.3672364},
doi = {10.1145/3672359.3672364},
abstract = {The rapid advent of Large Language Models (LLMs), such as ChatGPT and Claude, is revolutionizing various fields, from education and healthcare to the engineering of reliable software systems. These LLMs operate through "prompts," which are natural language inputs that users employ to query and leverage the models' capabilities. Given the novelty of LLMs, the understanding of how to effectively use prompts remains largely anecdotal, based on isolated use cases. This fragmented approach limits the reliability and utility of LLMs, especially when they are applied in mission-critical software environments. To harness the full potential of LLMs in such crucial contexts, therefore, we need a systematic, disciplined approach to "prompt engineering" that guides interactions with and evaluations of these LLMs.},
journal = {Ada Lett.},
month = jun,
pages = {43–51},
numpages = {9}
}

@article{10.1145/3722231,
author = {Cimino, Gaetano and Deufemia, Vincenzo},
title = {SIGFRID: Unsupervised, Platform-Agnostic Interference Detection in IoT Automation Rules},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3722231},
doi = {10.1145/3722231},
abstract = {Smart home technology has profoundly changed modern living by interconnecting devices, services, dataflows, and user interactions into integrated, automated environments. Homeowners can easily program smart devices using conditional IF-THEN rules, where triggers prompt corresponding actions. However, as smart homes incorporate more multifunctional devices, conflicting trigger-action rules can simultaneously control devices in inconsistent ways, causing unexpected and potentially unsafe interference situations. This article introduces Sigfrid, a novel interference detection approach using scene interaction graphs constructed through Large Language Models (LLMs). To enhance LLM reasoning, we propose a new prompt engineering methodology that integrates automated and manual editing techniques to formulate queries for deriving causal insights in the smart home domain. Interferences are identified through efficient exploration of the graph constructed from the extracted relations. We evaluate Sigfrid on real-world If-This-Then-That (IFTTT) and SmartThings rule sets, demonstrating its superiority over state-of-the-art methods by more than 21% in F1-score.},
journal = {ACM Trans. Internet Things},
month = apr,
articleno = {13},
numpages = {33},
keywords = {IoT, trigger-action platforms, interference detection, behavioral modeling, smart home}
}

@inproceedings{10.1145/3597503.3639184,
author = {Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael},
title = {PyTy: Repairing Static Type Errors in Python},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639184},
doi = {10.1145/3597503.3639184},
abstract = {Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {87},
numpages = {13},
keywords = {automatic program repair, type annotation, transfer learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3696630.3728724,
author = {Robert, John and Olea, Carlos and Hindka, Yash and Brown, Nanette},
title = {Generative AI Detection of Document Incompleteness, Inconsistencies, and Discrepancies},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728724},
doi = {10.1145/3696630.3728724},
abstract = {Software development activities must adhere to industry, national, and/or international regulatory standards to ensure security, safety, and other important properties in high-stakes, regulated domains, such as healthcare, automotive, and aerospace. These software standards are often codified via multiple documents, ranging from specific requirements to general guidance. Software developers must demonstrate—and regulatory authorities must then verify—the completeness and consistency between the software artifacts and the software regulatory documents. Detecting regulatory document incompleteness, inconsistencies, and/or discrepancies (DIID) with respect to software artifacts is important for both developers and regulators because they indicate potential non-conformance and/or software regulatory risk.DIID detection is currently a human-intensive activity where people learn the standards and then review or search software artifacts to find or detect potential issues. This tedious and error-prone manual process is even more challenging with continuous integration and continuous delivery (CI/CD) pipelines that create frequent software updates. In addition, software artifacts (such as requirements or architecture documents) and software standards also change as the software is maintained, requiring ongoing manual reviews and revisions.Generative AI can accelerate DIID detection for different documents and data sources. This paper explores how generative AI, specifically Large Language Models (LLMs), can accelerate DIID detection in software artifacts, particularly when comparing trusted documents like safety standards with software development artifacts. Integrating LLMs into the software development lifecycle (SDLC) to detect DIID can improve specific development activities, as well as enable transformative opportunities by accelerating end-to-end SDLC workflows and regulatory compliance. Current LLM benchmarks and testing frameworks do not sufficiently test for detecting DIID and require extension to adequately and consistently detect DIID across multiple SDLC activities. This position paper describes how LLMs can accelerate software assurance processes and ensure greater compliance with industry standards and government regulatory requirements.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1381–1385},
numpages = {5},
keywords = {generative artificial intelligence (GenAI), large language models (LLMs), software assurance, regulatory compliance},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inbook{10.1145/3677389.3702588,
author = {Keya, Farhana and Jaradeh, Mohamad Yaser and Auer, S\'{o}ren},
title = {Leveraging LLMs for Scientific Abstract Summarization: Unearthing the Essence of Research in a Single Sentence},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702588},
abstract = {There are lots of scientific articles are being published every year, it is increasingly challenging for researchers to maintain oversight and track scientific progress. Meanwhile, Large Language Models (LLMs) have revolutionized natural language processing tasks. This research focuses on generating summaries from research paper abstracts by utilizing LLMs and comprehensively evaluating the performance of the summarization. LLMs offer customizable outputs through Prompt Engineering by leveraging descriptive instructions including instructive examples and injection of context knowledge. We investigate the performance of various prompting techniques for various LLMs using both GPT-4 and human evaluation. For that purpose, we created a comprehensive benchmark dataset for scholarly summarization covering multiple scientific domains. We integrated our approach in the Open Research Knowledge Graph (ORKG) to enable quicker syn- thesis of research findings and trends across multiple studies, facilitating the dissemination of scientific knowledge to policymakers, practitioners, and the public.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {9},
numpages = {7}
}

@article{10.1145/3728947,
author = {Fu, Yingjie and Li, Bozhou and Li, Linyi and Zhang, Wentao and Xie, Tao},
title = {The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-Based Code Generation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728947},
doi = {10.1145/3728947},
abstract = {The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. As an alternative to natural language, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs’ capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities (derived from HumanEval and CodeHunt). The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs’ score decreases by over 60%, indicating that example-based code generation remains challenging for the evaluated LLMs. Notably, the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of the iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements. Furthermore, we find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization. These findings highlight the importance of early prompts during interactions and offer critical insights and implications for enhancing LLM-based code generation.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA070},
numpages = {24},
keywords = {Empirical Study, Example-Based Code Generation, Large Language Models, Multi-Turn Interaction, Prompt Engineering}
}

@inproceedings{10.1145/3701716.3715465,
author = {Zou, Minghui and Guo, Ronghui and Zhang, Sai and Zhang, Xiaowang and Feng, Zhiyong},
title = {TAT: Improving Stance Detection on Social Media through Thought Alignment with LLMs},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715465},
doi = {10.1145/3701716.3715465},
abstract = {Stance detection, which identifies a given statement's stance toward a specific target, plays a crucial role in various fields. With the development of large language models (LLMs), researchers have sought to integrate them into stance detection systems in two main approaches: finetuning-based approaches, which leverage additional data generated by LLMs or directly finetune LLMs with existing datasets, and prompt engineering-based approaches, which use task-specific prompts to guide LLMs without additional training. However, these methods face significant challenges, including limited accuracy and complexity of the synthesized data, reliance on resource-intensive models, and inefficiencies during inference. To address these limitations, this paper proposes a novel framework that integrates thought-chain data augmentation to systematically enrich training data by generating logically consistent reasoning chains, and thought-aligned finetuning to internalize reasoning capabilities into the model by harmonizing reasoning-intensive and direct prediction paradigms. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance in both in-target and cross-target settings, validating its effectiveness.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1558–1562},
numpages = {5},
keywords = {data augmentation, stance detection, thought alignment},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3661167.3661183,
author = {Watanabe, Miku and Kashiwa, Yutaro and Lin, Bin and Hirao, Toshiki and Yamaguchi, Ken'Ichi and Iida, Hajimu},
title = {On the Use of ChatGPT for Code Review: Do Developers Like Reviews By ChatGPT?},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661183},
doi = {10.1145/3661167.3661183},
abstract = {Code review is a critical but time-consuming process for ensuring code quality in modern software engineering. To alleviate the effort of reviewing source code, recent studies have investigated the possibility of automating the review process. Moreover, tools based on large language models such as ChatGPT are playing an increasingly important role in this vision. Understanding how these tools are used during code review can provide valuable insights for code review automation. This study investigates for what purposes developers use ChatGPT during code review and how developers react to the information and suggestions provided by ChatGPT. We manually analyze 229 review comments in 205 pull requests from 179 projects. We find that developers often use ChatGPT for outsourcing their work as frequently as asking for references. Moreover, we observe that only 30.7% of responses to the answers provided by ChatGPT are negative. We further analyze the reasons behind the negative reactions. Our results provide valuable insights for improving the effectiveness of LLMs in code reviews.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {375–380},
numpages = {6},
keywords = {ChatGPT, Code Review, Empirical Study},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3629104.3672429,
author = {Delouee, Majid Lotfian and Pernes, Daria G. and Degeler, Victoria and Koldehofe, Boris},
title = {Towards Federated LLM-Powered CEP Rule Generation and Refinement},
year = {2024},
isbn = {9798400704437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629104.3672429},
doi = {10.1145/3629104.3672429},
abstract = {In traditional event processing systems, patterns representing situations of interest are typically defined by domain experts or learned from historical data, making rule generation reactive, time-consuming, and susceptible to human error. This paper proposes integrating large language models (LLMs) to automate and accelerate query translation and rule generation into event-based systems. Also, we introduce a federated learning schema to refine the initially generated rules by examining them over distributed event streams, ensuring greater accuracy and adaptability. Preliminary results demonstrate the potential of LLMs as a key component in proactively expediting the autonomous rule-generation process. Moreover, our findings suggest that employing customized prompt engineering techniques can further enhance the quality of the generated rules.},
booktitle = {Proceedings of the 18th ACM International Conference on Distributed and Event-Based Systems},
pages = {185–186},
numpages = {2},
keywords = {Autonomous Rule Generation, Complex Event Processing, Federated Learning, Large Language Models, Rule Refinement},
location = {Villeurbanne, France},
series = {DEBS '24}
}

@inproceedings{10.1145/3639477.3639743,
author = {Fakih, Mohamad and Dharmaji, Rahul and Moghaddas, Yasamin and Quiros, Gustavo and Ogundare, Oluwatosin and Al Faruque, Mohammad Abdullah},
title = {LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639743},
doi = {10.1145/3639477.3639743},
abstract = {Although Large Language Models (LLMs) have established predominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools - including grammar checkers, compilers and SMV verifiers - to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally-flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10.To promote open research, we share the complete experimental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage1.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {192–203},
numpages = {12},
keywords = {industrial control, verifiable synthesis, large language models, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@article{10.1145/3697010,
author = {Ouyang, Shuyin and Zhang, Jie M. and Harman, Mark and Wang, Meng},
title = {An Empirical Study of the Non-Determinism of ChatGPT in Code Generation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697010},
doi = {10.1145/3697010},
abstract = {There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; non-deterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is.To fill this gap, this article conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00% and 47.56% for three different code generation datasets (i.e., CodeContests, APPS and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature  (=)  1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {42},
numpages = {28},
keywords = {code generation, non-determinism, large language model}
}

@article{10.1145/3715326,
author = {Xiao, Zhe and He, Xu and Wu, Haoying and Yu, Bei and Guo, Yang},
title = {EDA-Copilot: A RAG-Powered Intelligent Assistant for EDA Tools},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3715326},
doi = {10.1145/3715326},
abstract = {With the rise of Large Language Models (LLMs), researchers have become increasingly interested in their applications in EDA flows, particularly in specific subdomains like serving as knowledge assistants and generating RTL code. In this study, we present a Retrieval-Augmented Generation (RAG) framework tailored to EDA task processing, named EDA-Adaptive RAG. This framework addresses the implicit semantics of EDA data and facilitates efficient knowledge acquisition through classification and enhanced retrieval, significantly enhancing LLMs ability to acquire EDA knowledge. Furthermore, we aim to integrate RAG into the design process as an EDA assistant application. Using RTL code generation as a case study, we demonstrate that the performance of RTL code generation can be enhanced through highly relevant retrievals provided by our RAG. The experimental analysis involves EDA Q&amp;A tasks and RTL code generation evaluation. It is shown that our method outperforms the latest works in terms of both answer stability and code quality.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jan,
keywords = {Large Language Models, Electronic design automation, RTL-to-GDSII, Retrieval-Augmented Generation}
}

@article{10.1145/3660807,
author = {Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi},
title = {Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660807},
doi = {10.1145/3660807},
abstract = {Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {100},
numpages = {24},
keywords = {Attention, Code Generation, Large Language Models}
}

@inproceedings{10.1145/3708359.3712102,
author = {Joshi, Ishika and Shahid, Simra and Venneti, Shreeya Manasvi and Vasu, Manushree and Zheng, Yantao and Li, Yunyao and Krishnamurthy, Balaji and Chan, Gromit Yeuk-Yin},
title = {CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712102},
doi = {10.1145/3708359.3712102},
abstract = {Ensuring large language models’ (LLMs) responses align with prompt instructions is crucial for application development. Based on our formative study with industry professionals, the alignment requires heavy human involvement and tedious trial-and-error especially when there are many instructions in the prompt. To address these challenges, we introduce CoPrompter, a framework that identifies misalignment based on assessing multiple LLM responses with criteria. It proposes a method to generate evaluation criteria questions derived directly from prompt requirements and an interface to turn these questions into a user-editable checklist. Our user study with industry prompt engineers shows that CoPrompter improves the ability to identify and refine instruction alignment with prompt requirements over traditional methods, helps them understand where and how frequently models fail to follow user’s prompt requirements, and helps in clarifying their own requirements, giving them greater control over the response evaluation process. We also present the design lessons to underscore our system’s potential to streamline the prompt engineering process.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {341–365},
numpages = {25},
keywords = {HCI, LLM Evaluation, Prompt Optimization},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3706598.3713827,
author = {Wagman, Kelly B. and Dearing, Matthew T. and Chetty, Marshini},
title = {Generative AI Uses and Risks for Knowledge Workers in a Science Organization},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713827},
doi = {10.1145/3706598.3713827},
abstract = {Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1199},
numpages = {17},
keywords = {generative AI, genAI, artificial intelligence, large language models, LLMs, copilot, workflow agent, agents, future of work, enterprise AI, AI for science, knowledge work, responsible AI, security},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1109/ASE56229.2023.00109,
author = {Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.},
title = {What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00109},
doi = {10.1109/ASE56229.2023.00109},
abstract = {Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {761–773},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3675395,
author = {Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
title = {AceCoder: An Effective Prompting Technique Specialized in Code Generation},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3675395},
doi = {10.1145/3675395},
abstract = {Large language models (LLMs) have shown great success in code generation. LLMs take as the input a prompt and output the code. How to make prompts (i.e., Prompting Techniques) is a key question. Existing prompting techniques are designed for natural language generation and have low accuracy in code generation.In this article, we propose a new prompting technique named AceCoder. Our motivation is that code generation meets two unique challenges (i.e., requirement understanding and code implementation). AceCoder contains two novel mechanisms (i.e., guided code generation and example retrieval) to solve these challenges. ❶ Guided code generation asks LLMs first to analyze requirements and output an intermediate preliminary (e.g., test cases). The preliminary clarifies requirements and tells LLMs “what to write.” ❷ Example retrieval selects similar programs as examples in prompts, which provide lots of relevant content (e.g., algorithms, APIs) and teach LLMs “how to write.” We apply AceCoder to four LLMs (e.g., GPT-3.5, CodeGeeX) and evaluate it on three public benchmarks using the Pass@ (k) . Results show that AceCoder can significantly improve the performance of LLMs on code generation. In terms of Pass@1, AceCoder outperforms the SOTA baseline by up to 56.4% in MBPP, 70.7% in MBJP, and 88.4% in MBJSP. AceCoder is effective in LLMs with different sizes (i.e., 6B–13B) and different languages (i.e., Python, Java, and JavaScript). Human evaluation shows human developers prefer programs from AceCoder.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {204},
numpages = {26},
keywords = {Code generation, large language models, prompting engineering}
}

@inproceedings{10.1145/3691620.3695506,
author = {Zhang, Huan and Cheng, Wei and Wu, Yuhan and Hu, Wei},
title = {A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695506},
doi = {10.1145/3691620.3695506},
abstract = {Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%--162.43% compared to prompting LLMs directly.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1319–1331},
numpages = {13},
keywords = {code generation, large language model, agent, pair programming},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3706598.3714154,
author = {Zamfirescu-Pereira, J.D. and Jun, Eunice and Terry, Michael and Yang, Qian and Hartmann, Bjoern},
title = {Beyond Code Generation: LLM-supported Exploration of the Program Design Space},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714154},
doi = {10.1145/3706598.3714154},
abstract = {In this work, we explore explicit Large Language Model (LLM)-powered support for the iterative design of computer programs. Program design, like other design activity, is characterized by navigating a space of alternative problem formulations and associated solutions in an iterative fashion. LLMs are potentially powerful tools in helping this exploration; however, by default, code-generation LLMs deliver code that represents a particular point solution. This obscures the larger space of possible alternatives, many of which might be preferable to the LLM’s default interpretation and its generated code. We contribute an IDE that supports program design through generating and showing new ways to frame problems alongside alternative solutions, tracking design decisions, and identifying implicit decisions made by either the programmer or the LLM. In a user study, we find that with our IDE, users combine and parallelize design phases to explore a broader design space—but also struggle to keep up with LLM-originated changes to code and other information overload. These findings suggest a core challenge for future IDEs that support program design through higher-level instructions given to LLM-based agents: carefully managing attention and deciding what information agents should surface to program designers and when.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {153},
numpages = {17},
keywords = {Program design, Code generation, Design space exploration, Generative AI, LLMs},
location = {
},
series = {CHI '25}
}

@article{10.1145/3745764,
author = {Chen, Junkai and Zhenhao, Li and Xing, Hu and Xin, Xia},
title = {NLPerturbator: Studying the Robustness of Code LLMs to Natural Language Variations},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3745764},
doi = {10.1145/3745764},
abstract = {Large language models achieve promising results in code generation based on a given natural language description. They have been integrated into open-source projects and commercial products to facilitate daily coding activities. The natural language description in the prompt is crucial for LLMs to comprehend users’ requirements. Prior studies have uncovered that LLMs are sensitive to changes in the prompts, including slight changes that look inconspicuous. However, the natural language descriptions often vary in real-world scenarios (e.g., different formats, grammar, and wording). Prior studies on the robustness of LLMs were often based on random perturbations, and such perturbations may not actually happen. In this paper, we conduct a comprehensive study to investigate how code LLMs are robust to variations of natural language descriptions in real-world scenarios. We summarize 18 categories of perturbations of natural language and 3 combinations of co-occurred categories based on our literature review and online survey with practitioners. We propose an automated framework, NLPerturbator, which can perform perturbations of each category given a set of prompts. Through a series of experiments on code generation using sevencode LLMs, we find that the perturbed prompts can decrease the performance of code generation by a considerable margin. Our study highlights the importance of enhancing the robustness of LLMs to real-world variations in the prompts, as well as the essentiality of attentively constructing the prompts.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {Robustness, Code Generation, Large Language Model}
}

@inproceedings{10.1145/3716815.3729010,
author = {Luo, Yining and Li, Baobao and Singhal, Anoop and Tseng, Peiyu and Zhang, Lan and Zou, Qingtian and Sun, Xiaoyan and Liu, Peng},
title = {Exploring Prompt Patterns for Effective Vulnerability Repair in Real-World Code by Large Language Models},
year = {2025},
isbn = {9798400715013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716815.3729010},
doi = {10.1145/3716815.3729010},
abstract = {Large Language Models (LLMs) have shown promise in automating code vulnerability repair, but their effectiveness in handling real-world code remains limited. This paper investigates the capability of LLMs,  in repairing vulnerabilities and proposes a systematic approach to enhance their performance through specialized prompt engineering. Through extensive evaluation of 5,826 code samples, we found that while LLMs successfully repair vulnerabilities in simple cases, they struggle with complex real-world code that involves intricate dependencies, contextual requirements, and multi-file interactions. To address these limitations, we first incorporated Control Flow Graphs (CFGs) as supplementary prompts, achieving a 14.4% success rate in fixing previously unresolvable vulnerabilities. Through analysis of repair failures, we identified three primary challenge categories and developed corresponding prompt patterns incorporating techniques such as granular contextual information provision and progressive code simplification. Evaluation on real-world projects demonstrated that our approach significantly improved LLMs' repair capabilities, achieving over 85% success rates across all identified challenge categories. Our findings suggest that while LLMs have inherent limitations in handling complex vulnerabilities independently, they can become effective tools for automated vulnerability repair when guided by carefully crafted prompts.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {23–33},
numpages = {11},
keywords = {large language models, program repair, deep learning},
location = {Pittsburgh, PA, USA},
series = {IWSPA '25}
}

@inproceedings{10.1145/3691620.3695527,
author = {Mathews, Noble Saji and Nagappan, Meiyappan},
title = {Test-Driven Development and LLM-based Code Generation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695527},
doi = {10.1145/3691620.3695527},
abstract = {Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1583–1594},
numpages = {12},
keywords = {code generation, LLM, TDD, testing, software engineering},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3652988.3673932,
author = {Steenstra, Ian and Nouraei, Farnaz and Arjmand, Mehdi and Bickmore, Timothy},
title = {Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673932},
doi = {10.1145/3652988.3673932},
abstract = {We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors’ empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {20},
numpages = {10},
keywords = {Alcohol Use Counseling, Embodied Conversational Agents, Intelligent Virtual Agents, Large Language Models, Motivational Interviewing, Persuasive Technology},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@article{10.1145/3724117,
author = {Huang, Dong and Zhang, Jie M. and Bu, Qingwen and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
title = {Bias Testing and Mitigation in LLM-based Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3724117},
doi = {10.1145/3724117},
abstract = {As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4)1.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Fairness testing, code generation}
}

@article{10.5555/3722577.3722831,
author = {Zhu, Kaijie and Zhao, Qinlin and Chen, Hao and Wang, Jindong and Xie, Xing},
title = {PromptBench: a unified library for evaluation of large language models},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that can be easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed as an open, general, and flexible codebase for research purpose. It aims to facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {254},
numpages = {22},
keywords = {evaluation, large language models, framework}
}

@inproceedings{10.1145/3639478.3643076,
author = {Huang, Tao and Sun, Zhihong and Jin, Zhi and Li, Ge and Lyu, Chen},
title = {KareCoder: A New Knowledge-Enriched Code Generation System},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643076},
doi = {10.1145/3639478.3643076},
abstract = {Large Language Models (LLMs) demonstrate proficiency in handling fundamental programming problems but struggle with complex programming in new types. The study presents KareCoder, integrating programming knowledge into code generation. Initial tests reveal KareCoder's significant success in the Pass@1 metric for complex competitive programming problems.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {270–271},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3630106.3658984,
author = {Wang, Ruotong and Cheng, Ruijia and Ford, Denae and Zimmermann, Thomas},
title = {Investigating and Designing for Trust in AI-powered Code Generation Tools},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658984},
doi = {10.1145/3630106.3658984},
abstract = {Trust is a crucial factor for the adoption and responsible usage of generative AI tools in complex tasks such as software engineering. However, we have a limited understanding of how software developers evaluate the trustworthiness of AI-powered code generation tools in real-world settings. To address this gap, we conducted Study 1, an interview study with 17 developers who use AI-powered code generation tools in professional or personal settings. We found that developers’ trust is rooted in the AI tool’s perceived ability, integrity, and benevolence, and is situational, varying according to the context of usage. Existing AI code generation tools lack the affordances for developers to efficiently and effectively evaluate the trustworthiness of AI-powered code generation tools. To explore designs that can augment the existing interface of AI-powered code generation tools, we explored three sets of design concepts (suggestion quality indicators, usage stats, and control mechanisms) that derived from Study 1 findings. In Study 2, a design probe study with 12 developers, we investigated the potential of these design concepts to help developers make effective trust judgments. We discuss the implication of our findings on the design of AI-powered code generation tools and future research on trust in AI.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1475–1493},
numpages = {19},
keywords = {generative AI, human-AI interaction, software engineering tooling, trust in AI},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{10.1145/3736167,
author = {He, Zhuolun and Pu, Yuan and Wu, Haoyuan and Qiu, Tairu and Yu, Bei},
title = {Large Language Models for EDA: Future or Mirage?},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3736167},
doi = {10.1145/3736167},
abstract = {In this paper, we explore the burgeoning intersection of large language models (LLMs) and electronic design automation (EDA). We critically assess whether LLMs represent a transformative future for EDA or merely a fleeting mirage. By organizing existing research into four critical domains of EDA — code generation, verification and debugging, knowledge representation and retreival, and optimization/modeling — we provide a comprehensive overview of the current state-of-the-art. The survey concludes with a 5-level roadmap to guide the progressive integration and advancement of LLMs in EDA. Ultimately, this paper aims to provide a comprehensive, evidence-based perspective on the role of LLMs in shaping the future of EDA.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may
}

@inproceedings{10.1145/3685650.3685667,
author = {Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim \O{}. and Alexandrov, Boian S.},
title = {TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs},
year = {2024},
isbn = {9798400711695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685650.3685667},
doi = {10.1145/3685650.3685667},
abstract = {Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2024},
articleno = {8},
numpages = {4},
keywords = {chain of thought, llm, nmf, prompt tuning, topic labeling},
location = {San Jose, CA, USA},
series = {DocEng '24}
}

@inproceedings{10.1145/3701716.3715472,
author = {Gawin, Cole and Sun, Yidan and Kejriwal, Mayank},
title = {Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715472},
doi = {10.1145/3701716.3715472},
abstract = {Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {971–975},
numpages = {5},
keywords = {abstract common sense, conceptnet, llm prompting},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3679240.3734601,
author = {Kuchenbuch, Ren\'{e} and Lehnhoff, Sebastian and Sauer, J\"{u}rgen},
title = {Smart Grid Assistive AI in Requirement Engineering: Improving the Modeling of Use Cases and Architecture Models with LLMs},
year = {2025},
isbn = {9798400711251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679240.3734601},
doi = {10.1145/3679240.3734601},
abstract = {The IEC 62559 Use Case Methodology and Smart Grid Architecture Model (SGAM) Framework are crucial for fostering a shared understanding in the development of ICT-based power systems and their components within energy-related Requirements Engineering. However, discrepancies in interpretation among diverse stakeholders often diminish the quality of IEC 62559 Use Cases and SGAM Models, potentially leading to errors and costly setbacks in later project stages. This paper introduces the Smart Grid Assistive AI in Requirements Engineering (SGAAIRE), an intelligent system that leverages Large Language Models to enhance the quality of IEC 62559 Use Case descriptions and SGAM Models. Through a demonstration scenario featuring built-in quality defects, SGAAIRE is shown to effectively address common issues as an extension to a Use Case Management Repository. The proposed system supports the improvement of IEC 62559 Use Cases and SGAM Models, enabling the development of third-party tools aimed at advancing research in AI for energy-related requirements management.},
booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems},
pages = {495–504},
numpages = {10},
keywords = {Requirement Engineering, IEC 62559 Use Cases, Smart Grid Architecture Model, Artificial Intelligence, Natural Language Processing},
location = {
},
series = {E-Energy '25}
}

@inproceedings{10.1145/3698322.3698346,
author = {Holtel, Stefan},
title = {Phrasebooks Can Teach Us ChatGPT: Decoding Prompt Crafting as Function Allocation},
year = {2024},
isbn = {9798400716836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698322.3698346},
doi = {10.1145/3698322.3698346},
abstract = {The discipline of Prompt Engineering is rapidly evolving, with varied and often conflicting approaches to crafting effective prompts for Large Language Models like ChatGPT. This divergence has led to a significant underutilization of AI technologies by individuals and organizations, largely due to a fundamental misconception about the nature of prompt writing. This paper advocates for a paradigm shift from a command-based to a language-centric approach, integrating concepts from metacognition and knowledge management. It introduces the novel metaphor of ‘Prompting Phrasebooks,’ akin to traveler phrasebooks, as a structured method to facilitate rich, interactive dialogues with AI systems. By drawing on a culturally ingrained analogy, the Prompting Phrasebook offers a practical framework for users with no or little expertise in prompt crafting to effectively engage with AI-driven chatbots. This concept is further operationalized through the creation of so-called ‘Prompt Pattern Languages,’ each representing a curated set of prompts designed to achieve a specific cognitive tasks The paper presents the theoretical foundation of this approach and demonstrates its application through a Prompt Pattern Language for AI-assisted email composition, offering an unprecedented direction for teaching and mastering Prompt Engineering.},
booktitle = {Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices},
articleno = {29},
numpages = {9},
keywords = {Large Language Models, Prompt Engineering, Prompt Pattern},
location = {
},
series = {EuroPLoP '24}
}

@article{10.1145/3729274,
author = {M\"{u}ndler, Niels and He, Jingxuan and Wang, Hao and Sen, Koushik and Song, Dawn and Vechev, Martin},
title = {Type-Constrained Code Generation with Language Models},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {PLDI},
url = {https://doi.org/10.1145/3729274},
doi = {10.1145/3729274},
abstract = {Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic features of general-purpose programming languages. However, LLMs frequently generate code with typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. For this purpose, we develop novel prefix automata and a search over inhabitable types, forming a sound approach to enforce well-typedness on LLM-generated code. We formalize our approach on a foundational simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on the HumanEval and MBPP datasets shows that our approach reduces compilation errors by more than half and significantly increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including state-of-the-art open-weight models with more than 30B parameters. The results demonstrate the generality and effectiveness of our approach in constraining LLM code generation with formal rules of type systems.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {171},
numpages = {26},
keywords = {Code Generation, Constrained Decoding, Language Model, Program Repair, Program Synthesis, Program Translation, Type System}
}

@inproceedings{10.1145/3726302.3730189,
author = {Kruff, Andreas Konstantin and Breuer, Timo and Schaer, Philipp},
title = {Evaluating Contrastive Feedback for Effective User Simulations},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730189},
doi = {10.1145/3726302.3730189},
abstract = {The use of Large Language Models (LLMs) for simulating user behavior in the domain of Interactive Information Retrieval has recently gained significant popularity. However, their application and capabilities remain highly debated and understudied. This study explores whether the underlying principles of contrastive training techniques, which have been effective for fine-tuning LLMs, can also be applied beneficially in the area of prompt engineering for user simulations.Previous research has shown that LLMs possess comprehensive world knowledge, which can be leveraged to provide accurate estimates of relevant documents. This study attempts to simulate a knowledge state by enhancing the model with additional implicit contextual information gained during the simulation. This approach enables the model to refine the scope of desired documents further. The primary objective of this study is to analyze how different modalities of contextual information influence the effectiveness of user simulations.Various user configurations were tested, where models are provided with summaries of already judged relevant, irrelevant, or both types of documents in a contrastive manner. The focus of this study is the assessment of the impact of the prompting techniques on the simulated user agent performance. We hereby lay the foundations for leveraging LLMs as part of more realistic simulated users.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2931–2935},
numpages = {5},
keywords = {contrastive feedback, interactive information retrieval (iir), simiir 3, user simulation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3650212.3652135,
author = {Lu, You and Tian, Yifan and Bi, Yuyang and Chen, Bihuan and Peng, Xin},
title = {DiaVio: LLM-Empowered Diagnosis of Safety Violations in ADS Simulation Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652135},
doi = {10.1145/3650212.3652135},
abstract = {Simulation testing has been widely adopted by leading companies to ensure the safety of autonomous driving systems (ADSs). Anumber of scenario-based testing approaches have been developed to generate diverse driving scenarios for simulation testing, and demonstrated to be capable of finding safety violations. However, there is no automated way to diagnose whether these violations are caused by the ADS under test and which category these violations belong to. As a result, great effort is required to manually diagnose violations.     To bridge this gap, we propose DiaVio to automatically diagnose safety violations in simulation testing by leveraging large language models (LLMs). It is built on top of a new domain specific language (DSL) of crash to align real-world accident reports described in natural language and violation scenarios in simulation testing. DiaVio fine-tunes a base LLM with real-world accident reports to learn diagnosis capability, and uses the fine-tuned LLM to diagnose violation scenarios in simulation testing. Our evaluation has demonstrated the effectiveness and efficiency of DiaVio in violation diagnosis.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {376–388},
numpages = {13},
keywords = {Automated Driving System, Large Language Models, Scenario-based Testing, Violation Diagnosis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3653081.3653111,
author = {Liang, Zijing and Fang, Zirui and Huang, Haitao and Wang, Zhiyuan and Hong, Yifan and Liu, Ke and Shang, Penghui},
title = {Engineering Exploration and Research on Large Language Models},
year = {2024},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653081.3653111},
doi = {10.1145/3653081.3653111},
abstract = {Large Language Models, as a booming industry in artificial intelligence, empower various sectors, enhancing the efficiency of developers and application practitioners. However, deploying large models in private scenarios poses numerous challenges. This review focuses on the engineering applications of large language models, delving into the technical aspects of retrieval-enhanced generation, prompt engineering for large models, fine-tuning and incremental pre-training of large models, and the intelligent agent mode of large models. Through a detailed analysis of these technologies, the goal is to improve model performance, enhance adaptability in diverse scenarios, and address various challenges that may arise in practical deployment. Through this research, we aim to systematically outline the technical means for the engineering application of large models, providing insights and experiences for the deployment of large models.},
booktitle = {Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
pages = {174–179},
numpages = {6},
location = {Nanchang, China},
series = {IoTAAI '23}
}

@article{10.1145/3672459,
author = {Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
title = {Self-Collaboration Code Generation via ChatGPT},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672459},
doi = {10.1145/3672459},
abstract = {Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {189},
numpages = {38},
keywords = {Code generation, large language models, multi-agent collaboration, software development}
}

@inproceedings{10.1145/3589335.3641292,
author = {Graux, Damien and Montella, S\'{e}bastien and Jabeen, Hajira and Gardent, Claire and Pan, Jeff Z.},
title = {[PromptEng] First International Workshop on Prompt Engineering for Pre-Trained Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641292},
doi = {10.1145/3589335.3641292},
abstract = {The recent achievements and availability of Large Language Models have paved the road to a new range of applications and use-cases. Pre-trained language models are now being involved at-scale in many fields where they were until now absent from. More specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. Unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. This first international workshop on prompt engineering aims at gathering practitioners (both from Academia and Industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient prompts to make use of LLMs.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1311–1312},
numpages = {2},
keywords = {best practices, collective task, llm, prompt engineering},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3709355,
author = {Wang, Qing and Wang, Junjie and Li, Mingyang and Wang, Yawen and Liu, Zhe},
title = {A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709355},
doi = {10.1145/3709355},
abstract = {Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in AI have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this article explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as Large Language Models (LLMs) and the AI model-centric development paradigm.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {148},
numpages = {17},
keywords = {Software Testing, Artificial Intelligence, AI, Large Language Model, LLM, Open Source, Open Collaborative}
}

@inproceedings{10.1145/3723178.3723224,
author = {Siam, Md Kamrul and Gu, Huanying and Cheng, Jerry Q.},
title = {Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers},
year = {2025},
isbn = {9798400713828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723178.3723224},
doi = {10.1145/3723178.3723224},
abstract = {Our everyday lives now heavily rely on artificial intelligence (AI) powered large language models (LLMs). Like regular users, programmers are also benefiting from the newest large language models. In response to the critical role that AI models play in modern software development, this study presents a thorough evaluation of leading programming assistants, including ChatGPT, Gemini (Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on tasks like natural language processing and code generation accuracy in different programming languages like Java, Python and C++. Based on the results, it has emphasized their strengths and weaknesses and the importance of further modifications to increase the reliability and accuracy of the latest popular models. Although these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion. With time, developing more refined AI technology is essential for achieving advanced solutions in various fields, especially with the knowledge of the feature intricacies of these models and their implications. This study offers a comparison of different LLMs and provides essential feedback on the rapidly changing area of AI models. It also emphasizes the need for ethical developmental practices to actualize AI models’ full potential.},
booktitle = {Proceedings of the 3rd International Conference on Computing Advancements},
pages = {346–354},
numpages = {9},
keywords = {AI models, chatbot, Gemini, GitHub Copilot, ChatGPT, AlphaCode, LLM, code generation, ethical considerations, responsible deployment, AI model accuracy},
location = {
},
series = {ICCA '24}
}

@inproceedings{10.1145/3643991.3645081,
author = {AlOmar, Eman Abdullah and Venkatakrishnan, Anushkrishna and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali},
title = {How to refactor this code? An exploratory study on developer-ChatGPT refactoring conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645081},
doi = {10.1145/3643991.3645081},
abstract = {Large Language Models (LLMs), like ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with ChatGPT. In this paper, our goal is to explore conversations between developers and ChatGPT related to refactoring to better understand how developers identify areas for improvement in code and how ChatGPT addresses developers' needs. Our approach relies on text mining refactoring-related conversations from 17,913 ChatGPT prompts and responses, and investigating developers' explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while ChatGPT typically includes the refactoring intention; and (3) various learning settings when prompting ChatGPT in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {202–206},
numpages = {5},
keywords = {refactoring documentation, ChatGPT, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inbook{10.1145/3708897,
author = {Giacaman, Nasser and Terragni, Valerio},
title = {Empowering Computing Students with Large Language Models by Developing an Escape Room Game},
year = {2025},
isbn = {9798400714450},
abstract = {In this project, computing students learn to integrate large language models (LLMs) into a software system. Students develop a Java application with a basic graphical user interface (GUI) using JavaFX, gain practical experience with prompt engineering, and learn about the impact of LLM parameters and conversational roles. Students are provided with a Javabased API that connects with OpenAI's GPT model. The project emphasizes teaching students to manage LLM API calls, enhance GUI responsiveness, and improve the user experience all in the context of an AI-powered application. This experience equips them with critical skills in software development and AI application. It prepares them for advanced software development by learning how to create effective LLM prompts to create intelligent and user-friendly applications. We share the experience of using this project and provide guidelines for assessing it in a second-year software engineering undergraduate course, where students' prior programming experience is limited to the prerequisite CS2 course on object-oriented programming. In the case study we present, the project involved developing a riddle-solving escape room, which we called EscAIpe Room.},
numpages = {6}
}

@article{10.14778/3641204.3641221,
author = {Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yichen and Ding, Bolin and Zhou, Jingren},
title = {Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation},
year = {2024},
issue_date = {January 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3641204.3641221},
doi = {10.14778/3641204.3641221},
abstract = {Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar.To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {1132–1145},
numpages = {14}
}

@inproceedings{10.1109/ASE56229.2023.00047,
author = {Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming},
title = {The Plastic Surgery Hypothesis in the Era of Large Language Models},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00047},
doi = {10.1109/ASE56229.2023.00047},
abstract = {Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {522–534},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3687123.3698286,
author = {Gramacki, Piotr and Martins, Bruno and Szyma\'{n}ski, Piotr},
title = {Evaluation of Code LLMs on Geospatial Code Generation},
year = {2024},
isbn = {9798400711763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687123.3698286},
doi = {10.1145/3687123.3698286},
abstract = {Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a "mentor" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository1, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {54–62},
numpages = {9},
keywords = {code generation, geospatial data science, large language models},
location = {Atlanta, GA, USA},
series = {GeoAI '24}
}

@inproceedings{10.1145/3676536.3689909,
author = {Yu, Zhongzhi and Li, Chaojian and Zhang, Yongan and Liu, Mingjie and Pinckney, Nathaniel and Zhou, Wenfei and Yang, Haoyu and Liang, Rongjian and Ren, Haoxing and Lin, Yingyan Celine},
title = {Invited Paper: LLM4HWDesign Contest: Constructing a Comprehensive Dataset for LLM-Assisted Hardware Code Generation with Community Efforts},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3689909},
doi = {10.1145/3676536.3689909},
abstract = {Large Language Models (LLMs) show promise in streamlining hardware design, particularly in hardware code generation. However, the development of LLMs for this domain is severely hindered by the scarcity of large-scale, high-quality, and publicly accessible hardware code datasets. This shortage limits the effective fine-tuning of LLMs, impeding their ability to acquire hardware domain knowledge and generate practical designs. To address this challenge, we have organized the first-of-its-kind LLM4HWDesign contest, a community-driven initiative aimed at constructing a large-scale, high-quality dataset for hardware code generation. The contest adopts a two-phase approach, focusing on expanding the scale and quality of an existing hardware code generation dataset, respectively. By harnessing the collective efforts of the hardware design community, the LLM4HWDesign contest seeks to establish a critical resource for advancing LLM-assisted hardware design workflows. The primary goal of this initiative is to deliver a comprehensive dataset compiled from participants' submissions. We hope the released dataset will significantly advance the field of LLM-assisted hardware design and provide substantial benefits to the broader hardware community.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {5},
numpages = {5},
keywords = {LLM for EDA, code generation, dataset, contest},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/3709026.3709030,
author = {Liew, Pei Yee and Tan, Ian K. T.},
title = {On Automated Essay Grading using Large Language Models},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709030},
doi = {10.1145/3709026.3709030},
abstract = {Automated Essay Grading (AEG), combining Automated Essay Scoring (AES) and Automated Writing Evaluation (AWE), is a time-saving solution to the challenges of manual essay evaluation. It aims to reduce the workload on educators by offering a more consistent grading approach. Inspired by ChatGPT’s impressive language comprehension and generation capabilities, this study explored the potential of various Large Language Models (LLMs) in AEG tasks. The models examined include GPT-4, GPT-3.5, PaLM, and LLaMA2. Tailored prompts were designed and their performance was assessed in conjunction with each LLM through prompt engineering. Our study shows that LLMs can achieve substantial agreement with human markers in AES, with a Quadratic Weighted Kappa (QWK) score of 0.68. In AWE, the feedback on the essay was assessed qualitatively. It achieved an agreement level score of 4.9 (out of 5) with a standard deviation of 0.05, closely aligned with human assessment. This study provided valuable insights into the effectiveness of LLMs in automated essay grading. It highlighted their potential to enhance educational assessment practices.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {204–211},
numpages = {8},
keywords = {automated essay scoring, automated writing evaluation, large language models, prompt engineering},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3638530.3664162,
author = {Custode, Leonardo Lucio and Migliore Rambaldi, Chiara Camilla and Roveri, Marco and Iacca, Giovanni},
title = {Comparing Large Language Models and Grammatical Evolution for Code Generation},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664162},
doi = {10.1145/3638530.3664162},
abstract = {Code generation is one of the most valuable applications of AI, as it allows for automated programming and "self-building" programs. Both Large Language Models (LLMs) and evolutionary methods, such as Genetic Programming (GP) and Grammatical Evolution (GE), are known to be capable of performing code generation with reasonable performance. However, to the best of our knowledge, little work has been done so far on a systematic comparison between the two approaches. Most importantly, the only studies that conducted such comparisons used benchmarks from the GP community, which, in our opinion, may have provided possibly GP-biased results. In this work, we perform a comparison of LLMs and evolutionary methods, in particular GE, using instead a well-known benchmark originating from the LLM community. Our results show that, in this scenario, LLMs can solve significantly more tasks than GE, indicating that GE struggles to match the performance of LLMs on code generation tasks that have different properties from those commonly used in the GP community.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1830–1837},
numpages = {8},
keywords = {grammatical evolution, genetic programming, large language models, code generation},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3650105.3652299,
author = {Li, Junjie and Sangalay, Aseem and Cheng, Cheng and Tian, Yuan and Yang, Jinqiu},
title = {Fine Tuning Large Language Model for Secure Code Generation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652299},
doi = {10.1145/3650105.3652299},
abstract = {AI pair programmers, such as GitHub's Copilot, have shown great success in automatic code generation. However, such large language model-based code generation techniques face the risk of introducing security vulnerabilities to codebases. In this work, we explore the direction of fine-tuning large language models for generating more secure code. We use real-world vulnerability fixes as our fine-tuning dataset. We craft a code-generation scenario dataset (C/C++) for evaluating and comparing the pre-trained and fine-tuned models. Our experiments on GPT-J show that the fine-tuned GPT-J achieved 70.4% and 64.5% ratios of non-vulnerable code generation for C and C++, respectively, which has a 10% increase for C and a slight increase for C++ compared with the pre-trained large language model.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {86–90},
numpages = {5},
keywords = {code generation, cybersecurity, artificial intelligence (AI), common weakness enumerations (CWEs)},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3691620.3695346,
author = {Sarschar, Mahja and Zhang, Gefei and Nowak, Annika},
title = {PACGBI: A Pipeline for Automated Code Generation from Backlog Items},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695346},
doi = {10.1145/3691620.3695346},
abstract = {While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables nontechnical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2338–2341},
numpages = {4},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3640794.3665563,
author = {Bozkir, Efe and \"{O}zdel, S\"{u}leyman and Lau, Ka Hei Carrie and Wang, Mengdi and Gao, Hong and Kasneci, Enkelejda},
title = {Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665563},
doi = {10.1145/3640794.3665563},
abstract = {Advances in artificial intelligence and human-computer interaction will likely lead to extended reality (XR) becoming pervasive. While XR can provide users with interactive, engaging, and immersive experiences, non-player characters are often utilized in pre-scripted and conventional ways. This paper argues for using large language models (LLMs) in XR by embedding them in avatars or as narratives to facilitate inclusion through prompt engineering and fine-tuning the LLMs. We argue that this inclusion will promote diversity for XR use. Furthermore, the versatile conversational capabilities of LLMs will likely increase engagement in XR, helping XR become ubiquitous. Lastly, we speculate that combining the information provided to LLM-powered spaces by users and the biometric data obtained might lead to novel privacy invasions. While exploring potential privacy breaches, examining user privacy concerns and preferences is also essential. Therefore, despite challenges, LLM-powered XR is a promising area with several opportunities.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {38},
numpages = {7},
keywords = {ChatGPT, artificial intelligence, augmented reality, engagement, extended reality, generative AI, inclusion, large language models, privacy, virtual reality},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3639478.3643081,
author = {Lian, Xiaoli and Wang, Shuaisong and Ma, Jieping and Tan, Xin and Liu, Fang and Shi, Lin and Gao, Cuiyun and Zhang, Li},
title = {Imperfect Code Generation: Uncovering Weaknesses in Automatic Code Generation by Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643081},
doi = {10.1145/3639478.3643081},
abstract = {The task of code generation has received significant attention in recent years, especially when the pre-trained large language models (LLMs) for code have consistently achieved state-of-the-art performance. However, there is currently a lack of a comprehensive weakness taxonomy in the field, uncovering weaknesses in automatic code generation by LLMs. This may lead the community to invest excessive efforts into well-known hotspots while neglecting many crucial yet unrecognized issues that deserve more attention. To bridge this gap, we conduct a systematic study on analyzing the weaknesses based on three state-of-the-art LLMs across three widely-used code generation datasets. Our study identifies eight types of weaknesses and assesses their prevalence across each LLM and dataset, aiming to inform and shape the trajectory of future research in the domain.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {422–423},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3724363.3729100,
author = {Hebbar, Sannidhi V and Harini S, Sasmita and Kumar, Viraj},
title = {Refuting LLM-generated Code with Reactive Task Comprehension},
year = {2025},
isbn = {9798400715679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724363.3729100},
doi = {10.1145/3724363.3729100},
abstract = {Large Language Models (LLMs) for code generation have improved to the point where they are being integrated into professional software development workflows. Since these models occasionally generate buggy code, it is important for students to develop the ability to refute such code (typically, by identifying a counterexample input on which the code fails to perform the desired task). To create counterexamples manually, prior work has suggested code comprehension and task comprehension as two necessary skills. In this paper, we anticipate advances in software development tools and consider a limited form of the latter skill -- reactive task comprehension -- where students only need to correctly state the code's desired behavior on inputs suggested by an automated system. We make two contributions. First, we demonstrate the feasibility of such a system based on existing LLMs and code coverage tools. Second, we show that reactive task comprehension is surprisingly effective in refuting LLM-generated buggy Python functions in the HumanEval+ dataset. Bearing in mind that students are likely to have access to increasingly sophisticated code generation models and assistive systems, we discuss the implications of our findings for introductory programming education.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {30–36},
numpages = {7},
keywords = {code comprehension, cs0/cs1, refute problems, task comprehension},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@article{10.1145/3643762,
author = {Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram},
title = {CORE: Resolving Code Quality Issues using LLMs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643762},
doi = {10.1145/3643762},
abstract = {As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer. We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark), (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively), and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8% in these cases. CORE produced revisions that passed the static analysis tool in 76.8% Java files (across 10 quality checks) comparable to 78.3% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {36},
numpages = {23},
keywords = {Code quality, LLMs, code revision, static analysis}
}

@article{10.1145/3747288,
author = {Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Du, Xiaoning and Xing, Zhenchang},
title = {Bypassing Guardrails: Lessons Learned from Red Teaming ChatGPT},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3747288},
doi = {10.1145/3747288},
abstract = {Warning: this paper may contain content that is offensive or upsetting.Ethical and social risks persist as a crucial yet challenging topic in human-artificial Intelligence interactions, especially in ensuring the safe usage of natural language processing (NLP). The emergence of large language models (LLMs) like ChatGPT introduces the potential for exacerbating this concern. However, prior works on the ethics and risks of emergent LLMs either overlook the practical implications in real-world scenarios, lag behind rapid NLP advancements, lack user consensus on ethical risks, or fail to holistically address the entire spectrum of ethical considerations. In this paper, we comprehensively evaluate, qualitatively explore and catalog ethical dilemmas and risks in ChatGPT through benchmarking with eight representative datasets and red teaming involving diverse case studies. Our findings show that while ChatGPT demonstrates superior safety performance on benchmark datasets, its guardrails can be bypassed via our manually curated examples, revealing not only the limitations of current benchmarks for risk assessment but also unexplored risks in five distinct scenarios, including social bias in code generation, bias in cross-lingual question answering, toxic language in personalized dialogue, misleading information from hallucination, and prompt injections for unethical behaviors. We conclude with implications from red teaming ChatGPT and recommendations for designing future responsible large language models.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {AI Ethics, Red Teaming, Large Language Models}
}

@article{10.1145/3716848,
author = {Fu, Yujia and Liang, Peng and Tahir, Amjed and Li, Zengyang and Shahin, Mojtaba and Yu, Jiaxin and Chen, Jinfu},
title = {Security Weaknesses of Copilot-Generated Code in GitHub Projects: An Empirical Study},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716848},
doi = {10.1145/3716848},
abstract = {Modern code generation tools utilizing AI models like Large Language Models (LLMs) have gained increased popularity due to their ability to produce functional code. However, their usage presents security challenges, often resulting in insecure code merging into the code base. Thus, evaluating the quality of generated code, especially its security, is crucial. While prior research explored various aspects of code generation, the focus on security has been limited, mostly examining code produced in controlled environments rather than open source development scenarios. To address this gap, we conducted an empirical study, analyzing code snippets generated by GitHub Copilot and two other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub projects. Our analysis identified 733 snippets, revealing a high likelihood of security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets affected. These issues span 43 Common Weakness Enumeration (CWE) categories, including significant ones like CWE-330: Use of Insufficiently Random Values, CWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site Scripting. Notably, eight of those CWEs are among the 2023 CWE Top-25, highlighting their severity. We further examined using Copilot Chat to fix security issues in Copilot-generated code by providing Copilot Chat with warning messages from the static analysis tools, and up to 55.5% of the security issues can be fixed. We finally provide the suggestions for mitigating security issues in generated code.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code Generation, Security Weakness, CWE, GitHub Copilot, GitHub Project}
}

@inproceedings{10.1145/3697355.3697376,
author = {Sasaki, Iori and Arikawa, Masatoshi and Lu, Min and Utsumi, Tomihiro and Sato, Ryo},
title = {Generative Live Commentaries Interacting with Geospatial Context for Promoting Local Festivals},
year = {2024},
isbn = {9798400717529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3697355.3697376},
doi = {10.1145/3697355.3697376},
abstract = {With the advancement of robust network connections and the rise of live streaming platforms, it has become possible to experience local events and walking tours remotely in real-time. This study introduces a city-wide audio augmented reality system designed to enhance engagement at local festivals by combining livestreamed video with character-driven commentaries on local history and event details. To ensure continuous storytelling, even in areas with few points of interest, this study proposes a novel geofencing architecture that considers multilayered city features, enabling seamless audio guide experiences. Additionally, this paper introduces prompt engineering for generating entertaining and listener-friendly guide scripts, tailored for large language models, named the geofence-to-conversation technique. A preliminary study of our prototype system, deployed at an actual local festival, demonstrates its effectiveness and potential for enhancing regional promotion.},
booktitle = {Proceedings of the 2024 8th International Conference on Big Data and Internet of Things},
pages = {125–131},
numpages = {7},
keywords = {Audio augmented reality, Character-driven live commentaries, Geofencing, Large language models, Prompt engineering, Regional promotion},
location = {
},
series = {BDIOT '24}
}

@inproceedings{10.1145/3675094.3677545,
author = {Zhang, Shiquan and Ma, Ying and Fang, Le and Jia, Hong and D'Alfonso, Simon and Kostakos, Vassilis},
title = {Enabling On-Device LLMs Personalization with Smartphone Sensing},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3677545},
doi = {10.1145/3675094.3677545},
abstract = {This demo presents a novel end-to-end framework that combines on-device large language models (LLMs) with smartphone sensing technologies to achieve context-aware and personalized services. The framework addresses critical limitations of current personalization solutions via cloud LLMs, such as privacy concerns, latency and cost, and limited personal information. To achieve this, we innovatively proposed deploying LLMs on smartphones with multimodal sensor data through context-aware sensing and customized prompt engineering, ensuring privacy and enhancing personalization performance. A case study involving a university student demonstrated the capability of the framework to provide tailored recommendations. In addition, we show that the framework achieves the best trade-off in privacy, performance, latency, cost, battery and energy consumption between on-device and cloud LLMs. To the best of our knowledge, this is the first framework to provide on-device LLMs personalization with smartphone sensing. Future work will incorporate more diverse sensor data and involve extensive user studies to enhance personalization. Our proposed framework has the potential to substantially improve user experiences across domains including healthcare, productivity, and entertainment.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {186–190},
numpages = {5},
keywords = {end-to-end framework, llm, on-device, personalization, smartphone sensing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3597503.3639219,
author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
title = {Evaluating Large Language Models in Class-Level Code Generation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639219},
doi = {10.1145/3597503.3639219},
abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {81},
numpages = {13},
keywords = {class-level code generation, large language model, benchmark},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3678884.3681850,
author = {Liu, Jiaying (Lizzy) and Wang, Yunlong and Lyu, Yao and Su, Yiheng and Niu, Shuo and Xu, Xuhai "Orson" and Zhang, Yan},
title = {Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression},
year = {2024},
isbn = {9798400711145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678884.3681850},
doi = {10.1145/3678884.3681850},
abstract = {Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM's video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM's capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.},
booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing},
pages = {190–196},
numpages = {7},
keywords = {images, large language model, large language-and-vision assistant (llava), mental health, multimodal information, user generated content, visual content},
location = {San Jose, Costa Rica},
series = {CSCW Companion '24}
}

@inproceedings{10.1145/3726302.3729932,
author = {Cui, Ziqiang and Weng, Yunpeng and Tang, Xing and Lyu, Fuyuan and Liu, Dugang and He, Xiuqiang and Ma, Chen},
title = {Comprehending Knowledge Graphs with Large Language Models for Recommender Systems},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729932},
doi = {10.1145/3726302.3729932},
abstract = {In recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing methods still face several limitations. First, most KGs suffer from missing facts or limited scopes. Second, existing methods convert textual information in KGs into IDs, resulting in the loss of natural semantic connections between different items. Third, existing methods struggle to capture high-order connections in the global KG. To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) to improve KG-based recommendations. The extensive knowledge and remarkable reasoning capabilities of LLMs enable our method to supplement missing facts in KGs, and their powerful text understanding abilities allow for better utilization of semantic information. Specifically, CoLaKG extracts useful information from KGs at both local and global levels. By employing the item-centered subgraph extraction and prompt engineering, it can accurately understand the local information. In addition, through the semantic-based retrieval module, each item is enriched by related items from the entire knowledge graph, effectively harnessing global information. Furthermore, the local and global information are effectively integrated into the recommendation model through a representation fusion module and a retrieval-augmented representation learning module, respectively. Extensive experiments on four real-world datasets demonstrate the superiority of our method.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1229–1239},
numpages = {11},
keywords = {knowledge graphs, large language models, recommendation, recommender systems},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3706598.3714115,
author = {Ma, Jenny GuangZhen and Sreedhar, Karthik and Liu, Vivian and Perez, Pedro A. and Wang, Sitong and Sahni, Riya and Chilton, Lydia B},
title = {DynEx: Dynamic Code Synthesis with Structured Design Exploration for Accelerated Exploratory Programming},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714115},
doi = {10.1145/3706598.3714115},
abstract = {Recent advancements in large language models have significantly expedited the process of generating front-end code. This allows users to rapidly prototype user interfaces and ideate through code, a process known as exploratory programming. However, existing LLM code generation tools focus more on technical implementation details rather than finding the right design given a particular problem. We present DynEx, an LLM-based method for design exploration in accelerated exploratory programming. DynEx introduces a technique to explore the design space through a structured Design Matrix before creating the prototype with a modular, stepwise approach to LLM code generation. Code is generated sequentially, and users can test and approve each step before moving onto the next. A user study of 10 experts found that DynEx increased design exploration and enabled the creation of more complex and varied prototypes compared to a Claude Artifact baseline. We conclude with a discussion of the implications of design exploration for exploratory programming.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {873},
numpages = {27},
keywords = {code synthesis, exploratory programming, design exploration, design matrix, user interface, prototyping},
location = {
},
series = {CHI '25}
}

@article{10.1145/3695868,
author = {Li, Xinze and Wang, Hanbin and Liu, Zhenghao and Yu, Shi and Wang, Shuo and Yan, Yukun and Fu, Yukai and Gu, Yu and Yu, Ge},
title = {Building a Coding Assistant via the Retrieval-Augmented Language Model},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3695868},
doi = {10.1145/3695868},
abstract = {Pretrained language models have shown strong effectiveness in code-related tasks, such as code retrieval, code generation, code summarization, and code completion tasks. In this article, we propose COde assistaNt viA retrieval-augmeNted language model (CONAN), which aims to build a code assistant by mimicking the knowledge-seeking behaviors of humans during coding. Specifically, it consists of a code structure-aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and Masked Entity Prediction tasks to make language models code structure-aware and learn effective representations for code snippets and documentation. Then CONAN-G designs a dual-view code representation mechanism for implementing a retrieval-augmented code generation model. CONAN-G regards the code documentation descriptions as prompts, which help language models better understand the code semantics. Our experiments show that CONAN achieves convincing performance on different code generation tasks and significantly outperforms previous retrieval augmented code generation models. Our further analyses show that CONAN learns tailored representations for both code snippets and documentation by aligning code-documentation data pairs and capturing structural semantics by masking and predicting entities in the code data. Additionally, the retrieved code snippets and documentation provide necessary information from both program language and natural language to assist the code generation process. CONAN can also be used as an assistant for Large Language Models (LLMs), providing LLMs with external knowledge in shorter code document lengths to improve their effectiveness on various code tasks. It shows the ability of CONAN to extract necessary information and help filter out the noise from retrieved code documents.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {39},
numpages = {25},
keywords = {Code Assistant, Code Generation, Code Retrieval, Retrieval Augmented Language Model}
}

@inproceedings{10.1145/3674912.3674919,
author = {Andreeva, Anna and Lekova, Anna and Tsvetkova, Paulina and Simonska, Miglena},
title = {Expanding the Capabilities of Robot NAO to Enable Human-Like Communication with Children with Speech and Language Disorders},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674919},
doi = {10.1145/3674912.3674919},
abstract = {The humanoid robot NAO is widely used in therapy scenarios for children with neurodevelopmental disorders, however it has poor speech recognition and dialog based on a predefined lexicon that results in limited vocabulary and limited number of predefined dialog scenarios. The integration of Conversational Artificial Intelligence (AI) in NAO can significantly enhance and expand the robot's capabilities for intensive speech and listening exercises for children with speech and language disorders. Applying design-based research, the authors present the ongoing effective iteration in the building-testing cycles of a software architecture for Conversational AI in the robot NAO by integrating different AI cloud services for NLP into NAO's native software. Examining the aims and methods of stakeholders interested in integrating Conversational AI in NAO for speech and language therapy, we revealed several technical and ethical challenges. These challenges were successfully addressed with solutions implemented in the Node-RED platform, such as achieving more accurate and speech recognition, generating human-like text based on a context, and implementing multilingual text-to-speech synthesis. Additionally, implementation raises ethical considerations for both developers and therapists, especially regarding the assessment of risks linked with AI systems. We followed the guidelines set in the European AI Act, which categorizes AI systems according to their associated risk levels.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {63–68},
numpages = {6},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@article{10.14778/3717755.3717772,
author = {Zhao, Fuheng and Deep, Shaleen and Psallidas, Fotis and Floratou, Avrilia and Agrawal, Divyakant and Abbadi, Amr El},
title = {Sphinteract: Resolving Ambiguities in NL2SQL through User Interaction},
year = {2025},
issue_date = {December 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3717755.3717772},
doi = {10.14778/3717755.3717772},
abstract = {Translating natural language questions into SQL queries (NL2SQL) is a challenging task of great practical importance. Prior work has extensively studied how to address NL2SQL using Large Language Models (LLMs) with solutions ranging from careful prompt engineering, to fine-tuning existing LLMs, or even training custom models. However, a remaining challenging problem in NL2SQL is the inherent ambiguity in the natural language questions asked by users. In this paper, we introduce Sphinteract, a framework designed to assist LLMs in generating high-quality SQL answers that accurately reflect the user intent. Our key insight to resolve ambiguity is to take into account minimal user feedback interactively. We introduce the Summarize, Review, Ask (SRA) paradigm, which guides LLMs in identifying ambiguities in NL2SQL tasks and generates targeted questions for the user to answer. We propose three different methods of how to process user feedback and generate SQL queries based on user input. Our experiments on the challenging KaggleDBQA and BIRD benchmarks demonstrate that by means of asking clarification questions to the user, LLMs can efficiently incorporate the feedback, resulting in accuracy improvements of up to 42%.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1145–1158},
numpages = {14}
}

@article{10.1145/3715107,
author = {Molina, Facundo and Gorla, Alessandra and d’Amorim, Marcelo},
title = {Test Oracle Automation in the Era of LLMs},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715107},
doi = {10.1145/3715107},
abstract = {The effectiveness of a test suite in detecting faults highly depends on the quality of its test oracles. Large Language Models (LLMs) have demonstrated remarkable proficiency in tackling diverse software testing tasks. This article aims to present a roadmap for future research on the use of LLMs for test oracle automation. We discuss the progress made in the field of test oracle automation before the introduction of LLMs, identifying the main limitations and weaknesses of existing techniques. Additionally, we discuss recent studies on the use of LLMs for this task, highlighting the main challenges that arise from their use, e.g., how to assess quality and usefulness of the generated oracles. We conclude with a discussion about the directions and opportunities for future research on LLM-based oracle automation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {150},
numpages = {24},
keywords = {Test Oracle Problem, Large Language Models}
}

@inproceedings{10.1145/3654777.3676356,
author = {Vu, Minh Duc and Wang, Han and Chen, Jieshan and Li, Zhuang and Zhao, Shengdong and Xing, Zhenchang and Chen, Chunyang},
title = {GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through Dynamic Interface Exploration and Learning},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676356},
doi = {10.1145/3654777.3676356},
abstract = {Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. For unprecedented tasks, GptVoiceTasker utilises the contextual information and on-screen content to continuously explore and execute the tasks. In addition, the system continually learns from historical user commands to automate subsequent task invocations, further enhancing execution efficiency. From our experiments, GptVoiceTasker achieved 84.5% accuracy in parsing human commands into executable actions and 85.7% accuracy in automating multi-step tasks. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {48},
numpages = {17},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3663433.3663456,
author = {Sinha, Ravi and Solola, Idris and Nguyen, Ha and Swanson, Hillary and Lawrence, LuEttaMae},
title = {The Role of Generative AI in Qualitative Research: GPT-4's Contributions to a Grounded Theory Analysis},
year = {2024},
isbn = {9798400717222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663433.3663456},
doi = {10.1145/3663433.3663456},
abstract = {We present reflections on our experience using a generative AI model in qualitative research, to illuminate the AI's contributions to our analytic process. Our analytic focus was a segment of classroom transcript, which captured a teacher introducing scientific theory-building practices to middle school students. We used a grounded theory approach to produce a fine-grained characterization of the teacher's talk moves during the lesson implementation. Our eventual goal is to build a more nuanced conceptualization of responsive teaching in the context of theory-building activities. We involved GPT-4 during the initial exploratory and later focused coding stages. For our analysis of GPT-4’s contributions to the analytic process, we analyzed our notes and analytic memos, along with video recordings of meetings where we discussed insights in response to GPT-4’s input. We present vignettes to illustrate pivotal moments where AI contributed to the coding process, including code generation, comparison, and refinement. The paper presents our experiences of conducting qualitative research in partnership with generative AI, underscoring the role that emerging technologies can play in the analysis of data and the development of grounded theory.},
booktitle = {Proceedings of the 2024 Symposium on Learning, Design and Technology},
pages = {17–25},
numpages = {9},
keywords = {Generative AI, Grounded theory, Qualitative methodologies, Responsive teaching},
location = {Delft, Netherlands},
series = {LDT '24}
}

@inproceedings{10.1145/3691620.3695480,
author = {Chen, Jiachi and Zhong, Qingyuan and Wang, Yanlin and Ning, Kaiwen and Liu, Yongkun and Xu, Zenan and Zhao, Zhe and Chen, Ting and Zheng, Zibin},
title = {RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695480},
doi = {10.1145/3691620.3695480},
abstract = {Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36% in text-to-code scenario and 11.52% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71%; ChatGPT-4 has a refusal rate of only 35.73%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {995–1006},
numpages = {12},
keywords = {large language models, malicious code, code generation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3728938,
author = {Ji, Zhenlan and Ma, Pingchuan and Li, Zongjie and Wang, Zhaoyu and Wang, Shuai},
title = {Causality-Aided Evaluation and Explanation of Large Language Model-Based Code Generation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728938},
doi = {10.1145/3728938},
abstract = {While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLM)-based code generation, where LLMs, deemed a complex and powerful black-box model, are instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency.    Inspired by recent progress in causality analysis and its software engineering applications, this paper proposes a causality-driven approach to systematically analyze prompt-code causal relationships. However, this endeavor faces three key technical challenges: (1) representing textual prompts and code in a canonical form, (2) establishing causal relations between high-level concepts and code features, and (3) systematically analyzing diverse prompt variations. To address these challenges, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over four popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA061},
numpages = {24},
keywords = {Causality, Code Generation, Explainability, Large Language Models}
}

@inproceedings{10.1145/3627673.3679987,
author = {Liu, Dairui and Yang, Boming and Du, Honghui and Greene, Derek and Hurley, Neil and Lawlor, Aonghus and Dong, Ruihai and Li, Irene},
title = {RecPrompt: A Self-tuning Prompting Framework for News Recommendation Using Large Language Models},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679987},
doi = {10.1145/3627673.3679987},
abstract = {News recommendations heavily rely on Natural Language Processing (NLP) methods to analyze, understand, and categorize content, enabling personalized suggestions based on user interests and reading behaviors. Large Language Models (LLMs) like GPT-4 have shown promising performance in understanding natural language. However, the extent of their applicability to news recommendation systems remains to be validated. This paper introduces RecPrompt, the first self-tuning prompting framework for news recommendation, leveraging the capabilities of LLMs to perform complex news recommendation tasks. This framework incorporates a news recommender and a prompt optimizer that applies an iterative bootstrapping process to enhance recommendations through automatic prompt engineering. Extensive experimental results with 400 users show that RecPrompt can achieve an improvement of 3.36% in AUC, 10.49% in MRR, 9.64% in nDCG@5, and 6.20% in nDCG@10 compared to deep neural models. Additionally, we introduce TopicScore, a novel metric to assess explainability by evaluating LLM's ability to summarize topics of interest for users. The results show LLM's effectiveness in accurately identifying topics of interest and delivering comprehensive topic-based explanations.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3902–3906},
numpages = {5},
keywords = {automatic prompt engineering, large language models, news recommendation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@article{10.1145/3731756,
author = {Ma, Qianou and Peng, Weirui and Yang, Chenyang and Shen, Hua and Koedinger, Kenneth and Wu, Tongshuang},
title = {What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3731756},
doi = {10.1145/3731756},
abstract = {Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot) needs humans to clearly articulate customized requirements (e.g., “start the response with a tl;dr”). However, existing prompt engineering instructions often lack focused training on requirement articulation and instead tend to emphasize increasingly automatable strategies (e.g., tricks like adding role-plays and “think step-by-step”). To address the gap, we introduce Requirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human attention on generating clear, complete requirements during prompting. We implement ROPE through an assessment and training suite that provides deliberate practice with LLM-generated feedback. In a randomized controlled experiment with 30 novices, ROPE significantly outperforms conventional prompt engineering training (20% vs. 1% gains), a gap that automatic prompt optimization cannot close. Furthermore, we demonstrate a direct correlation between the quality of input requirements and LLM outputs. Our work paves the way to empower more end-users to build complex LLM applications.},
note = {Just Accepted},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
keywords = {LLM, Human-AI Interaction, Prompt Engineering, Requirement Engineering, End-User Programming}
}

@inproceedings{10.1145/3670474.3685966,
author = {Vijayaraghavan, Prashanth and Nitsure, Apoorva and Mackin, Charles and Shi, Luyao and Ambrogio, Stefano and Haran, Arvind and Paruthi, Viresh and Elzein, Ali and Coops, Dan and Beymer, David and Baldwin, Tyler and Degan, Ehsan},
title = {Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685966},
doi = {10.1145/3670474.3685966},
abstract = {Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets - VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {28},
numpages = {10},
keywords = {Chain-of-Descriptions, LLM, VHDL, VHDL Code Generation, VHDL Code Summarization, VHDL-Eval, VHDL-Xform},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@inproceedings{10.1145/3661638.3661653,
author = {Wei, Zizhong and Guo, Dongsheng and Huang, Dengrong and Zhang, Qilai and Zhang, Sijia and Jiang, Kai and Li, Rui},
title = {Detecting and Mitigating the Ungrounded Hallucinations in Text Generation by LLMs},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661653},
doi = {10.1145/3661638.3661653},
abstract = {Large language models (LLMs) have achieved impressive success in generating fluent and coherent texts in natural language. However, the presence of inaccurate or low-quality data can unintentionally lead to the retention of incorrect knowledge, resulting in hallucinations that hinder progress in content generation. In this paper, we propose a comprehensive framework aimed at detecting and mitigating these hallucinations. Our approach uses Named Entity Recognition (NER) and Entity Relationship (ER) models to identify hallucination entities and sentences during the detection phase. Furthermore, by incorporating prompt engineering, we effectively correct these hallucination sentences using LLM in the mitigation phase. Tests on real articles confirm the effectiveness of our approach in rectifying LLM-associated hallucinations without adding new ones, thereby enhancing their reliability and credibility.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {77–81},
numpages = {5},
location = {Mianyang, China},
series = {AISNS '23}
}

@inproceedings{10.1145/3660650.3660673,
author = {Rajabi, Parsa and Kerslake, Chris},
title = {Can You Spot the AI? Incorporating GenAI into Technical Writing Assignments},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660673},
doi = {10.1145/3660650.3660673},
abstract = {In an effort to foster critical reflection on the usage of generative AI (genAI) during computer science writing assignments, this three-part assignment challenges students to predict whether their peers can detect which essays are generated using AI. Implemented as part of a third-year professional responsibility and technical writing course for N=200 students during Spring 2024, students individually generated two short persuasive essays, one using genAI and the other without. They then combined the two essays into a single document and submitted it for peer-review. Additionally, they formulated a guess on whether their peers would be able to detect which essay was generated as well as a rationale for their guess. Following the peer-review process, students reflected on their own experience trying to detect which essays were generated as well as the outcome of their guess about their peers abilities as well. Feedback indicates its effectiveness in engaging students in their understanding of the potentials and limitations of genAI. Recommended prerequisites include a clear course AI-usage policy and a brief overview of genAI prompt engineering.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {23},
numpages = {2},
keywords = {AI Literacy, AI in Education, AI-usage Policy, ChatGPT, Generative AI, Technical Writing},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1145/3641237.3691661,
author = {Muthyala, Mayank P. and Lauer, Claire and Carradini, Stephen},
title = {So You Want to Build a Chatbot?: A Systematic Case Study Comparing the Design and Development of Two Water Chatbots},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691661},
doi = {10.1145/3641237.3691661},
abstract = {Chatbots have become a popular method through which to deliver conversational-style information to users about a range of topics, including providing customer service, news and weather updates, educational content, and medical information. This article compares two chatbots created with different methods, including via custom architecture and custom GPT to determine the strengths and limitations of the development methods. The bots that our research team developed were built to deliver information about water and drought to Arizona residents. We compare the initial setup process, customization capabilities, the training process, prompt engineering requirements, file handling, costs, and outputs of each bot. The custom architecture bot offers the flexibility and control of answers, but it costs more than its comparator and takes more time. The custom GPT requires little experience with Large Language Models (LLMs) and no experience with coding, but offers less control. Because we recognize that public agencies often don't have the expertise or funding to build a fully-customized bot architecture, we conclude with suggestions about the contexts and purposes or which each type of bot should be developed.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {128–137},
numpages = {10},
keywords = {AI, API, Artificial intelligence, Chatbot, Drought, GPT, Open AI, Science Communication, User Experience, Water},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@inproceedings{10.1145/3691621.3694934,
author = {Siddiq, Mohammed Latif and da Silva Santos, Joanna Cecilia and Devareddy, Sajith and Muller, Anna},
title = {SALLM: Security Assessment of Generated Code},
year = {2024},
isbn = {9798400712494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691621.3694934},
doi = {10.1145/3691621.3694934},
abstract = {With the growing popularity of Large Language Models (LLMs) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate LLMs do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. Second, existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations. Therefore, in this paper, we described Sallm, a framework to benchmark LLMs' abilities to generate secure code systematically. This framework has three major components: a novel dataset of security-centric Python prompts, configurable assessment techniques to evaluate the generated code, and novel metrics to evaluate the models' performance from the perspective of secure code generation.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
pages = {54–65},
numpages = {12},
keywords = {security evaluation, large language models, pre-trained transformer model, metrics},
location = {Sacramento, CA, USA},
series = {ASEW '24}
}

@inbook{10.1145/3676536.3676801,
author = {Xu, Ke and Sun, Jialin and Hu, Yuchen and Fang, Xinwei and Shan, Weiwei and Wang, Xi and Jiang, Zhe},
title = {MEIC: Re-thinking RTL Debug Automation using LLMs},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676801},
abstract = {The deployment of Large Language Models (LLMs) for code debugging (e.g., C and Python) is widespread, benefiting from their ability to understand and interpret intricate concepts. However, in the semiconductor industry, utilising LLMs to debug Register Transfer Level (RTL) code is still insufficient, largely due to the underrepre-sentation of RTL-specific data in training sets. This work introduces a novel framework, Make Each Iteration Count (MEIC), which contrasts with traditional one-shot LLM-based debugging methods that heavily rely on prompt engineering, model tuning, and model training. MEIC utilises LLMs in an iterative process to overcome the limitation of LLMs in RTL code debugging, which is suitable for identifying and correcting both syntax and function errors, while effectively managing the uncertainties inherent in LLM operations. To evaluate our framework, we provide an open-source dataset comprising 178 common RTL programming errors. The experimental results demonstrate that the proposed debugging framework achieves fix rate of 93% for syntax errors and 78% for function errors, with up to 48x speedup in debugging processes when compared with experienced engineers. The Repo. of dataset and code: https://github.com/SEU-ACAL/reproduce-MEIC-ICCAD.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {100},
numpages = {9}
}

@inproceedings{10.1145/3613904.3641965,
author = {Calle, Paul and Shao, Ruosi and Liu, Yunlong and H\'{e}bert, Emily T and Kendzor, Darla and Neil, Jordan and Businelle, Michael and Pan, Chongle},
title = {Towards AI-Driven Healthcare: Systematic Optimization, Linguistic Analysis, and Clinicians’ Evaluation of Large Language Models for Smoking Cessation Interventions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641965},
doi = {10.1145/3613904.3641965},
abstract = {Creating intervention messages for smoking cessation is a labor-intensive process. Advances in Large Language Models (LLMs) offer a promising alternative for automated message generation. Two critical questions remain: 1) How to optimize LLMs to mimic human expert writing, and 2) Do LLM-generated messages meet clinical standards? We systematically examined the message generation and evaluation processes through three studies investigating prompt engineering (Study 1), decoding optimization (Study 2), and expert review (Study 3). We employed computational linguistic analysis in LLM assessment and established a comprehensive evaluation framework, incorporating automated metrics, linguistic attributes, and expert evaluations. Certified tobacco treatment specialists assessed the quality, accuracy, credibility, and persuasiveness of LLM-generated messages, using expert-written messages as the benchmark. Results indicate that larger LLMs, including ChatGPT, OPT-13B, and OPT-30B, can effectively emulate expert writing to generate well-written, accurate, and persuasive messages, thereby demonstrating the capability of LLMs in augmenting clinical practices of smoking cessation interventions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {436},
numpages = {16},
keywords = {Computational Linguistic Analysis, Expert Review, Large Language Model, Message Generation, Smoking Cessation Intervention},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3640794.3665570,
author = {Guyre, Melissa and Holland, Liz and Shah, Nirva and Divekar, Rahul R.},
title = {Prompt Engineering an LLM into Roleplaying a Management Coach: a Short Guide by and for Non-NLP Experts},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665570},
doi = {10.1145/3640794.3665570},
abstract = {Large Language Models (LLMs) combined with prompt engineering have democratized the creation of chatbots, thereby making it possible for domain experts to be directly part of the chatbot-building process. This paper describes the process followed by management domain experts who built a chatbot to coach new managers. We describe the information sources we used as context, the prompts, and the edge cases that led to a viable management coach chatbot. We describe our process as we recognize that while many role-play chatbots exist, their creators rarely share knowledge about the process or the prompts, thereby hindering replicability. In our paper, we share this knowledge so any management expert or researcher can create a chatbot customized to reflect their management values rather than relying on an opaque product built by a third party. Further, we share results from our pilot tests, where external management experts and to-be managers reviewed our chatbot.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {49},
numpages = {10},
keywords = {Management coaching bots, prompting process},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@article{10.5555/3737313.3737334,
author = {Fernandez, Amanda S. and Patrick, David and Gomez, Mauricio and Cornell, Kimberly A.},
title = {Incorporating LLM Activities into Established CS1 Curriculum: An Experience Report},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {8},
issn = {1937-4771},
abstract = {Large Language Models (LLMs), including Gemini, CoPilot, and ChatGPT, have experienced significant growth in usage and adoption in recent years. As these models become more sophisticated, particularly in code generation capabilities, educators need to adapt their CS1 courses. In this experience report, we share observations we made while designing and teaching LLM activities for CS1 students at two academic institutions during the spring 2024 term. Drawing on recent research, our activities consist of four short 10-15 minute exercises that guide students in how to properly utilize LLMs within their CS1 coursework. These activities can be easily added to the existing CS1 course curriculum to supplement the existing course materials. Post-activity surveys indicated a positive impact on students' understanding of CS concepts and indicated enthusiasm for learning how to use LLMs safely in programming.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {79–93},
numpages = {15}
}

@inproceedings{10.1145/3704217.3705008,
author = {Zhai, Nai-ji and Tuo, Xianyi and Jin, Zhishuo},
title = {Addressing Past Tense Temporal Attacks in Large Language Models through Multiple Methods},
year = {2025},
isbn = {9798400707094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704217.3705008},
doi = {10.1145/3704217.3705008},
abstract = {Several previous studies highlight the problems that several state-of-art large language models are vulnerable under past tense attack (e.g.,” How to make a Molotov cocktail?” to” How did people make a Molotov cocktail?”). In this paper, we try several methods to address this problem on GPT-3.5-Turbo. The single prompt-engineering method reached a 1% attack success rate&nbsp;and a&nbsp;17% over-refusal rate, and llama-2-7b trained on the Latent Adversarial Training method reached a 0% asr. However, its over-refusal rate was 48%. As an alternative method, we tried to build an advisor with a filter based on LLM, which can perform internet searching to check the safety of the content and provide more information with more timely related resources. With more information, the model can give more detailed insight even if it gives a possible refusal with “Sorry.” Source code at https://github.com/t0rit0/2024-summer-project},
booktitle = {Proceedings of the 2024 8th International Conference on E-Society, E-Education and E-Technology},
pages = {106–111},
numpages = {6},
keywords = {Content Safety, Contextual Analysis, Ethical AI, Large Language Models (LLMs), Model Robustness, Past Tense Temporal Attack, Prompt Engineering},
location = {
},
series = {ESET '24}
}

@inproceedings{10.1145/3698385.3699876,
author = {Jiang, Baozheng and Zhang, Haoxiang and Li, Yanxia and Zhou, Hexiao and Xiao, Zexiao and He, Sijia and Qiu, Wenying and Li, You},
title = {A Practical Investigation of the Accuracy of Large Language Models in Various Industrial Application Scenarios},
year = {2024},
isbn = {9798400712975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698385.3699876},
doi = {10.1145/3698385.3699876},
abstract = {Large language models have revolutionized natural language processing, facilitating a wide range of industrial applications. However, their precision and reliability in executing specific tasks across various industries remain incompletely understood. This study investigates the performance of these models across eight industrial scenarios, including code generation, safety monitoring, industrial prototype design, industrial knowledge inquiry, etc. By assessing their accuracy in these domains, the research highlights their practical utility, inherent limitations, and pinpoints areas for enhancement to optimize their deployment in industrial settings.},
booktitle = {Proceedings of the First International Workshop on IoT Datasets for Multi-Modal Large Model},
pages = {44–49},
numpages = {6},
keywords = {Industrial Applications, Large Language Models, Model Accuracy Evaluation},
location = {Hangzhou, China},
series = {IOTMMIM '24}
}

@inproceedings{10.1145/3716368.3735300,
author = {Jha, Manvi and Wan, Jiaxin and Zhang, Huan and Chen, Deming},
title = {PREFACE - A Reinforcement Learning Framework for Code Verification via LLM Prompt Repair},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735300},
doi = {10.1145/3716368.3735300},
abstract = {Large Language Models (LLMs) have emerged as powerful tools for code generation. Yet, they often struggle to produce code that is both syntactically and semantically correct, particularly when correctness must be formally verified. Addressing this gap, we present a novel, model-agnostic framework that couples LLMs with a lightweight reinforcement learning (RL) agent to enable scalable, robust, and formally verifiable code generation without the need for costly model fine-tuning. Centered on the generation of Dafny code, a formally verifiable programming language, our system initiates with LLM-generated code, which is rigorously evaluated within an integrated verification environment. Upon failure, we feed the erroneous code and error metadata to an RL agent trained to explore the prompt-code space (i.e., the set of possible prompt edits paired with their resulting generated code variants). This agent strategically selects corrective prompts to minimize verification iterations, effectively steering the LLM toward correct outputs using its latent capabilities. Once verified, Dafny code can be systematically translated to C for high-level synthesis (HLS), ensuring correctness-by-construction in downstream hardware design workflows. On a 100‑task benchmark, PREFACE’s error‑guided prompt refinement raises verification success by up to 21% — 14% for ChatGPT‑4o, 17% for ChatGPT‑o1‑mini, 10% for Qwen2.5‑Coder‑14B, 4% for Qwen2.5‑7B, and 21% for Gemini‑2‑Flash—demonstrating substantial gains across diverse LLMs.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {547–553},
numpages = {7},
keywords = {Dafny, Formal verification, Large Language Models (LLMs), Reinforcement learning, Prompt optimization, Automated reasoning},
location = {
},
series = {GLSVLSI '25}
}

@inbook{10.1145/3728725.3728728,
author = {Yang, Kai and Shi, Huaifeng and Wang, Rui and Zhou, Lei and Peng, Jiahui and Liu, Chaofan},
title = {Trend Decomposition Enhanced Large Language Model for Non-Stationary Network Traffic Prediction},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728728},
abstract = {Regular network traffic prediction techniques, which generally rely on data normalization, often fail to account for the non-stable characteristics of network traffic, leading to a decline in estimate reliability. A novel approach to this problem is the Trend Decomposition Enhanced Large Language Model for Non-Static Network Traffic Prediction (TD-LLM). Applying trend decomposition as the preparation approach, the proposed strategy first divides raw information into resolved and non-fixed parts. The predetermined portion is standardized and processed, implementing a multi-head cross-attention mechanism, while a better self-attention mechanism improves the non-stationary element. Prompt engineering is used to create specially designed causes that serve as a guideline for the design throughout the forecast process. Finally, the processed data and creates are integrated into the large language model to produce the predicted results. In contrast to foundation models like Time- LLM, LSTM, Informer, and Transformer, the TD-LLM type drastically reduces mean squared error for network traffic prediction across many regions, according to exploratory evaluations, with decreases of 19.73 %, 54.71 %, 76.88 %, and 61.48 %, respectively.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {12–19},
numpages = {8}
}

@article{10.1145/3717061,
author = {Yang, Zezhou and Chen, Sirong and Gao, Cuiyun and Li, Zhenhao and Hu, Xing and Liu, Kui and Xia, Xin},
title = {An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3717061},
doi = {10.1145/3717061},
abstract = {Code generation aims to automatically generate code snippets of specific programming language according to natural language descriptions. The continuous advancements in deep learning, particularly pre-trained models, have empowered the code generation task to achieve remarkable performance. One main challenge of pre-trained models for code generation is the semantic gap between developers’ natural language requirements and source code. To address the issue, prior studies typically adopt a retrieval-augmented framework for the task, where the similar code snippets collected by a retrieval process can be leveraged to help understand the requirements and provide guidance for the generation process. In a retrieval-augmented framework, similar data can be retrieved from the database using a retrieval algorithm, and original input data can be fused with retrieved data by different fusion strategies. However, there is a lack of systematic study on the application of this framework for code generation, including the impact of the final generated results and the specific usage of the framework. In this paper, we choose three popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to assess the impact of the quality and utilization of retrieved code on the retrieval-augmented framework. Our analysis shows that the retrieval-augmented framework is beneficial for improving the performance of the existing pre-trained models. We also provide suggestions on the utilization of the retrieval-augmented code generation framework: BM25 and Sequential Integration Fusion are recommended due to their convenience and superior performance. Sketch Filling Fusion, which extracts a sketch of relevant code, could help the model improve its performance further. Additionally, we conduct experiments to investigate the influence of the retrieval-augmented framework on large language models for code generation, showing the effectiveness of the framework, and we discuss the trade-off between performance improvement and computational costs in each phase within the framework.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {code generation, retrieval-augmented methods, empirical study}
}

@inproceedings{10.1145/3701551.3703562,
author = {Wu, Qinyuan and Khan, Mohammad Aflah and Das, Soumi and Nanda, Vedant and Ghosh, Bishwamittra and Kolling, Camila and Speicher, Till and Bindschaedler, Laurent and Gummadi, Krishna and Terzi, Evimaria},
title = {Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3703562},
doi = {10.1145/3701551.3703562},
abstract = {In this paper, we focus on the challenging task of reliably estimating factual knowledge that is embedded inside large language models (LLMs). To avoid reliability concerns with prior approaches, we propose to eliminate prompt engineering when probing LLMs for factual knowledge. Our approach, called Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the in-context learning ability of LLMs to communicate both the factual knowledge question as well as the expected answer format. Our knowledge estimator is both conceptually simpler (i.e., doesn't depend on meta-linguistic judgments of LLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ZP-LKE. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts. Code available at: https://github.com/QinyuanWu0710/ZeroPrompt_LKE},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {754–763},
numpages = {10},
keywords = {in-context learning, knowledge extraction, large language models},
location = {Hannover, Germany},
series = {WSDM '25}
}

@article{10.1145/3745766,
author = {Wen, Jinfeng and Chen, Zhenpeng and Zhu, Zixi and Sarro, Federica and Liu, Yi and Ping, Haodi and Wang, Shangguang},
title = {LLM-Based Misconfiguration Detection for AWS Serverless Computing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3745766},
doi = {10.1145/3745766},
abstract = {Serverless computing is a popular cloud computing paradigm that enables developers to build applications at the function level, known as serverless applications. The Serverless Application Model (AWS SAM) is the most widely adopted configuration schema. However, misconfigurations pose a significant challenge due to the complexity of serverless configurations and the limitations of traditional data-driven techniques. Recent advancements in Large Language Models (LLMs), pre-trained on large-scale public data, offer promising potential for identifying and explaining misconfigurations. In this paper, we present SlsDetector, the first framework that harnesses the capabilities of LLMs to perform static misconfiguration detection in serverless applications. SlsDetector utilizes effective prompt engineering with zero-shot prompting to identify configuration issues. It designs multi-dimensional constraints aligned with serverless configuration characteristics and leverages the Chain of Thought technique to enhance LLM inferences, alongside generating structured responses. We evaluate SlsDetector on a curated dataset of 110 configuration files, which includes correct configurations, real-world misconfigurations, and intentionally injected errors. Our results show that SlsDetector, based on ChatGPT-4o (one of the most representative LLMs), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, outperforming state-of-the-art data-driven methods by 53.82, 17.40, and 49.72 percentage points, respectively. We further investigate the generalization capability of SlsDetector across recent LLMs, including Llama 3.1 (405B) Instruct Turbo, Gemini 1.5 Pro, and DeepSeek V3, with consistently high effectiveness.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Serverless computing, Software configuration, Large language model}
}

@inproceedings{10.1145/3663529.3663829,
author = {Toslali, Mert and Snible, Edward and Chen, Jing and Cha, Alan and Singh, Sandeep and Kalantar, Michael and Parthasarathy, Srinivasan},
title = {AgraBOT: Accelerating Third-Party Security Risk Management in Enterprise Setting through Generative AI},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663829},
doi = {10.1145/3663529.3663829},
abstract = {In the contemporary business landscape, organizations often rely on third-party services for many functions, including IT services, cloud computing, and business processes. To identify potential security risks, organizations conduct rigorous assessments before engaging with third-party vendors, referred to as Third-Party Security Risk Management (TPSRM). Traditionally, TPSRM assessments are executed manually by human experts and involve scrutinizing various third-party documents such as System and Organization Controls Type 2 (SOC 2) reports and reviewing comprehensive questionnaires along with the security policy and procedures of vendors—a process that is time-intensive and inherently lacks scalability.     AgraBOT, a Retrieval Augmented Generation (RAG) framework, can assist TPSRM assessors by expediting TPSRM assessments and reducing the time required from days to mere minutes. AgraBOT utilizes cutting-edge AI techniques, including information retrieval (IR), large language models (LLMs), multi-stage ranking, prompt engineering, and in-context learning to accurately generate relevant answers from third-party documents to conduct assessments. We evaluate AgraBOT on seven real TPSRM assessments, consisting of 373 question-answer pairs, and attain an F1 score of 0.85.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {74–79},
numpages = {6},
keywords = {AI, Document Understanding, LLM, RAG, TPSRM},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3717383.3717394,
author = {Alam, Md Tauseef and Goswami, Sorbajit and Singh, Khushi and Halder, Raju and Maiti, Abyayananda and Banerjee, Soumyadip},
title = {SolGen: Secure Smart Contract Code Generation Using Large Language Models Via Masked Prompting},
year = {2025},
isbn = {9798400714245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717383.3717394},
doi = {10.1145/3717383.3717394},
abstract = {Owing to the swift advancement of technology and the unfamiliarity of the execution environment, the development of Solidity smart contracts from scratch often results in significant vulnerabilities. In contrast, automated code generation enhances productivity, minimizes development time, and enables developers to focus on high-level tasks and fundamental logic. In consideration of these two viewpoints, this paper examines the utilization of large language models (LLMs) for the automatic generation of Solidity smart contracts based on specified criteria, while simultaneously ensuring the elimination of vulnerabilities through a novel masking strategy. To achieve this, we propose SolGen, a framework for generating secure Solidity smart contract code using LLMs. We assess the performance of existing LLMs (i.e. ChatGPT and Meta AI) for secure Solidity code generation. Our research indicates that ChatGPT outperforms Meta AI in performance, yielding a greater percentage of syntactically accurate and secure code. Additionally, we examine the impact of temperature adjustment on the security of generated contracts using an open-source LLM, Llama3. Our findings suggest that a temperature setting of 0.7 is optimal for the generation of Solidity code, considerably exceeding the performance of both lower and higher settings (0.1 and 1.2), especially with regard to the compilability of the code.},
booktitle = {Proceedings of the 18th Innovations in Software Engineering Conference},
articleno = {13},
numpages = {11},
keywords = {Smart Contracts, Code Generation, Large Language Models, Prompt Engineering},
location = {
},
series = {ISEC '25}
}

@inproceedings{10.1145/3686215.3688378,
author = {Molto, Joaquin and Fields, Jonathan and Visser, Ubbo and Lisetti, Christine},
title = {An LLM-powered Socially Interactive Agent with Adaptive Facial Expressions for Conversing about Health},
year = {2024},
isbn = {9798400704635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686215.3688378},
doi = {10.1145/3686215.3688378},
abstract = {Virtual Socially Interactive Agents (SIA) have shown great promise for human interactions with computer applications in which not only domain-relevant content is needed, but also the way in which the content is delivered (e.g. socio-emotionally adaptive tutoring agents, socio-emotionally responsive health agents). While recent progress on Large Language Models (LLMs) has made rich verbal interactions possible, LLMs cannot communicate nonverbal social cues through a simple text-based interface. We propose an expressive conversational SIA system, powered by an OpenAI Large Language Model (LLM) for text generation, integrated with a 3D humanoid model with real-time behavior generation of FACS-based facial expressions that mirror the user’s to increase rapport and engagement using HumeAI’s Facial Expression Recognition and Empathic Voice Interface (EVI) models to drive the model’s animations. As a case study, we use prompt-engineering to focus the conversation on discussing health-related behaviors. We ground the generation of the LLM’s questions based on the World Health Organization’s (WHO) Alcohol Use Disorders Identification Test (AUDIT) 10-question inventory, a test that help identify whether someone is at risk of alcohol use disorder.},
booktitle = {Companion Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {75–77},
numpages = {3},
keywords = {Adaptation, Health Information Technologies, LLMs, Virtual Agent},
location = {San Jose, Costa Rica},
series = {ICMI Companion '24}
}

@inproceedings{10.1145/3698322.3698324,
author = {Maranh\~{a}o, Jo\~{a}o Jos\'{e} and Guerra, Eduardo Martins},
title = {A Prompt Pattern Sequence Approach to Apply Generative AI in Assisting Software Architecture Decision-making},
year = {2024},
isbn = {9798400716836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698322.3698324},
doi = {10.1145/3698322.3698324},
abstract = {This paper proposes an approach that employs generative AI, specifically GPT models, to enhance decision-making in software architecture through a sequence of prompt patterns. Five prompt patterns are introduced, each targeting software architects’ specific challenges when navigating complex design decisions. Through a structured and context-aware decision flow, we demonstrate how these patterns can mitigate risks, manage uncertainties, and optimize functional and non-functional requirements. The proposed approach is evaluated in two real-world scenarios and one fictional case, illustrating its practical application in optimizing operations and ensuring scalability, security, and performance. While AI demonstrates transformative potential in aiding architectural practices, we also highlight its limitations, emphasizing the importance of human oversight in validating technical assumptions and avoiding over-reliance on automated tools. This paper contributes to the ongoing dialogue on how AI can be integrated into software architecture to foster more efficient, informed, and context-sensitive decision-making processes for architects.},
booktitle = {Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices},
articleno = {1},
numpages = {12},
keywords = {Prompt Engineering, Prompt Patterns Sequence, Design Patterns, Software Architecture, Artificial Intelligence, AI Generative, Architecture Decision-Making},
location = {
},
series = {EuroPLoP '24}
}

@inproceedings{10.1145/3696630.3728702,
author = {Bose, Dibyendu Brinto},
title = {From Prompts to Properties: Rethinking LLM Code Generation with Property-Based Testing},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728702},
doi = {10.1145/3696630.3728702},
abstract = {Large Language Models (LLMs) have shown promise in automated code generation, but ensuring correctness remains a significant challenge. Traditional unit testing evaluates functional correctness but often fails to capture deeper logical constraints. We apply Property-Based Testing (PBT) as an alternative evaluation strategy to StarCoder and CodeLlama on MBPP and HumanEval. Our results reveal that while pass@k evaluation shows moderate success, PBT exposes additional correctness gaps. A significant portion of generated solutions only partially adhere to correctness properties (30–32%), while 18–23% fail outright. Property extraction is also imperfect, with 9–13% of constraints missing. These findings highlight that unit test-based evaluations may overestimate solution correctness by not capturing fundamental logical errors. Our study demonstrates that combining unit testing with PBT can offer a more comprehensive assessment of generated code correctness, revealing limitations that traditional verification approaches miss.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1660–1665},
numpages = {6},
keywords = {code generation, property-based-testing, large language model(LLM)},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3691620.3695360,
author = {Xu, Jia and Du, Weilin and Liu, Xiao and Li, Xuejun},
title = {LLM4Workflow: An LLM-based Automated Workflow Model Generation Tool},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695360},
doi = {10.1145/3691620.3695360},
abstract = {Workflows are pervasive in software systems where business processes and scientific methods are implemented as workflow models to achieve automated process execution. However, despite the benefit of no/low-code workflow automation, creating workflow models requires in-depth domain knowledge and nontrivial workflow modeling skills, which becomes a hurdle for the proliferation of workflow applications. Recently, Large language models (LLMs) have been widely applied in software code generation given their outstanding ability to understand complex instructions and generate accurate, context-aware code. Inspired by the success of LLMs in code generation, this paper aims to investigate how to use LLMs to automate workflow model generation. We present LLM4Workflow, an LLM-based automated workflow model generation tool. Using workflow descriptions as the input, LLM4Workflow can automatically embed relevant API knowledge and leverage LLM's powerful contextual learning abilities to generate correct and executable workflow models. Its effectiveness was validated through functional verification and simulation tests on a real-world workflow system. LLM4Workflow is open sourced at https://github.com/ISEC-AHU/LLM4Workflow, and the demo video is provided at https://youtu.be/XRQ0saKkuxY.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2394–2398},
numpages = {5},
keywords = {automated workflow model generation, large language models, low code development},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3634737.3661134,
author = {Mousavi, Zahra and Islam, Chadni and Moore, Kristen and Abuadbba, Alsharif and Babar, M. Ali},
title = {An Investigation into Misuse of Java Security APIs by Large Language Models},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3661134},
doi = {10.1145/3634737.3661134},
abstract = {The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1299–1315},
numpages = {17},
keywords = {security API, misuse, ChatGPT, LLM-generated code, software security, secure software development},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3613904.3642647,
author = {Zhang, Chao and Liu, Xuechen and Ziska, Katherine and Jeon, Soobin and Yu, Chi-Lin and Xu, Ying},
title = {Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642647},
doi = {10.1145/3613904.3642647},
abstract = {Mathematical language is a cornerstone of a child’s mathematical development, and children can effectively acquire this language through storytelling with a knowledgeable and engaging partner. In this study, we leverage the recent advances in large language models to conduct free-form, creative conversations with children. Consequently, we developed Mathemyths, a joint storytelling agent that takes turns co-creating stories with children while integrating mathematical terms into the evolving narrative. This paper details our development process, illustrating how prompt-engineering can optimize LLMs for educational contexts. Through a user study involving 35 children aged 4-8 years, our results suggest that when children interacted with Mathemyths, their learning of mathematical language was comparable to those who co-created stories with a human partner. However, we observed differences in how children engaged with co-creation partners of different natures. Overall, we believe that LLM applications, like Mathemyths, offer children a unique conversational experience pertaining to focused learning objectives.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {274},
numpages = {23},
keywords = {Storytelling, child–AI collaboration, children, co-creativity, conversational interfaces, large language models, mathematical language},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3595916.3626351,
author = {Du, Wenlong and Li, Qingquan and Zhou, Jian and Ding, Xu and Wang, Xuewei and Zhou, Zhongjun and Liu, Jin},
title = {FinGuard: A Multimodal AIGC Guardrail in Financial Scenarios},
year = {2024},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3595916.3626351},
doi = {10.1145/3595916.3626351},
abstract = {Recently, the development of foundation models has led to significant advances in the ability of artificial intelligence (AI) to generate multimodal content such as text and images. However, specialized industrial scenarios such as finance, which require high levels of security and compliance, pose challenges for the application of generative AI due to its uncontrollability. To address this issue, we propose FinGuard, a multimodal AI-generated content (AIGC) guardrail specifically designed for financial scenarios. We provide detailed definitions of the general quality, financial compliance, and security dimensions of AIGC, and implement the evaluation and inspection of multimodal AIGC including text and images. Our proposed FinGuard has been applied to a financial marketing application serving hundreds of millions of users.},
booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia},
articleno = {104},
numpages = {3},
keywords = {AIGC, finance, guardrail, multimodal},
location = {Tainan, Taiwan},
series = {MMAsia '23}
}

@inproceedings{10.1145/3657604.3664693,
author = {Henkel, Owen and Hills, Libby and Boxer, Adam and Roberts, Bill and Levonian, Zach},
title = {Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability To Mark Short Answer Questions in K-12 Education},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664693},
doi = {10.1145/3657604.3664693},
abstract = {This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {300–304},
numpages = {5},
keywords = {formative assessment, llms, science education},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3661167.3661221,
author = {Coignion, Tristan and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {A Performance Study of LLM-Generated Code on Leetcode},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661221},
doi = {10.1145/3661167.3661221},
abstract = {This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform’s measurement reliability. We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {79–89},
numpages = {11},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3649158.3657036,
author = {Ahmed, Mohiuddin and Wei, Jinpeng and Al-Shaer, Ehab},
title = {Prompting LLM to Enforce and Validate CIS Critical Security Control},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657036},
doi = {10.1145/3649158.3657036},
abstract = {Proper security control enforcement reduces the attack surface and protects the organizations against attacks. Organizations like NIST and CIS (Center for Internet Security) provide critical security controls (CSCs) as a guideline to enforce cyber security. Automated enforcement and measurability mechanisms for these CSCs still need to be developed. Analyzing the implementations of security products to validate security control enforcement is non-trivial. Moreover, manually analyzing and developing measures and metrics to monitor, and implementing those monitoring mechanisms are resource-intensive tasks and massively dependent on the security analyst's expertise and knowledge. To tackle those problems, we use large language models (LLMs) as a knowledge base and reasoner to extract measures, metrics, and monitoring mechanism implementation steps from security control descriptions to reduce the dependency on security analysts. Our approach used few-shot learning with chain-of-thought (CoT) prompting to generate measures and metrics and generated knowledge prompting for metrics implementation. Our evaluation shows that prompt engineering to extract measures, metrics, and monitoring implementation mechanisms can reduce dependency on humans and semi-automate the extraction process. We also demonstrate metric implementation steps using generated knowledge prompting with LLMs.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {93–104},
numpages = {12},
keywords = {account management., critical security control, llm, prompt engineering},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3640544.3645228,
author = {Guo, Jiajing and Mohanty, Vikram and Hao, Hongtao and Gou, Liang and Ren, Liu},
title = {Can LLMs Infer Domain Knowledge from Code Exemplars? A Preliminary Study},
year = {2024},
isbn = {9798400705090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640544.3645228},
doi = {10.1145/3640544.3645228},
abstract = {As organizations recognize the potential of Large Language Models (LLMs), bespoke domain-specific solutions are emerging, which inherently face challenges of knowledge gaps and contextual accuracy. Prompt engineering techniques such as chain-of-thoughts and few-shot prompting have been proposed to enhance LLMs’ capabilities by dynamically presenting relevant exemplars. Are LLMs able to infer domain knowledge from code exemplars involving similar domain concepts and analyze the data correctly? To investigate this, we curated a synthetic dataset containing 45 tabular databases, each has domain concepts and definitions, natural language data analysis queries, and responses in the form of Python code, visualizations, and insights. Using this dataset, we conducted a within-subjects experiment to evaluate the effectiveness of domain-specific exemplars versus randomly selected, generic exemplars. Our study underscores the significance of tailored exemplars in enhancing LLMs’ accuracy and contextual understanding in domain-specific tasks, paving the way for more intuitive and effective data analysis solutions.},
booktitle = {Companion Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {95–100},
numpages = {6},
keywords = {LLM, LLM evaluation, domain-specific data analysis, large language model, prompt engineering},
location = {Greenville, SC, USA},
series = {IUI '24 Companion}
}

@inproceedings{10.1145/3639592.3639611,
author = {Hussain, Musarrat and Rehman, Ubaid Ur and Nguyen, Tri D.T. and Lee, Sungyoung},
title = {CoT-STS: A Zero Shot Chain-of-Thought Prompting for Semantic Textual Similarity},
year = {2024},
isbn = {9798400716225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639592.3639611},
doi = {10.1145/3639592.3639611},
abstract = {The emergence of Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by changing the focus of technical development from features engineering, architecture engineering, and objective engineering to prompt engineering. The main goal of the prompt engineering is to craft clear and concise instructions, known as input prompts, for LLMs to effectively perform the targeted NLP task. Semantic Textual Similarity (STS) is one such significant NLP task, which aims to assess the similarity between the semantic meanings of two input sentences. Numerous approaches have been proposed in the literature, including syntactic similarity evaluations, word-embedding based methods, and dedicated model training. However, these approaches require substantial effort, such as creating extensive annotated datasets and training dedicated STS models.This research introduces CoT-STS, which aims to customize the use of the chain-of-thought prompting with LLMs for the STS task. We proposed four influential factors as part of the Chain-of-Thought approach, including theme similarity, participating object similarity, similarity of the activity being carried out, and the evaluation of other factors before arriving at the final similarity assessments. The application of the proposed CoT-STS on the BIOSSES dataset achieved a Pearson's correlation of 0.72, surpassing the 0.45 correlation achieved by the standard prompting and the correlation of 0.71 achieved by the existing zero-shot CoT methodology. The result achieved demonstrates the potential of LLMs with an appropriate prompting strategy to significantly improve the performance of the STS task.},
booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference},
pages = {135–139},
numpages = {5},
keywords = {Chain-of-Thought Prompting, Large Language Models, Semantic Textual Similarity},
location = {Kyoto, Japan},
series = {AICCC '23}
}

@inproceedings{10.1145/3652620.3686246,
author = {Siddeshwar, Vaishali and Alwidian, Sanaa and Makrehchi, Masoud},
title = {A Comparative Study of Large Language Models for Goal Model Extraction},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686246},
doi = {10.1145/3652620.3686246},
abstract = {User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {253–263},
numpages = {11},
keywords = {goal-oriented requirement language (GRL), goal modeling, user story, agile development, requirements engineering, large language models (LLMS), GPT-4, llama, cohere},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3728902,
author = {Dong, Jinhao and Sun, Jun and Zhang, Wenjie and Dong, Jin Song and Hao, Dan},
title = {ConTested: Consistency-Aided Tested Code Generation with LLM},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728902},
doi = {10.1145/3728902},
abstract = {Recent advancements in large language models (LLMs) have significantly improved code generation, which generates code snippets automatically based on natural language requirements. Despite achieving state-of-the-art performance, LLMs often struggle to generate accurate and reliable code, requiring developers to spend substantial effort debugging and evaluating the generated output. Researchers have proposed leveraging Consistency to select code that passes more tests (inter-consistency) and demonstrates consistent behavior across more counterparts (intra-consistency). However, since the tests themselves are also generated by LLMs, relying on majority voting based on incorrect tests leads to unreliable results. To address this, we propose a lightweight interaction framework that incorporates user feedback to effectively guide consistency. Our results demonstrate that, with minimal human effort, performance can be significantly improved. In each iteration, we introduce a rank-correct-fix co-evolution process between code and tests. This process iteratively   enhances the quality of both, making the consistency voting between code and tests more reliable.   We evaluate ConTested through extensive experiments, demonstrating its effectiveness across multiple LLMs, including GPT-3.5 and GPT-4o. Our results show improvements of 32.9% over GPT-3.5 and 16.97% over GPT-4o. Additionally, ConTested achieves an 11.1% improvement over the SOTA post-processing technique, MPSC. This improvement is achieved with only a 4-round interaction with users, requiring minimal user effort. A user study further confirms the feasibility and cost-effectiveness of ConTested, highlighting its ability to enhance code generation without introducing substantial overhead.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA027},
numpages = {22},
keywords = {Code Generation, Iterative Interaction, Self-Consistency}
}

@inproceedings{10.5555/3721488.3721690,
author = {Kobzarev, Oleg and Lykov, Artem and Tsetserukou, Dzmitry},
title = {GestLLM: Advanced Hand Gesture Interpretation via Large Language Models for Human-Robot Interaction},
year = {2025},
publisher = {IEEE Press},
abstract = {This paper introduces GestLLM, an advanced system for human-robot interaction that enables intuitive robot control through hand gestures. Unlike conventional systems, which rely on a limited set of predefined gestures, GestLLM leverages large language models and feature extraction via MediaPipe [1] to interpret a diverse range of gestures. This integration addresses key limitations in existing systems, such as restricted gesture flexibility and the inability to recognize complex or unconventional gestures commonly used in human communication.By combining state-of-the-art feature extraction and language model capabilities, GestLLM achieves performance comparable to leading vision-language models while supporting gestures underrepresented in traditional datasets. For example, this includes gestures from popular culture, such as the ''Vulcan salute'' from Star Trek, without any additional pretraining, prompt engineering, etc. This flexibility enhances the naturalness and inclusivity of robot control, making interactions more intuitive and user-friendly.GestLLM provides a significant step forward in gesture-based interaction, enabling robots to understand and respond to a wide variety of hand gestures effectively. This paper outlines its design, implementation, and evaluation, demonstrating its potential applications in advanced human-robot collaboration, assistive robotics, and interactive entertainment.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1413–1417},
numpages = {5},
keywords = {gesture recognition, llm, robot control},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3706598.3713668,
author = {O'Brien, Gabrielle},
title = {How Scientists Use Large Language Models to Program},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713668},
doi = {10.1145/3706598.3713668},
abstract = {Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization. As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development. We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university. Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries. We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {876},
numpages = {16},
keywords = {Code assistant, Copilot, generative AI, program synthesis, data science, data analysis},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3716368.3735172,
author = {Li, Shijie and Fu, Weimin and Zhao, Yifang and Guo, Xiaolong and Jin, Yier},
title = {Intelligence In The Fence: Construct A Privacy and Reliable Hardware Design Assistant LLM},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735172},
doi = {10.1145/3716368.3735172},
abstract = {Large language models (LLMs) have been widely used for code assistance in the software and hardware domains. The trend of training a local model for code generation is growing because of the security concerns of releasing proprietary data to third-party providers. However, due to the lack of high-quality training datasets in the hardware domain, researchers have to rely on commercial LLMs, facing the issue of training data leakage. This paper adheres to the principle of zero data upload to address data privacy concerns. Instead of commercial LLMs, we propose a localized and transparent solution leveraging local LLMs to synthesize data and eliminate data leakage risks. We propose an innovative approach to constructing high-quality data by interpretation. To enable efficient local deployment, we fine-tune compact open-source LLMs. The proposed training process and the new dataset structure help us locally train a hardware design assistant LLM named PrivacyGen. PrivacyGen outperforms GPT-4 in the VerilogEval&nbsp;[21] benchmark and performs similarly to GPT-4 in RTLLM&nbsp;[24] while exhibiting a significantly smaller model size and lower total cost of ownership.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {659–666},
numpages = {8},
keywords = {Domain-Specific Large Language Model, Verilog Code Generation, Dataset Construction, Data Privacy},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3660354.3660356,
author = {Gressel, Gilad and Pankajakshan, Rahul and Mirsky, Yisroel},
title = {Discussion Paper: Exploiting LLMs for Scam Automation: A Looming Threat},
year = {2024},
isbn = {9798400706493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660354.3660356},
doi = {10.1145/3660354.3660356},
abstract = {Large Language Models (LLMs) have enabled powerful new AI capabilities, but their potential misuse for automating scams and fraud poses a serious emerging threat. In this paper, we investigate how LLMs combined with speech synthesis and speech recognition could be leveraged to build automated systems for executing phone scams at scale. Our research reveals that current publicly accessible language models can, through advanced prompt engineering, mimic authorities and seek personal financial information, bypassing existing safeguards. As these models become more widely available, they significantly lower the barriers for executing complex AI-driven scams, including potential future threats like voice cloning for virtual kidnapping. Existing defences, such as passive detection is not suitable for synthetic voice over compressed channels. Therefore, we urgently call for multi-disciplinary research into user education, media forensics, regulatory measures, and AI safety enhancements to combat this growing risk. Without proactive measures, the rise in AI-enabled fraud could undermine consumer trust in the digital and economic landscape, emphasizing the need for a comprehensive strategy to prevent automated fraud.},
booktitle = {Proceedings of the 3rd ACM Workshop on the Security Implications of Deepfakes and Cheapfakes},
pages = {20–24},
numpages = {5},
keywords = {AI Security, Deepfakes, LLM, Vishing},
location = {Singapore, Singapore},
series = {WDC '24}
}

@inproceedings{10.1145/3696630.3731663,
author = {Zi, Yangtian and Li, Luisa and Guha, Arjun and Anderson, Carolyn and Feldman, Molly Q},
title = {``I Would Have Written My Code Differently': Beginners Struggle to Understand LLM-Generated Code},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731663},
doi = {10.1145/3696630.3731663},
abstract = {Large language models (LLMs) are being increasingly adopted for programming work. Prior work shows that while LLMs accelerate task completion for professional programmers, beginning programmers struggle to prompt models effectively. However, prompting is just half of the code generation process– when code is generated, it must be read, evaluated, and integrated (or rejected). How accessible are these tasks for beginning programmers?This paper measures how well beginners comprehend LLM-generated code and explores the challenges students face in judging code correctness. We compare how well students understand natural language descriptions of functions and LLM-generated implementations, studying 32 CS1 students on 160 task instances. Our results show a low per-task success rate of 32.5%, with indiscriminate struggles across demographic populations. Key challenges include barriers for non-native English speakers, unfamiliarity with Python syntax, and automation bias. Our findings highlight the barrier that code comprehension presents to beginning programmers seeking to write code with LLMs.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1479–1488},
numpages = {10},
keywords = {large language models, code comprehension, computer science education, CS1},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.5555/3729857.3729874,
author = {Bandi, Ajay and Blackford, Benjamin and Fellah, Aziz and Linville, Diana and Meyer, Trevor C. and Voss, Robert J.},
title = {Prompting Collaboration: Development of an Multidisciplinary Applied AI Minor Program},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {6},
issn = {1937-4771},
abstract = {Artificial Intelligence (AI) has rapidly transformed industries and research, becoming a driving force for technological innovation and development [1]. As AI continues to grow and change, it is reshaping the way we approach problem-solving, decision-making, and creative processes across various sectors. Northwest Missouri State University is developing a new multidisciplinary AI minor open to all undergraduate students on campus. The program is tailored for students from any discipline who want to explore how AI can be utilized and integrated into their fields such as computer science, humanities, business, sciences, healthcare, agriculture, and education, among others. The curriculum integrates topics such as foundational AI concepts, prompt engineering and writing processes, ethical considerations in AI, AI in the workplace, and a capstone project. This program also promotes interdisciplinary collaboration and emphasizes the ethical use of AI.By the end of the program, students will be able to use AI to enhance efficiency and accuracy in tasks, develop and evaluate effective prompts, apply generative AI tools across various input formats, and assess the ethical considerations of AI in real-world applications. The panel members are experts from diverse fields, including management, humanities, technical writing, and computer science. The panel discusses the development of the AI minor curriculum and explores opportunities to extend the AI curriculum by offering AI certificates for undergraduate and graduate online professional students. By attending this panel, the audience will gain valuable insights into developing comprehensive AI programs, fostering cross-disciplinary innovation, and preparing students to use AI ethically and effectively across diverse fields.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {129–132},
numpages = {4}
}

@inproceedings{10.1145/3672608.3707778,
author = {Kudriavtseva, Arina and Hotak, Nisar Ahmad and Gadyatskaya, Olga},
title = {My Code Is Less Secure with Gen AI: Surveying Developers' Perceptions of the Impact of Code Generation Tools on Security},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707778},
doi = {10.1145/3672608.3707778},
abstract = {Background: Generative AI (GAI) tools like GitHub Copilot and ChatGPT are transforming software development by automating code generation and enhancing developers' productivity. However, since these tools are often trained on open-source repositories, they may inadvertently reproduce vulnerable code, raising concerns about the security of AI-generated outputs. Aims: In this paper, we aim to investigate how developers perceive code security when using GAI tools. Method: We conducted a survey with 105 software developers with diverse experience levels to gather their perceptions regarding the security of generated code and their suggestions for improving it. Results: While developers reported increased development speed when using GAI tools, many spend additional time on security reviews and documentation of the generated code, and they are worried about the overreliance on AI and vulnerabilities in the code. Only about a quarter of the developers expressed confidence in the code generated by AI, and, moreover, experienced developers perceive that their proficiency in secure coding decreases when using GAI tools. Our results provide organizations with a better understanding of the risks associated with GAI tools and help improve their software security programs.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1637–1646},
numpages = {10},
keywords = {secure software development, developer study, generated code security},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3701716.3719642,
author = {Liu, Junwen and Huang, Haikuan and Huang, Gang and Ge, Shuang and Hu, Jinlian},
title = {Multimodal Intent Recognition in E-Commerce: Challenges, Innovations, and Lightweight Solutions},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3719642},
doi = {10.1145/3701716.3719642},
abstract = {The "Multimodal Dialogue System Intent Recognition Challenge", jointly organized by Alibaba Taobao and Tmall Group and the World Wide Web Conference (WWW), focuses on multimodal intent recognition in e-commerce scenarios, aiming to address technical challenges in joint understanding of images and text for customer service applications. During the preliminary round, over 1,500 teams participated, with 11 advancing to the semi-finals and 9 ultimately presenting in the final. Innovative approaches from participants centered on three key directions: multimodal data augmentation (e.g., synthetic sample generation, image-text co-augmentation), model optimization (discriminative fine-tuning, model soup fusion, etc.), and prompt engineering. Significantly, the top three teams elevated the weighted F1-score from a baseline of 0.78 to above 0.9. This improvement was achieved by incorporating a diverse set of techniques, including but not limited to vision-language models, structure-aware retrieval, and hierarchical label optimization. The competition outcomes validate the potential of lightweight models in data-scarce scenarios and provide open-source technical pathways for applying multimodal large language models to e-commerce customer service. These advancements drive progress in fine-grained semantic comprehension, domain adaptation, and efficient inference, offering valuable insights for the industrial deployment of intelligent customer service systems.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {3008–3011},
numpages = {4},
keywords = {RAG, data augmentation, e-commerce customer service, model fusion, multimodal intent recognition, vision-language models},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3627673.3680117,
author = {Xu, Anbang and Yu, Tan and Du, Min and Gundecha, Pritam and Guo, Yufan and Zhu, Xinliang and Wang, May and Li, Ping and Chen, Xinyun},
title = {Generative AI and Retrieval-Augmented Generation (RAG) Systems for Enterprise},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680117},
doi = {10.1145/3627673.3680117},
abstract = {This workshop introduces generative AI applications for enterprise, with a focus on retrieval-augmented generation (RAG) systems. Generative AI is a field of artificial intelligence that can create new content and solve complex problems. RAG systems are a novel generative AI technique that combines information retrieval with text generation to generate rich and diverse responses. RAG systems can leverage enterprise data, which is often specific, structured, and dynamic, to provide customized solutions for various domains. However, enterprise data also poses challenges such as scalability, security, and data quality. This workshop convenes researchers and practitioners to explore RAG and other generative AI systems in real-world enterprise scenarios, fostering knowledge exchange, collaboration, and identification of future directions. Relevant to the CIKM community, the workshop intersects with core areas of data science and machine learning, offering potential benefits across various domains.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5599–5602},
numpages = {4},
keywords = {enterprise application, generation, rag, retrieval},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3731715.3733328,
author = {Han, Bowen and Deng, Shizhuo and Gan, Zehua and Teng, Da and Chen, Dongyue and Jia, Tong},
title = {Ensemble CLIPs: Effective Zero-shot Classification with Hundreds of Multi-modal CLIPs},
year = {2025},
isbn = {9798400718779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731715.3733328},
doi = {10.1145/3731715.3733328},
abstract = {CLIP is a multi-modal model that bridges the gap between images and text, showing remarkable zero-shot inference power and image-text retrieval capabilities. However, it remains a weaker classifier compared to fine-tuned CLIP on downstream task datasets. Therefore, we aim to leverage ensemble learning to further explore CLIP's zero-shot potential. Current ensemble learning method utilizes different CLIP encoder backbones as weak classifiers, achieving excellent results. However, the number of weak classifiers is limited to the available CLIP backbones, which we believe is insufficient to fully unlock CLIP's capabilities. Inspired by recent advancements in prompt engineering, where large language models (LLMs) such as GPT generate diverse descriptions for each class. These descriptions are concatenated to form robust textual embeddings, leading to improved zero-shot performance. To create more classifiers, we combine LLMs' capabilities with Different Backbones. Specifically, we employ LLMs to generate numerous descriptions. Unlike previous methods, we input these descriptions separately into the text encoder to obtain multiple textual embeddings, which are then freely paired with different backbones, thereby constructing hundreds of weak classifiers. However, this approach introduces a new challenge: the lazy behavior of LLMs may produce meaningless class descriptions, leading to ineffective classifiers that break overall performance. To address this, we further design an algorithm to construct a robust classifier that guides classification and assign appropriate weights to different classifiers. Comprehensive experiments are conducted to demonstrate the effectiveness of our proposed method. Our approach even surpasses some methods that leverage external data.},
booktitle = {Proceedings of the 2025 International Conference on Multimedia Retrieval},
pages = {407–415},
numpages = {9},
keywords = {clip, ensemble learning, multi-modal learning, prompt engineering, zero-shot classification},
location = {Chicago, IL, USA},
series = {ICMR '25}
}

@inproceedings{10.1145/3663649.3664368,
author = {Aerts, Willem and Fletcher, George and Miedema, Daphne},
title = {A Feasibility Study on Automated SQL Exercise Generation with ChatGPT-3.5},
year = {2024},
isbn = {9798400706783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663649.3664368},
doi = {10.1145/3663649.3664368},
abstract = {SQL is the standard for database query languages and is taught in most introductory database courses. Query languages are illustrated and tested through toy examples: small, accessible, instances of databases. These are not always engaging, but coming up with new examples and questions is time-consuming. Existing research in Computer Science Education has shown that Large Language Models (LLMs) can generate coding exercises. However, this has not been demonstrated for SQL yet but could save teachers much time. In this paper, we study whether it is feasible to have ChatGPT-3.5 generate database schemas and associated SQL questions for teachers through a two-part study. Through a survey of educators, we found that creating a story and database schema for the SQL part is more time-consuming than the questions themselves. In our prompt engineering study, we identified prompts that were successful at creating database schemas, mock data, and exercises. However, although ChatGPT could help reduce the time required to create exams, some participants indicated that they are skeptical about using LLMs.},
booktitle = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
pages = {13–19},
numpages = {7},
keywords = {Assessment, ChatGPT, Education, LLM, SQL},
location = {Santiago, AA, Chile},
series = {DataEd '24}
}

@inproceedings{10.1145/3643991.3645074,
author = {Jin, Kailun and Wang, Chung-Yu and Pham, Hung Viet and Hemmati, Hadi},
title = {Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645074},
doi = {10.1145/3643991.3645074},
abstract = {Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {167–171},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inbook{10.1145/3658617.3697621,
author = {Tasnia, Kimia and Rahman, Sazadur},
title = {OPL4GPT: An Application Space Exploration of Optimal Programming Language for Hardware Design by LLM},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697621},
abstract = {Despite the emergence of Large Language Models (LLMs) as potential tools for automating hardware design, the optimal programming language to describe hardware functions remains unknown. Prior works extensively explored optimizing Verilog-based HDL design, which often overlooked the potential capabilities of alternative programming languages for hardware designs. This paper investigates the efficacy of C++ and Verilog as input languages in extensive application space exploration, tasking an LLM to generate implementations for various System-on-chip functional blocks. We proposed an automated Optimal Programming Language (OPL) framework that leverages OpenAI's GPT-4o LLM to translate natural language specifications into hardware descriptions using both high-level and low-level programming paradigms. The OPL4GPT demonstration initially employs a novel prompt engineering approach that decomposes design specifications into manageable submodules, presented to the LLM to generate code in both C++ and Verilog. A closed-loop feedback mechanism automatically incorporates error logs from the LLM's outputs, encompassing both syntax and functionality. Finally, functionally correct outputs are synthesized using either RTL (Register-Transfer Level) for Verilog or High-Level Synthesis for C++ to assess area, power, and performance. Our findings illuminate the strengths and weaknesses of each language across various application domains, empowering hardware designers to select the most effective approach.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {981–987},
numpages = {7}
}

@inproceedings{10.1145/3691620.3695503,
author = {Huang, Junjie and Guo, Daya and Wang, Chenglong and Gu, Jiazhen and Lu, Shuai and Inala, Jeevana Priya and Yan, Cong and Gao, Jianfeng and Duan, Nan and Lyu, Michael R.},
title = {Contextualized Data-Wrangling Code Generation in Computational Notebooks},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695503},
doi = {10.1145/3691620.3695503},
abstract = {Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts' overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose Data-Coder, which encodes data context and code&amp;textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1282–1294},
numpages = {13},
keywords = {code generation, data wrangling, computational notebooks, large language models},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3663408.3663424,
author = {Wang, Tongze and Xie, Xiaohui and Zhang, Lei and Wang, Chuyi and Zhang, Liang and Cui, Yong},
title = {ShieldGPT: An LLM-based Framework for DDoS Mitigation},
year = {2024},
isbn = {9798400717581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663408.3663424},
doi = {10.1145/3663408.3663424},
abstract = {The constantly evolving Distributed Denial of Service&nbsp;(DDoS) attacks pose a significant threat to the cyber realm, which underscores the importance of DDoS mitigation as a pivotal area of research. While existing AI-driven approaches, including deep neural networks, show promise in detecting DDoS attacks, their inability to elucidate prediction rationales and provide actionable mitigation measures limits their practical utility. The advent of large language models (LLMs) offers a novel avenue to overcome these limitations. In this work, we introduce ShieldGPT, a comprehensive DDoS mitigation framework that harnesses the power of LLMs. ShieldGPT comprises four components: attack detection, traffic representation, domain-knowledge injection and role representation. To bridge the gap between the natural language processing capabilities of LLMs and the intricacies of network traffic, we develop a representation scheme that captures both global and local traffic features. Furthermore, we explore prompt engineering specific to the network domain and design two prompt templates that leverage LLMs to produce traffic-specific, comprehensible explanations and mitigation instructions. Our preliminary experiments and case studies validate the effectiveness and applicability of ShieldGPT, demonstrating its potential to enhance DDoS mitigation efforts with nuanced insights and tailored strategies.},
booktitle = {Proceedings of the 8th Asia-Pacific Workshop on Networking},
pages = {108–114},
numpages = {7},
keywords = {Distributed Denial of Service&nbsp;(DDoS), Large Language Model&nbsp;(LLM), ShieldGPT},
location = {Sydney, Australia},
series = {APNet '24}
}

@article{10.1145/3715908,
author = {Li, Jia and Tao, Chongyang and Li, Jia and Li, Ge and Jin, Zhi and Zhang, Huangzhao and Fang, Zheng and Liu, Fang},
title = {Large Language Model-Aware In-Context Learning for Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715908},
doi = {10.1145/3715908},
abstract = {Large Language Models (LLMs) have shown impressive In-Context Learning (ICL) ability in code generation. LLMs take a prompt context consisting of a few demonstration examples and a new requirement as input, and output new programs without any parameter update. Existing studies have found that the performance of ICL-based code generation heavily depends on the quality of demonstration examples and thus arises research on selecting demonstration examples: given a new requirement, a few demonstration examples are selected from a candidate pool, where LLMs are expected to learn the pattern hidden in these selected demonstration examples. Existing approaches are mostly based on heuristics or randomly selecting examples. However, the distribution of randomly selected examples usually varies greatly, making the performance of LLMs less robust. The heuristics retrieve examples by only considering textual similarities of requirements, leading to sub-optimal performance.To fill this gap, we propose a Large language model-Aware selection approach for In-context-Learning-based code generation named LAIL. LAIL uses LLMs themselves to select examples. It requires LLMs themselves to label a candidate example as a positive example or a negative example for a requirement. Positive examples are helpful for LLMs to generate correct programs, while negative examples are trivial and should be ignored. Based on the labeled positive and negative data, LAIL trains a model-aware retriever to learn the preference of LLMs and select demonstration examples that LLMs need. During the inference, given a new requirement, LAIL uses the trained retriever to select a few examples and feed them into LLMs to generate desired programs. We apply LAIL to four widely used LLMs and evaluate it on five code generation datasets. Extensive experiments demonstrate that LAIL outperforms the state-of-the-art (SOTA) baselines by 11.58%, 3.33%, and 5.07% on CodeGen-Multi-16B, 1.32%, 2.29%, and 1.20% on CodeLlama-34B, and achieves 4.38%, 2.85%, and 2.74% improvements on Text-davinci-003 in terms of Pass@1 at MBJP, MBPP, and MBCPP, respectively. In addition to function-level code generation, LAIL improves the performance of LLMs on DevEval, a repository-level code generation dataset, which achieves 10.04%, 8.12%, and 4.63% improvements compared to the SOTA baselines at Pass@1, 3, and 5 on CodeLlama-7B. Human evaluation further verifies that the generated programs of LAIL are superior in correctness, code quality, and maintainability. Besides, LAIL has satisfactory transferability across different LLMs and datasets, where the retriever learned on one LLM (dataset) can be transferred to other LLMs (datasets).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code generation, in-context-learning, large language model}
}

@inproceedings{10.1145/3639476.3639776,
author = {Rukmono, Satrio Adi and Ochoa, Lina and Chaudron, Michel},
title = {Deductive Software Architecture Recovery via Chain-of-thought Prompting},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639776},
doi = {10.1145/3639476.3639776},
abstract = {As software evolves, software architecture recovery techniques can help for effective maintenance. We envision a deductive software architecture recovery approach supported by Large Language Models (LLMs). Unlike existing inductive (bottom-up) recovery techniques, which reconstruct architecture by considering the properties observed at implementation level, our top-down approach starts with architectural properties and seeks their manifestations in the implementation. It employs a known Reference Architecture (RA) and involves two phases: RA definition and code units classification. A proof-of-concept with GPT-4 emulates deductive reasoning via chain-of-thought prompting. It demonstrates the deductive SAR approach, applying it to the Android application K-9 Mail and achieving a 70% accuracy in classifying 54 classes and 184 methods. The future plans focus on evaluating and refining the approach through ground-truth assessments, deeper exploration of reference architectures, and advancing toward automated human-like software architecture explanations. We highlight the potential for LLMs in achieving more comprehensive and explainable software architecture recovery.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {92–96},
numpages = {5},
keywords = {software architecture, software architecture recovery, deductive SAR, chain-of-thought prompting},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3698322.3698348,
author = {Schuckart, Adrian},
title = {GenAI and Prompt Engineering: A Progressive Framework for Empowering the Workforce},
year = {2024},
isbn = {9798400716836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698322.3698348},
doi = {10.1145/3698322.3698348},
abstract = {This paper introduces a sequential framework for integrating Large Language Models like text based GenAI into professional development and training, articulated through three progressive educational patterns: “Pathways to Potential”, “Journey to Application” and “Crafting Proficiency”. Beginning with foundational knowledge, the curriculum progresses to immersive applications and culminates in advanced Prompt Engineering techniques. Each pattern is designed to incrementally enhance participants’ understanding and practical skills with text based GenAI, addressing initial misconceptions, fostering deep technical insights and promoting the application of text based GenAI to complex problem-solving within diverse professional contexts. Through interactive sessions, case studies and hands-on workshops, the framework aims to develop a proficient, innovative workforce capable of leveraging text based GenAI transformative potential. The paper highlights the need for a comprehensive approach to text based GenAI education, emphasizing collaboration, technical understanding and the creation of a self-sustaining learning community. The proposed patterns offer a scalable model for organizational text based GenAI integration, promising significant improvements in operational efficiency and innovation capabilities.},
booktitle = {Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices},
articleno = {12},
numpages = {8},
keywords = {AI in professional development, AI-powered learning, Digital Education, Digital Transformation, GenAI, Generative AI, Human-AI interaction, LLMs, Large language models, Pattern-based education, Problem based learning, Prompt Engineering, Prompt Pattern, Text generation, Workforce Upskilling},
location = {
},
series = {EuroPLoP '24}
}

@inproceedings{10.1145/3728179.3728192,
author = {Guo, Ce and Zhao, Tong},
title = {ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs},
year = {2025},
isbn = {9798400714320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728179.3728192},
doi = {10.1145/3728179.3728192},
abstract = {Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware design, yet writing Hardware Description Language (HDL) code for FPGA implementation remains a complex and time-consuming task. Large Language Models (LLMs) have emerged as a promising tool for HDL generation, but existing benchmarks for LLM-based code generation primarily focus on functional correctness while overlooking hardware resource usage. Furthermore, current benchmarks offer limited diversity and do not fully represent the wide range of real-world FPGA applications. To address these shortcomings, we introduce ResBench, the first resource-focused benchmark explicitly designed to distinguish between resource-optimized and inefficient LLM-generated HDL code. ResBench consists of 56 problems across 12 categories, covering applications from finite state machines to financial computing. Our open-source evaluation framework automatically tests LLMs by generating Verilog code, verifying correctness, and measuring resource usage. The experiments, which primarily analyze Lookup Table (LUT) usage, reveal significant differences among LLMs, demonstrating ResBench’s capability to identify models that generate more resource-optimized FPGA designs.},
booktitle = {Proceedings of the 15th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
pages = {25–34},
numpages = {10},
keywords = {Large Language Models (LLMs), Hardware Description Languages (HDLs), Verilog Code Generation, FPGA Resource Utilization, Automated Benchmarking, Empirical Evaluation of LLMs},
location = {
},
series = {HEART '25}
}

@inproceedings{10.1145/3672608.3707960,
author = {Malandri, Lorenzo and Mercorio, Fabio and Serino, Antonio},
title = {SkiLLMo: Normalized ESCO Skill Extraction through Transformer Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707960},
doi = {10.1145/3672608.3707960},
abstract = {In recent years, natural language processing (NLP) technologies have made a significant contribution in addressing a number of labour market tasks. One of the most interesting challenges is the automatic extraction of competences from unstructured texts.This paper presents a pipeline for efficiently extracting and standardizing skills from job advertisements using NLP techniques. The proposed methodology leverages open-source Transformer and Large Language Models to extract skills and map them to the European labour market taxonomy, ESCO.To address the computational challenges of processing lengthy job advertisements, a BERT model was fine-tuned to identify text segments likely containing skills. This filtering step reduces noise and ensures that only relevant content is processed further. The filtered text is then passed to an LLM, which extracts implicit and explicit hard and soft skills through prompt engineering. The extracted skills are subsequently matched with entries in a vector store containing the ESCO taxonomy to achieve standardization.Evaluation by domain experts shows that the pipeline achieves a precision of 91% for skill extraction, 80% for skill standardization and a combined overall precision of 79%. These results demonstrate the effectiveness of the proposed approach in facilitating structured and standardized skill extraction from job postings.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1969–1978},
numpages = {10},
keywords = {skill extraction, large language models, transformer models, information extraction, labor market},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3643991.3644910,
author = {Lin, Hong Yi and Thongtanunam, Patanamon and Treude, Christoph and Charoenwet, Wachiraphan},
title = {Improving Automated Code Reviews: Learning From Experience},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644910},
doi = {10.1145/3643991.3644910},
abstract = {Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {278–283},
numpages = {6},
keywords = {code review, review comments, neural machine translation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3711919.3728678,
author = {Cheung, Kuen Sum and Kaul, Mayuri and Jahangirova, Gunel and Mousavi, Mohammad Reza and Zie, Eric},
title = {Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development},
year = {2025},
isbn = {9798400714610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711919.3728678},
doi = {10.1145/3711919.3728678},
abstract = {Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.},
booktitle = {Proceedings of the 1st International Workshop on Responsible Software Engineering},
pages = {13–20},
numpages = {8},
keywords = {LLM code generation, carbon footprint, manual code generation, software engineering, sustainability},
location = {Trondheim, Norway},
series = {ResponsibleSE '25}
}

@inproceedings{10.1145/3696630.3730538,
author = {Guimaraes, Everton and Nascimento, Nathalia},
title = {AI in the Software Development Lifecycle: Insights and Open Research Questions},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730538},
doi = {10.1145/3696630.3730538},
abstract = {The rapid advancements in Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping software engineering and automating tasks such as code generation, debugging, testing, and maintenance. AI-powered tools (i.e. ChatGPT, DeepSeek), have demonstrated significant potential in enhancing developer productivity and accelerating software development processes. Integrating AI and LLMs into software engineering presents notable challenges despite these advancements. Concerns regarding the reliability of AI-generated code, security vulnerabilities, and the propagation of biases in training data pose substantial risks. Additionally, ethical considerations, including intellectual property rights, transparency, and the need for human oversight, highlight the complexities of AI adoption in critical software systems. The rapid evolution of these technologies requires continuous adaptation of software engineering methodologies to mitigate risks while maximizing benefits. This paper analyzes AI's role in software engineering, identifying key applications, challenges, and future research directions. We examine AI's impact across various phases of the software development lifecycle, This paper contributes to the ongoing discussion on AI-driven software engineering and outlines a research agenda for navigating this rapidly evolving field.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1353–1357},
numpages = {5},
keywords = {artificial intelligence, large language models (LLMs) software engineering, AI-assisted software development, LeetCode},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3708531,
author = {Wang, Shenao and Zhao, Yanjie and Hou, Xinyi and Wang, Haoyu},
title = {Large Language Model Supply Chain: A Research Agenda},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708531},
doi = {10.1145/3708531},
abstract = {The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This article provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security and privacy (S&amp;P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {147},
numpages = {46},
keywords = {LLM Supply Chain, Large Language Models}
}

@inproceedings{10.1145/3626253.3635356,
author = {AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem},
title = {How can We Leverage Static Analysis and Large Language Models to Engage Students in Software Quality Improvement},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635356},
doi = {10.1145/3626253.3635356},
abstract = {Static analysis tools are frequently used to scan the source code and detect deviations from the project coding guidelines. Yet, their adoption is challenged by their high false positive rate, which makes them not suitable for students and novice developers. However, Large Language Models (LLMs), such as ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including testing, code review, and program comprehension. Such models represent an opportunity to reduce the ambiguity of static analysis tools and support their adoption. Yet, the effectiveness of using static analysis (i.e., PMD) to detect coding issues, and relying on LLMs (i.e., ChatGPT) to explain and recommend fix, has not yet been explored. In this talk, we aim to shed light on our experience in teaching the use of ChatGPT to cultivate a bugfix culture and leverage LLMs to improve software quality in educational settings. We share our findings to support educators in teaching students better code review strategies, and to increase students' awareness about LLM and promote software quality in education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1930},
numpages = {1},
keywords = {computing, education, large language models, quality},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3658644.3670392,
author = {Liu, Zeyan and Yao, Zijun and Li, Fengjun and Luo, Bo},
title = {On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670392},
doi = {10.1145/3658644.3670392},
abstract = {With ChatGPT under the spotlight, utilizing large language models (LLMs) to assist academic writing has drawn a significant amount of debate in the community. In this paper, we aim to present a comprehensive study of the detectability of ChatGPT-generated content within the academic literature, particularly focusing on the abstracts of scientific papers, to offer holistic support for the future development of LLM applications and policies in academia. Specifically, we first present GPABench2, a benchmarking dataset of over 2.8 million comparative samples of human-written, GPT-written, GPT-completed, and GPT-polished abstracts of scientific writing in computer science, physics, and humanities and social sciences. Second, we explore the methodology for detecting ChatGPT content. We start by examining the unsatisfactory performance of existing ChatGPT detecting tools and the challenges faced by human evaluators (including more than 240 researchers or students). We then test the hand-crafted linguistic features models as a baseline and develop a deep neural framework named CheckGPT to better capture the subtle and deep semantic and linguistic patterns in ChatGPT written literature. Last, we conduct comprehensive experiments to validate the proposed CheckGPT framework in each benchmarking task over different disciplines. To evaluate the detectability of ChatGPT content, we conduct extensive experiments on the transferability, prompt engineering, and robustness of CheckGPT.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2236–2250},
numpages = {15},
keywords = {aigc detection, large language models, responsible ai},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inbook{10.1145/3658617.3697616,
author = {Gai, Jiahao and Chen, Hao and Wang, Zhican and Zhou, Hongyu and Zhao, Wanru and Lane, Nicholas and Fan, Hongxiang},
title = {Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697616},
abstract = {Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C++, opening new opportunities for automating software development and enhancing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring automated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs) such as Verilog and SystemVerilog, several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to software programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores leveraging LLMs to generate High-Level Synthesis (HLS)-based hardware design. Although code generation for domain-specific programming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infrastructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pretrained models for HLS-based hardware generation, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then proposed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS- design generation. Comprehensive experiments demonstrate the effectiveness of our methods.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {988–994},
numpages = {7}
}

@inproceedings{10.1145/3699538.3699541,
author = {Korpimies, Kai and Laaksonen, Antti and Luukkainen, Matti},
title = {Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699541},
doi = {10.1145/3699538.3699541},
abstract = {Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {23},
numpages = {7},
keywords = {Large language models, Computer Science Education, User Study, Code generation, Software project},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3650215.3650386,
author = {Zhang, Bo and Xiang, Yang},
title = {Knowledge Base Enhanced ChatGLM for RPA Robot Code Generation},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650386},
doi = {10.1145/3650215.3650386},
abstract = {The advent of ChatGPT demonstrates the powerful NLP capabilities of large language models, and current research efforts are focused on improving the model's performance while minimizing costs. This paper proposes a method for constructing domain-specific knowledge bases to enhance the performance of large language models. By extracting knowledge from the RPA robot code domain and creating a vectorized knowledge base for retrieval and query, the method integrates the knowledge base with the large language model, thereby boosting the performance of the large language model in RPA robot code generation tasks. The RPA robot external knowledge base-enhanced ChatGLM model constructed in this paper achieves a notably superior performance compared to ChatGPT under GPT4’s evaluation. This outcome validates the effectiveness of the proposed approach for enhancing large language models with domain-specific knowledge bases.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {961–965},
numpages = {5},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@inproceedings{10.1145/3661167.3661267,
author = {Bavota, Gabriele},
title = {AI-based Code Generation: Achievements and Open Problems},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661267},
doi = {10.1145/3661167.3661267},
abstract = {Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as GitHub Copilot. In this talk, I will discuss recent successful applications of LLMs for code generation, showing how they are changing the way in which developers approach coding. Then, I will present open problems in the area of AI-based code generators; this part will cover both issues arising from their usage as well as challenges that the research community working in this area are facing (e.g., related to the empirical evaluation of these tools).},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {1},
numpages = {1},
keywords = {AI for Software Engineering, Generative Models},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3690635,
author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
title = {Structured Chain-of-Thought Prompting for Code Generation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3690635},
doi = {10.1145/3690635},
abstract = {Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still cannot satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29% Pass@1 in HumanEval. In this article, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures (i.e., sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs (i.e., gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct- ({) 1.3B, 6.7B, 33B (}) ) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {37},
numpages = {23},
keywords = {Code Generation, Large Language Models, Prompting Engineering}
}

@inproceedings{10.1145/3696630.3728541,
author = {Shi, Kensen and Alt\i{}nb\"{u}ken, Deniz and Anand, Saswat and Christodorescu, Mihai and Gr\"{u}nwedel, Katja and Koenings, Alexa and Naidu, Sai and Pathak, Anurag and Rasi, Marc and Ribeiro, Fredde and Ruffin, Brandon and Sanyam, Siddhant and Tabachnyk, Maxim and Toth, Sara and Tu, Roy and Welp, Tobias and Yin, Pengcheng and Zaheer, Manzil and Chandra, Satish and Sutton, Charles},
title = {Natural Language Outlines for Code: Literate Programming in the LLM Era},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728541},
doi = {10.1145/3696630.3728541},
abstract = {We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL, where a developer can change either code or NL and have the LLM automatically update the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and malware detection.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {150–161},
numpages = {12},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3626111.3628189,
author = {Xiang, Qiao and Lin, Yuling and Fang, Mingjun and Huang, Bang and Huang, Siyong and Wen, Ridi and Le, Franck and Kong, Linghe and Shu, Jiwu},
title = {Toward Reproducing Network Research Results Using Large Language Models},
year = {2023},
isbn = {9798400704154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626111.3628189},
doi = {10.1145/3626111.3628189},
abstract = {Reproducing research results is important for the networking community. The current best practice typically resorts to: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; or (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private ones are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). We first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report our observations and lessons and discuss future open research questions of this proposal.},
booktitle = {Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
pages = {56–62},
numpages = {7},
keywords = {Large language models, Networking systems},
location = {Cambridge, MA, USA},
series = {HotNets '23}
}

@article{10.1145/3714462,
author = {Wang, Chong and Zhang, Jian and Feng, Yebo and Li, Tianlin and Sun, Weisong and Liu, Yang and Peng, Xin},
title = {Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714462},
doi = {10.1145/3714462},
abstract = {Recent code large language models (LLMs) have shown promising performance in generating standalone functions. However, they face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen, an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding descriptions, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one through constrained greedy search.We conduct comprehensive experiments to evaluate ToolGen’s effectiveness in repository-level code generation across three distinct code LLMs: CodeGPT, CodeT5, and CodeLlama. To facilitate this evaluation, we create a benchmark comprising 671 real-world code repositories and introduce two new dependency-based metrics: Dependency Coverage and Static Validity Rate. The results demonstrate that ToolGen significantly improves Dependency Coverage by 31.4% to 39.1% and Static Validity Rate by 44.9% to 57.7% across the three LLMs, while maintaining competitive or improved performance in widely recognized similarity metrics such as BLEU-4, CodeBLEU, Edit Similarity, and Exact Match. On the CoderEval dataset, ToolGen achieves improvements of 40.0% and 25.0% in test pass rate (Pass@1) for CodeT5 and CodeLlama, respectively, while maintaining the same pass rate for CodeGPT. ToolGen also demonstrates high efficiency in repository-level code generation, with latency ranging from 0.63 to 2.34 seconds for generating each function. Furthermore, our generalizability evaluation confirms ToolGen’s consistent performance when applied to diverse code LLMs, encompassing various model architectures and scales.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {repository-level code generation, code LLMs, tool integration}
}

@inproceedings{10.1145/3613905.3651122,
author = {Park, Minju and Kim, Sojung and Lee, Seunghyun and Kwon, Soonwoo and Kim, Kyuseok},
title = {Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651122},
doi = {10.1145/3613905.3651122},
abstract = {As the recent Large Language Models(LLM’s) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM’s in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system’s framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at http://rlearning-its.com.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {123},
numpages = {10},
keywords = {Intelligent Tutoring System, Personalized Learning, Student Modeling},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3724363.3729097,
author = {Smith, David H., IV and Fowler, Max and Denny, Paul and Zilles, Craig},
title = {ReDefining Code Comprehension: Function Naming as a Mechanism for Evaluating Code Comprehension},
year = {2025},
isbn = {9798400715679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724363.3729097},
doi = {10.1145/3724363.3729097},
abstract = { ''Explain in Plain English'' (EiPE) questions are widely used to assess code comprehension skills but are challenging to grade automatically. Recent approaches like Code Generation Based Grading (CGBG) leverage large language models (LLMs) to generate code from student explanations and validate its equivalence to the original code using unit tests. However, this approach does not differentiate between high-level, purpose-focused responses and low-level, implementation-focused ones, limiting its effectiveness in assessing comprehension level. We propose a modified approach where students generate function names, emphasizing the function's purpose over implementation details. We evaluate this method in an introductory programming course and analyze it using Item Response Theory (IRT) to assess the difficulty and discrimination of function naming exercises as exam items and to compare their alignment with traditional EiPE grading standards. We also publish this work as an open source Python package for auto-grading EiPE questions, providing a scalable solution for adoption.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {44–50},
numpages = {7},
keywords = {code comprehension, eipe, eipl, explain in plain english, explain in plain language, function naming, gpt-4o, large language model, llm, solo taxonomy},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@article{10.14778/3685800.3685918,
author = {Ozcan, Fatma},
title = {Harmonizing ML and Databases: A Symphony of Data (VLDB 2024 Keynote)},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685918},
doi = {10.14778/3685800.3685918},
abstract = {Large language models (LLMs) are rapidly transforming the landscape of computing and daily life, demonstrating immense potential across diverse applications like natural language processing, machine translation, and code generation. This talk delves into the impact of LLMs on database research. Specifically, we'll examine how LLMs are fueling innovation in natural language interfaces for data interaction, highlighting current limitations and advocating for semantic data models and enhanced context to improve the accuracy of these solutions. Drawing inspiration from LLMs, we'll introduce a novel paradigm for database cost modeling, leveraging pre-trained models and fine-tuning techniques. We'll share our early-stage prototype, initial results, and outline a research roadmap highlighting numerous exciting challenges in this evolving field.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4556},
numpages = {1}
}

@inproceedings{10.1145/3597926.3598135,
author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
title = {How Effective Are Neural Networks for Fixing Security Vulnerabilities},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598135},
doi = {10.1145/3597926.3598135},
abstract = {Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs’ vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1282–1294},
numpages = {13},
keywords = {Vulnerability, Language Model, Automated Program Repair, AI and Software Engineering},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3641554.3701800,
author = {Shah, Anshul and Chernova, Anya and Tomson, Elena and Porter, Leo and Griswold, William G. and Soosai Raj, Adalbert Gerald},
title = {Students' Use of GitHub Copilot for Working with Large Code Bases},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701800},
doi = {10.1145/3641554.3701800},
abstract = {Large language models (LLMs) are already heavily used by professional software engineers. An important skill for new university graduates to possess will be the ability to use such LLMs to effectively navigate and modify a large code base. While much of the prior work related to LLMs in computing education focuses on novice programmers learning to code, less work has focused on how upper-division students use and trust these tools, especially while working with large code bases. In this study, we taught students about various GitHub Copilot features, including Copilot chat, in an upper-division software engineering course and asked students to add a feature to a large code base using Copilot. Our analysis revealed a novel interaction pattern that we call one-shot prompting, in which students ask Copilot to implement the entire feature at once and spend the next few prompts asking Copilot to debug the code or asking Copilot to regenerate its incorrect response. Finally, students reported significantly more trust in the code comprehension features than code generation features of Copilot, perhaps due to the presence of trust affordances in the Copilot chat that are absent in the code generation features. Our study takes the first steps in understanding how upper-division students use Github Copilot so that our instruction can adequately prepare students for a career in software engineering.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1050–1056},
numpages = {7},
keywords = {github copilot, large code bases, program comprehension, trust},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3696630.3728493,
author = {Zhang, Yiran and Li, Ruiyin and Liang, Peng and Sun, Weisong and Liu, Yang},
title = {Knowledge-Based Multi-Agent Framework for Automated Software Architecture Design},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728493},
doi = {10.1145/3696630.3728493},
abstract = {Architecture design is a critical step in software development. However, creating a high-quality architecture is often costly due to the significant need for human expertise and manual effort. Recently, agents built upon Large Language Models (LLMs) have achieved remarkable success in various software engineering tasks. Despite this progress, the use of agents to automate the architecture design process remains largely unexplored. To address this gap, we envision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework. MAAD uses agents to simulate human roles in the traditional software architecture design process, thereby automating the design process. To empower these agents, MAAD incorporates knowledge extracted from three key sources: 1) existing system designs, 2) authoritative literature, and 3) architecture experts. By envisioning the MAAD framework, we aim to advance the full automation of application-level system development.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {530–534},
numpages = {5},
keywords = {large language model, multi-agent system, software architecture},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1109/ASE56229.2023.00143,
author = {Ren, Xiaoxue and Ye, Xinyuan and Zhao, Dehai and Xing, Zhenchang and Yang, Xiaohu},
title = {From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00143},
doi = {10.1109/ASE56229.2023.00143},
abstract = {Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name Kpc, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {976–987},
numpages = {12},
keywords = {large language model, code generation, knowledge-driven prompt, API misuse},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3658644.3691367,
author = {Ton, Khiem and Nguyen, Nhi and Nazzal, Mahmoud and Khreishah, Abdallah and Borcea, Cristian and Phan, NhatHai and Jin, Ruoming and Khalil, Issa and Shen, Yelong},
title = {Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3691367},
doi = {10.1145/3658644.3691367},
abstract = {This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {5078–5080},
numpages = {3},
keywords = {demonstration system, llms, prompt optimization, secure code},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3706598.3713083,
author = {Krau\ss{}, Veronika and McGill, Mark and Kosch, Thomas and Thiel, Yolanda Maira and Sch\"{o}n, Dominik and Gugenheimer, Jan},
title = {"Create a Fear of Missing Out" - ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713083},
doi = {10.1145/3706598.3713083},
abstract = {With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., “increase the likelihood of us selling our product”). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT’s recommendations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {857},
numpages = {20},
keywords = {ChatGPT, LLM, Deceptive Design, Dark Patterns, Design Inspiration},
location = {
},
series = {CHI '25}
}

@article{10.1145/3714461,
author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and Lo, David and Sahraoui, Houari},
title = {Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714461},
doi = {10.1145/3714461},
abstract = {Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {code generation, large language models, parameter-efficient fine-tuning, quantization, retrieval-augmented generation, empirical study}
}

@inproceedings{10.1145/3676536.3676781,
author = {Xiong, Chenwei and Liu, Cheng and Li, Huawei and Li, Xiaowei},
title = {HLSPilot: LLM-based High-Level Synthesis},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676781},
doi = {10.1145/3676536.3676781},
abstract = {Large language models (LLMs) have catalyzed an upsurge in automatic code generation, garnering significant attention for register transfer level (RTL) code generation. Despite the potential of RTL code generation with natural language, it remains error-prone and limited to relatively small modules because of the substantial semantic gap between natural language expressions and hardware design intent. In response to the limitations, we propose a methodology that reduces the semantic gaps by utilizing C/C++ for generating hardware designs via High-Level Synthesis (HLS) tools. Basically, we build a set of C-to-HLS optimization strategies catering to various code patterns, such as nested loops and local arrays. Then, we apply these strategies to sequential C/C++ code through in-context learning, which provides the LLMs with exemplary C/C++ to HLS prompts. With this approach, HLS designs can be generated effectively. Since LLMs still face problems in determining the optimized pragma parameters precisely, we have a design space exploration (DSE) tool integrated for pragma parameter tuning. Furthermore, we also employ profiling tools to pinpoint the performance bottlenecks within a program and selectively convert bottleneck components to HLS code for hardware acceleration. By combining the LLM-based profiling, C/C++ to HLS translation, and DSE, we have established HLSPilot---the first LLM-enabled high-level synthesis framework, which can fully automate the high-level application acceleration on hybrid CPU-FPGA architectures. According to our experiments on real-world application benchmarks, HLSPilot achieve comparable performance in general and can even outperform manually crafted counterparts, thereby underscoring the substantial promise of LLM-assisted hardware designs.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {226},
numpages = {9},
keywords = {large language model, high-level synthesis, C-to-HLS, code generation},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/3604237.3626902,
author = {Teixeira, Ana Clara and Marar, Vaishali and Yazdanpanah, Hamed and Pezente, Aline and Ghassemi, Mohammad},
title = {Enhancing Credit Risk Reports Generation using LLMs: An Integration of Bayesian Networks and Labeled Guide Prompting},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626902},
doi = {10.1145/3604237.3626902},
abstract = {Credit risk analysis is a process that involves a wide range of complex cognitive abilities. Automating the credit risk analysis process using Large Language Models can bring transformative changes to the finance industry, but not without appropriate measures to ensure trustworthy responses. In this work, we propose a novel prompt-engineering method that enhances the ability of Large Language Models to generate reliable credit risk reports - Labeled Guide Prompting (LGP). LGP consists of: (1) providing annotated few-shot examples to the LLM that denote sets of tokens in an exemplary prompt that are of greater importance when generating sets of tokens in the exemplary response and (2) providing text in the prompt that describes the direction, pathways and interactions between variables from a Bayesian network used for credit risk assessment, thus promoting abductive reasoning. Using data from 100 credit applications, we demonstrate that LGP enables LLMs to generate credit risk reports that are preferred by human credit analysts (in 60-90% of cases) over alternative credit risk reports created by their peers in a blind review. Additionally, we found a statistically significant improvement (p-value &lt; 10− 10) in the insightfulness of the responses generated using LGP when compared to identical prompts without LGP components. We conclude that Labeled Guide Prompting can enhance LLM performance in complex problem-solving tasks, achieving a level of competency comparable to or exceeding human experts.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {340–348},
numpages = {9},
keywords = {Bayesian network, GPT-4, credit risk report, labeled guide prompting, prompt engineering},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3650105.3652295,
author = {Niu, Changan and Zhang, Ting and Li, Chuanyi and Luo, Bin and Ng, Vincent},
title = {On Evaluating the Efficiency of Source Code Generated by LLMs},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652295},
doi = {10.1145/3650105.3652295},
abstract = {Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {103–107},
numpages = {5},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3643795.3648384,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inbook{10.1109/DAC56929.2023.10247987,
author = {Pujar, Saurabh and Buratti, Luca and Guo, Xiaojie and Dupuis, Nicolas and Lewis, Burn and Suneja, Sahil and Sood, Atin and Nalawade, Ganesh and Jones, Matt and Morari, Alessandro and Puri, Ruchir},
title = {Invited: Automated Code Generation for Information Technology Tasks in YAML through Large Language Models},
year = {2025},
isbn = {9798350323481},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC56929.2023.10247987},
abstract = {The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible YAML code generation tool, aimed at improving IT automation productivity. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models.},
booktitle = {Proceedings of the 60th Annual ACM/IEEE Design Automation Conference},
pages = {1–4},
numpages = {4}
}

@inproceedings{10.1145/3690624.3709225,
author = {Tong, Zhenyu and Qin, Chuan and Fang, Chuyu and Yao, Kaichun and Chen, Xi and Zhang, Jingshuai and Zhu, Chen and Zhu, Hengshu},
title = {From Missteps to Mastery: Enhancing Low-Resource Dense Retrieval through Adaptive Query Generation},
year = {2025},
isbn = {9798400712456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690624.3709225},
doi = {10.1145/3690624.3709225},
abstract = {Document retrieval, designed to recall query-relevant documents from expansive collections, is essential for information-seeking tasks, such as web search and open-domain question-answering. Advances in representation learning and pretrained language models (PLMs) have driven a paradigm shift from traditional sparse retrieval methods to more effective dense retrieval approaches, forging enhanced semantic connections between queries and documents and establishing new performance benchmarks. However, reliance on extensive annotated document-query pairs limits their competitiveness in low-resource scenarios. Recent research efforts employing the few-shot capabilities of large language models (LLMs) and prompt engineering for synthetic data generation have emerged as a promising solution. Nonetheless, these approaches are hindered by the generation of lower-quality data within the conventional dense retrieval training process. To this end, in this paper, we introduce iGFT, a framework aimed at enhancing low-resource dense retrieval by integrating a three-phase process --- Generation, Filtering, and Tuning --- coupled with an iterative optimization strategy. Specifically, we first employ supervised fine-tuning on limited ground truth data, enabling an LLM to function as the generator capable of producing potential queries from given documents. Subsequently, we present a multi-stage filtering module to minimize noise in the generated data while retaining samples poised to significantly improve the dense retrieval model's performance in the follow-up fine-tuning process. Furthermore, we design a novel iterative optimization strategy that dynamically optimizes the query generator for producing more informative queries, thereby enhancing the efficacy of the entire framework. Finally, extensive experiments conducted on a series of publicly available retrieval benchmark datasets have demonstrated the effectiveness of the proposed iGFT.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1},
pages = {1373–1384},
numpages = {12},
keywords = {dense retrieval, large language model, query generation},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inbook{10.1145/3676536.3676830,
author = {Cui, Fan and Yin, Chenyang and Zhou, Kexing and Xiao, Youwei and Sun, Guangyu and Xu, Qiang and Guo, Qipeng and Liang, Yun and Zhang, Xingcheng and Song, Demin and Lin, Dahua},
title = {OriGen: Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676830},
abstract = {Recent studies have demonstrated the significant potential of Large Language Models (LLMs) in generating Register Transfer Level (RTL) code, with notable advancements showcased by commercial models such as GPT-4 and Claude3-Opus. However, these proprietary LLMs often raise concerns regarding privacy and security. While open-source LLMs offer solutions to these concerns, they typically underperform commercial models in RTL code generation tasks, primarily due to the scarcity of high-quality open-source RTL datasets. To address this challenge, we introduce OriGen, a fully open-source framework that incorporates self-reflection capabilities and a novel dataset augmentation methodology for generating high-quality, large-scale RTL code. Our approach employs a code-to-code augmentation technique to enhance the quality of open-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors through a self-reflection process that leverages compiler feedback.Experimental results demonstrate that OriGen significantly outperforms other open-source alternatives in RTL code generation. It surpasses the previous best-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the pass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits superior capabilities in self-reflection and error correction, outperforming GPT-4 by 19.9% on a benchmark designed to evaluate self-reflection capabilities.OriGen is open source at GitHub(https://github.com/pku-liang/OriGen)},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {99},
numpages = {9}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473904,
author = {Lu, Yao and Liu, Shang and Zhang, Qijun and Xie, Zhiyao},
title = {RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473904},
doi = {10.1109/ASP-DAC58780.2024.10473904},
abstract = {Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {722–727},
numpages = {6},
location = {Incheon, Republic of Korea},
series = {ASPDAC '24}
}

@inproceedings{10.1145/3677389.3702605,
author = {Azher, Ibrahim Al and Seethi, Venkata Devesh Reddy and Akella, Akhil Pandey and Alhoori, Hamed},
title = {LimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702605},
doi = {10.1145/3677389.3702605},
abstract = {The "limitations" sections of scientific articles play a crucial role in highlighting the boundaries and shortcomings of research, thereby guiding future studies and improving research methods. Analyzing these limitations benefits researchers, reviewers, funding agencies, and the broader academic community. We introduce LimTopic, a strategy where Topic generation in Limitation sections in scientific articles with Large Language Models (LLMs). Here, each topic contains the title and `Topic Summary.' This study focuses on effectively extracting and understanding these limitations through topic modeling and text summarization, utilizing the capabilities of LLMs. We extracted limitations from research articles and applied an LLM-based topic modeling integrated with the BERtopic approach to generate a title for each topic and `Topic Sentences.' To enhance comprehension and accessibility, we employed LLM-based text summarization to create concise and generalizable summaries for each topic's Topic Sentences and produce a `Topic Summary.' Our experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and integrating BERTopic with LLM to generate topics, titles, and a topic summary. We also experimented with various LLMs with BERTopic for topic modeling and various LLMs for text summarization tasks. Our results showed that the combination of BERTopic and GPT 4 performed the best in terms of silhouette and coherence scores in topic modeling, and the GPT4 summary outperformed other LLM tasks as a text summarizer. Our code and dataset are available at https://github.com/IbrahimAlAzhar/LimTopic/tree/master.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {30},
numpages = {12},
keywords = {research limitations, limitations sections, large language models, information extraction, science of science},
location = {Hong Kong, China},
series = {JCDL '24}
}

@inproceedings{10.1145/3691620.3695257,
author = {Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal},
title = {Understanding Developer-Analyzer Interactions in Code Reviews},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695257},
doi = {10.1145/3691620.3695257},
abstract = {Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1945–1955},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3711542.3711582,
author = {Sachdev, Jayant and D Rosario, Sean and Phatak, Abhijeet and Wen, He and Kirti, Swati and Tripathy, Chittaranjan},
title = {Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711582},
doi = {10.1145/3711542.3711582},
abstract = {Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM’s performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing e-commerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {32–40},
numpages = {9},
keywords = {Large Language Models, Search Relevance, Information Retrieval, Data Annotation, Prompt Engineering, Retrieval Augmented Generation, Maximum Marginal Relevance.},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.1145/3722237.3722399,
author = {Lin, Yan and Zhang, Lu},
title = {Applications and Challenges of Generative Artificial Intelligence Enabling Critical Thinking Development in International Undergraduate Education},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722399},
doi = {10.1145/3722237.3722399},
abstract = {In today's fast-changing information-exploding era, developing students' critical thinking has become one of the most important tasks in international undergraduate education. Generative AI can simulate human creativity and imagination, providing brand-new resources and tools for critical thinking development. This paper details the application of generative AI technology in providing intelligent teaching resources, implementing personalized learning tutoring, promoting interdisciplinary integrated learning, cultivating the spirit of questioning and reforming assessment methods, etc. It also points out that the application of this intelligent technology in the teaching process is also facing the main challenges of data bias and false information, data privacy and security, and the enhancement of teachers' application ability, and gives specific countermeasures. Therefore, this paper aims to provide a useful reference for international undergraduate education practice and promote the integration of generative AI technology to empower students' critical thinking development.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {944–949},
numpages = {6},
keywords = {Critical Thinking, Generative Artificial Intelligence, Intelligent Teaching, Technological Risks},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1145/3657054.3657277,
author = {Bachinger, Sarah T. and Feddoul, Leila and Mauch, Marianne Jana and K\"{o}nig-Ries, Birgitta},
title = {Extracting Legal Norm Analysis Categories from German Law Texts with Large Language Models},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657277},
doi = {10.1145/3657054.3657277},
abstract = {The digitization of public services in Germany is always based on a legal basis (e.g., laws). In the digitization process, first relevant entities in law documents (e.g., actors) are detected, then a list of possible process steps of their interactions is derived. The final process is constructed and transformed to a digital service for citizens and companies. Today, the discovery of custom entities in German law documents is still manual high effort work. In our study, we investigate the capabilities of Large Language Models (LLMs) to automate this task, choose five LLMs from 61 evaluated candidates, and perform prompt engineering to create five different prompt variants with differing parts. We examine the automatic annotation by two LLMs (LeoLM and BLOOM CLP German) in detail and find that the inclusion of more information in the prompts as well as an increased number of examples per prompt are beneficial. We report micro F1-scores for the optimal scenario of 0.91 for BLOOM CLP German, and 0.82 for LeoLM, with a higher balanced accuracy for LeoLM. The results indicate that LLMs have a good potential to perform named entity recognition, especially for supporting legal norm analysis in the context of the digitization of public administration.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {481–493},
numpages = {13},
keywords = {Digital Transformation, Federal Information Management, Large Language Models, Named Entity Recognition, Public Administration},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@inproceedings{10.1145/3641554.3701863,
author = {Raihan, Nishat and Siddiq, Mohammed Latif and Santos, Joanna C.S. and Zampieri, Marcos},
title = {Large Language Models in Computer Science Education: A Systematic Literature Review},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701863},
doi = {10.1145/3641554.3701863},
abstract = {Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {938–944},
numpages = {7},
keywords = {code generation, cs education, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3677389.3702522,
author = {Zhang, Yining and Peng, Yinan and Tu, Chengying and Zhang, Zherui and Yan, Hongfei and Chen, Chong and Ma, Hao and Yang, Jia and Zhang, Yan and Liao, Rikun},
title = {Exploring Efficient Optimization Techniques in Online Retrieval-Augmented Generation Application},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702522},
doi = {10.1145/3677389.3702522},
abstract = {Recent advances in large language models (LLM) have brought an explosive growth to chat-bot applications. Among them, retrieval-augmented generation, which provides extra context to make LLM capable of answering out-of-domain questions is becoming a popular method. However, naive implementation of RAG usually cannot reach ideal answer quality in complicated real-world scenarios. Researchers have proposed a number of methods to improve RAG, but many of them involves extra LLM calls which is too time-consuming for online application. In this paper, we explored practical techniques and designs in RAG that improve answers to user-satisfying quality while keeping the response latency at a moderate level in the scenario of a research data QA application in university. Our main findings include introducing a relevance judge with small-scale LLM for retrieved documents can effectively filter out less relevant ones, which can otherwise disrupt the generated answer greatly, and decomposing the generation task into multiple independent sub-tasks can reduce the chance of hallucination and also accelerates the generation. As for model performance, prompt engineering and fine-tuning (through learning from strong LLM) are effective yet simple ways to enhance answer quality. Our results and experience provide insights for building future real-world LLM applications.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {51},
numpages = {11},
keywords = {retrieval-augmented generation, document relevance, research data, fine-tuning},
location = {Hong Kong, China},
series = {JCDL '24}
}

@inproceedings{10.1145/3696630.3728572,
author = {Liu, Yuhe and Pei, Changhua and Xu, Longlong and Chen, Bohan and Sun, Mingze and Zhang, Zhirui and Sun, Yongqian and Zhang, Shenglin and Wang, Kun and Zhang, Haiming and Li, Jianhui and Xie, Gaogang and Wen, Xidao and Nie, Xiaohui and Ma, Minghua and Pei, Dan},
title = {OpsEval: A Comprehensive Benchmark Suite for Evaluating Large Language Models’ Capability in IT Operations Domain},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728572},
doi = {10.1145/3696630.3728572},
abstract = {In recent decades, the field of software engineering has driven the rapid evolution of Information Technology (IT) systems, including advances in cloud computing, 5G networks, and financial information platforms. Ensuring the stability, reliability, and robustness of these complex IT systems has emerged as a critical challenge. Large language models (LLMs) that have exhibited remarkable capabilities in NLP-related tasks are showing great potential in AIOps, such as root cause analysis of failures, generation of operations and maintenance scripts, and summarizing of alert information. Unlike knowledge in general corpora, knowledge of Ops varies with the different IT systems, encompassing various private sub-domain knowledge, sensitive to prompt engineering due to various sub-domains, and containing numerous terminologies. Existing NLP-related benchmarks can not guide the selection of suitable LLMs for Ops (OpsLLM), and current metrics (e.g., BLEU, ROUGE) can not adequately reflect the question-answering (QA) effectiveness in the Ops domain. We propose a comprehensive benchmark suite, OpsEval, including an Ops-oriented evaluation dataset, an Ops evaluation benchmark, and a specially designed Ops QA evaluation method. Our dataset contains 7,334 multiple-choice questions and 1,736 QA questions. We have carefully selected and released 20% of the dataset written by domain experts in various sub-domains to assist current researchers in preliminary evaluations of OpsLLMs1. We test over 24 latest LLMs under various settings such as self-consistency, chain-of-thought, and in-context learning, revealing findings when applying LLMs to Ops. We also propose an evaluation method for QA in Ops, which has a coefficient of 0.9185 with human experts and is improved by 0.4471 and 1.366 compared to BLEU and ROUGE, respectively. Over the past one year, our dataset and leaderboard have been continuously updated.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {503–513},
numpages = {11},
keywords = {large language models, operations, benchmark, evaluation, prompt engineering},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3643745,
author = {Zan, Daoguang and Yu, Ailun and Shen, Bo and Chen, Bei and Li, Wei and Gong, Yongshun and Chen, Xiaolin and Yao, Yafen and Luo, Weihua and Guan, Bei and Liu, Yan and Wang, Yongji and Wang, Qianxiang and Cui, Lizhen},
title = {DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643745},
doi = {10.1145/3643745},
abstract = {The task of code generation aims to generate code solutions based on given programming problems. Recently, code large language models (code LLMs) have shed new light on this task, owing to their formidable code generation capabilities. While these models are powerful, they seldom focus on further improving the accuracy of library-oriented API invocation. Nonetheless, programmers frequently invoke APIs in routine coding tasks. In this paper, we aim to enhance the proficiency of existing code LLMs regarding API invocation by mimicking analogical learning, which is a critical learning strategy for humans to learn through differences among multiple instances. Motivated by this, we propose a simple yet effective approach, namely DiffCoder, which excels in API invocation by effectively training on the differences (diffs) between analogical code exercises. To assess the API invocation capabilities of code LLMs, we conduct experiments on seven existing benchmarks that focus on mono-library API invocation. Additionally, we construct a new benchmark, namely PanNumEval, to evaluate the performance of multi-library API invocation. Extensive experiments on eight benchmarks demonstrate the impressive performance of DiffCoder. Furthermore, we develop a VSCode plugin for DiffCoder, and the results from twelve invited participants further verify the practicality of DiffCoder.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {19},
numpages = {21},
keywords = {Code Generation, Code Library, Instruction Tuning, Large Language Model}
}

@inproceedings{10.1145/3638529.3654049,
author = {Saletta, Martina and Ferretti, Claudio},
title = {Exploring the Prompt Space of Large Language Models through Evolutionary Sampling},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638529.3654049},
doi = {10.1145/3638529.3654049},
abstract = {Large language models (LLMs) are increasingly gaining relevance in every-day life, due to their apparent ability in solving tasks that demand intricate linguistic comprehension. Recent studies state that one of the key points that impact their outcome is the quality of the prompt used to interact with them. This work proposes a grammar-based evolutionary approach for exploring the prompt space of LLMs, driven by a fitness function that aims at optimizing the performance on a given task. We tested our technique by steering two state-of-the-art models through evolved prompts, and by comparing the performance they obtain on 8 benchmark tasks with that obtained when using other baseline prompts on the same tasks, showing that in most cases our prompts yield better results. Further, we defined a constrained mutation operator that limits the changes to specific grammar non-terminals, allowing to study and highlight the elements in the prompt that mostly affect the output of the LLM. Finally, a thorough discussion points out some issues that limit the relevance of the emerging prompt engineering discipline, given the existence of many effective prompt structures and the possible diversity that can be observed in the LLM output given the same input to the model.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1345–1353},
numpages = {9},
keywords = {large language models, prompt evolution, evolution strategies, local search},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@inproceedings{10.1145/3643795.3648396,
author = {Bhatia, Shreya and Gandhi, Tarushi and Kumar, Dhruv and Jalote, Pankaj},
title = {Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648396},
doi = {10.1145/3643795.3648396},
abstract = {Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.*These authors contributed equally.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {54–61},
numpages = {8},
keywords = {large language models, unit test generation, ChatGPT, generative AI},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3716640.3716649,
author = {Prather, James and Reeves, Brent N and Denny, Paul and Leinonen, Juho and MacNeil, Stephen and Luxton-Reilly, Andrew and Orvalho, Jo\~{a}o and Alipour, Amin and Alfageeh, Ali and Amarouche, Thezyrie and Kimmel, Bailey and Wright, Jared and Blake, Musa and Barbre, Gweneth},
title = {Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716649},
doi = {10.1145/3716640.3716649},
abstract = {Non-native English speakers (NNES) face multiple barriers to learning programming. These barriers can be obvious, such as the fact that programming language syntax and instruction are often in English, or more subtle, such as being afraid to ask for help in a classroom full of native English speakers. However, these barriers are frustrating because many NNES students know more about programming than they can articulate in English. Advances in generative AI (GenAI) have the potential to break down these barriers because state of the art models can support interactions in multiple languages. Moreover, recent work has shown that GenAI can be highly accurate at code generation and explanation. In this paper, we provide the first exploration of NNES students prompting in their native languages (Arabic, Chinese, and Portuguese) to generate code to solve programming problems. Our results show that students are able to successfully use their native language to solve programming problems, but not without some difficulty specifying programming terminology and concepts. We discuss the challenges they faced, the implications for practice in the short term, and how this might transform computing education globally in the long term.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {74–84},
numpages = {11},
keywords = {AI; Artificial Intelligence; Automatic Code Generation; Codex; Copilot; CS1; GenAI; GitHub; GPT; GPT-4; ChatGPT; HCI; Introductory Programming; Large Language Models; LLM; Non-Native English Speakers; Novice Programming; OpenAI; Prompt Problems},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3691620.3694987,
author = {Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan},
title = {Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3694987},
doi = {10.1145/3691620.3694987},
abstract = {Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28% on EM, 13% on BLEU, and 6.8% on CodeBLEU.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {65–77},
numpages = {13},
keywords = {retrieval-augmented code generation, preference-guided refactorer, deep reinforcement learning},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3626111.3628205,
author = {Sharma, Prakhar and Yegneswaran, Vinod},
title = {PROSPER: Extracting Protocol Specifications Using Large Language Models},
year = {2023},
isbn = {9798400704154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626111.3628205},
doi = {10.1145/3626111.3628205},
abstract = {We explore the application of Large Language Models (LLMs) (specifically GPT-3.5-turbo) to extract specifications and automating understanding of networking protocols from Internet Request for Comments (RFC) documents. LLMs have proven successful in specialized domains like medical and legal text understanding, and this work investigates their potential in automatically comprehending RFCs. We develop Artifact Miner, a tool to extract diagram artifacts from RFCs. We then couple extracted artifacts with natural language text to extract protocol automata using GPT-turbo 3.5 (chatGPT) and present our zero-shot and few-shot extraction results. We call this framework for FSM extraction 'PROSPER: Protocol Specification Miner'. We compare PROSPER with existing state-of-the-art techniques for protocol FSM state and transition extraction. Our experiments indicate that employing artifacts along with text for extraction can lead to lower false positives and better accuracy for both extracted states and transitions. Finally, we discuss efficient prompt engineering techniques, the errors we encountered, and pitfalls of using LLMs for knowledge extraction from specialized domains such as RFC documents.},
booktitle = {Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
pages = {41–47},
numpages = {7},
keywords = {Large language models, automated extraction, protocol FSMs, protocol specifications, request for comments},
location = {Cambridge, MA, USA},
series = {HotNets '23}
}

@article{10.1145/3728963,
author = {Wang, Ruiqi and Guo, Jiyu and Gao, Cuiyun and Fan, Guodong and Chong, Chun Yong and Xia, Xin},
title = {Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728963},
doi = {10.1145/3728963},
abstract = {Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored.    In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide insights and implications, concluding that current state-of-the-art LLM-as-a-judge methods can potentially replace human evaluations in certain SE tasks.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA086},
numpages = {23},
keywords = {human preference, large language models, model evaluation}
}

@inproceedings{10.1145/3636534.3697447,
author = {Shen, Leming and Zheng, Yuanqing},
title = {IoTCoder: A Copilot for IoT Application Development},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3697447},
doi = {10.1145/3636534.3697447},
abstract = {Existing code Large Language Models are primarily designed for generating simple and general algorithms but are not dedicated to IoT applications. To fill this gap, we present IoTCoder, a coding copilot specifically designed to synthesize programs for IoT application development. IoTCoder features three locally deployed small language models (SLMs): a Task Decomposition SLM that decomposes a complex IoT application into multiple tasks with detailed descriptions, a Requirement Transformation SLM that converts the decomposed tasks described in natural language to well-structured specifications, and a Modularized Code Generation SLM that generates modularized code based on the task specifications. Experiment results show that IoTCoder can synthesize programs adopting more IoT-specific algorithms and outperform state-of-the-art code LLMs in terms of both task accuracy (by more than 24.2% on average) and memory usage (by less than 358.4 MB on average).},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {1647–1649},
numpages = {3},
keywords = {large language models, IoT applications},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/3593078.3593933,
author = {Urban, Matthias and Nguyen, Duc Dat and Binnig, Carsten},
title = {OmniscientDB: A Large Language Model-Augmented DBMS That Knows What Other DBMSs Do Not Know},
year = {2023},
isbn = {9798400701931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593078.3593933},
doi = {10.1145/3593078.3593933},
abstract = {In this paper, we present our vision of OmniscientDB, a novel database that leverages the implicitly-stored knowledge in large language models to augment datasets for analytical queries or even machine learning tasks. OmiscientDB empowers its users to augment their datasets by means of simple SQL queries and thus has the potential to dramatically reduce the manual overhead associated with data integration. It uses automatic prompt engineering to construct appropriate prompts for given SQL queries and passes them to a large language model like GPT-3 to contribute additional data (i.e., new rows, columns, or entire tables), augmenting the explicitly stored data. Our initial evaluation demonstrates the general feasibility of our vision, explores different prompting techniques in greater detail, and points towards several directions for future research.},
booktitle = {Proceedings of the Sixth International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {4},
numpages = {7},
keywords = {data augmentation, large language models, databases},
location = {Seattle, WA, USA},
series = {aiDM '23}
}

@inproceedings{10.1145/3708657.3708752,
author = {Sun, Qiyao and Li, Ziqing and Ma, Ruize and Wang, Yuxuan and Zhao, Zheyun and Lv, Xiongce},
title = {All Commentary by AI: An End-to-End Automated Basketball Commentary System Integrating Computer Vision and Large Language Models},
year = {2025},
isbn = {9798400717444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708657.3708752},
doi = {10.1145/3708657.3708752},
abstract = {Small-scale sports events have gained widespread popularity through social media platforms, but the lack of professional commentators leads to suboptimal viewer experiences. Traditional automated commentary methods lack deep understanding of game events and generate content that lacks vividness and cultural adaptability. To address this issue, we propose an end-to-end automated basketball commentary system. We combine You Only Look Once(YOLO) object detection and multi-object tracking techniques to perform real-time detection and tracking of basketballs, players, and hoops. By setting fixed regions at hoop positions and detecting whether the basketball’s trajectory crosses these regions, we achieve automatic score recognition. Using human pose estimation, we conduct temporal analysis of players’ keypoints and accurately identify violations such as holding, traveling, and double dribbling based on rule-based judgments. Compared to methods relying on complex action classification models, our approach reduces dependence on large-scale training data and improves recognition efficiency. Finally, we integrate prompt engineering with large language models to generate real-time commentary that aligns with the audience’s language habits and cultural backgrounds. Experimental results show that audience satisfaction increased by 21% compared to a baseline model without style adjustments. This study provides an effective and innovative solution for the automated commentary of small-scale sports events.},
booktitle = {Proceedings of the 2024 10th International Conference on Communication and Information Processing},
pages = {584–592},
numpages = {9},
keywords = {automated sports commentary, computer vision, human pose estimation, large language models, prompt engineering},
location = {
},
series = {ICCIP '24}
}

@inproceedings{10.1145/3589335.3651945,
author = {Ayoub, Michael Antonios Kruse and Su, Zhan and Li, Qiuchi},
title = {A Case Study of Enhancing Sparse Retrieval using LLMs},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651945},
doi = {10.1145/3589335.3651945},
abstract = {While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1609–1615},
numpages = {7},
keywords = {information retrieval, large language models, query expansion, query writing},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3658271.3658342,
author = {Albuquerque, Beatriz Ventorini Lins de and Cunha, Antonio Fernando Souza da and Souza, Leonardo and Siqueira, Sean Wolfgand Matsui and Santos, Rodrigo Pereira dos},
title = {Generating and Reviewing Programming Codes with Large Language Models: A Systematic Mapping Study},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658342},
doi = {10.1145/3658271.3658342},
abstract = {Context: The proliferation of technologies based on Large Language Models (LLM) is reshaping various domains, also impacting on programming code creation and review. Problem: The decision-making process in adopting LLM in software development demands an understanding of associated challenges and diverse application possibilities. Solution: This study addresses the identified challenges linked to LLM utilization in programming code processes. It explores models, utilization strategies, challenges, and coping mechanisms, focusing on the perspectives of researchers in software development. IS Theory: Drawing on Task-Technology Fit (TTF) theory, the research examines the alignment between task characteristics in code generation and review, and LLM technology attributes to discern performance impacts and utilization patterns. Method: Employing the Systematic Mapping of the Literature method, the research analyzes 19 selected studies from digital databases—IEEE Digital Library, Compendex Engineering Village, and Scopus—out of 1,257 retrieved results. Summary of Results: The research reveals 23 models, 13 utilization strategies, 15 challenges, and 14 coping mechanisms associated with LLM in programming code processes, offering a comprehensive understanding of the application landscape. Contributions to IS: Contributing to the Information Systems (IS) field, This study provides valuable insights into the utilization of LLM in programming code generation and review. The identified models, strategies, challenges, and coping mechanisms offer practical guidance for decision-making processes related to LLM technology adoption. The research aims to support the IS community in effectively navigating the complexities of integrating large language models into the dynamic software development lifecycle.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {70},
numpages = {10},
keywords = {Code Generation, LLM, automatic refactoring, code auto-suggestion, code completion, natural language models, neural network, systematic mapping study, transformer architecture},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@inproceedings{10.1145/3658644.3690298,
author = {Nazzal, Mahmoud and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai},
title = {PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690298},
doi = {10.1145/3658644.3690298},
abstract = {The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, recent literature and our empirical investigation in this work show that while LLMs can generate functioning code, they inherently tend to introduce security vulnerabilities, limiting their potential. This problem is mainly due to their training on massive open-source corpora exhibiting insecure and inefficient programming practices. Therefore, automatic optimization of LLM prompts for generating secure and functioning code is a demanding need. This paper introduces PromSec, an algorithm for &lt;u&gt;prom&lt;/u&gt;pt optimization for &lt;u&gt;sec&lt;/u&gt;ure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate the code-clearing and generation loop as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. As a result, PromSec becomes a cost-effective and practical solution for generating secure and functioning codes.Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that despite the comprehensive application of a state-of-the-art approach, it falls short in addressing all vulnerabilities within the code, whereas PromSec effectively resolves each of them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operational time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study presents an essential step towards improving the trustworthiness of LLMs for secure and functioning code generation, significantly enhancing their large-scale integration in real-world software code development practices.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2266–2280},
numpages = {15},
keywords = {LLMs, code generation, graph generative adversarial networks, secure and functioning codes},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3643795.3648375,
author = {Grandel, Skyler and Schmidt, Douglas C. and Leach, Kevin},
title = {Applying Large Language Models to Enhance the Assessment of Parallel Functional Programming Assignments},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648375},
doi = {10.1145/3643795.3648375},
abstract = {Courses in computer science (CS) often assess student programming assignments manually, with the intent of providing in-depth feedback to each student regarding correctness, style, efficiency, and other quality attributes. As class sizes increase, however, it is hard to provide detailed feedback consistently, especially when multiple assessors are required to handle a larger number of assignment submissions. Large language models (LLMs), such as ChatGPT, offer a promising alternative to help automate this process in a consistent, scalable, and minimally-biased manner.This paper explores ChatGPT-4's scalablility and accuracy in assessing programming assignments based on predefined rubrics in the context of a case study we conducted in an upper-level undergraduate and graduate CS course at Vanderbilt University. In this case study, we employed a method that compared assessments generated by ChatGPT-4 against human graders to measure the accuracy, precision, and recall associated with identifying programming mistakes. Our results show that when ChatGPT-4 is used properly (e.g., with appropriate prompt engineering and feature selection) it can improve objectivity and grading efficiency, thereby acting as a complementary tool to human graders for advanced computer science graduate and undergraduate students.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {102–110},
numpages = {9},
keywords = {ChatGPT, education, generative AI, large language models, prompt engineering, automated grading},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3696410.3714889,
author = {Gui, Yi and Li, Zhen and Wan, Yao and Shi, Yemin and Zhang, Hongyu and Chen, Bohua and Su, Yi and Chen, Dongping and Wu, Siyuan and Zhou, Xing and Jiang, Wenbin and Jin, Hai and Zhang, Xiangliang},
title = {WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714889},
doi = {10.1145/3696410.3714889},
abstract = {Automatically generating webpage code from webpage designs can significantly reduce the workload of front-end developers, and recent Multimodal Large Language Models (MLLMs) have shown promising potential in this area. However, our investigation reveals that most existing MLLMs are constrained by the absence of high-quality, large-scale, real-world datasets, resulting in inadequate performance in automated webpage code generation. To fill this gap, this paper introduces WebCode2M, a new dataset comprising 2.56 million instances, each containing a design image along with the corresponding webpage code and layout details. Sourced from real-world web resources, WebCode2M offers a rich and valuable dataset for webpage code generation across a variety of applications. The dataset quality is ensured by a scoring model that filters out instances with aesthetic deficiencies or other incomplete elements. To validate the effectiveness of WebCode2M, we introduce a baseline model based on the Vision Transformer (ViT), named WebCoder, and establish a benchmark for fair comparison. Additionally, we introduce a new metric, TreeBLEU, to measure the structural hierarchy recall. The benchmarking results demonstrate that our dataset significantly improves the ability of MLLMs to generate code from webpage designs, confirming its effectiveness and usability for future applications in front-end design tools. Finally, we highlight several practical challenges introduced by our dataset, calling for further research. The code and dataset are publicly available at our project homepage: https://webcode2m.github.io.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1834–1845},
numpages = {12},
keywords = {code generation, dataset, design to code, ui automation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3725812,
author = {Tan, Hanzhuo and Luo, Qi and Jiang, Ling and Zhan, Zizheng and Li, Jing and Zhang, Haotian and Zhang, Yuqun},
title = {Prompt-based Code Completion via Multi-Retrieval Augmented Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3725812},
doi = {10.1145/3725812},
abstract = {Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Code Completion, Multi-Retriever, Prompting}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473893,
author = {Wan, Lily Jiaxin and Huang, Yingbing and Li, Yuhong and Ye, Hanchen and Wang, Jinghua and Zhang, Xiaofan and Chen, Deming},
title = {Software/Hardware Co-Design for LLM and Its Application for Design Verification},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473893},
doi = {10.1109/ASP-DAC58780.2024.10473893},
abstract = {The widespread adoption of Large Language Models (LLMs) is impeded by their demanding compute and memory resources. The first task of this paper is to explore optimization strategies to expedite LLMs, including quantization, pruning, and operation-level optimizations. One unique direction is to optimize LLM inference through novel software/hardware co-design methods. Given the accelerated LLMs, the second task of this paper is to study LLMs' performance in the usage scenario of circuit design and verification. Specifically, we place a particular emphasis on functional verification. Through automated prompt engineering, we harness the capabilities of the established LLM, GPT-4, to generate High-Level Synthesis (HLS) designs with predefined errors based on 11 open-source synthesizable HLS benchmark suites. This dataset is a comprehensive collection of over 1000 function-level designs, and each of which is afflicted with up to 45 distinct combinations of defects injected into the source code. This dataset, named Chrysalis, expands upon what's available in current HLS error models, offering a rich resource for training to improve how LLMs debug code. The dataset can be accessed at: https://github.com/UIUC-ChenLab/Chrysalis-HLS.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {435–441},
numpages = {7},
keywords = {large language models, software/hardware co-design, functional verification},
location = {Incheon, Republic of Korea},
series = {ASPDAC '24}
}

@inproceedings{10.1145/3744367.3744395,
author = {Xu, Chang},
title = {Research on Security Risks and Supervision of Generative AI Large Models},
year = {2025},
isbn = {9798400715068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744367.3744395},
doi = {10.1145/3744367.3744395},
abstract = {Generative AI Large Models represented by DeepSeek, ChatGPT,Midjourney and Sora have become the focus of discussions in the global tech circle once again. While the comprehensive rise of Generative AI has brought a lot of convenience to people, it has also brought a lot of risks. This paper studies the principles of large models and the differences from traditional ones, through analyzes the challenges or risks in multiple fields and dimensions such as ethical risks, security risks, social risks, legal risks brought by generative artificial intelligence, etc. By analyzing the regulatory agencies and mechanisms of different countries, targeted countermeasures and suggestions have been put forward for the supervision and governance in the future development process of generative artificial intelligence.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems},
pages = {176–181},
numpages = {6},
keywords = {Generative AI, Large Models, Regulatory strategy, Security Risks},
location = {
},
series = {ICAIES '25}
}

@inproceedings{10.1145/3626252.3630880,
author = {Sheard, Judy and Denny, Paul and Hellas, Arto and Leinonen, Juho and Malmi, Lauri and Simon},
title = {Instructor Perceptions of AI Code Generation Tools - A Multi-Institutional Interview Study},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630880},
doi = {10.1145/3626252.3630880},
abstract = {Much of the recent work investigating large language models and AI Code Generation tools in computing education has focused on assessing their capabilities for solving typical programming problems and for generating resources such as code explanations and exercises. If progress is to be made toward the inevitable lasting pedagogical change, there is a need for research that explores the instructor voice, seeking to understand how instructors with a range of experiences plan to adapt. In this paper, we report the results of an interview study involving 12 instructors from Australia, Finland and New Zealand, in which we investigate educators' current practices, concerns, and planned adaptations relating to these tools. Through this empirical study, our goal is to prompt dialogue between researchers and educators to inform new pedagogical strategies in response to the rapidly evolving landscape of AI code generation tools.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1223–1229},
numpages = {7},
keywords = {ai code generation, generative ai, instructor perceptions, interview study, large language models, llms, programming education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3597503.3639183,
author = {Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl},
title = {Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639183},
doi = {10.1145/3597503.3639183},
abstract = {Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. Researchers are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of "code analysis" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code &amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {220},
numpages = {13},
keywords = {LLM, code summarization, program analysis, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3745238.3745456,
author = {Zhang, Jiahao},
title = {Artificial Intelligence Generation Content Empowering Java Framework Application Course for Software Technology Major},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745456},
doi = {10.1145/3745238.3745456},
abstract = {With the rapid development of artificial intelligence-generated content (AIGC) technology, its application in the field of education has gradually become an important direction of teaching reform. This paper takes the course of “Java Framework Application “in software technology major as the research object, discusses how to empower the traditional teaching mode through AIGC technology, and improve the teaching quality and practical effect of the course. Firstly, the pain points existing in the current Java framework course teaching were analyzed, including the shortage of case resources, the single practice scene, and the lack of personalized guidance. Secondly, combined with the technical characteristics of AIGC, a multi-dimensional application model covering code generation, intelligent question answering, scene simulation and learning effect evaluation was constructed. By introducing a large language model to automatically generate customized case codes of mainstream frameworks such as Spring Boot and MyBatis, conversational AI is used to assist students to solve development problems in real time, and a virtual project scene is constructed based on generative technology to strengthen engineering practice ability. Teaching practice shows that the application of AIGC technology greatly improves students 'code standardization and project development efficiency, and teachers can accurately locate learning weaknesses through intelligent analysis tools. This study provides a reusable implementation path for the intelligent transformation of technology courses, and verifies the empowering value of generative artificial intelligence in the field of vocational education, which has important reference significance for the optimization of future curriculum system.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {1397–1402},
numpages = {6},
keywords = {Artificial intelligence generated content(AIGC), Curriculum reform, Generative artificial intelligence, Intelligent education, Java framework teaching},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1145/3691620.3695505,
author = {Qu, Muzi and Liu, Jie and Kang, Liangyi and Wang, Shuai and Ye, Dan and Huang, Tao},
title = {Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695505},
doi = {10.1145/3691620.3695505},
abstract = {Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions---correctness, performance, and similarity---and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30%. Furthermore, 30% of the codes exhibited a performance improvement of more than 20%, underscoring the effectiveness and potential of our framework for practical applications.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1308–1318},
numpages = {11},
keywords = {code generation, large language model, performance optimization},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3701625.3701687,
author = {Sampaio, Savio Sousa and Lima, M\'{a}rcia Sampaio and de Souza, Eriky Rodrigues and Meireles, Maria Alcimar and Pessoa, Marcela Savia and Conte, Tayana Uchoa},
title = {Exploring the Use of Large Language Models in Requirements Engineering Education: An Experience Report with ChatGPT 3.5},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701687},
doi = {10.1145/3701625.3701687},
abstract = {Large Language Models (LLMs) are becoming common in educational settings. This trend presents a challenge for teachers, who must focus on teaching the proper usage of LLMs. In the context of Software Engineering (SE), ChatGPT can support various software development tasks. This work reports an experience with students using ChatGPT 3.5 to support the Requirements Engineering (RE) phase. We conducted a two-phase study with 42 students. First, the students elicited requirements for systems using RE techniques. Then, the students used ChatGPT 3.5 to generate requirements for the same systems. Finally, they compared both sets of requirements based on equivalence, innovation, and relevance. On average, 65.26% of the requirements generated by ChatGPT were considered equivalents to the requirements the students had elicited. However, students reported that ChatGPT generates broad and non-specific requirements. Students also reported that ChatGPT 3.5 can foster the requirements elicitation, but it is necessary to establish well-defined prompts for generating requirements.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {624–634},
numpages = {11},
keywords = {Requirement Elicitation, ChatGPT 3.5, Software engineering education},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1145/3664646.3676276,
author = {Cai, Jiahao},
title = {Agents for Data Science: From Raw Data to AI-Generated Notebooks using LLMs and Code Execution (Invited Talk)},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3676276},
doi = {10.1145/3664646.3676276},
abstract = {Data science tasks involve a complex interplay of datasets, code and code outputs for answering questions, deriving insights, or building models from data. Tasks and chosen methods may require specialized data domain or scientific domain knowledge. Queries range from high-level (low-code) or highly technical (high-code). Code execution results, such as plots and tables are artifacts used by data scientists to interpret and reason about the current and future states of a solution towards completing the task. This presents unique challenges in designing, deploying and evaluating LLM-based agents for automating data science workflows. In this talk we will introduce an end-to-end, autonomous Data Science Agent (DSA) built around Gemini and available as an experiment at labs.google/code. DSA leverages agentic flows, planning and orchestration to tackle open-ended data science explorations. It uses LLMs for planning, task decomposition, code generation, reasoning and error-correction through code execution. DSA is designed to streamline the entire data science process, enabling users to query data in natural language, and get from a dataset and prompt to a fully AI-generated, populated notebook. We’ll discuss design choices (prompting, SFT, orchestration), iterative development cycles, evaluation, lessons learned and future challenges. Where applicable, we will showcase real-world case studies demonstrating how DSA can assist with bootstrapping the analysis of data from complex scientific domains.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {181},
numpages = {1},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3708518,
author = {Widyasari, Ratnadira and Zhang, Ting and Bouraffa, Abir and Maalej, Walid and Lo, David},
title = {Explaining Explanations: An Empirical Study of Explanations in Code Reviews},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708518},
doi = {10.1145/3708518},
abstract = {Code reviews are central for software quality assurance. Ideally, reviewers should explain their feedback to enable authors of code changes to understand the feedback and act accordingly. Different developers might need different explanations in different contexts. Therefore, assisting this process first requires understanding the types of explanations reviewers usually provide. The goal of this article is to study the types of explanations used in code reviews and explore the potential of Large Language Models (LLMs), specifically ChatGPT, in generating these specific types. We extracted 793 code review comments from Gerrit and manually labeled them based on whether they contained a suggestion, an explanation, or both. Our analysis shows that 42% of comments only include suggestions without explanations. We categorized the explanations into seven distinct types including rule or principle, similar examples, and future implications. When measuring their prevalence, we observed that some explanations are used differently by novice and experienced reviewers. Our manual evaluation shows that, when the explanation type is specified, ChatGPT can correctly generate the explanation in 88 out of 90 cases. This foundational work highlights the potential for future automation in code reviews, which can assist developers in sharing and obtaining different types of explanations as needed, thereby reducing back-and-forth communication.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {177},
numpages = {30},
keywords = {code review, explanation, empirical study, large language model}
}

@inproceedings{10.1145/3716554.3716561,
author = {Zosimadis, Ilias and Stamelos, Ioannis},
title = {LLM-Enhanced Test Case Prioritization for Complex Software Systems},
year = {2025},
isbn = {9798400713170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716554.3716561},
doi = {10.1145/3716554.3716561},
abstract = {This paper presents a novel approach to test case prioritization using Large Language Models (LLMs) for complex software systems. Traditional prioritization methods often struggle with the dynamic nature of modern software development and the large amounts of unstructured data generated during the software lifecycle. Our method leverages LLMs to analyze diverse data sources, including code changes, user feedback, and system documentation, creating a more adaptive and context-aware prioritization strategy. We applied our approach to an Internet of Things (IoT) based system for motion tracking in ten-pin bowling. The experimental results show significant improvements over a baseline Additional Statement Coverage method. Our LLM-enhanced approach achieved a 12.12% higher Average Percentage of Faults Detected (APFD) score and reduced test suite execution time by 26 %. These findings demonstrate the potential of LLMs to enhance software testing practices, particularly in early fault detection and efficient resource utilization. The paper discusses implementation details, evaluation metrics, and future directions for integrating this approach into continuous integration and deployment pipelines.},
booktitle = {Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {46–50},
numpages = {5},
location = {
},
series = {PCI '24}
}

@inproceedings{10.1145/3626252.3630863,
author = {Del Carpio Gutierrez, Andre and Denny, Paul and Luxton-Reilly, Andrew},
title = {Evaluating Automatically Generated Contextualised Programming Exercises},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630863},
doi = {10.1145/3626252.3630863},
abstract = {Introductory programming courses often require students to solve many small programming exercises as part of their learning. Researchers have previously suggested that the context used in the problem description for these exercises is likely to impact student engagement and motivation. Furthermore, supplying programming exercises that use a broad range of contexts or even allowing students to select contexts to personalize their own exercises, may support the interests of a diverse student population. Unfortunately, it is time-consuming for instructors to create large numbers of programming exercises that provide a wide range of contextualized problems. However, recent work has shown that large language models may be able to automate the mass production of programming exercises, reducing the burden on instructors. In this research, we explore the potential of OpenAI's GPT-4 to create high-quality and novel programming exercises that implement various contexts. Finally, through prompt engineering, we compare different prompting strategies used to generate many programming exercises with various contextualized problem descriptions and then evaluate the quality of the exercises generated.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {289–295},
numpages = {7},
keywords = {chatgpt, cs1, gpt-4, large language models, novice programmers, openai, programming exercises, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3709358,
author = {Fan, Lishui and Liu, Jiakun and Liu, Zhongxin and Lo, David and Xia, Xin and Li, Shanping},
title = {Exploring the Capabilities of LLMs for Code-Change-Related Tasks},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709358},
doi = {10.1145/3709358},
abstract = {Developers deal with code-change-related tasks daily, e.g., reviewing code. Pre-trained code and code-change-oriented models have been adapted to help developers with such tasks. Recently, large language models (LLMs) have shown their effectiveness in code-related tasks. However, existing LLMs for code focus on general code syntax and semantics rather than the differences between two code versions. Thus, it is an open question how LLMs perform on code-change-related tasks.To answer this question, we conduct an empirical study using  (&gt;) 1B parameters LLMs on three code-change-related tasks, i.e., code review generation, commit message generation, and just-in-time comment update, with in-context learning (ICL) and parameter-efficient fine-tuning (PEFT, including LoRA and prefix-tuning). We observe that the performance of LLMs is poor without examples and generally improves with examples, but more examples do not always lead to better performance. LLMs tuned with LoRA have comparable performance to the state-of-the-art small pre-trained models. Larger models are not always better, but Llama 2 and Code Llama families are always the best. The best LLMs outperform small pre-trained models on the code changes that only modify comments and perform comparably on other code changes. We suggest future work should focus more on guiding LLMs to learn the knowledge specific to the changes related to code rather than comments for code-change-related tasks.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {159},
numpages = {36},
keywords = {Code-change-related task, large language model, empirical study}
}

@inproceedings{10.1145/3696630.3728560,
author = {Yang, Longxing and Luo, Yixing and Gao, Hao and Fan, Yingshuang and Zhang, Jingru and Li, Xiaofeng and Dong, Xiaogang and Gu, Bin and Jin, Zhi and Yang, Mengfei},
title = {Evaluating Large Language Models for Requirements Question Answering in Industrial Aerospace Software},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728560},
doi = {10.1145/3696630.3728560},
abstract = {Aerospace software presents significant challenges to requirements engineering due to its design complexity and stringent safety standards. When manually drafting requirement documents, engineers need strong domain knowledge while also navigating heterogeneous data, which leads to errors and inefficiencies. This paper evaluates the capabilities of large language models (LLMs) in understanding aerospace software requirements and their potential to assist in requirements question answering (QA). We develop an aerospace requirements QA benchmark based on industrial software assets, books, and research materials, creating a total of 6, 696 QA pairs across ten tasks and three heterogeneous data formats: text, tables, and formulas. We then evaluate the domain-specific performance of five mainstream open-source LLMs using zero-shot learning, few-shot learning, and retrieval-augmented generation (RAG) techniques. We further categorize hallucinations from LLMs and quantitatively analyze error distributions. Moreover, we conduct a user study to assess the LLM's practical usefulness when applying to requirements QA. The evaluation results show that (1) LLMs demonstrate limited performance in the aerospace software domain, (2) RAG techniques significantly enhance the capabilities of LLMs for text-based tasks, while few-shot learning improves the performance of most LLMs, (3) four distinct types of QA hallucinations are identified, and (4) LLM QA is particularly beneficial for junior engineers. This research provides valuable perspectives for the future application of LLMs in aerospace software.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {366–377},
numpages = {12},
keywords = {aerospace software, requirements question answering, large language models, evaluation},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3722212.3725097,
author = {Fathollahzadeh, Saeed and Mansour, Essam and Boehm, Matthias},
title = {Demonstrating CatDB: LLM-based Generation of Data-centric ML Pipelines},
year = {2025},
isbn = {9798400715648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722212.3725097},
doi = {10.1145/3722212.3725097},
abstract = {AutoML systems automate finding Machine Learning (ML) pipelines but struggle to scale with large datasets due to time-consuming data analysis and complex hyper-parameter search spaces. LLMs (Large Language Models) offer flexibility and scalability for code generation with strong generalization across coding tasks. However, generating data-centric ML pipeline scripts is more challenging, as it requires complex reasoning to align the needs of a dataset with coding tasks, such as data cleaning or feature transformation. Thus, LLMs struggle to generate effective and efficient ML pipelines. This demo paper presents CatDB, which overcomes these challenges by dynamically generating dataset-specific instructions to guide LLMs in generating effective pipelines. CatDB profiles datasets to extract metadata, including refined data catalog information and statistics, and then uses this metadata to break down pipeline generation into instructions of tasks such as data cleaning, transformation, and model training, tailored to specifics of the dataset at hand. This process enables CatDB to leverage LLM coding capabilities more effectively. Our evaluation shows CatDB outperforms existing LLM-based and AutoML systems with up to orders of magnitude faster runtime on large datasets. The audience will experience CatDB's capabilities with commercial and open-source LLMs, using a variety of real datasets, as shown in our demo video and Colab notebook.},
booktitle = {Companion of the 2025 International Conference on Management of Data},
pages = {87–90},
numpages = {4},
keywords = {LLM code generation, data catalogs, data-centric ML pipelines},
location = {Berlin, Germany},
series = {SIGMOD/PODS '25}
}

@inproceedings{10.1145/3695053.3731096,
author = {Shin, Yongwon and Kang, Dookyung and Sung, Hyojin},
title = {ATiM: Autotuning Tensor Programs for Processing-in-DRAM},
year = {2025},
isbn = {9798400712616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695053.3731096},
doi = {10.1145/3695053.3731096},
abstract = {Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for accelerating memory-intensive operations in modern applications, such as Large Language Models (LLMs). Despite its potential, current software stacks for DRAM-PIM face significant challenges, including reliance on hand-tuned libraries that hinder programmability, limited support for high-level abstractions, and the lack of systematic optimization frameworks. To address these limitations, we present ATiM, a search-based optimizing tensor compiler for UPMEM. Key features of ATiM include: (1) automated searches of the joint search space for host and kernel tensor programs, (2) PIM-aware optimizations for efficiently handling boundary conditions, and (3) improved search algorithms for the expanded search space of UPMEM systems. Our experimental results on UPMEM hardware demonstrate performance gains of up to 6.18 \texttimes{} for various UPMEM benchmark kernels and 8.21 \texttimes{} for GPT-J layers. To the best of our knowledge, ATiM is the first tensor compiler to provide fully automated, autotuning-integrated code generation support for a DRAM-PIM system. By bridging the gap between high-level tensor computation abstractions and low-level hardware-specific requirements, ATiM establishes a foundation for advancing DRAM-PIM programmability and enabling streamlined optimization.},
booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture},
pages = {899–915},
numpages = {17},
location = {
},
series = {ISCA '25}
}

@inproceedings{10.1145/3639478.3643108,
author = {Liu, Yilun and Tao, Shimin and Meng, Weibin and Yao, Feiyu and Zhao, Xiaofeng and Yang, Hao},
title = {LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643108},
doi = {10.1145/3639478.3643108},
abstract = {Automated log analysis plays a crucial role in software maintenance as it allows for efficient identification and resolution of issues. However, traditional methods employed in log analysis heavily rely on extensive historical data for training purposes and lack rationales for its predictions. The performance of these traditional methods significantly deteriorates when in-domain logs for training are limited and unseen log data are the majority, particularly in rapidly changing online environments. Additionally, the lack of rationales hampers the interpretability of analysis results and impacts analysts' subsequent decision-making processes. To address these challenges, we proposes LogPrompt, an novel approach that leverages large language models (LLMs) and advanced prompting techniques to achieve performance improvements in zero-shot scenarios (i.e., no in-domain training). Moreover, LogPrompt has garnered positive evaluations from experienced practitioners in its log interpretation ability. Code available at https://github.com/lunyiliu/LogPrompt.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {364–365},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3600061.3603141,
author = {Li, Fu and Huang, Jiaming and Gao, Yi and Dong, Wei},
title = {ChatIoT: Zero-code Generation of Trigger-action Based IoT Programs with ChatGPT},
year = {2023},
isbn = {9798400707827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600061.3603141},
doi = {10.1145/3600061.3603141},
abstract = {Trigger-Action Program (TAP) is a popular and significant form of Internet of Things (IoT) applications, commonly utilized in smart homes. Existing works either just perform actions based on commands or require human intervention to generate TAPs. With the emergence of Large Language Models (LLMs), it becomes possible for users to create IoT TAPs in zero-code manner using natural language. Thus, we propose ChatIoT, which employs LLMs to process natural language in chats and realizes the zero-code generation of TAPs for existing devices.},
booktitle = {Proceedings of the 7th Asia-Pacific Workshop on Networking},
pages = {219–220},
numpages = {2},
location = {Hong Kong, China},
series = {APNet '23}
}

@inproceedings{10.1109/ASE56229.2023.00096,
author = {Yan, Dapeng and Gao, Zhipeng and Liu, Zhiming},
title = {A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00096},
doi = {10.1109/ASE56229.2023.00096},
abstract = {Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of "code cleanness", we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1887–1898},
numpages = {12},
keywords = {code generation, program competition, Chat-GPT, large language model, clean code},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3657604.3664669,
author = {Bradford, Allison and Li, Weiying and Gerard, Libby and Linn, Marcia C.},
title = {Comparing Expert and ChatGPT-authored Guidance Prompts},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664669},
doi = {10.1145/3657604.3664669},
abstract = {Students bring a multitude of ideas and experiences to the classroom while they are reasoning about scientific phenomena. They often need timely guidance to refine build upon their initial ideas. In this study we explore the development of guidance prompts to provide students with personalized, real-time feedback in the context of a pedagogically grounded chatbot. In the current version of the tool, guidance prompts are authored by learning scientists who are experts in the content of the items and in Knowledge Integration pedagogy. When students engage with the chatbot, an idea detection model is used to determine the ideas that are present in a student explanation and then the expert-authored guidance prompts are assigned based on rules about which ideas are or are not present in the student explanation. While this approach allows for close attention to and control of the pedagogical intent of each prompt, it is time consuming and not easily generalizable. Further this rule-based approach limits the ways in which students can interact with the chatbot. The work in progress study presented in this paper explores the potential of using generative AI to create similarly pedagogically grounded guidance prompts as a first step towards increasing the generalizability and scalability of this approach. Specifically, we ask: using criteria from the Knowledge Integration Pedagogical Framework, how do ChatGPT 3.5-authored guidance prompts compare to human expert-authored guidance prompts? We find that while prompt engineering can enhance the alignment of ChatGPT-authored guidance prompts with pedagogical criteria, the human expert-authored guidance prompts more consistently meet the pedagogical criteria.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {388–392},
numpages = {5},
keywords = {automated guidance, generative AI, knowledge integration pedagogy},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3708635.3708655,
author = {Yu, Hong Qing and Sutton, Jack and O'Neill, Sam and Reiff-Marganiec, Stephan},
title = {Case Studies on LLM Centric and Services Oriented Data Analytics Agent Development},
year = {2025},
isbn = {9798400717765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708635.3708655},
doi = {10.1145/3708635.3708655},
abstract = {This paper presents a novel service orchestration framework for a chatbot application focused on data analytics questions. The framework integrates Large Language Models (LLMs) with service-oriented computing to transform data analytics into a dynamic, conversational experience. The approach leverages advancements in LLM technology to enable real-time, automated data insights via chatbot interfaces, making complex data analytics accessible across various industries. In addition, the data will be processed and analysis at edge-machine rather than post all the data directly to the LLMs on the cloud. Therefore, the Central to the framework is the local Micro Analytics Service (MAS) and a dynamic service-data coordination framework, which together facilitate the decoupling of data from business logic, allowing for intuitive engagement with analytics processes. Through two case studies, retail data analysis and regional healthcare planning, the ability of the framework to provide actionable insights through natural language prompts is demonstrated, showcasing its potential to significantly reduce barriers to sophisticated data analytics. The evaluation reveals strong performance in data connection and code generation, with identified areas for improvement in visualizations and handling complex data scenarios.},
booktitle = {Proceedings of the 2024 13th International Conference on Software and Information Engineering},
pages = {69–76},
numpages = {8},
keywords = {LLM-driven service orchestration, Dynamic data analytics services, Services Computing},
location = {
},
series = {ICSIE '24}
}

@inproceedings{10.1145/3616855.3635743,
author = {Saxena, Shreya and Prasad, Siva and I, Muneeswaran and Shankar, Advaith and V, Varun and Gopalakrishnan, Saisubramaniam and Vaddina, Vishal},
title = {Automated Tailoring of Large Language Models for Industry-Specific Downstream Tasks},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635743},
doi = {10.1145/3616855.3635743},
abstract = {Foundational Large Language Models (LLMs) are pre-trained generally on huge corpora encompassing broad subjects to become versatile and generalize to future downstream tasks. However, their effectiveness falls short when dealing with tasks that are highly specialized to a specific use case. Even when adopting current prompt engineering techniques like few-shot or Chain-of-Thought reasoning prompts, the required level of results is not yet achievable directly with foundational models alone. The alternative approach is to fine-tune the LLM, but a common challenge is the limited availability of task-specific training data. In this talk, we will introduce an end-to-end automated framework to tailor a model to specific downstream tasks for an industry where the first step is to generate task-specific custom data from unstructured documents. Next, we will discuss our optimized distributed training pipeline for fine-tuning LLMs on the generated data. Finally, we will provide an overview of the statistical metrics and customized metrics we employ for assessing the performance of the fine-tuned LLM. This automated framework alleviates the burden of manual adjustments and streamlines the process to provide a model that is fully customized to suit the unique requirements of any specific business use case.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1184–1185},
numpages = {2},
keywords = {data generation, evaluation, finetuning, large language model},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3643916.3644403,
author = {Ma, Zexiong and An, Shengnan and Xie, Bing and Lin, Zeqi},
title = {Compositional API Recommendation for Library-Oriented Code Generation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644403},
doi = {10.1145/3643916.3644403},
abstract = {Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task.To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation.To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {87–98},
numpages = {12},
keywords = {API recommendation, code generation, requirements decomposition, large language model},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3715014.3722064,
author = {Shen, Leming and Yang, Qiang and Huang, Xinyu and Ma, Zijing and Zheng, Yuanqing},
title = {GPIoT: Tailoring Small Language Models for IoT Program Synthesis and Development},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3722064},
doi = {10.1145/3715014.3722064},
abstract = {Code Large Language Models (LLMs) enhance software development efficiency by automatically generating code and documentation based on user requirements. However, code LLMs cannot synthesize specialized programs when tasked with IoT applications that require domain knowledge. While Retrieval-Augmented Generation (RAG) offers a promising solution by fetching relevant domain knowledge, it necessitates powerful cloud LLMs (e.g., GPT-4) to process user requirements and retrieved contents, which raises significant privacy concerns. This approach also suffers from unstable networks and prohibitive LLM query costs. Moreover, it is challenging to ensure the correctness and relevance of the fetched contents. To address these issues, we propose GPIoT, a code generation system for IoT applications by fine-tuning locally deployable Small Language Models (SLMs) on IoT-specialized datasets. SLMs have smaller model sizes, allowing efficient local deployment and execution to mitigate privacy concerns and network uncertainty. Furthermore, by fine-tuning SLMs with our IoT-specialized datasets, the SLMs' ability to synthesize IoT-related programs can be substantially improved. To evaluate GPIoT's capability in synthesizing programs for IoT applications, we develop a benchmark, IoTBench. Extensive experiments and user trials demonstrate the effectiveness of GPIoT in generating IoT-specialized code, outperforming state-of-the-art code LLMs with an average task accuracy increment of 64.7% and significant improvements in user satisfaction.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {199–212},
numpages = {14},
keywords = {small language model, IoT program synthesis, fine-tuning},
location = {UC Irvine Student Center., Irvine, CA, USA},
series = {SenSys '25}
}

@inproceedings{10.1145/3650105.3652301,
author = {Macedo, Marcos and Tian, Yuan and Cogo, Filipe and Adams, Bram},
title = {Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652301},
doi = {10.1145/3650105.3652301},
abstract = {Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {57–68},
numpages = {12},
keywords = {code translation, output format, large language model, LLM, software engineering, benchmarking, evaluation, empirical study, case study},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3716640.3716657,
author = {Arora, Utkarsh and Garg, Anupam and Gupta, Aryan and Jain, Samyak and Mehta, Ronit and Oberoi, Rupin and Prachi and Raina, Aryaman and Saini, Manav and Sharma, Sachin and Singh, Jaskaran and Tyagi, Sarthak and Kumar, Dhruv},
title = {Analyzing LLM Usage in an Advanced Computing Class in India},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716657},
doi = {10.1145/3716640.3716657},
abstract = {This study examines the use of large language models (LLMs) by undergraduate and graduate students for programming assignments in advanced computing classes. Unlike existing research, which primarily focuses on introductory classes and lacks in-depth analysis of actual student-LLM interactions, our work fills this gap. We conducted a comprehensive analysis involving 411 students from a Distributed Systems class at an Indian university, where they completed three programming assignments and shared their experiences through Google Form surveys and interviews.Our findings reveal that students leveraged LLMs for a variety of tasks, including code generation, debugging, conceptual inquiries, and test case creation. They employed a spectrum of prompting strategies, ranging from basic contextual prompts to advanced techniques like chain-of-thought prompting and iterative refinement. While students generally viewed LLMs as beneficial for enhancing productivity and learning, we noted a concerning trend of over-reliance, with many students submitting entire assignment descriptions to obtain complete solutions. Given the increasing use of LLMs in the software industry, our study highlights the need to update undergraduate curricula to include training on effective prompting strategies and to raise awareness about the benefits and potential drawbacks of LLM usage in academic settings.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {154–163},
numpages = {10},
keywords = {Large Language Models, Computing Education, User Study},
location = {
},
series = {ACE '25}
}

@article{10.1145/3660810,
author = {Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
title = {ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660810},
doi = {10.1145/3660810},
abstract = {Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {103},
numpages = {23},
keywords = {Code Generation, Large Language Model, Prompt Engineering}
}

@article{10.1145/3734217,
author = {Niu, Feifei and Li, Chuanyi and Liu, Kui and Xia, Xin and Lo, David},
title = {When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3734217},
doi = {10.1145/3734217},
abstract = {Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model’s capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {296},
numpages = {41},
keywords = {Information retrieval, bug localization, deep learning, survey}
}

@inproceedings{10.1145/3732945.3732990,
author = {Ding, Zhiguo and Wu, Songyang and Zhang, Yilin and Yang, Tao and Li, Yingna and Li, Zhuhua and Shao, Hang and Shi, Yicong},
title = {CodingCare: AI Code Generation Security Framework for Common Vulnerability Mitigation},
year = {2025},
isbn = {9798400715204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732945.3732990},
doi = {10.1145/3732945.3732990},
abstract = {This article provides a comprehensive review of code generation LLMs (Large Language Models) focusing on security issues and possible solutions to software development workflows. Recent literature suggests that more than 70% of developers are using AI programming assistants in their day-to-day activities. Code generated by AI contains a plethora of serious vulnerabilities such as Cross-site scripting (XSS), SQL injection vulnerabilities, and unsafe credentials which impact the overall security of software systems. Most of the research, thus far, has focused on either the fine-tuning of larger models, working with the methods for optimal prompting, or looking at security evaluations to detect outcomes in coding LLMs, rather than exploring generative models with an overall design of a security framework. To contribute, we develop an overall security framework for code LLMs including prompt libraries, bibliography databases, vulnerability databases, repositories of programming use-cases for several ways through the software development life-cycles, including requirement analysis, code development, code vetting changes, code iteration, and code submission. We developed an experimental system and we will use it to conduct comparative experiments with three code LLMs including Deepseek-coder-7B, Mistral-7B, and Code Llama-7B. The results of this research indicate that our proposed security framework reduces the number of CVEs reported. This research lays a groundwork for projects that lead to other research issues related to software security LLMs considerations.},
booktitle = {Proceedings of the 2025 4th International Conference on Intelligent Systems, Communications and Computer Networks},
pages = {307–312},
numpages = {6},
keywords = {AI code generation, Common vulnerability, Large language model, Security framework},
location = {
},
series = {ISCCN '25}
}

@inproceedings{10.1145/3613904.3642803,
author = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
title = {PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642803},
doi = {10.1145/3613904.3642803},
abstract = {The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model’s interpretation and the user’s intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user’s initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model’s attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user’s expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {185},
numpages = {21},
keywords = {Generative AI, Large Language Models, Prompt Engineering},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3696630.3728507,
author = {Yoo, Shin and Feldt, Robert and Kim, Somin and Kim, Naryeong},
title = {Capturing Semantic Flow of ML-based Systems},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728507},
doi = {10.1145/3696630.3728507},
abstract = {ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {601–605},
numpages = {5},
keywords = {program analysis, large language models, agents},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00053,
author = {Tufano, Rosalia},
title = {Automating Code Review},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00053},
doi = {10.1109/ICSE-Companion58688.2023.00053},
abstract = {Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, code review comes at the cost of spending developers' time on reviewing their teammates' code. The goal of this research is to investigate the possibility of using Deep Learning (DL) to automate specific code review tasks. We started by training vanilla Transformer models to learn code changes performed by developers during real code review activities. This gives the models the possibility to automatically (i) revise the code submitted for review without any input from the reviewer; and (ii) implement changes required to address a specific reviewer's comment. While the preliminary results were encouraging, in this first work we tested DL models in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. Thus, in a subsequent work, we exploited a pre-trained Text-To-Text-Transfer-Transformer (T5) to overcome some of these limitations and experiment DL models for code review automation in more realistic and challenging scenarios. The achieved results show the improvements brought by T5 both in terms of applicability (i.e., scenarios in which it can be applied) and performance. Despite this, we are still far from performance levels making these techniques deployable in practice, thus calling for additional research in this area, as we discuss in our future work agenda.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {192–196},
numpages = {5},
keywords = {deep learning, code review},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3663529.3663873,
author = {Patel, Hetvi and Shah, Kevin Amit and Mondal, Shouvick},
title = {Do Large Language Models Generate Similar Codes from Mutated Prompts? A Case Study of Gemini Pro},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663873},
doi = {10.1145/3663529.3663873},
abstract = {In this work, we delve into the domain of source code similarity detection using Large Language Models (LLMs). Our investigation is motivated by the necessity to identify similarities among different pieces of source code, a critical aspect for tasks such as plagiarism detection and code reuse. We specifically focus on exploring the effectiveness of leveraging LLMs for this purpose. To achieve this, we utilized the LLMSecEval dataset, comprising 150 NL prompts for code generation across two languages: C and Python, and employed radamsa, a mutation-based input generator, to create 26 different mutations per NL prompt. Next, using the Gemini Pro LLM, we generated code for the original and mutated NL prompts. Finally, we detect code similarities using the recently proposed CodeBERTScore metric that utilizes the CodeBERT LLM. Our experiment aims to uncover the extent to which LLMs can consistently generate similar code despite mutations in the input NL prompts, providing insights into the robustness and generalizability of LLMs in understanding and comparing code syntax and semantics.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {671–672},
numpages = {2},
keywords = {Gemini Pro, LLMs, NL Prompt Mutation, Source Code Similarity},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3696630.3728523,
author = {Jin, Bihui and Wang, Jiayue and Nie, Pengyu},
title = {Learning to Edit Interactive Machine Learning Notebooks},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728523},
doi = {10.1145/3696630.3728523},
abstract = {Machine learning (ML) developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Notebooks provide a convenient tool for writing ML pipelines and interactively observing outputs. However, maintaining notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the ML pipeline code. Moreover, there is no existing benchmark related to developer edits on notebooks.In this paper, we present early results of the first study on learning to edit ML pipeline code in notebooks using large language models (LLMs). We collect the first dataset of 48,398 notebook edits derived from 20,095 revisions of 792 ML-related GitHub repositories. Our dataset captures granular details of file-level and cell-level modifications, offering a foundation for understanding real-world maintenance patterns in ML pipelines. We observe that the edits on notebooks are highly localized. Although LLMs have been shown to be effective on general-purpose code generation and editing, our results reveal that the same LLMs, even after finetuning, have low accuracy on notebook editing, demonstrating the complexity of real-world ML pipeline maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing LLMs' capabilities in engineering ML code.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {681–685},
numpages = {5},
keywords = {interactive computational notebook, machine learning pipeline, software evolution, code editing, large language model},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3626252.3630817,
author = {Fernandez, Amanda S. and Cornell, Kimberly A.},
title = {CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630817},
doi = {10.1145/3626252.3630817},
abstract = {As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself - teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create "black box" code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {345–351},
numpages = {7},
keywords = {ai, artificial intelligence, code generation, copilot, cs1, gpt-4, introductory programming, large language model, llm, machine learning, novice programmers, programming, prompt engineering, secure code, software verification},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3658617.3697624,
author = {Akyash, Mohammad and Mardani Kamali, Hadi},
title = {SimEval: Investigating the Similarity Obstacle in LLM-based Hardware Code Generation},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697624},
doi = {10.1145/3658617.3697624},
abstract = {The increasing use and efficiency of large language models (LLMs) in digital hardware circuit design has started to revolutionize the early stages of integrated circuit (IC) supply chain design and implementation, pushing towards enhanced automation. Despite these advances, hardware circuits' inherent complexity and limited data present significant challenges. Recent studies have begun to explore various attributes of LLM-generated hardware code, including semantics, syntax, fluency, and flexibility. Given that many code generation methodologies rely on fine-tuned LLMs and face constraints due to the limited availability of datasets for hardware designs, this paper investigates the "diversity" of codes generated by LLMs. We introduce SimEval, a comprehensive, multifaceted metric vector designed to assess the similarity of LLM-generated hardware codes at the syntactic, structural, and behavioral levels, from high-level register transfer (RT-level) to synthesized (gate-level) netlists. SimEval uniquely combines sub-tree matching from abstract syntax trees (AST) with structural similarity based on kernel graphs for control flow graphs (CFG). Our experiments focusing on samples from GPT-3.5 datasets and evaluating their similarity using SimEval, highlight the critical role of SimEval in evaluating LLM-based hardware code generators w.r.t. diversity1.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {1002–1007},
numpages = {6},
keywords = {large language model (LLM), hardware design, similarity, finetuning, dataset, code synthesis},
location = {Tokyo, Japan},
series = {ASPDAC '25}
}

@inproceedings{10.1145/3649217.3653600,
author = {Villegas Molina, Ismael and Montalvo, Audria and Zhong, Shera and Jordan, Mollie and Soosai Raj, Adalbert Gerald},
title = {Generation and Evaluation of a Culturally-Relevant CS1 Textbook for Latines using Large Language Models},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653600},
doi = {10.1145/3649217.3653600},
abstract = {In the United States, culturally relevant computing (CRC) is one of the most popular pedagogical implementations for Latin American (Latine) students. Culturally-relevant learning resources are a valuable tool for implementing CRC. However, the traditional method of creation and maintenance of textbooks takes a significant amount of time and effort. Given the duration required for textbook production, the development of culturally-relevant learning resources may become lengthened, as it requires close attention both on the material and the incorporation of cultural referents. In order to accelerate the process, we used the advancement of large language models (LLMs) to our advantage. Through prompt engineering, we created a series of prompts to produce a textbook for an introductory computer science course (CS1) that incorporates Latine culture. This textbook was evaluated on metrics regarding sensibility, correctness, readability, linguistic approachability, appropriateness of examples, and cultural relevance. Overall, the generated textbook was mainly sensible, correct, readable, and linguistically approachable. Code examples were not always appropriate due to the usage of libraries that are not typically used in a CS1 course. The cultural relevance was apparent, but it often included surface-level cultural referents. The main incorporation of culture was through geographical locations and people's names. This suggests that the use of LLMs to generate textbooks may serve as a valuable first step for writing culturally-relevant learning resources. Though this study focuses on Latines, our results and prompts may be applicable for generating culturally-relevant CS1 textbooks for other cultures.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {325–331},
numpages = {7},
keywords = {Latina, Latine, Latino, Latinx, computer science textbook, culturally relevant resources, large language models, resource generation},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3706599.3706693,
author = {Naik, Suchismita and Snellinger, Amanda and Toombs, Austin L. and Saponas, Scott and Hall, Amanda K},
title = {Exploring Early Adopters' Use of AI Driven Multi-Agent Systems to Inform Human-Agent Interaction Design: Insights from Industry Practice},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706693},
doi = {10.1145/3706599.3706693},
abstract = {This case study explores the experiences of Microsoft employees, who are early adopters of multi-agent generative AI systems, as they experiment with these technologies to design, test, and deploy new tools attempting to bridge the gap between existing Microsoft products and emerging AI capabilities. Thirteen developers and creators participated in 60-minute semi-structured interviews to elicit their challenges, use cases, and lessons learned from their experimentation with multi-agent AI frameworks. A thematic qualitative analysis process was conducted to analyze the interview data. Participants reported building multi-agent AI tools to address tasks in team collaboration, productivity, customer support, creative processes, and security. Strategies for managing complexity, enhancing transparency, and balancing agent autonomy with human oversight were found to be important human-agent interaction design considerations. Findings from this study highlight the capabilities and limitations of specialized multi-agents within the contexts of participants’ use cases and provide insights to inform the human-agent interaction design of future multi-agent generative AI systems.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {677},
numpages = {8},
keywords = {Artificial Intelligence, User Experience, Challenges and Limitations of AI Use},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3654777.3676401,
author = {Padmanabha, Akhil and Yuan, Jessie and Gupta, Janavi and Karachiwalla, Zulekha and Majidi, Carmel and Admoni, Henny and Erickson, Zackory},
title = {VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676401},
doi = {10.1145/3654777.3676401},
abstract = {Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos, code, and supporting files are located on our project website1},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {116},
numpages = {18},
keywords = {assistive robotics, large language models (LLMs), speech interfaces},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@article{10.1145/3742475,
author = {Grandel, Skyler and Andersen, Scott Thomas and Huang, Yu and Leach, Kevin},
title = {ComCat: Expertise-Guided Context Generation to Enhance Code Comprehension},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3742475},
doi = {10.1145/3742475},
abstract = {Software maintenance constitutes a substantial portion of the total lifetime costs of software, with a significant portion attributed to code comprehension. Software comprehension is eased by documentation such as comments that summarize and explain code. We present ComCat, an approach to automate comment generation by augmenting Large Language Models (LLMs) with expertise-guided context to target the annotation of source code with comments that improve comprehension. Our approach enables the selection of the most relevant and informative comments for a given snippet or file containing source code. We develop the ComCat pipeline to comment C/C++ files by (1) automatically identifying suitable locations in which to place comments, (2) predicting the most helpful type of comment for each location, and (3) generating a comment based on the selected location and comment type. In a human subject evaluation, we demonstrate that ComCat-generated comments significantly improve developer code comprehension across three indicative software engineering tasks by up to 13% for 80% of participants. In addition, we demonstrate that ComCat-generated comments are at least as accurate and readable as human-generated comments and are preferred over standard ChatGPT-generated comments for up to 92% of snippets of code. Furthermore, we develop and release a dataset containing source code snippets, human-written comments, and human-annotated comment categories. ComCat leverages LLMs to offer a significant improvement in code comprehension across a variety of human software engineering tasks.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Code Comprehension, Automated Comment Generation, Code Summarization, Generative AI}
}

@inproceedings{10.1145/3649158.3657043,
author = {Kundu, Ashish},
title = {AI/ML, Graphs and Access Control: Towards Holistic Identity and Access Management},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657043},
doi = {10.1145/3649158.3657043},
abstract = {Vulnerabilities in identity and access management (IAM) are one of the most common reasons for data breaches leading to adversarial impacts on security, privacy and compliance postures. Account breaches, incorrectly designed access control policies, weaknesses in authentication and credential management, vulnerable session management are some of the several security issues that lead to eventual compromise of the crown jewels leading to data breaches. The lifecycles of subjects and their identities, of objects and re- sources, and of the permissions and authorization policies are in- tertwined in a complex manner for each specific scenario. Often subjects, objects and permissions often are hard to be defined or isolated from each other, especially in the context of machine learn-ing. The evolution of these entities, and how their provenance is analyzed often is essential not only for forensic analysis of a breach but also should be a proactive ongoing process.  In order to manage the security issues and risks thereof, holistic end-to-end identity and access management in a secure and privacy- preserving manner is the need of yesterday, today and of the future. In the past couple of decades, we have encountered this problem time and again in various contexts in the settings of academic and industry research and in development/deployment of products, services and processes.  Three elements are the key ingredients in order to address this problem in a holistic manner: (1) graphs, (2) machine learning, and (3) decentralized computing (i.e., web3, blockchains). Further, with the advent of generative AI and large language models, the question arises about what problems they can help solve, or they can excerbate further, or what new challenges they can introduce. In this talk, I plan to delve into a discussion of the following: (a) the holistic and end-to-end nature of IAM, (b) the interplay between these three elements - graphs, machine learning, Web3 as well as generative AI, and how they can help, and (c) the research challenges that need to be addressed in order to reduce the security, privacy and compliance risks in identity and access management.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {1},
numpages = {1},
keywords = {access control, generative ai, identity, machine learning},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3696410.3714891,
author = {Gui, Yi and Wan, Yao and Li, Zhen and Zhang, Zhongyi and Chen, Dongping and Zhang, Hongyu and Su, Yi and Chen, Bohua and Zhou, Xing and Jiang, Wenbin and Zhang, Xiangliang},
title = {UICopilot: Automating UI Synthesis via Hierarchical Code Generation from Webpage Designs},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714891},
doi = {10.1145/3696410.3714891},
abstract = {Automating the synthesis of User Interfaces (UIs) plays a crucial role in enhancing productivity and accelerating the development lifecycle, reducing both development time and manual effort. Recently, the rapid development of Multimodal Large Language Models (MLLMs) has made it possible to generate front-end Hypertext Markup Language (HTML) code directly from webpage designs. However, real-world webpages encompass not only a diverse array of HTML tags but also complex stylesheets, resulting in significantly lengthy code. The lengthy code poses challenges for the performance and efficiency of MLLMs, especially in capturing the structural information of UI designs. To address these challenges, this paper proposes UICopilot, a novel approach to automating UI synthesis via hierarchical code generation from webpage designs. To validate the effectiveness of UICopilot, we conduct experiments on a real-world dataset, i.e., WebCode2M. Experimental results demonstrate that UICopilot significantly outperforms existing baselines in both automatic evaluation metrics and human evaluations. Specifically, statistical analysis reveals that the majority of human annotators prefer the webpages generated by UICopilot over those produced by GPT-4V.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1846–1855},
numpages = {10},
keywords = {UI automation, UI synthesis, code generation, design to code},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3650212.3680371,
author = {Li, Dong and Yan, Meng and Zhang, Yaosheng and Liu, Zhongxin and Liu, Chao and Zhang, Xiaohong and Chen, Ting and Lo, David},
title = {CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680371},
doi = {10.1145/3650212.3680371},
abstract = {Large Language Models (LLMs) specialized in code have shown exceptional proficiency across various programming-related tasks, particularly code generation. Nonetheless, due to its nature of pretraining on massive uncritically filtered data, prior studies have shown that code LLMs are prone to generate code with potential vulnerabilities. Existing approaches to mitigate this risk involve crafting data without vulnerability and subsequently retraining or fine-tuning the model. As the number of parameters exceeds a billion, the computation and data demands of the above approaches will be enormous. Moreover, an increasing number of code LLMs tend to be distributed as services, where the internal representation is not accessible, and the API is the only way to reach the LLM, making the prior mitigation strategies non-applicable.    To cope with this, we propose CoSec, an on-the-fly Security hardening method of code LLMs based on security model-guided Co-decoding, to reduce the likelihood of code LLMs to generate code containing vulnerabilities. Our key idea is to train a separate but much smaller security model to co-decode with a target code LLM. Since the trained secure model has higher confidence for secure tokens, it guides the generation of the target base model towards more secure code generation. By adjusting the probability distributions of tokens during each step of the decoding process, our approach effectively influences the tendencies of generation without accessing the internal parameters of the target code LLM. We have conducted extensive experiments across various parameters in multiple code LLMs (i.e., CodeGen, StarCoder, and DeepSeek-Coder), and the results show that our approach is effective in security hardening. Specifically, our approach improves the average security ratio of six base models by 5.02%-37.14%, while maintaining the functional correctness of the target model.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1428–1439},
numpages = {12},
keywords = {AI Safety, Code Generation, Large Language Models, Software Security},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3748505,
author = {Yang, Chen and Chen, Junjie and Lin, Bin and Wang, Ziqi and Zhou, Jianyi},
title = {Advancing Code Coverage: Incorporating Program Analysis with Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3748505},
doi = {10.1145/3748505},
abstract = {Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-enhanced techniques, achieving an average improvement of 34.10% and 25.93% in terms of branch coverage.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {Test Generation, Program Analysis, Large Language Models}
}

@inproceedings{10.1145/3640771.3643717,
author = {Luan, Zhirong and Lai, Yujun and Huang, Rundong and Yan, Yan and Wang, Jingwei and Lu, Jizhou and Chen, Badong},
title = {Hierarchical Large Language Models in Cloud-Edge-End Architecture for Heterogeneous Robot Cluster Control},
year = {2024},
isbn = {9798400708954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640771.3643717},
doi = {10.1145/3640771.3643717},
abstract = {Despite their powerful semantic understanding and code generation capabilities, Large Language Models (LLMs) still face challenges when dealing with complex tasks. Multi-agent strategy generation and motion control are highly complex domains that inherently require experts from multiple fields to collaborate. To enhance multi-agent strategy generation and motion control, we propose an innovative architecture that employs the concept of a cloud-edge-end hierarchical structure. By leveraging multiple large language models with distinct areas of expertise, we can efficiently generate strategies and perform task decomposition. Introducing the cosine similarity approach, aligning task decomposition instructions with robot task sequences at the vector level, we can identify subtasks with incomplete task decomposition and iterate on them multiple times to ultimately generate executable machine task sequences.The robot is guided through these task sequences to complete tasks of higher complexity. With this architecture, we implement the process of natural language control of robots to perform complex tasks, and successfully address the challenge of multi-agent execution of open tasks in open scenarios and the problem of task decomposition.},
booktitle = {Proceedings of the 2023 2nd International Symposium on Computing and Artificial Intelligence},
pages = {102–105},
numpages = {4},
keywords = {Cosine similarity, LLMS, Task decomposition},
location = {Shanghai, China},
series = {ISCAI '23}
}

@inproceedings{10.1145/3663529.3663869,
author = {Chavan, Sagar Bhikan and Mondal, Shouvick},
title = {Do Large Language Models Recognize Python Identifier Swaps in Their Generated Code?},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663869},
doi = {10.1145/3663529.3663869},
abstract = {Large Language Models (LLMs) have transformed natural language processing and generation activities in recent years. However, as the scale and complexity of these models grow, their ability to write correct and secure code has come under scrutiny. In our research, we delve into the critical examination of LLMs including ChatGPT-3.5, legacy Bard, and Gemini Pro, and their proficiency in generating accurate and secure code, particularly focusing on the occurrence of identifier swaps within the code they produce. Our methodology encompasses the creation of a diverse dataset comprising a range of coding tasks designed to challenge the code generation capabilities of these models. Further, we employ Pylint for an extensive code quality assessment and undertake a manual multi-turn prompted “Python identifier-swap” test session to evaluate the models’ ability to maintain context and coherence over sequential coding prompts. Our preliminary findings indicate a concern for developers: LLMs capable of generating better quality codes can perform worse when queried to recognize identifier swaps.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {663–664},
numpages = {2},
keywords = {Gemini Pro, LLMs, Python Identifier Swap},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3731567,
author = {van Stein, Niki and Vermetten, Diederick and B\"{a}ck, Thomas},
title = {In-the-loop Hyper-Parameter Optimization for LLM-Based Automated Design of Heuristics},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731567},
doi = {10.1145/3731567},
abstract = {Large Language Models (LLMs) have shown great potential in automatically generating and optimizing (meta)heuristics, making them valuable tools in heuristic optimization tasks. However, LLMs are generally inefficient when it comes to fine-tuning hyper-parameters of the generated algorithms, often requiring excessive queries that lead to high computational and financial costs. This paper presents a novel hybrid approach, LLaMEA-HPO, which integrates the open source LLaMEA (Large Language Model Evolutionary Algorithm) framework with a Hyper-Parameter Optimization (HPO) procedure in the loop. By offloading hyper-parameter tuning to an HPO procedure, the LLaMEA-HPO framework allows the LLM to focus on generating novel algorithmic structures, reducing the number of required LLM queries and improving the overall efficiency of the optimization process.We empirically validate the proposed hybrid framework on benchmark problems, including Online Bin Packing, Black-Box Optimization, and the Traveling Salesperson Problem. Our results demonstrate that LLaMEA-HPO achieves superior or comparable performance compared to existing LLM-driven frameworks while significantly reducing computational costs. This work highlights the importance of separating algorithmic innovation and structural code search from parameter tuning in LLM-driven code optimization and offers a scalable approach to improve the efficiency and effectiveness of LLM-based code generation.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = apr,
keywords = {Code Generation, Heuristic Optimization, Large Language Models, Evolutionary Computation, Black-Box Optimization, Traveling Salesperson Problems}
}

@inproceedings{10.1145/3703323.3704279,
author = {Pendyala, Vishnu S},
title = {The impact of Artificial Intelligence on Ecojustice and Ethics},
year = {2025},
isbn = {9798400711244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703323.3704279},
doi = {10.1145/3703323.3704279},
abstract = {Artificial Intelligence (AI) has attained human-level performance in tasks like text summarization, machine translation, and code generation while handling multimodal data. Despite these advances, AI poses challenges such as generating false or biased outputs, causing discrimination, and exhibiting opaque decision-making processes that hinder bias mitigation. Additionally, training large language models (LLMs) for AI consumes substantial energy and can enable autonomous warfare, posing a threat to eco-justice, and raising sustainability concerns. Their ability to create realistic content poses the risk of misuse and perpetuating misinformation. This article addresses these ethical issues and presents solutions through regulation and technology, with examples of global legislation tackling algorithmic discrimination and consumer rights. It emphasizes the need for a workforce skilled in navigating an AI-driven world. Future research directions touch upon mechanistic interpretability to understand AI outputs, new approaches to natural language processing and multimodal data understanding, and developing more interpretable neural network architectures with fewer parameters. This article summarizes the tutorial on the topic that includes some of the author’s work in related areas.},
booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)},
pages = {353–357},
numpages = {5},
keywords = {Generative AI, Sustainability, Environment},
location = {
},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3745238.3745397,
author = {Sun, Jian and Wang, Shuyi and Zhang, Runnan},
title = {Research Status and Prospects of Generative AI Governance in China Based on Visual Analysis},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745397},
doi = {10.1145/3745238.3745397},
abstract = {The data in this paper comes from CNKI, the Chinese literature database. This paper researched papers from 2015 to 2024 on Chinese generative AI governance in this database. With the help of the visualization software Citerspace and VOSviewer, the paper conducts data processing and visualization analyses from annual publication volumes, main agencies, core authors, and keyword trends. The paper studied the conditions and future perspectives of the Chinese generative AI governance studies. Our study shows that the paper volumes of this field are witnessing a rising trend. The research mainly aims at digital security as the key research topic; AI algorithms in technical difficulty-related topics; agile governance, risk governance, digital governance, technical governance, comprehensive governance, and global governance as solution-related topics. Trends in this field are becoming more diverse in governance fields and patterns. National security is becoming more and more important macroscopically. As for governance tools, papers focus more on the development of legal regulations. Microly, papers pay attention more to the privacy and security of personal information.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {1017–1023},
numpages = {7},
keywords = {Artificial Intelligence, China, Generative AI Governance, Visual Analysis},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1145/3632410.3632504,
author = {Bhowmick, Archisman and Mishra, Mayank and Singhal, Rekha},
title = {TASCA : Tool for Automatic SCalable Acceleration of ML pipelines✱},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632504},
doi = {10.1145/3632410.3632504},
abstract = {Data Scientists use Python for building ML pipelines including pre-processing on data for cleansing and transformation. The computational overheads, due to performance anti-patterns, on data processing can be expensive, especially on large data size. FASCA&nbsp;[10] is a framework to identify such performance anti-patterns, with their corresponding performant versions, from ML pipelines. However, it needs human intervention to generate the performant code. Recent growth in maturity of Large Language Models (LLM) for code generation motivated us to exploit them for automating the process of transforming performance anti-patterns with their performant versions, the feasibility of which has been discussed in &nbsp;[5]. This paper presents, TASCA, a tool which automatically detects potential performance anti-patterns for large data size in ML pipelines and replaces them with their performant version using Large Language Models like GPTNeo3.5/4. The tool has been tested empirically on three real-world workloads, showing substantial performance improvements, including a 70% speedup for a Netflix Series Recommendation pipeline, a 50% boost for a Movie Recommendation pipeline, and a 40% enhancement for an In-house recommendation system training pipeline.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {514–518},
numpages = {5},
keywords = {Code Acceleration, ML Pipeline, Scalability Bottlenecks.},
location = {Bangalore, India},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3712335.3712410,
author = {Xu, Chao and Zhang, Qi and Li, Baiyan and Wang, Anmin and Bao, Jingsong},
title = {Visual Analysis of Time Series Data for Multi-Agent Systems Driven by Large Language Models},
year = {2025},
isbn = {9798400710834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712335.3712410},
doi = {10.1145/3712335.3712410},
abstract = {The application of digitalization in manufacturing involves using sensors to collect and transmit large amounts of data in real-time. These complex, timestamped sequence data require effective analytical support to drive the generation of analysis summaries. Visual analysis helps researchers identify hidden patterns by intuitively displaying complex data, but its implementation relies on specialized knowledge and a deep understanding of data analysis techniques. Moreover, the analysis process often requires users to recall and process large amounts of information, which increases the complexity of the analysis. Therefore, there is an increasing demand for intelligent visual analysis technology. Large language models, with their powerful reasoning, summarization, and code generation capabilities, provide the possibility of visual analysis for general users. We propose a large language model-based multi-agent framework that integrates domain knowledge to achieve an automated data analysis workflow, from data acquisition to analysis summarization, helping users extract valuable analytical results from complex data and improving both efficiency and experience.},
booktitle = {Proceedings of the 3rd International Conference on Signal Processing, Computer Networks and Communications},
pages = {427–431},
numpages = {5},
keywords = {Interactive Data Analysis, Large Language Model, Time Series Data, Visual Analytics},
location = {
},
series = {SPCNC '24}
}

@article{10.1145/3737286,
author = {Li, Cangyuan and Chen, Chujie and Pan, Yudong and Xu, Wenjun and Liu, Yiqi and Chang, Kaiyan and Wang, Yujie and Wang, Mengdi and Wang, Ying and Li, Huawei and Han, Yinhe},
title = {AutoSilicon: Scaling Up RTL Design Generation Capability of Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3737286},
doi = {10.1145/3737286},
abstract = {Hardware description language (HDL) code designing is a critical component of the chip design process, requiring substantial engineering and time resources. Recent advancements in large language models (LLMs), such as GPT series, have shown promise in automating HDL code generation. However, current LLM-based approaches face significant challenges in meeting real-world hardware design requirements, particularly in handling complex designs and ensuring code correctness. Our evaluations reveal that the functional correctness rate of LLM-generated HDL code significantly decreases as design complexity increases. In this paper, we propose the AutoSilicon framework, which aims to scale up the hardware design capability of LLMs. AutoSilicon incorporates an agent system, which 1) allows for the decomposition of large-scale, complex code design tasks into smaller, simpler tasks; 2) provides a compilation and simulation environment that enables LLMs to compile and test each piece of code it generates; and 3) introduces a series of optimization strategies. Experimental results demonstrate that AutoSilicon can scale hardware designs to projects with code equivalent to over 10,000 tokens. In terms of design quality, it further improves the syntax correctness rate and functional correctness rate compared with approaches that do not employ any extensions. For example, compared to directly generating HDL code using GPT-4-turbo, AutoSilicon enhances the syntax correctness rate by an average of 35.8% and improves functional correctness by an average of 35.6%.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {Verilog Code Generation, LLM, Agent, Verilog}
}

@inproceedings{10.1109/CGO57630.2024.10444830,
author = {Okuda, Katsumi and Amarasinghe, Saman},
title = {AskIt: Unified Programming Interface for Programming with Large Language Models},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444830},
doi = {10.1109/CGO57630.2024.10444830},
abstract = {Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 % reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {41–54},
numpages = {14},
keywords = {domain specific language, code generation, large language model, software engineering, artificial intelligence},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@inproceedings{10.1145/3719487.3719519,
author = {Chang, Chi-In and Choi, Wan-Chong and Choi, Iek-Chong and Lei, Huey},
title = {A Systematic Literature Review of the Practical Applications of Artificial Intelligence-Generated Content (AIGC) Using OpenAI ChatGPT, Copilot, and Codex in Programming Education},
year = {2025},
isbn = {9798400717413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719487.3719519},
doi = {10.1145/3719487.3719519},
abstract = {This systematic literature review investigated the practical applications of Artificial Intelligence-Generated Content (AIGC) tools, specifically ChatGPT, Copilot, and Codex, in programming education. The review synthesized current research trends and key applications, with a particular focus on large language models (LLMs) and advanced chatbots. The findings revealed a significant increase in research interest following the release of ChatGPT, with most studies concentrating on university-level programming education. The review identified eight primary applications of AIGC tools in programming education, including evaluating AI performance in solving programming tasks, providing AI-driven code generation and assistance, automating assessment review and feedback, delivering personalized learning and tutoring, supporting educators and instructional design, investigating educator and student perceptions, checking for plagiarism, and exploring AI's impact on curriculum design. Despite the promising potential of these tools, critical gaps remained, particularly in primary and secondary education, as well as in online and blended learning environments. Future research was recommended to address these areas and to explore a broader range of AIGC tools to unlock their full potential for enhancing programming education.},
booktitle = {Proceeding of the 2024 8th International Conference on Education and E-Learning},
pages = {13–19},
numpages = {7},
keywords = {Applications of AIGC, Artificial intelligence generated content, ChatGPT, Chatbots, Educational technology, Large language models, Programming education, Systematic literature review},
location = {
},
series = {ICEEL '24}
}

@inproceedings{10.1145/3637528.3671542,
author = {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul and Akay, Gozde and Feris, Rogerio and Johnson, Tony and Hammer, Stephen and Karlinsky, Leonid},
title = {Large Scale Generative AI Text Applied to Sports and Music},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671542},
doi = {10.1145/3637528.3671542},
abstract = {We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the GRAMMY awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforementioned events, supporting 90 million fans around the world with 8 billion page views, continuously pushing the bounds on what is possible at the intersection of sports, entertainment, and AI.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4784–4792},
numpages = {9},
keywords = {applied computing, generative ai, large scale computing, neural networks, sports and entertainment},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3639478.3643526,
author = {Fakhoury, Sarah and Chakraborty, Saikat and Musuvathi, Madanlal and Lahiri, Shuvendu K.},
title = {NL2Fix: Generating Functionally Correct Code Edits from Bug Descriptions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643526},
doi = {10.1145/3639478.3643526},
abstract = {Despite the notable advancement of Large Language Models for Code Generation, there is a distinct gap in benchmark datasets and evaluation of LLMs' proficiency in generating functionally correct code edits based on natural language descriptions of intended changes. We address this void by presenting the challenge of translating natural language descriptions of code changes, particularly bug fixes outlined in Issue reports within repositories, into accurate code fixes. To tackle this issue, we introduce Defects4J-Nl2fix, a dataset comprising 283 Java programs from the widely-used Defects4J dataset, augmented with high-level descriptions of bug fixes. Subsequently, we empirically evaluate three state-of-the-art LLMs on this task, exploring the impact of different prompting strategies on their ability to generate functionally correct edits. Results show varied ability across models on this novel task. Collectively, the studied LLMs are able to produce plausible fixes for 64.6% of the bugs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {410–411},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3696630.3728569,
author = {Chen, Yujia and Chen, Mingyu and Gao, Cuiyun and Jiang, Zhihan and Li, Zhongqi and Ma, Yuchi},
title = {Towards Mitigating API Hallucination in Code Generated by LLMs with Hierarchical Dependency Aware},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728569},
doi = {10.1145/3696630.3728569},
abstract = {Application Programming Interfaces (APIs) are crucial in modern software development. Large Language Models (LLMs) assist in automated code generation but often struggle with API hallucination, including invoking non-existent APIs and misusing existing ones in practical development scenarios. Existing studies resort to Retrieval-Augmented Generation (RAG) methods for mitigating the hallucination issue, but tend to fail since they generally ignore the structural dependencies in practical projects and do not indeed validate whether the generated APIs are available or not. To address these limitations, we propose MARIN, a framework for mitigating API hallucination in code generated by LLMs with hierarchical dependency aware. MARIN consists of two phases: Hierarchical Dependency Mining, which analyzes local and global dependencies of the current function, aiming to supplement comprehensive project context in LLMs' input, and Dependency Constrained Decoding, which utilizes mined dependencies to adaptively constrain the generation process, aiming to ensure the generated APIs align with the project's specifications. To facilitate the evaluation of the degree of API hallucination, we introduce a new benchmark APIHul-Bench and two new metrics including Micro Hallucination Number (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six state-of-the-art LLMs demonstrate that MARIN effectively reduces API hallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in MaHR compared to the RAG approach. Applied to Huawei's internal projects and two proprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41% in MaHR.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {468–479},
numpages = {12},
keywords = {large language model, code generation, LLM hallucination},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3606038.3616157,
author = {Sarfati, Noah and Yerushalmy, Ido and Chertok, Michael and Keller, Yosi},
title = {Generating Factually Consistent Sport Highlights Narrations},
year = {2023},
isbn = {9798400702693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606038.3616157},
doi = {10.1145/3606038.3616157},
abstract = {Sports highlights are an important form of media for fans worldwide, as they provide short videos that capture key moments from games, often accompanied by the original commentaries of the game's announcers. However, traditional forms of presenting sports highlights have limitations in conveying the complexity and nuance of the game. In recent years, the use of Large Language Models (LLMs) for natural language generation has emerged and is a promising approach for generating narratives that can provide a more compelling and accessible viewing experience. In this paper, we propose an end-to-end solution to enhance the experience of watching sports highlights by automatically generating factually consistent narrations using LLMs and crowd noise extraction. Our solution involves several steps, including extracting the source of information from the live broadcast using a transcription model, prompt engineering, and comparing out-of-the-box models for consistency evaluation. We also propose a new dataset annotated on generated narratives from 143 Premier League plays and fine-tune a Natural Language Inference (NLI) model on it, achieving 92% precision. Furthermore, we extract crowd noise from the original video to create a more immersive and realistic viewing experience for sports fans by adapting speech enhancement SOTA models on a brand new dataset created from 155 Ligue 1 games.},
booktitle = {Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports},
pages = {15–22},
numpages = {8},
keywords = {speech enhancing, prompt engineering, natural language inference, large language models (llms), hallucinations, factual consistency evaluation},
location = {Ottawa ON, Canada},
series = {MMSports '23}
}

@inproceedings{10.1145/3643991.3645069,
author = {Wu, Liangxuan and Zhao, Yanjie and Hou, Xinyi and Liu, Tianming and Wang, Haoyu},
title = {ChatGPT Chats Decoded: Uncovering Prompt Patterns for Superior Solutions in Software Development Lifecycle},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645069},
doi = {10.1145/3643991.3645069},
abstract = {The advent of Large Language Models (LLMs) like ChatGPT has markedly transformed software development, aiding tasks from code generation to issue resolution with their human-like text generation. Nevertheless, the effectiveness of these models greatly depends on the nature of the prompts given by developers. Therefore, this study delves into the DevGPT dataset, a rich collection of developer-ChatGPT dialogues, to unearth the patterns in prompts that lead to effective problem resolutions. The underlying motivation for this research is to enhance the collaboration between human developers and AI tools, thereby improving productivity and problem-solving efficacy in software development. Utilizing a combination of textual analysis and data-driven approaches, this paper seeks to identify the attributes of prompts that are associated with successful interactions, providing crucial insights for the strategic employment of ChatGPT in software engineering environments.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {142–146},
numpages = {5},
keywords = {data mining, large language model, LLM, ChatGPT},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3698364.3705347,
author = {Chang, Chen-Chia and Ho, Chia-Tung and Li, Yaguang and Chen, Yiran and Ren, Haoxing},
title = {DRC-Coder: Automated DRC Checker Code Generation Using LLM Autonomous Agent},
year = {2025},
isbn = {9798400712937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698364.3705347},
doi = {10.1145/3698364.3705347},
abstract = {In the advanced technology nodes, the integrated design rule checker (DRC) is often utilized in place and route tools for fast optimization loops for power-performance-area. Implementing integrated DRC checkers to meet the standard of commercial DRC tools demands extensive human expertise to interpret foundry specifications, analyze layouts, and debug code iteratively. However, this labor-intensive process, requiring to be repeated by every update of technology nodes, prolongs the turnaround time of designing circuits. In this paper, we present DRC-Coder, a multi-agent framework with vision capabilities for automated DRC code generation. By incorporating vision language models and large language models (LLM), DRC-Coder can effectively process textual, visual, and layout information to perform rule interpretation and coding by two specialized LLMs. We also design an auto-evaluation function for LLMs to enable DRC code debugging. Experimental results show that targeting on a sub-3nm technology node for a state-of-the-art standard cell layout tool, DRC-Coder achieves perfect F1 score 1.000 in generating DRC codes for meeting the standard of a commercial DRC tool, highly outperforming standard prompting techniques (F1=0.631). DRC-Coder can generate code for each design rule within four minutes on average, which significantly accelerates technology advancement and reduces engineering costs.},
booktitle = {Proceedings of the 2025 International Symposium on Physical Design},
pages = {143–151},
numpages = {9},
keywords = {code generation, design rule checking, large language model},
location = {Austin, TX, USA},
series = {ISPD '25}
}

@inproceedings{10.1145/3643795.3648379,
author = {Rasnayaka, Sanka and Wang, Guanlin and Shariffdeen, Ridwan and Iyer, Ganesh Neelakanta},
title = {An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648379},
doi = {10.1145/3643795.3648379},
abstract = {Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {111–118},
numpages = {8},
keywords = {LLM for code generation, software engineering},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3689187.3709607,
author = {Clear, Tony and Cajander, \r{A}sa and Clear, Alison and McDermott, Roger and Daniels, Mats and Divitini, Monica and Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria and Kleanthous, Styliani and Kultur, Can and Parvini, Ghazaleh and Polash, Mohammad and Zhu, Tingting},
title = {AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709607},
doi = {10.1145/3689187.3709607},
abstract = {As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction.IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies.The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace.The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {34–67},
numpages = {34},
keywords = {artificial intelligence, computing competencies, computing curricula, generative ai, it profession, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3706598.3713894,
author = {Krol, Stephen James and Llano Rodriguez, Maria Teresa and Loor Paredes, Miguel J},
title = {Exploring the Needs of Practising Musicians in Co-Creative AI Through Co-Design},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713894},
doi = {10.1145/3706598.3713894},
abstract = {Recent advances in generative AI music have resulted in new technologies that are being framed as co-creative tools for musicians with early work demonstrating their potential to add to music practice. While the field has seen many valuable contributions, work that involves practising musicians in the design and development of these tools is limited, with the majority of work including them only once a tool has been developed. In this paper, we present a case study that explores the needs of practising musicians through the co-design of a musical variation system, highlighting the importance of involving a diverse range of musicians throughout the design process and uncovering various design insights. This was achieved through two workshops and a two week ecological evaluation, where musicians from different musical backgrounds offered valuable insights not only on a musical system’s design but also on how a musical AI could be integrated into their musical practices.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {196},
numpages = {13},
keywords = {Co-Creative AI, Human-AI interaction, Co-creative music composition},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3726010.3726029,
author = {Liu, Baiqiang},
title = {Network Security Issues Caused by Generative Artificial Intelligence},
year = {2025},
isbn = {9798400712845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726010.3726029},
doi = {10.1145/3726010.3726029},
abstract = {It has recently emerged that generative Artificial Intelligence (AI) has been significantly developed and can create realistic speech, images, and video. Although such innovations provide many advantages, they pose major threats to the network's security. The research question of the study will, therefore, be the role and effects of generative artificial intelligence on network security, especially deepfake, artificial intelligence-driven phishing, and multimedia content manipulation. This research uses a case study analysis of real-life experiences of security threats in networked systems to show how generative AI can be misused. The findings reveal that: 1. Generative AI can create highly developed cyberattacks such as deepfake social engineering and AI-supplemented phishing, which use visually and audibly persuasive appeals to manipulate users. 2. The problem with this kind of security threat highlighted by AI is that most traditional security solutions lack awareness of such threats since they are incapable of effective real-time user authentication, over-dependence on visual and biometric verification, and lack of AI-awareness training. 3. There are significant gaps in cybersecurity policies and training. Many organizations lack formal protocols for addressing AI-related security risks, and often, organizations are applying generative AI technologies. In addressing the aforementioned vulnerabilities, the study recommends an improved security model that incorporates deepfake detection, more secure authentication, and even advanced anomaly detection systems. In furtherance, there should be improvements in policy that aim at commanding the AI media, ensuring comprehensive training of the employees on cybersecurity matters, and drives that aim at educating the public on how to enhance the strength for discouraging AI cybercrimes.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence, Digital Media Technology and Interaction Design},
pages = {132–136},
numpages = {5},
keywords = {Deepfakes, Generative AI, Manipulation of Multimedia Content, Network Security},
location = {
},
series = {ICADI '24}
}

@inproceedings{10.1145/3639477.3639751,
author = {Pinto, Gustavo and De Souza, Cleidson and Neto, Joao Batista and Souza, Alberto and Gotto, Tarci­sio and Monteiro, Edward},
title = {Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639751},
doi = {10.1145/3639477.3639751},
abstract = {With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and CoPilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information.In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot AI. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {408–417},
numpages = {10},
keywords = {LLM, LLM-based applications, LLM for code, LLM4code, code LLMs, challenges},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inbook{10.5555/3716662.3716671,
author = {Barnett, Julia and Kieslich, Kimon and Diakopoulos, Nicholas},
title = {Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation},
year = {2025},
publisher = {AAAI Press},
abstract = {The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n = 234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {82–93},
numpages = {12}
}

@inproceedings{10.1145/3544548.3581388,
author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
title = {Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581388},
doi = {10.1145/3544548.3581388},
abstract = {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {437},
numpages = {21},
keywords = {design tools, end-users, language models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3749986,
author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
title = {HLSRewriter: Efficient Refactoring and Optimization of C/C++ Code with LLMs for High-Level Synthesis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3749986},
doi = {10.1145/3749986},
abstract = {In High-Level Synthesis (HLS), refactoring a standard C/C++ code into its HLS-compatible version (HLS-C) still requires significant human effort. While various program scripts have been introduced to automate this process, the resulting code still contains many HLS-incompatible issues that need to be manually refactored and optimized by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated code refactoring and optimization in HLS. However, due to the limited training of LLMs, considering hardware and software simultaneously, hallucinations may occur when using LLMs for HLS, leading to synthesis failures. To address these challenges, we introduce HLSRewriter, an LLM-aided code refactoring and optimization framework that takes regular C/C++ code as input and automatically generates its corresponding optimized HLS-C code for hardware synthesis with minimal human intervention. To mitigate LLM hallucinations, a step-wise reasoning process is employed to analyze and detect HLS-incompatible errors. Afterwards, a repair library containing reference templates is efficiently created by scanning the HLS tool manual, followed by cooperation with a Retrieval-Augmented Generation (RAG) paradigm to guide the LLMs toward correct refactoring. In addition, a pipeline-aware decomposition strategy is introduced to progressively break down complex loop structures into smaller tasks with a balanced trade-off between latency and area, thereby enabling efficient pipelining and parallel execution. To further improve hardware efficiency, a bit width adjuster module is incorporated into this framework to optimize the precision of floating-point variables. Moreover, LLM-aided HLS optimization strategies are introduced to add/tune hardware directives in HLS-C code, thereby enhancing the performance of the final synthesized hardware. Experimental results demonstrate that the proposed LLM-aided framework can achieve higher refactoring pass rates and superior hardware performance in 24 real-world tasks compared with traditional approaches and the direct application of LLMs for code refactoring and optimization. The codes are open-sourced at this link: https://github.com/code-source1/catapult.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jul,
keywords = {Large language models, electronic design automation, high-level synthesis}
}

@proceedings{10.1145/3643795,
title = {LLM4Code '24: Proceedings of the 1st International Workshop on Large Language Models for Code},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the first edition of the InternationalWorkshop on Large Language Models for Code (LLM4Code). Large Language Models (LLMs), which are large-scale models being trained on massive textual corpora, have achieved significant advances in various domains, including Software Engineering (SE). Recently, there has been a growing interest in applying LLMs to assist software development and maintenance, such as code generation and comprehension, test generation, and program repair. Although the application of LLMs on code-relevant tasks has shown very promising performance, there is a huge potential to explore this growing domain further. The motivation of the LLM4Code workshop is to provide a platform for academics and practitioners to discuss and share their ideas on applying and developing LLMs to solve code-relevant problems in SE activities.The LLM4Code workshop is concerned with the research on how to better apply LLMs to solve code-relevant tasks, how to design better LLMs for code-relevant tasks, and how to better benchmark LLMs on code-relevant tasks. The workshop aims to achieve multiple goals as follows. Firstly, the workshop aims to provide an opportunity for participants to discuss novel ideas and preliminary results on LLMs for solving code-relevant SE problems, to exchange the latest progress in this domain. Secondly, the workshop aims to encourage participants to discuss the open challenges and problems of LLM4code, to identify important future directions in this domain. Finally, the workshop aims to encourage participants to share infrastructures and benchmarks that are foundational and beneficial for future research in this domain.},
location = {Lisbon, Portugal}
}

@article{10.14778/3705829.3705832,
author = {Tang, Xiu and Liu, Wenhao and Wu, Sai and Yao, Chang and Yuan, Gongsheng and Ying, Shanshan and Chen, Gang},
title = {QueryArtisan: Generating Data Manipulation Codes for Ad-hoc Analysis in Data Lakes},
year = {2024},
issue_date = {October 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3705829.3705832},
doi = {10.14778/3705829.3705832},
abstract = {Query processing over data lakes is a challenging task, often requiring extensive data pre-processing activities such as data cleaning, transformation, and loading. However, the advent of Large Language Models (LLMs) has illuminated a new pathway to address these complexities by offering a unified approach to understanding the diverse datasets submerged in data lakes. In this paper, we introduce QueryArtisan, a novel LLM-powered analytic tool specifically designed for data lakes. QueryArtisan transcends traditional ETL (Extract, Transform, Load) processes by generating just-intime code for dataset-specific queries. It eliminates the need for an intermediary schema, enabling users to query the data lake directly using natural language. To achieve this, we have developed a suite of heterogeneous operators capable of processing data across various modalities. Additionally, QueryArtisan incorporates a cost model-based query optimization technique, significantly enhancing its code generation capabilities for efficient query resolution. Our extensive experimental evaluations, conducted with real-life datasets, demonstrate that QueryArtisan markedly outperforms existing solutions in terms of effectiveness, efficiency and usability.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {108–116},
numpages = {9}
}

@inproceedings{10.1145/3691620.3695468,
author = {JianWang and Liu, Shangqing and Xie, Xiaofei and Li, Yi},
title = {An Empirical Study to Evaluate AIGC Detectors on Code Content},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695468},
doi = {10.1145/3691620.3695468},
abstract = {Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&amp;A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {844–856},
numpages = {13},
keywords = {AIGC detection, code generation, large language model},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3641555.3704749,
author = {Hare, Brian K. and Gladbach, Joan and Shah, S. Jawad and Xu, Dianxiang},
title = {Building AI-Powered Responsible Workforce by Integrating Large Language Models into Computer Science Curriculum},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704749},
doi = {10.1145/3641555.3704749},
abstract = {Software development is undergoing a revolutionary transformation, fueled by remarkable advancements in Large Language Models (LLMs). This wave of innovation is reshaping the entire landscape and holds the promise of streamlining the development process, leading to increased productivity and efficiency. By providing text prompts, developers can now receive entirely generated code outputs, representing a fundamental shift in how software is built. This paradigm change can accelerate development cycles and unlock new levels of creativity and ingenuity, resulting in the realization of novel applications and business outcomes. However, this paradigm shift also brings new challenges and necessitates acquiring additional skills for software developers to fully harness the capabilities of LLM-powered tools. These skills include prompt engineering for software development, structural complexity management, debugging of AI errors, and compliance with ethical guidelines and principles.The special session will introduce our NSF-sponsored 3-year project, which aims to integrate LLMs into the standard CS curriculum. To the best of our knowledge, this project is among the first department-level initiatives to renovate CS curriculum, rather than individual courses, with the new developments of LLMs. Our project focuses on (a) enhancing students' problem-solving and programming skills by leveraging LLMs as a learning tool in core programming courses, (b) improving students' software development skills by integrating LLM-powered tools into the software engineering course sequence, and (c) educating students on ethical and responsible AI practices. The special session will discuss the objectives and methods of our project, as well as the current results and lessons learned.This NSF-supported project aims to integrate LLMs into the standard CS curriculum. The revolutionized computer science education will cultivate a new generation of AI-powered responsible developers. The objectives are to enhance student programming, software development, and problem-solving skills; educate students on ethical and responsible AI practices; and develop faculty development materials and workshops. Our presentation will discuss the objectives and methods of our project, currently in year 1 of a 3-year timeline.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1709–1710},
numpages = {2},
keywords = {AI, curriculum development, large language models, undergraduate education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3638529.3654017,
author = {Nasir, Muhammad Umair and Earle, Sam and Togelius, Julian and James, Steven and Cleghorn, Christopher},
title = {LLMatic: Neural Architecture Search Via Large Language Models And Quality Diversity Optimization},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638529.3654017},
doi = {10.1145/3638529.3654017},
abstract = {Large language models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. Here, we propose using the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks. We test LLMatic on the CIFAR-10 and NAS-bench-201 benchmarks, demonstrating that it can produce competitive networks while evaluating just 2, 000 candidates, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark. The open-sourced code is available at https://github.com/umair-nasir14/LLMatic.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1110–1118},
numpages = {9},
keywords = {large language models, neural networks, quality-diversity optimization, neural architecture search},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@inproceedings{10.1145/3663532.3664466,
author = {Paduraru, Ciprian and Stefanescu, Alin and Jianu, Augustin},
title = {Unit Test Generation using Large Language Models for Unity Game Development},
year = {2024},
isbn = {9798400706745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663532.3664466},
doi = {10.1145/3663532.3664466},
abstract = {Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development.  This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.},
booktitle = {Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games},
pages = {7–13},
numpages = {7},
keywords = {game development, large language models, unit testing},
location = {Porto de Galinhas, Brazil},
series = {FaSE4Games 2024}
}

@inproceedings{10.1145/3664647.3680665,
author = {Luo, Daqin and Feng, Chengjian and Nong, Yuxuan and Shen, Yiqing},
title = {AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680665},
doi = {10.1145/3664647.3680665},
abstract = {Automated Machine Learning (AutoML) offers a promising approach to streamline the training of machine learning models. However, existing AutoML frameworks are often limited to unimodal scenarios and require extensive manual configuration. Recent advancements in Large Language Models (LLMs) have showcased their exceptional abilities in reasoning, interaction, and code generation, presenting an opportunity to develop a more automated and user-friendly framework. To this end, we introduce AutoM3L, an innovative Automated Multimodal Machine Learning framework that leverages LLMs as controllers to automatically construct multimodal training pipelines. AutoM3L comprehends data modalities and selects appropriate models based on user requirements, providing automation and interactivity. By eliminating the need for manual feature engineering and hyperparameter optimization, our framework simplifies user engagement and enables customization through directives, addressing the limitations of previous rule-based AutoML approaches. We evaluate the performance of AutoM3L on six diverse multimodal datasets spanning classification, regression, and retrieval tasks, as well as a comprehensive set of unimodal datasets. The results demonstrate that AutoM3L achieves competitive or superior performance compared to traditional rule-based AutoML methods. Furthermore, a user study highlights the user-friendliness and usability of our framework, compared to the rule-based AutoML methods.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {8586–8594},
numpages = {9},
keywords = {automated machine learning, human-ai interaction, large language model, usability, user study},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3696461,
author = {Hota, Aritra and Chatterjee, Soumyajit and Chakraborty, Sandip},
title = {Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3696461},
doi = {10.1145/3696461},
abstract = {Traditional human-in-the-loop-based annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent large language models (LLMs) are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore the opportunities in using these LLMs as virtual annotators where the LLMs will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems of the traditional human-in-the-loop approach. Motivated by this observation, we perform a detailed study in this paper to assess whether the state-of-the-art (SOTA) LLMs can be used as virtual annotators for labeling time-series physical sensing data. To perform this in a principled manner, we segregate the study into two major phases. In the first phase, we investigate the challenges an LLM like GPT-4 faces in comprehending raw sensor data. Considering the observations from phase 1, in the next phase, we investigate the possibility of encoding the raw sensor data using SOTA SSL approaches and utilizing the projected time-series data to get annotations from the LLM. Detailed evaluation with four benchmark HAR datasets shows that SSL-based encoding and metric-based guidance allow the LLM to make more reasonable decisions and provide accurate annotations without requiring computationally expensive fine-tuning or sophisticated prompt engineering.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
keywords = {Large Language Models, Human-in-the-Loop, Time-series Data}
}

@inproceedings{10.1145/3638530.3654104,
author = {Liu, Yueyue and Zhang, Hongyu and Le, Van-Hoang and Miao, Yuantian and Li, Zhiqiang},
title = {Local Search-based Approach for Cost-effective Job Assignment on Large Language Models},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3654104},
doi = {10.1145/3638530.3654104},
abstract = {Large Language Models (LLMs) have garnered significant attention due to their impressive capabilities. However, leveraging LLMs can be expensive due to the computational resources required, with costs depending on invocation numbers and input prompt lengths. Generally, larger LLMs deliver better performance but at a higher cost. In addition, prompts that provide more guidance to LLMs can increase the probability of correctly processing the job but also tend to be longer, increasing the processing cost. Therefore, selecting an appropriate LLM and prompt template is crucial for achieving an optimal trade-off between cost and performance. This paper formulates the job assignment on LLMs as a multi-objective optimisation problem and proposes a local search-based algorithm, termed LSAP, which aims to minimise the invocations cost while maximising overall performance. First, historical data is used to estimate the accuracy of each job submitted to a candidate LLM with a chosen prompt template. Subsequently, LSAP combines heuristic rules to select an appropriate LLM and prompt template based on the invocation cost and estimated accuracy. Extensive experiments on LLM-based log parsing, a typical software maintenance task that utilizes LLMs, demonstrate that LSAP can efficiently generate solutions with significantly lower cost and higher accuracy compared to the baselines.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {719–722},
numpages = {4},
keywords = {large language models, job assignment, local search, log parsing},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3691422.3691471,
author = {Ma, Xiaoqian},
title = {The potential legal risks of artificial intelligence},
year = {2025},
isbn = {9798400717260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691422.3691471},
doi = {10.1145/3691422.3691471},
abstract = {ChatGPT, as a typical application of generative artificial intelligence, means that human society is moving towards the "era of high knowledge revolution". From the perspective of functionalism, generative artificial intelligence will certainly have an important impact on the reshaping of legal society at the level of "instrument" and "Tao". However, while generative AI is deeply embedded in Chinese society, it also impacts personal information security on a large scale, challenges national security, causes intellectual property rights disputes, and academic ethics irregularities. Therefore, it is necessary for the national regulatory authorities to exercise careful governance, formulate accurate and fair market access guidelines and mechanism accountability systems, improve the public's awareness of risk prevention, and gradually improve the generative AI governance system.},
booktitle = {Proceedings of the 2024 15th International Conference on E-Business, Management and Economics},
pages = {366–370},
numpages = {5},
keywords = {ChatGPT, Generative artificial intelligence, Legal regulation, Risk management},
location = {
},
series = {ICEME '24}
}

@article{10.1145/3661145,
author = {Weber, Thomas and Brandmaier, Maximilian and Schmidt, Albrecht and Mayer, Sven},
title = {Significant Productivity Gains through Programming with Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3661145},
doi = {10.1145/3661145},
abstract = {Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {256},
numpages = {29},
keywords = {github copilot, gpt, language models, programming, software development, user study}
}

@inproceedings{10.1145/3696630.3728555,
author = {Krishna, Rahul and Pan, Rangeet and Sinha, Saurabh and Tamilselvam, Srikanth and Pavuluri, Raju and Vukovic, Maja},
title = {Codellm-Devkit: A Framework for Contextualizing Code LLMs with Program Analysis Insights},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728555},
doi = {10.1145/3696630.3728555},
abstract = {Large Language Models for Code (or code LLMs) are increasingly gaining popularity and capabilities, offering a wide array of functionalities such as code completion, code generation, code summarization, test generation, code translation, and more. To leverage code LLMs to their full potential, developers must provide code-specific contextual information to the models. These are typically derived and distilled using program analysis tools. However, there exists a significant gap—these static analysis tools are often language-specific and come with a steep learning curve, making their effective use challenging. These tools are tailored to specific program languages, requiring developers to learn and manage multiple tools to cover various aspects of the their code base. Moreover, the complexity of configuring and integrating these tools into the existing development environments add an additional layer of difficulty. This challenge limits the potential benefits that could be gained from more widespread and effective use of static analysis in conjunction with LLMs.To address this challenge, we present codellm-devkit (hereafter, cldk), an open-source library that significantly simplifies the process of performing program analysis at various levels of granularity for different programming languages to support code LLM use cases. As a Python library, cldk offers developers an intuitive and user-friendly interface, making it incredibly easy to provide rich program analysis context to code LLMs. With this library, developers can effortlessly integrate detailed, code-specific insights that enhance the operational efficiency and effectiveness of LLMs in coding tasks. CLDK is available as an open-source library at https://github.com/codellm-devkit.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {308–318},
numpages = {11},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3639476.3639757,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Roney, Lindsay and Santos, Joanna C. S.},
title = {Re(gEx|DoS)Eval: Evaluating Generated Regular Expressions and their Proneness to DoS Attacks},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639757},
doi = {10.1145/3639476.3639757},
abstract = {With the recent advances of code generation techniques based on Large Language Models (LLMs), developers are using them for a vast range of tasks, including regex generation. Despite the efforts to generate regexes from natural language, there is no benchmark for LLMs with real-world data and robust test sets. Moreover, a regex can be prone to Denial of Service (DoS) attacks due to catastrophic backtracking. Hence, we need a systematic evaluation process to evaluate the correctness and security of the regexes generated by the language models. In this paper, we describe Re(gEx|DoS)Eval, a framework that includes a dataset of 762 regex descriptions (prompts) from real users, refined prompts with examples, and a robust set of tests. We introduce the pass@k and vulnerable@k metrics to evaluate the generated regexes based on the functional correctness and proneness of ReDoS attacks. Moreover, we demonstrate the Re(gEx|DoS)Eval with three LLMs (T5, Phi, and GPT-3), and describe the future plans to extend this framework.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {52–56},
numpages = {5},
keywords = {regex generation, ReDoS, DoS attack, evaluation, dataset},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3637528.3671622,
author = {Fischer, Sophie and Gemmell, Carlos and Tecklenburg, Niklas and Mackie, Iain and Rossetto, Federico and Dalton, Jeffrey},
title = {GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671622},
doi = {10.1145/3637528.3671622},
abstract = {We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4951–4961},
numpages = {11},
keywords = {conversational task assistants, large language models},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3691620.3695536,
author = {Chen, Mouxiang and Liu, Zhongxin and Tao, He and Hong, Yusu and Lo, David and Xia, Xin and Sun, Jianling},
title = {B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695536},
doi = {10.1145/3691620.3695536},
abstract = {Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy ℬ4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50% over the strongest heuristic and 246% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1693–1705},
numpages = {13},
keywords = {code generation, software engineering, large language models},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3561833.3568496,
author = {Rajamani, Sriram},
title = {AI Assisted Programming},
year = {2022},
isbn = {9781450397759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561833.3568496},
doi = {10.1145/3561833.3568496},
abstract = {We present a vision for a futuristic programming environment, where ML models and logical rules co-exist and evolve over time. We substantiate our vision using three case studies: (1) safe code generation using large language models, (2) heterogeneous data extraction, and (2) regulatory compliance for online advertising. Using these case studies, we hypothesize how such a futuristic programming system can balance productivity with safety, security and reliability, present new research results, and point to directions for future work.},
booktitle = {Proceedings of the 15th Annual ACM India Compute Conference},
pages = {5},
numpages = {1},
location = {Jaipur, India},
series = {COMPUTE '22}
}

@inproceedings{10.1145/3672919.3672971,
author = {Gao, Peng and Qiu, Feng and Hua, Baojian},
title = {ChemGen: Towards Understanding First-Principles Calculation Code Generation Based on Large Language Models},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672919.3672971},
doi = {10.1145/3672919.3672971},
abstract = {First-principles calculation software, grounded in quantum chemistry theories, is indispensable in scientific research. However, the development of such software requires the amalgamation of multidisciplinary knowledge, posing a significant challenge to developers. We propose an approach to utilize large language models (LLMs) for automatically generating code for first-principles calculations. Building on this concept, we have designed and implemented ChemGen, a fully automated framework to assist in generating and evaluating code for first-principles calculations. Meanwhile, we have developed a benchmark named ChemEval, which includes 24 code generation tasks tailored for first-principles calculations. Our experiments, conducted using three leading LLMs—GPT-3.5 Turbo, Gemini Pro, and WizardCoder-Python-13B—indicate that these models can generate functionally correct code for 79.17% of the tasks in ChemEval. Additionally, for each of the LLMs used, the median cyclomatic complexity of the generated code did not exceed 3. Furthermore, the application of the knowledge generation prompting technique improves the accuracy of the produced code.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
pages = {281–287},
numpages = {7},
location = {Nanjing, China},
series = {CSAIDE '24}
}

@inproceedings{10.1145/3587103.3594206,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Caspersen, Michael E. and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education: An ITiCSE Working Group Conducted by Humans},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594206},
doi = {10.1145/3587103.3594206},
abstract = {The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {561–562},
numpages = {2},
keywords = {AI, CS1, GPT, GitHub, LLM, artificial intelligence, code generation, codex, computer programming, copilot, large language models, novice programming, openAI, pedagogical practices},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3672919.3672955,
author = {Yao, Wenyan and Zhang, Tianbao},
title = {Design of Human Resources Management Decision System Based on Multi-Agent System and Reinforcement Learning Algorithm},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672919.3672955},
doi = {10.1145/3672919.3672955},
abstract = {This study aims to address the lack of scientific and systematic decision systems in the field of Human Resources Management (HRM). By designing a HRM decision support system based on Multi-Agent systems and reinforcement learning algorithms, effective tools are provided to HR managers to assist them in making more scientific and systematic HRM decisions. The research analyzes the current issues in HRM practices and proposes comprehensive solutions. Through the optimization of Multi-Agent reinforcement learning algorithms, experiments validate the effectiveness of the system in supporting decision-making in HRM. The results demonstrate that the improved algorithms outperform traditional methods, confirming the efficacy of the system's design and optimization. This HRM decision support system, based on Multi-Agent systems and reinforcement learning algorithms, holds the potential to drive organizational development and enhance the efficiency of HRM. However, further research and practical application are needed to refine and optimize the system to adapt to the constantly evolving HRM environment.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
pages = {183–187},
numpages = {5},
location = {Nanjing, China},
series = {CSAIDE '24}
}

@inproceedings{10.1145/3709025.3712216,
author = {Gray, Morgan and Zhang, Li and Ashley, Kevin D.},
title = {Generating Case-Based Legal Arguments with LLMs},
year = {2025},
isbn = {9798400714214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709025.3712216},
doi = {10.1145/3709025.3712216},
abstract = {Over its decades long history, the field of AI and Law has made significant progress developing and researching formal models of case based reasoning that are capable of producing legal arguments. These models employ argument schemes to replicate legal argumentation. Although their arguments are accurate and explainable, these systems are costly to produce and maintain, requiring manual case representations and expert-crafted algorithms that mimic argument. To address these limitations we employ a prompt-engineering strategy that leads state-of-the-art LLMs to follow argument schemes. We show that it is feasible for LLMs to produce basic case-based legal arguments.},
booktitle = {Proceedings of the 2025 Symposium on Computer Science and Law},
pages = {160–168},
numpages = {9},
keywords = {Argument Scheme, Factors, Large Language Models, Legal Arguments, Legal Reasoning},
location = {Munich, Germany},
series = {CSLAW '25}
}

@inproceedings{10.1145/3613905.3650794,
author = {Kwon, Huisung and Choi, Yunjae Josephine and Lee, Sunok and Lee, Sangsu},
title = {Unveiling the Inherent Needs: GPT Builder as Participatory Design Tool for Exploring Needs and Expectation of AI with Middle-Aged Users},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650794},
doi = {10.1145/3613905.3650794},
abstract = {A generative session that directly involves users in the design process is an effective way to design user-centered experiences by uncovering intrinsic needs. However, engaging users who lack coding knowledge in AI system design poses significant challenges. Recognizing this, the recently revealed GPT-creating tool, which allows users to customize ChatGPT through simple dialog interactions, is a promising solution. We aimed to identify the possibility of using this tool to uncover intrinsic users’ needs and expectations towards AI. We conducted individual participatory design workshops with generative sessions focusing on middle-aged individuals. This approach helped us to delve into the latent needs and expectations of conversational AI among this demographic. We discovered a wide range of unexpressed needs and expectations for AI among them. Our research highlights the potential and value of using the GPT-creating tool as a design method, particularly for revealing the users’ unexpressed needs and expectations.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {358},
numpages = {6},
keywords = {Artificial Intelligence (AI), GPT, Generative Design, Inherent needs, LLM-based CAs, Participatory Design (PD)},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3650212.3680342,
author = {He, Yifeng and Huang, Jiabo and Rong, Yuyang and Guo, Yiwen and Wang, Ethan and Chen, Hao},
title = {UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680342},
doi = {10.1145/3650212.3680342},
abstract = {The remarkable capability of large language models (LLMs) in                                                                 generating high-quality code has drawn increasing attention                                                                 in the software testing community.                                                                However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests                                                                since they were trained on code snippets collected without                                                                 differentiating between code for testing and for other purposes.                                                                In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis.                                                                 Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified.                                                                By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale.                                                                Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs.                                                                Our experiments demonstrate that,                                                                 by building an autoregressive LLM based on UniTSyn,                                                                we can achieve significant benefits in learning and understanding unit test representations,                                                                 resulting in improved generation accuracy and code coverage                                                                 across all the evaluated programming languages.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1061–1072},
numpages = {12},
keywords = {Large language models, dataset, software testing, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3757062,
author = {Yang, Fu-Chia and Guo, Siqi and Mousas, Christos},
title = {Exploring Familiarity and Knowledgeability in Conversational Virtual Agents},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3558},
url = {https://doi.org/10.1145/3757062},
doi = {10.1145/3757062},
abstract = {In this study, we examined the impact of agent familiarity and knowledgeability on several variables spanning agent perceptions (i.e., perceived knowledge, familiarity, trust, anthropomorphism, uncanny valley effect, and likability), social and emotional experiences (i.e., co-presence, rapport, cognitive process expectations, and willingness for future interaction), and conversation dynamics (i.e., conversation transcript, participants’ response word count, and response time). We created two virtual agents for the study: a digital replica of a professor from our department (i.e., familiar agent) and an agent with similar demographic variables (i.e., age, gender, and ethnicity) but with a fabricated appearance and voice (i.e., unfamiliar agent). We implemented both agents to exhibit two levels of knowledgeability (i.e., low and high) in the domain of game development and course-specific information. We used large language models (LLMs) to provide the agents with persona information and domain knowledge through prompt engineering. For our user study, we followed a 2 (familiarity: unfamiliar vs. familiar agent)  (times)  2 (knowledgeability: low vs. high knowledgeability) within-group study design and recruited 32 participants who engaged in a five-minute, conversation-based virtual reality (VR) interaction with all four experimental conditions: unfamiliar agent with low knowledgeability (ULK), unfamiliar agent with high knowledgeability (UHK), familiar agent with low knowledgeability (FLK), and familiar agent with high knowledgeability (FHK). The findings demonstrated a significant main effect of agent familiarity on perceived knowledge, suggesting that familiarity plays a crucial role in shaping users’ perception of the agent's knowledgeability level. Besides perceived knowledge, familiarity also affected all other variables, apart from co-presence. Conversely, agent knowledgeability affected perceived familiarity, trust, anthropomorphism, cognitive process expectations, willingness for future interaction, conversation content, and participants’ response word count. Finally, we found an interaction effect between agent familiarity and perceived knowledge, indicating that familiarity has a significant influence on users’ perceptions of the agent's knowledgeability. This study contributes to the field of conversational human-agent interaction in VR by providing empirical evidence on how adapting both familiarity and knowledgeability of virtual agents can significantly enhance user experience, offering valuable insights into designing more engaging, trustworthy, and effective embodied conversational agents.},
note = {Just Accepted},
journal = {ACM Trans. Appl. Percept.},
month = jul,
keywords = {Virtual Reality, Familiarity, Knowledgeability, Embodied Conversational Agents, Large Language Models}
}

@inproceedings{10.1145/3650212.3680308,
author = {Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik},
title = {Oracle-Guided Program Selection from Large Language Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680308},
doi = {10.1145/3650212.3680308},
abstract = {While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {628–640},
numpages = {13},
keywords = {code generation, differential testing, large language model, oracle inference},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3691620.3695552,
author = {Feng, Jia and Liu, Jiachen and Gao, Cuiyun and Chong, Chun Yong and Wang, Chaozheng and Gao, Shan and Xia, Xin},
title = {ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695552},
doi = {10.1145/3691620.3695552},
abstract = {In recent years, with the widespread attention of academia and industry on the application of large language models (LLMs) to code-related tasks, an increasing number of large code models (LCMs) have been proposed and corresponding evaluation benchmarks have continually emerged. Although existing evaluation benchmarks are helpful for comparing different LCMs, they may not reflect the performance of LCMs in various development scenarios. Specifically, they might evaluate model performance in only one type of scenario (e.g., code generation or code completion), whereas real development contexts are diverse and may involve multiple tasks such as code generation, code completion, API recommendation, and test function generation. Additionally, the questions may not originate from actual development practices, failing to capture the programming challenges faced by developers during the development process.To address the aforementioned issues, we propose Complex-CodeEval, a new benchmark for evaluating the performance of LCMs in various development scenarios. ComplexCodeEval includes 3,897 Java samples from 1,055 high-star GitHub repositories and 7,184 Python samples from 2,107 high-star repositories. Each function sample in ComplexCodeEval contains multiple annotations (e.g., function signatures, docstrings and reference APIs) to accommodate various downstream tasks. Furthermore, to better reflect diverse development scenarios, each function sample is required to originate from a repository that depends on at least one selected library (based on popularity), and each function sample must invoke at least one API from the selected library. Additionally, each function sample has multiple timestamps to avoid data leakage. Based on ComplexCodeEval, we evaluate the performance of ten LCMs across four tasks (i.e., code generation, code completion, API recommendation, and test case generation) to explore their performance in complex development environments. Furthermore, we conduct an in-depth analysis of the impact of context and data leakage on model performance. Our experimental results reveal several key findings. For instance, LCMs exhibit varying performance across different coding tasks. Additionally, rich contextual information can greatly enhance the performance of LCMs. Moreover, using leaked data for evaluation may lead to an overestimation of model performance, resulting in inaccurate evaluation outcomes that deviate from the performance in practice.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1895–1906},
numpages = {12},
keywords = {large language models, code intelligence, benchmark},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3650212.3680328,
author = {Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Jin, Shunfu},
title = {CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680328},
doi = {10.1145/3650212.3680328},
abstract = {With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs’ realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM’s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs’ conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors’ workload and improving students’ learning experience, showing promise for code review and other software engineering tasks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {882–894},
numpages = {13},
keywords = {Large Language Model, Open Source, Program Repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3674805.3690746,
author = {Almeida, Aylton and Xavier, Laerte and Valente, Marco Tulio},
title = {Automatic Library Migration Using Large Language Models: First Results},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3690746},
doi = {10.1145/3674805.3690746},
abstract = {Despite being introduced only a few years ago, Large Language Models (LLMs) are already widely used by developers for code generation. However, their application in automating other Software Engineering activities remains largely unexplored. Thus, in this paper, we report the first results of a study in which we are exploring the use of ChatGPT to support API migration tasks, an important problem that demands manual effort and attention from developers. Specifically, in the paper, we share our initial results involving the use of ChatGPT to migrate a client application to use a newer version of SQLAlchemy, an ORM (Object Relational Mapping) library widely used in Python. We evaluate the use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts) and show that the best results are achieved by the One-Shot prompt, followed by the Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to successfully migrate all columns of our target application and upgrade its code to use new functionalities enabled by SQLAlchemy’s latest version, such as Python’s asyncio and typing modules, while preserving the original code behavior.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {427–433},
numpages = {7},
keywords = {API Migration, ChatGPT, Large Language Models, Python, SQLAlchemy},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@inproceedings{10.1109/ASE56229.2023.00148,
author = {Feldt, Robert and Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
title = {Towards Autonomous Testing Agents via Conversational Large Language Models},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00148},
doi = {10.1109/ASE56229.2023.00148},
abstract = {Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized "hallucination" of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1688–1693},
numpages = {6},
keywords = {software testing, machine learning, large language model, artificial intelligence, test automation},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3640543.3645148,
author = {Kim, Yoonsu and Lee, Jueon and Kim, Seoyoung and Park, Jaehyuk and Kim, Juho},
title = {Understanding Users’ Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645148},
doi = {10.1145/3640543.3645148},
abstract = {Large language models (LLMs) with chat-based capabilities, such as ChatGPT, are widely used in various workflows. However, due to a limited understanding of these large-scale models, users struggle to use this technology and experience different kinds of dissatisfaction. Researchers have introduced several methods, such as prompt engineering, to improve model responses. However, they focus on enhancing the model’s performance in specific tasks, and little has been investigated on how to deal with the user dissatisfaction resulting from the model’s responses. Therefore, with ChatGPT as the case study, we examine users’ dissatisfaction along with their strategies to address the dissatisfaction. After organizing users’ dissatisfaction with LLM into seven categories based on a literature review, we collected 511 instances of dissatisfactory ChatGPT responses from 107 users and their detailed recollections of dissatisfactory experiences, which we released as a publicly accessible dataset. Our analysis reveals that users most frequently experience dissatisfaction when ChatGPT fails to grasp their intentions, while they rate the severity of dissatisfaction related to accuracy the highest. We also identified four tactics users employ to address their dissatisfaction and their effectiveness. We found that users often do not use any tactics to address their dissatisfaction, and even when using tactics, 72% of dissatisfaction remained unresolved. Moreover, we found that users with low knowledge of LLMs tend to face more dissatisfaction on accuracy while they often put minimal effort in addressing dissatisfaction. Based on these findings, we propose design implications for minimizing user dissatisfaction and enhancing the usability of chat-based LLM.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {385–404},
numpages = {20},
keywords = {Chat-based LLM, ChatGPT, Knowledge-level, Large Language Models, Resolving tactics, User-side dissatisfaction, datasets},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3701625.3701656,
author = {Malheiros, Phelipe Silva and Lima, Rayfran Rocha and Oran, Ana Carolina},
title = {Impact of Generative AI Technologies on Software Development Professionals' Perceptions of Job Security},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701656},
doi = {10.1145/3701625.3701656},
abstract = {CONTEXT: The rapid evolution of generative AI technologies has sparked significant interest in the software development sector. These technologies promise to automate repetitive tasks, improve code quality, and assist in various managerial and technical activities. Despite the advancements and benefits of AI tools, there are concerns about how these technologies might impact job security for software development professionals. The main question is whether these professionals feel threatened by the possibility of being replaced by AI tools. OBJECTIVE: This study aims to investigate the impact of generative AI technologies on software development professionals, particularly focusing on their perceptions of job security. The research seeks to determine whether these professionals feel threatened by AI technologies and what their expectations are for the future. METHOD: An opinion survey was conducted with professionals in the software development field. The questionnaire, divided into sections on personal characteristics, AI usage, perceptions of job impact, confidence, and recommendation, was distributed online and analyzed using quantitative and qualitative methods. RESULTS: The survey revealed evidence that the majority of professionals do not currently feel threatened by AI tools, but there is uncertainty about the future, especially in the long term. The study also highlighted the main benefits of AI tools in reducing time spent on coding and improving documentation quality. However, challenges such as clarity in communication with AI and technical limitations were also noted. CONCLUSION: Generative AI technologies are seen as complementary to human work, enhancing the efficiency and quality of software development. However, the uncertainty about long-term job security suggests the need for balanced strategies to integrate these technologies in a way that maximizes benefits while minimizing risks.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {169–178},
numpages = {10},
keywords = {Job Security, Generative AI, AI-supported software development},
location = {
},
series = {SBQS '24}
}

@article{10.1145/3728940,
author = {Xue, Pengyu and Wu, Linhao and Yang, Zhen and Wang, Chengyi and Li, Xiang and Zhang, Yuxiang and Li, Jia and Jin, Ruikai and Pei, Yifei and Shen, Zhaoyan and Lyu, Xiran and Keung, Jacky Wai},
title = {ClassEval-T: Evaluating Large Language Models in Class-Level Code Translation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728940},
doi = {10.1145/3728940},
abstract = {In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development.    To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs' performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate eight recent LLMs of commercial, general, and code kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs' ability to dependency awareness when translating class samples. Finally, 1,243 failure cases made by the best-performing LLM under test are thoroughly analyzed and categorized in this paper for practical guidance and future enlightenment.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA063},
numpages = {24},
keywords = {Benchmark, Class-Level Code Translation, Large Language Models}
}

@inproceedings{10.1145/3643796.3648451,
author = {Zharov, Yaroslav and Khudyakov, Yury and Fedotova, Evgeniia and Grigorenko, Evgeny and Bogomolov, Egor},
title = {Tool-augmented LLMs as a Universal Interface for IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648451},
doi = {10.1145/3643796.3648451},
abstract = {Modern-day Integrated Development Environments (IDEs) have come a long way from the early text editing utilities to the complex programs encompassing thousands of functions to help developers. However, with the increasing number of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated software with a steep learning curve. The rise of the Large Language Models (LLMs) capable of both natural language dialogue and code generation leads to a discourse on the obsolescence of the concept of IDE. In this work, we offer a view on the place of the LLMs in the IDEs as the universal interface wrapping the IDE facilities. We envision a model that is able to perform complex actions involving multiple IDE features upon user command, stripping the user experience of the tedious work involved in searching through options and actions. For the practical part of the work, we engage with the works exploring the ability of LLMs to call for external tools to expedite a given task execution. We showcase a proof-of-concept of such a tool.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {40–42},
numpages = {3},
keywords = {IDE, LLM, ToolFormer},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3649217.3653582,
author = {Smith, David H. and Zilles, Craig},
title = {Code Generation Based Grading: Evaluating an Auto-grading Mechanism for "Explain-in-Plain-English" Questions},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653582},
doi = {10.1145/3649217.3653582},
abstract = {Comprehending and conveying the purpose of code is often cited as being a key learning objective within introductory programming courses. To address this objective, "Explain in Plain English'' questions, where students are shown a segment of code and asked to provide an abstract description of the code's purpose, have been adopted. However, given EiPE questions require a natural language response, they often require manual grading which is time-consuming for course staff and delays feedback for students. With the advent of large language models (LLMs) capable of generating code, responses to EiPE questions can be used to generate code segments, the correctness of which can then be easily verified using test cases. We refer to this approach as "Code Generation Based Grading'' (CGBG) and in this paper we explore its agreement with human graders using EiPE responses from past exams in an introductory programming course taught in Python. Overall, we find that all CGBG approaches achieve moderate agreement with human graders with the primary area of disagreement being its leniency with respect to low-level and line-by-line descriptions of code.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {171–177},
numpages = {7},
keywords = {auto-grading, eipe, gpt-4, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@proceedings{10.1145/3688864,
title = {ESGMFM '24: Proceedings of the 1st Workshop on Efficiency, Security, and Generalization of Multimedia Foundation Models},
year = {2024},
isbn = {9798400711916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 1st Workshop on Efficiency, Security, and Generalization of Multimedia Foundation Models (ESGMFM' 24). The workshop is co-located with ACM Multimedia 2024. The rapid progress in foundation models has notably enhanced the capabilities of deep learning models across a broad spectrum of tasks. Despite their exceptional accuracy, deploying these models in practical settings raises several concerns, particularly regarding efficiency, security, and generalization. As the utility of foundation models in multimedia applications becomes increasingly evident, addressing these issues is crucial. This workshop provides a timely venue for the discussion on efficiency, security, and generalization of foundation models in multimedia tasks or multi-modality methods. The mission of this workshop is to gather innovative contributions and perspectives from the community, unveiling advanced techniques for reinforcing multimedia foundation models. Our objective is to collectively tackle the challenges of multimedia foundation models, fostering their practical application and impact.},
location = {Melbourne VIC, Australia}
}

@inproceedings{10.1145/3664476.3664510,
author = {May, Richard and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {SoK: How Artificial-Intelligence Incidents Can Jeopardize Safety and Security},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664510},
doi = {10.1145/3664476.3664510},
abstract = {In the past years, a growing number of highly-automated systems has build on Artificial-Intelligence (AI) capabilities, for example, self-driving vehicles or predictive health-state diagnoses. As for any software system, there is a risk that misbehavior occurs (e.g., system failure due to bugs) or that malicious actors aim to misuse the system (e.g., generating attack scripts), which can lead to safety and security incidents. While software safety and security incidents have been studied in the past, we are not aware of research focusing on the specifics of AI incidents. With this paper, we aim to shed light on this gap through a case survey of 240 incidents that we elicited from four datasets comprising safety and security incidents involving AI from 2014 to 2023. Using manual data analyses and automated topic modeling, we derived relevant topics as well as the major issues and contexts in which the incidents occurred. We find that the topic of AI incidents is, not surprisingly, becoming more and more relevant, particularly in the contexts of autonomous driving and process-automation robotics. Regarding security and its intersection with safety, most incidents connect to generative AI (i.e., large-language models, deep fakes) and computer-vision systems (i.e., facial recognition). This emphasizes the importance of security to also ensure safety in the context of AI systems, with our results further revealing a high number of serious consequences (system compromise, human injuries) and major violations of confidentiality, integrity, availability, as well as authorization. We hope to support practitioners and researchers in understanding major safety and security issues to support the development of more secure, safe, and trustworthy AI systems.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {44},
numpages = {12},
keywords = {artificial intelligence, machine learning, safety, safety-critical systems, security, vulnerabilities},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3726302.3730058,
author = {Jiao, Yang and Wang, Xiaodong and Yang, Kai},
title = {PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730058},
doi = {10.1145/3726302.3730058},
abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods. These results highlight the potential risks posed by PR-Attack and emphasize the importance of securing RAG-based LLMs against such threats.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {656–667},
numpages = {12},
keywords = {bilevel optimization, large language models, retrieval-augmented generation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3689091.3690087,
author = {Shi, Yunxiao and Xu, Min and Zhang, Haimin and Zi, Xing and Wu, Qiang},
title = {A Learnable Agent Collaboration Network Framework for Personalized Multimodal AI Search Engine},
year = {2024},
isbn = {9798400712029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689091.3690087},
doi = {10.1145/3689091.3690087},
abstract = {Large language models (LLMs) and retrieval-augmented generation (RAG) techniques have revolutionized traditional information access, enabling AI agent to search and summarize information on behalf of users during dynamic dialogues. Despite their potential, current AI search engines exhibit considerable room for improvement in several critical areas. These areas include the support for multimodal information, the delivery of personalized responses, the capability to logically answer complex questions, and the facilitation of more flexible interactions. This paper proposes a novel AI Search Engine framework called the Agent Collaboration Network (ACN). The ACN framework consists of multiple specialized agents working collaboratively, each with distinct roles such as Account Manager, Solution Strategist, Information Manager, and Content Creator. This framework integrates mechanisms for picture content understanding, user profile tracking, and online evolution, enhancing the AI search engine's response quality, personalization, and interactivity. A highlight of the ACN is the introduction of a Reflective Forward Optimization method (RFO), which supports the online synergistic adjustment among agents. This feature endows the ACN with online learning capabilities, ensuring that the system has strong interactive flexibility and can promptly adapt to user feedback. This learning method may also serve as an optimization approach for agent-based systems, potentially influencing other domains of agent applications.},
booktitle = {Proceedings of the 2nd International Workshop on Deep Multimodal Generation and Retrieval},
pages = {12–20},
numpages = {9},
keywords = {information retrieval and generation, multi-agent system, multimodal, personalized search},
location = {Melbourne VIC, Australia},
series = {MMGR '24}
}

@article{10.1145/3745765,
author = {Zhang, Junwei and Hu, Xing and Xia, Xin and Cheung, Shing-Chi and Li, Shanping},
title = {Automated Unit Test Generation via Chain of Thought Prompt and Reinforcement Learning from Coverage Feedback},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3745765},
doi = {10.1145/3745765},
abstract = {Recently, large language models (LLMs) have shown promising results in code generation, and several automated test generation approaches based on LLMs have been proposed. Although these approaches achieve promising performance, they suffer from two limitations. First, they lack the intrinsic understanding of the semantic intricacies and logical constructs inherent to the focal method. Second, they ignore the diversity of the generated tests and generate tests with limited code coverage.To alleviate these two limitations, in this work, we propose a novel approach named TestCTRL that optimizes LLMs for unit test generation by the chain-of-thought (CoT) prompt and reinforcement learning (RL) strategy. Specifically, we first build a new CoT dataset, containing the focal methods, corresponding unit tests, and CoT prompts. The CoT prompt includes the intention and possible test input values. Then, the CoT dataset is used to fine-tune one LLM (i.e., CodeLlama 7B) that can be seen as the policy model in RL. Meanwhile, we fine-tune another LLM (i.e., CodeGPT) as the reward model by predicting the line coverage of the focal method and its test. Moreover, we employ the Proximal Policy Optimization (PPO) algorithm to optimize the policy model and generate unit tests. We use the Defects4J benchmark to evaluate our approach from three perspectives (i.e., naturalness, validity, and code coverage). To avoid data leakage threats, we filtered out data from the CoT dataset that have the same focal method and test case names as those in the Defects4J. The experimental results demonstrate that TestCTRL outperforms state-of-the-art baselines in line and branch coverages, respectively. Besides, TestCTRL improves bug detection performance. We also investigate the reason for the proposed approach's superiority.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {Unit Test Generation, Large Language Model, Reinforcement Learning, Chain of Thought}
}

@inproceedings{10.1145/3691620.3695290,
author = {Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang},
title = {Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695290},
doi = {10.1145/3691620.3695290},
abstract = {As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2230–2234},
numpages = {5},
keywords = {code smell, code quality, code refactoring, GitHub copilot},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3627673.3680087,
author = {Joshi, Ashutosh and Sarwar, Sheikh Muhammad and Varshney, Samarth and Nag, Sreyashi and Agrawal, Shrivats and Naik, Juhi},
title = {REAPER: Reasoning based Retrieval Planning for Complex RAG Systems},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680087},
doi = {10.1145/3627673.3680087},
abstract = {Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from heterogeneous data stores that are architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one (or few) retrieval source. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, which means that a (small) classification model dictates the performance of a large language model. To address this problem, we present REAPER (REAsoning-based PlannER), an LLM-based retrieval planner that we evaluate on a conversational shopping assistant, which shows significant gains in latency over Agent-based systems and scalability to new and unseen use cases when compared to classification-based planning.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4621–4628},
numpages = {8},
keywords = {chain-of-thought, multi-hop reasoning, rag},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3733155.3734917,
author = {Hebri, Aref and Pavel, Hamza Reza and Nikanfar, Sama and Farahanipad, Farnaz and Makedon, Fillia},
title = {Can Virtual AI Agents Improve Cognitive Assessment? A Virtual Reality Perspective},
year = {2025},
isbn = {9798400714023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733155.3734917},
doi = {10.1145/3733155.3734917},
abstract = {We introduce SmartSage, an intelligent virtual AI agent powered by Large Language Models (LLMs) and Vision Language Models (VLMs) that enhances cognitive assessment through adaptive, embodied interactions in virtual environments. As an integral component of our Virtual Reaility-based ADHD Assessment System, SmartSage serves as an interactive guide that demonstrates tasks, delivers real-time feedback, and dynamically adjusts task difficulty based on user performance. Based on the clinically validated Automated Test of Embodied Cognition (ATEC) framework, our system leverages motion tracking and haptic feedback of the Meta Quest platform to facilitate bidirectional sensorimotor interactions that boost engagement while maintaining clinical validity. By combining embodied cognition principles with virtual reality technology and AI-driven interaction, SmartSage makes cognitive assessment more accessible, engaging, and ecologically valid. In this paper, we provide an overview of the system design, present initial pilot results, and outline a roadmap for future validation studies.},
booktitle = {Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {424–428},
numpages = {5},
keywords = {cognitive assessment, virtual reality, executive function, embodied cognition},
location = {
},
series = {PETRA '25}
}

@inbook{10.1145/3696630.3734199,
author = {Harman, Mark and O'Hearn, Peter and Sengupta, Shubho},
title = {Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3734199},
abstract = {Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated 'just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper1 was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1–17},
numpages = {17}
}

@inproceedings{10.1145/3643916.3644418,
author = {Huang, Tao and Sun, Zhihong and Jin, Zhi and Li, Ge and Lyu, Chen},
title = {Knowledge-Aware Code Generation with Large Language Models},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644418},
doi = {10.1145/3643916.3644418},
abstract = {Large Language Models (LLMs) perform well on basic programming problems. However, they encounter challenges when dealing with complex tasks involving the use of diverse algorithmic and data structure skills, particularly programming competition-level problems. Notably, ChatGPT exhibits proficient performance on problems it has encountered during its pre-training phase, but this performance deteriorates when faced with novel problems. Consequently, enhancing the ability of LLMs to address unfamiliar problems has emerged as a pivotal research focus. The problem-solving process of LLMs mirrors human programmers' approach to a certain extent. When confronted with new programming tasks, human programmers engage in task planning and code writing with the previously acquired knowledge about algorithms and data structures. Despite having learned such knowledge, LLMs struggle to effectively apply it when faced with specific new problems. To address this issue, we constructed a novel dataset, CodeF, which contains a portion of programming problems that ChatGPT has not previously encountered. Furthermore, we developed a Knowledge Library tailored for Python programming contest problems and introduced the concept of Knowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models' understanding and problem-solving capabilities by integrating prompt and knowledge from the library into the LLMs' code generation reasoning process, especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets, KareCoder demonstrated outstanding performance in handling novel problems previously unencountered by LLMs. In contrast with the code directly generated by ChatGPT, KareCoder achieved a relative improvement of 23.3% on the Pass@1 metric on the CodeF post2021-9 dataset. Additionally, it performs well compared to other methods when dealing with problems that LLMs have previously encountered. Our dataset and experiment data are open-sourced and can be accessed at https://github.com/CodeGeneration3/KareCoder.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {52–63},
numpages = {12},
keywords = {code generation, large language models, knowledge library},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3637528.3672035,
author = {Fang, Yi and Fan, Dongzhe and Zha, Daochen and Tan, Qiaoyu},
title = {GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672035},
doi = {10.1145/3637528.3672035},
abstract = {This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {747–758},
numpages = {12},
keywords = {LLM for graph augmentation, graph contrastive learning, graph neural networks, text-attributed graphs},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3643991.3645080,
author = {Sagdic, Ertugrul and Bayram, Arda and Islam, Md Rakibul},
title = {On the Taxonomy of Developers' Discussion Topics with ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645080},
doi = {10.1145/3643991.3645080},
abstract = {Large language models (LLMs) like ChatGPT can generate text for various prompts. With exceptional reasoning capabilities, ChatGPT (particularly the GPT-4 model) has achieved widespread adoption across many tasks - from creative writing to domain-specific inquiries, code generation, and more. This research analyzed the DevGPT dataset to determine common topics posed by developers interacting with ChatGPT. The DevGPT dataset comprises ChatGPT interactions from GitHub issues, pull requests and discussions. By employing a mixed-methods approach combining unsupervised semantic modeling and expert qualitative analysis we categorize the topics developers discuss when interacting with ChatGPT.Our approach reveals 17 topics within seven categories, with over 25% of prompts focused on advanced programming guidance. Additional areas of significant query volume include DevOps workflows, SQL, databases, and specialized domains, such as localization, streaming media, and image processing. This research effectively illuminates core topics and dependencies that motivate developers to leverage ChatGPT. The taxonomy classification further clarifies critical areas to better customize AI tools for aligning with workflows and needs within software engineering contexts.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {197–201},
numpages = {5},
keywords = {DevGPT, ChatGPT, software engineering, topic taxonomy},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3650212.3652115,
author = {Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun},
title = {CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652115},
doi = {10.1145/3650212.3652115},
abstract = {In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {124–136},
numpages = {13},
keywords = {Benchmark, Code Generation, Large Language Models},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3698061.3726934,
author = {Davis, Nicholas and Sherson, Jacob and Rafner, Janet},
title = {The Co-Creative Design Framework for Hybrid Intelligence},
year = {2025},
isbn = {9798400712890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698061.3726934},
doi = {10.1145/3698061.3726934},
abstract = {With the rapid advancement of generative AI, co-creation has emerged as a key interaction paradigm, enabling humans and AI to collaborate in creative processes. However, despite decades of research on co-creativity, recent AI developments often lack a structured framework to integrate these insights effectively. To address this gap, we propose the Co-Creative Design Framework (CCDF), which formalizes human-AI co-creation through cognitive and interaction principles. The framework is structured around three core dimensions: agency, which defines the balance of autonomy and control between user and AI; interaction dynamics, which describe the evolving relationship between collaborators and their shared creative product; and communication, which governs information exchange between human and AI. The CCDF provides a systematic approach to modeling co-creative AI and hybrid intelligence systems, defining key dimensions of variance that shape the interaction space of co-creation. In particular, it highlights agency and interaction dynamics, which have been underexplored in recent co-creative AI frameworks. This paper details the iterative development of CCDF, synthesizing insights from co-creativity literature and AI research. We apply the framework in a comparative analysis of Traditional ChatGPT, ChatGPT Canvas Mode, and DALL-E, demonstrating its ability to capture fine-grained differences in system design and user experience.},
booktitle = {Proceedings of the 2025 Conference on Creativity and Cognition},
pages = {560–572},
numpages = {13},
keywords = {genAI, co-creation, hybrid intelligence, interaction design, cognition},
location = {
},
series = {C&amp;C '25}
}

@inproceedings{10.1145/3663529.3663801,
author = {Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
title = {ChatUniTest: A Framework for LLM-Based Test Generation},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663801},
doi = {10.1145/3663529.3663801},
abstract = {Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests.  Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage.  Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain.  ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {572–576},
numpages = {5},
keywords = {Automatic Unit Testing Generation, Large Language Models},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3643991.3645071,
author = {Siddiq, Mohammed Latif and Roney, Lindsay and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Quality Assessment of ChatGPT Generated Code and their Use by Developers},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645071},
doi = {10.1145/3643991.3645071},
abstract = {The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT's generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT's responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python &amp; Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers' discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {152–156},
numpages = {5},
keywords = {datasets, ChatGPT, security, quality, pull-request, open-coding},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3664646.3664762,
author = {de Oliveira Neto, Francisco Gomes},
title = {Unveiling Assumptions: Exploring the Decisions of AI Chatbots and Human Testers},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664762},
doi = {10.1145/3664646.3664762},
abstract = {The integration of Large Language Models (LLMs) and chatbots introduces new challenges and opportunities for decision-making in software testing. Decision-making relies on a variety of information, including code, requirements specifications, and other software artifacts that are often unclear or exist solely in the developer’s mind. To fill in the gaps left by unclear information, we often rely on assumptions, intuition, or previous experiences to make decisions. This paper explores the potential of LLM-based chatbots like Bard, Copilot, and ChatGPT, to support software testers in test decisions such as prioritizing test cases effectively. We investigate whether LLM-based chatbots and human testers share similar “assumptions” or intuition in prohibitive testing scenarios where exhaustive execution of test cases is often impractical. Preliminary results from a survey of 127 testers indicate a preference for diverse test scenarios, with a significant majority (96%) favoring dissimilar test sets. Interestingly, two out of four chatbots mirrored this preference, aligning with human intuition, while the others opted for similar test scenarios, chosen by only 3.9% of testers. Our initial insights suggest a promising avenue within the context of enhancing the collaborative dynamics between testers and chatbots.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {45–49},
numpages = {5},
keywords = {Chatbots, Software Testing, Test Prioritization},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3643753,
author = {Wang, Yan and Li, Xiaoning and Nguyen, Tien N. and Wang, Shaohua and Ni, Chao and Ding, Ling},
title = {Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643753},
doi = {10.1145/3643753},
abstract = {Pre-trained Large Language Models (LLM) have achieved remarkable successes in several domains. However, code-oriented LLMs are often heavy in computational complexity, and quadratically with the length of the input code sequence. Toward simplifying the input program of an LLM, the state-of-the-art approach has the strategies to filter the input code tokens based on the attention scores given by the LLM. The decision to simplify the input program should not rely on the attention patterns of an LLM, as these patterns are influenced by both the model architecture and the pre-training dataset. Since the model and dataset are part of the solution domain, not the problem domain where the input program belongs, the outcome may differ when the model is pre-trained on a different dataset. We propose SlimCode, a model-agnostic code simplification solution for LLMs that depends on the nature of input code tokens. As an empirical study on the LLMs including CodeBERT, CodeT5, and GPT-4 for two main tasks: code search and summarization, we reported that 1) the removal ratio of code has a linear-like relation with the saving ratio on training time, 2) the impact of categorized tokens on code simplification can vary significantly, 3) the impact of categorized tokens on code simplification is task-specific but model-agnostic, and 4) the above findings hold for the paradigm–prompt engineering and interactive in-context learning. The empirical results showed that SlimCode can improve the state-of-the-art technique by 9.46% and 5.15% in terms of MRR and BLEU score on code search and summarization, respectively. More importantly, SlimCode is 133 times faster than the state-of-the-art approach. Additionally, SlimCode can reduce the cost of invoking GPT-4 by up to 24% per API query, while still producing comparable results to those with the original code. With this result, we call for a new direction on code-based, model-agnostic code simplification solutions to further empower LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {27},
numpages = {23},
keywords = {AI4SE, Code Simplification, Machine Learning, Neural Networks, Pre-trained Large Language Models}
}

@inproceedings{10.1145/3709026.3709076,
author = {Qin, Zhike and Liu, Yanyan and Wang, Shaopu and Wang, Chunlei},
title = {CaFGD: Context Auto-Filling via Gradient Descent},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709076},
doi = {10.1145/3709026.3709076},
abstract = {Recently, large language models (LLMs) achieve remarkable success in domains beyond the traditional natural language processing, and there is a growing interest in applying LLMs to more general domains like code generation, travel planning, and robot controls, but these models still need improvement in targeted performance and generalization capabilities in specific fields or tasks. To enable models to more specifically understand various tasks, prompt learning has been proposed to convert downstream prediction tasks into language model tasks. Among prompt learning methods, most utilize gradient-based trigger token search methods for automatic context filling to complete tasks. However, these methods don’t always enhance the accuracy of LLMs in completing tasks, especially when meeting multiple task types and uncertain input sentences. The choice of trigger tokens often lacks specificity, leading to suboptimal model performance. To enhance model stability and generate more targeted trigger tokens, we propose a context auto-filling method via average gradient descent. Unlike other methods, our approach comprehensively considers the relationship between all trigger tokens and context. Proposed method modifies a template by replacing one trigger token with another token, using the model’s average gradient across all trigger tokens to select one token that maximize the likelihood function of the template. We conducted experiments on the SST-2 and SICK-E datasets for sentiment analysis (SA) and Natural Language Inference (NLI) tasks respectively. The experimental results demonstrate that the context auto-filling method with an average gradient of trigger token yields better performance.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {551–557},
numpages = {7},
keywords = {sentiment analysis, natural language inference, prompt learning, autoprompt generation},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3708035.3736046,
author = {Davies, Ronald H and Sanghvi, Krish and Nalam, Rahul and Ramnath, Rajiv},
title = {A Pre-Processing Framework for Securing LLM-RAG Interfaces Against Information Leakage},
year = {2025},
isbn = {9798400713989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708035.3736046},
doi = {10.1145/3708035.3736046},
abstract = {Large language model (LLM) interfaces with Retrieval-Augmented Generation (RAG) systems face a critical vulnerability: potential information leakage from generated outputs. We introduce a novel pre-processing framework featuring a dedicated "controlling path" screening module that evaluates inputs before committing full system resources. Our approach leverages a lightweight screening model enhanced with "crafted memories", curated dialogue histories that exemplify safe versus unsafe queries, and a reminder appended like RAG inputs. This system pre-filters potentially harmful or off-domain inputs while ensuring legitimate queries proceed for accurate processing. Experiments with several datasets, such as general instructions and medical question/answering, demonstrate success in filtering across multiple high-performance models achieving over 90% rejection of off-domain inputs on certain datasets. The framework not only poised to enhance security by substantially reducing risky input processing, but may also establish a new standard for implementing domain-specific restrictions in LLM pipelines. This work advances the development of modular security layers for next-generation conversational AI systems and interfaces.},
booktitle = {Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration},
articleno = {89},
numpages = {3},
keywords = {Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Pre-processing Framework, Information Flow Control, Input Screening, Domain-Specific Filtering},
location = {
},
series = {PEARC '25}
}

@inproceedings{10.1145/3703412.3703416,
author = {Gandhi, Shubham and Patwardhan, Manasi and Vig, Lovekesh and Shroff, Gautam},
title = {BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks},
year = {2025},
isbn = {9798400711619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703412.3703416},
doi = {10.1145/3703412.3703416},
abstract = {Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.},
booktitle = {Proceedings of the 4th International Conference on AI-ML Systems},
articleno = {3},
numpages = {9},
keywords = {Automated ML, Large Language Models, LLM Agents},
location = {
},
series = {AIMLSystems '24}
}

@inproceedings{10.1145/3591106.3592278,
author = {Alonso del Barrio, David and Gatica-Perez, Daniel},
title = {Framing the News: From Human Perception to Large Language Model Inferences},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592278},
doi = {10.1145/3591106.3592278},
abstract = {Identifying the frames of news is important to understand the articles’ vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {627–635},
numpages = {9},
keywords = {Covid-19 no-vax, GPT-3, large language models, news framing, prompt-engineering, transformers},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3650212.3680388,
author = {Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min},
title = {LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680388},
doi = {10.1145/3650212.3680388},
abstract = {FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.	Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18% and an average of 20%−110% improvement on business scenario coverage, and up to 93.72% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1643–1655},
numpages = {13},
keywords = {Software acceptance testing, fintech software, large language model, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3736407,
author = {Weyssow, Martin and Kamanda, Aton and Zhou, Xin and Sahraoui, Houari},
title = {CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3736407},
doi = {10.1145/3736407},
abstract = {Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavor that requires a deep assessment of LLMs’ outputs. Existing methods and benchmarks rely primarily on automated metrics and static analysis tools, which often fail to capture the nuances of user instructions and LLM outputs. To address this gap, we introduce the LLM-as-a-Judge evaluation framework and present CodeUltraFeedback, a comprehensive dataset for assessing and improving LLM alignment with coding preferences. CodeUltraFeedback consists of 10,000 coding instructions, each annotated with four responses generated from a diverse pool of 14 LLMs. These responses are annotated using GPT-3.5 as a judge, with both ranking-based scores and detailed textual feedback across five distinct coding preferences. Our analysis reveals that responses from GPT-3.5 and GPT-4 are consistently rated higher than those from open-weight models, underscoring substantial alignment gaps between closed- and open-weight LLMs. In turn, we explore the usage of CodeUltraFeedback as feedback data to fine-tune and align CodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO). The resulting aligned model achieves an average alignment improvement of 22.7% and 29.7% when evaluated with GPT-3.5 and GPT-4 judges, respectively. Notably, our aligned CodeLlama-7B-Instruct surpasses much larger models, such as CodeLlama-13B and 34B, in alignment with coding preferences. Despite not being explicitly trained for functional correctness, it also achieves a 10.5% and 26.6% relative improvement in Pass@ (1)  and Pass@ (10)  on the HumanEval+ benchmark. Our contributions demonstrate the practical value of preference tuning in code generation and set the stage for further progress in model alignment and RLAIF for automated software engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Large language models, code generation, automated software engineering, reinforcement learning from AI feedback, direct preference optimization, LLM-as-a-Judge}
}

@inproceedings{10.1145/3643991.3645075,
author = {Raj, Rachna and Costa, Diego Elias},
title = {The role of library versions in Developer-ChatGPT conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645075},
doi = {10.1145/3643991.3645075},
abstract = {The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {172–176},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3715073.3715075,
author = {Geren, Caleb and Board, Amanda and Dagher, Gaby G. and Andersen, Tim and Zhuang, Jun},
title = {Blockchain for Large Language Model Security and Safety: A Holistic Survey},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3715073.3715075},
doi = {10.1145/3715073.3715075},
abstract = {With the growing development and deployment of large language models (LLMs) in both industrial and academic fields, their security and safety concerns have become increasingly critical. However, recent studies indicate that LLMs face numerous vulnerabilities, including data poisoning, prompt injections, and unauthorized data exposure, which conventional methods have struggled to address fully. In parallel, blockchain technology, known for its data immutability and decentralized structure, offers a promising foundation for safeguarding LLMs. In this survey, we aim to comprehensively assess how to leverage blockchain technology to enhance LLMs' security and safety. Besides, we propose a new taxonomy of blockchain for large language models (BC4LLMs) to systematically categorize related works in this emerging field. Our analysis includes novel frameworks and definitions to delineate security and safety in the context of BC4LLMs, highlighting potential research directions and challenges at this intersection.Through this study, we aim to stimulate targeted advancements in blockchain-integrated LLM security.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {1–20},
numpages = {20}
}

@inproceedings{10.1145/3663529.3663794,
author = {Hora, Andre},
title = {Predicting Test Results without Execution},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663794},
doi = {10.1145/3663529.3663794},
abstract = {As software systems grow, test suites may become complex, making it challenging to run the tests frequently and locally. Recently, Large Language Models (LLMs) have been adopted in multiple software engineering tasks. It has demonstrated great results in code generation, however, it is not yet clear whether these models understand code execution. Particularly, it is unclear whether LLMs can be used to predict test results, and, potentially, overcome the issues of running real-world tests. To shed some light on this problem, in this paper, we explore the capability of LLMs to predict test results without execution. We evaluate the performance of the state-of-the-art GPT-4 in predicting the execution of 200 test cases of the Python Standard Library. Among these 200 test cases, 100 are passing and 100 are failing ones. Overall, we find that GPT-4 has a precision of 88.8%, recall of 71%, and accuracy of 81% in the test result prediction. However, the results vary depending on the test complexity: GPT-4 presented better precision and recall when predicting simpler tests (93.2% and 82%) than complex ones (83.3% and 60%). We also find differences among the analyzed test suites, with the precision ranging from 77.8% to 94.7% and recall between 60% and 90%. Our findings suggest that GPT-4 still needs significant progress in predicting test results.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {542–546},
numpages = {5},
keywords = {GPT-4, LLMs, large language models, software testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3639478.3639789,
author = {Deljouyi, Amirhossein},
title = {Understandable Test Generation Through Capture/Replay and LLMs},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639789},
doi = {10.1145/3639478.3639789},
abstract = {Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques.Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles.In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {261–263},
numpages = {3},
keywords = {automatic test generation, carving and replaying, large language models, readability, understandability, unit testing},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639477.3639719,
author = {Di, Peng and Li, Jianguo and Yu, Hang and Jiang, Wei and Cai, Wenting and Cao, Yang and Chen, Chaoyu and Chen, Dajun and Chen, Hongwei and Chen, Liang and Fan, Gang and Gong, Jie and Gong, Zi and Hu, Wen and Guo, Tingting and Lei, Zhichao and Li, Ting and Li, Zheng and Liang, Ming and Liao, Cong and Liu, Bingchang and Liu, Jiachen and Liu, Zhiwei and Lu, Shaojun and Shen, Min and Wang, Guangpei and Wang, Huan and Wang, Zhi and Xu, Zhaogui and Yang, Jiawei and Ye, Qing and Zhang, Gehao and Zhang, Yu and Zhao, Zelin and Zheng, Xunjin and Zhou, Hailian and Zhu, Lifu and Zhu, Xianying},
title = {CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639719},
doi = {10.1145/3639477.3639719},
abstract = {Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {418–429},
numpages = {12},
keywords = {code large language models, multi-lingual, chinese prompts},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@article{10.1145/3728925,
author = {Wu, Fan and Gao, Cuiyun and Li, Shuqing and Wen, Xin-Cheng and Liao, Qing},
title = {MLLM-Based UI2Code Automation Guided by UI Layout Information},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728925},
doi = {10.1145/3728925},
abstract = {Converting user interfaces into code (UI2Code) is a crucial step in website development, which is time-consuming and labor-intensive. The automation of UI2Code is essential to streamline this task, beneficial for improving the development efficiency. There exist deep learning-based methods for the task; however, they heavily rely on a large amount of labeled training data and struggle with generalizing to real-world, unseen web page designs. The advent of Multimodal Large Language Models (MLLMs) presents potential for alleviating the issue, but they are difficult to comprehend the complex layouts in UIs and generate the accurate code with layout preserved. To address these issues, we propose LayoutCoder, a novel MLLM-based framework generating UI code from real-world webpage images, which includes three key modules: (1) Element Relation Construction, which aims at capturing UI layout by identifying and grouping components with similar structures; (2) UI Layout Parsing, which aims at generating UI layout trees for guiding the subsequent code generation process; and (3) Layout-Guided Code Fusion, which aims at producing the accurate code with layout preserved. For evaluation, we build a new benchmark dataset which involves 350 real-world websites named Snap2Code, divided into seen and unseen parts for mitigating the data leakage issue, besides the popular dataset Design2Code. Extensive evaluation shows the superior performance of LayoutCoder over the state-of-the-art approaches. Compared with the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and 3.95% in the CLIP score on average across all datasets.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA050},
numpages = {23},
keywords = {Automated Software Engineering, Multimodal Large Language Models, Web App Development}
}

@inproceedings{10.1145/3672608.3707813,
author = {Sabatucci, Luca and Cossentino, Massimo and Di Napoli, Claudia and Susi, Angelo},
title = {Enhancing Device-Goal-Norm Modeling for Ambient Assisted Living with Large Language Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707813},
doi = {10.1145/3672608.3707813},
abstract = {In the rapidly evolving landscape of elderly care, designing personalized Ambient Assisted Living (AAL) applications relying on IoT-enabled devices presents complex challenges at the intersection of technology, human needs, and ethical considerations. This paper presents an innovative approach that enhances the design process of these applications by integrating large language models (LLMs) to automatically generate interaction patterns between IoT devices and users' needs and preferences. These patterns are used to select the more appropriate devices and their interaction modalities for each user at run-time. The LLM-enhanced process not only reduces analysts' workload but also may reveal nuanced interactions between IoT devices and user needs that might be missed in traditional methods, where the knowledge of the characteristics of available devices may be incomplete or not updated. The proposed approach, on the one hand, facilitates the design of highly adaptive and personalized assistive technologies and, on the other hand, demonstrates the potential of combining artificial intelligence with human expertise to improve software system design.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1481–1488},
numpages = {8},
keywords = {models at run-time, large language models, AAL},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3643916.3644408,
author = {Liu, Yilun and Tao, Shimin and Meng, Weibin and Wang, Jingyu and Ma, Wenbing and Chen, Yuhang and Zhao, Yanqing and Yang, Hao and Jiang, Yanfei},
title = {Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644408},
doi = {10.1145/3643916.3644408},
abstract = {Automated log analysis is crucial in modern software-intensive systems for facilitating program comprehension throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' comprehension of program status and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel interpretable log analysis approach for online scenarios. LogPrompt employs large language models (LLMs) to perform online log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' performance by up to 380.7% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite requiring no in-domain training, outperforms existing approaches trained on thousands of logs by up to 55.9%. We also conduct a human evaluation of LogPrompt's interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment. Code of LogPrompt is available at https://github.com/lunyiliu/LogPrompt.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {35–46},
numpages = {12},
keywords = {large language model, prompt engineering, log analysis, interpretability, online scenario},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inbook{10.1145/3728725.3728799,
author = {Wen, Chengjiang and Yue, Yang and Wang, Zhixiang},
title = {The Application of Membership Inference in Privacy Auditing of Large Language Models Based on Fine-Tuning Method},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728799},
abstract = {Thanks to the large datasets collected from the internet, large language models (LLMs) have made significant progress across various fields, with widespread applications in digital employees, intelligent customer service, and smart decision-making. However, during the training process, models may memorize sensitive private data, posing risks to data security and privacy protection. Member Inference Attacks (MIA) aim to determine whether a specific record was used in the training of the target model. This approach is crucial for detecting privacy risks in models and has seen substantial progress in both traditional and fine-tuned models. However, this remains an unresolved issue for large language models (LLMs). Recent studies indicate that under proper experimental setups, the AUC-ROC of existing methods approaches 0.5 (random guessing). Subsequently, some research has shifted MIA to dataset inference, achieving promising results in this area. Our study shows that when fine-tuning a target model with aggregated samples, it is possible to distinguish whether these samples were used in training the target model by observing the initial loss returned by the model. Based on this observation, we propose a fine-tuning-based member inference method for large language models. To validate the effectiveness of our method, we conducted extensive experiments on three mainstream models and seven datasets. The results indicate that member inference attacks with aggregated samples can achieve acceptable outcomes.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {473–479},
numpages = {7}
}

@inproceedings{10.1145/3643916.3645030,
author = {Khajezade, Mohamad and Wu, Jie JW and Fard, Fatemeh Hendijani and Rodriguez-Perez, Gema and Shehata, Mohamed Sami},
title = {Investigating the Efficacy of Large Language Models for Code Clone Detection},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3645030},
doi = {10.1145/3643916.3645030},
abstract = {Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are 'generative' tasks. However, there is limited research on the usage of LLMs for 'non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis1.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {161–165},
numpages = {5},
keywords = {large language models, code clone detection, zero-shot learning, few-shot learning},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3654777.3676358,
author = {Jennings, Nicholas and Wang, Han and Li, Isabel and Smith, James and Hartmann, Bjoern},
title = {What's the Game, then? Opportunities and Challenges for Runtime Behavior Generation},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676358},
doi = {10.1145/3654777.3676358},
abstract = {Procedural content generation (PCG), the process of algorithmically creating game components instead of manually, has been a common tool of game development for decades. Recent advances in large language models (LLMs) enable the generation of game behaviors based on player input at runtime. Such code generation brings with it the possibility of entirely new gameplay interactions that may be difficult to integrate with typical game development workflows. We explore these implications through GROMIT, a novel LLM-based runtime behavior generation system for Unity. When triggered by a player action, GROMIT generates a relevant behavior which is compiled without developer intervention and incorporated into the game. We create three demonstration scenarios with GROMIT to investigate how such a technology might be used in game development. In a system evaluation we find that our implementation is able to produce behaviors that result in significant downstream impacts to gameplay. We then conduct an interview study with n=13 game developers using GROMIT as a probe to elicit their current opinion on runtime behavior generation tools, and enumerate the specific themes curtailing the wider use of such tools. We find that the main themes of concern are quality considerations, community expectations, and fit with developer workflows, and that several of the subthemes are unique to runtime behavior generation specifically. We outline a future work agenda to address these concerns, including the need for additional guardrail systems for behavior generation.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {106},
numpages = {13},
keywords = {Generative AI, Human-AI interaction, Procedural Content Generation},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3650212.3680343,
author = {Guo, Lianghong and Wang, Yanlin and Shi, Ensheng and Zhong, Wanjun and Zhang, Hongyu and Chen, Jiachi and Zhang, Ruikai and Ma, Yuchi and Zheng, Zibin},
title = {When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680343},
doi = {10.1145/3650212.3680343},
abstract = {Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34% to 452%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1073–1085},
numpages = {13},
keywords = {Machine learning for analysis, Testing and development processes},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3644032.3644443,
author = {El Haji, Khalid and Brandt, Carolin and Zaidman, Andy},
title = {Using GitHub Copilot for Test Generation in Python: An Empirical Study},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644443},
doi = {10.1145/3644032.3644443},
abstract = {Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations.Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28% of the tests generated by Copilot are passing tests; 54.72% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {45–55},
numpages = {11},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3701625.3701657,
author = {de Almeida, \'{A}gatha and Collins, Eliane and Oran, Ana Carolina},
title = {AI in Service of Software Quality: How ChatGPT and Personas Are Transforming Exploratory Testing},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701657},
doi = {10.1145/3701625.3701657},
abstract = {Context: Exploratory testing is essential in the software validation process as a way to find unexpected and critical failures in a short time, complementing documented functional test cases. However, creating scenarios to explore the software (such as test charters) can be time-consuming, and depending on the team’s experience, it may lack adequate coverage of functionalities and scenarios that target specific user profiles of the application. Objective: This article investigates how AI, through LLMs (Large Language Models), can assist in creating exploratory test charters that reflect the characteristics and needs of different user personas. Method: To achieve this, an experimental study was conducted where personas were used as input in ChatGPT 3.5 to generate exploratory test charters. The effectiveness of the approach was evaluated by Software Engineering students, who analyzed the performance and usefulness of the generated charters through a questionnaire based on the TAM model, supplemented by qualitative and quantitative analyses. Results: Data analysis indicated positive acceptance of ChatGPT 3.5 by the participants, highlighting its ease of use and perceived usefulness. Conclusion: This study contributes to the field of Software Engineering by demonstrating a practical application of artificial intelligence in the automated generation of test charters. ChatGPT 3.5 has proven to be a promising tool to support the creation of personalized exploratory test charters, contributing to software quality improvement. The integration of artificial intelligence techniques with user-centered design methods can significantly optimize the software testing process.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {179–188},
numpages = {10},
keywords = {Exploratory Testing, ChatGPT, Personas, Software Quality, Artificial Intelligence},
location = {
},
series = {SBQS '24}
}

@article{10.1145/3721127,
author = {Alami, Adam and Jensen, Victor Vadmand and Ernst, Neil A.},
title = {Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3721127},
doi = {10.1145/3721127},
abstract = {Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals’ decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers’ sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study ( (textbf{interviews}rightarrowtextbf{focus groups}) ). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining one’s reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code quality, Accountability, Artificial Intelligence, Large Language Models, LLM, Code Review, Human and Social Aspects of Software Engineering}
}

@inproceedings{10.1145/3643660.3643942,
author = {Eisenreich, Tobias and Speth, Sandro and Wagner, Stefan},
title = {From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643942},
doi = {10.1145/3643660.3643942},
abstract = {Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {52–55},
numpages = {4},
keywords = {requirements, software architecture, architecture evaluation, LLM},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@article{10.1145/3643769,
author = {Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi},
title = {Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643769},
doi = {10.1145/3643769},
abstract = {Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage.   In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {Large Language Models, Test Generation}
}

@inproceedings{10.1145/3726302.3729882,
author = {Br\r{a}dland, Henrik and Goodwin, Morten and Andersen, Per-Arne and Nossum, Alexander S. and Gupta, Aditya},
title = {A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729882},
doi = {10.1145/3726302.3729882},
abstract = {Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p &gt; 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {170–179},
numpages = {10},
keywords = {document chunking, natural language processing, passage evaluation, retrieval-augmented generation, text embedding},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3696630.3730569,
author = {Wang, Fei and Xi, XueFeng and Cui, ZhiMing and Dai, Huan and Wang, XinYu},
title = {Embedding Traceability in Large Language Model Code Generation: Towards Trustworthy AI-Augmented Software Engineering},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730569},
doi = {10.1145/3696630.3730569},
abstract = {The rise of LLMs in software engineering introduces critical challenges in tracing generated code to its governing requirements. Traditional traceability methods, designed for human-authored code, fail to address LLMs' stochastic and opaque nature, which yields divergent outputs from identical prompts. This paper proposes embedding traceability as a first-class objective in LLM code-generation pipelines through three strategies: structured requirement prompting, metadata-aware fine-tuning, and retrieval-augmented validation. Our framework addresses LLM opacity, non-determinism, and scalability, while introducing metrics, dynamic adaptation, and ethical considerations for audit-ready code generation. Contributions include an end-to-end framework, a research agenda, and design principles to support trustworthy AI-augmented software engineering.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1760–1763},
numpages = {4},
keywords = {traceability, LLMs, code generation, requirements engineering, AI-augmented software engineering},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3589335.3651935,
author = {Stanley Jothiraj, Fiona Victoria and Mashhadi, Afra},
title = {Phoenix: A Federated Generative Diffusion Model},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651935},
doi = {10.1145/3589335.3651935},
abstract = {Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity or Non-IID (Non-Independent and Identically Distributed) data. We demonstrate how our approach outperforms the default diffusion model in a FL setting. These results indicate that high-quality samples can be generated by maintaining data diversity, preserving privacy, and reducing communication between data sources, offering exciting new possibilities in the field of generative AI.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1568–1577},
numpages = {10},
keywords = {diffusion models, federated learning, generative ai, heterogeneous data},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3733965.3733972,
author = {Liu, Haiyun and Xue, Jiahao and Zhao, Shangqing and Liu, Yao and Lu, Zhuo},
title = {The Dual Role of Large Language Models in Network Security: Survey and Research Trends},
year = {2025},
isbn = {9798400715310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733965.3733972},
doi = {10.1145/3733965.3733972},
abstract = {Large language models (LLMs) have profoundly shaped various domains, including several types of network systems. With their powerful capabilities, LLMs have recently been proposed to enhance network security. However, the development of LLMs can introduce new risks due to their potential vulnerabilities and misuse. In this paper, we are motivated to review the dual role of LLMs in network security. Our goal is to explore how LLMs impact network security and ultimately shed light on how to evaluate LLMs from a network security perspective. We further discuss several future research directions regarding how to scientifically enable LLMs to assist with network security.},
booktitle = {Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning},
pages = {20–25},
numpages = {6},
keywords = {large language model (llm), network security, adversarial attacks},
location = {USA},
series = {WiseML '25}
}

@inproceedings{10.1145/3589132.3625611,
author = {Rao, Jinmeng and Gao, Song and Mai, Gengchen and Janowicz, Krzysztof},
title = {Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625611},
doi = {10.1145/3589132.3625611},
abstract = {In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {41},
numpages = {4},
keywords = {multimodality, security, privacy, foundation model, GeoAI},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3717823.3718309,
author = {Alrabiah, Omar and Ananth, Prabhanjan and Christ, Miranda and Dodis, Yevgeniy and Gunn, Sam},
title = {Ideal Pseudorandom Codes},
year = {2025},
isbn = {9798400715105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717823.3718309},
doi = {10.1145/3717823.3718309},
abstract = {Pseudorandom codes are error-correcting codes with the property that no efficient adversary can distinguish encodings from uniformly random strings. They were recently introduced by Christ and Gunn [CRYPTO 2024] for the purpose of watermarking the outputs of randomized algorithms, such as generative AI models. Several constructions of pseudorandom codes have since been proposed, but none of them are robust to error channels that depend on previously seen codewords. This stronger kind of robustness is referred to as adaptive robustness, and it is important for meaningful applications to watermarking. In this work, we show the following. Adaptive robustness: We show that the pseudorandom codes of Christ and Gunn are adaptively robust, resolving a conjecture posed by Cohen, Hoover, and Schoenbach [S&amp;P 2025]. Our proof involves several new ingredients, combining ideas from both cryptography and coding theory and taking hints from the analysis of Boolean functions. Ideal security: We define an ideal pseudorandom code as one which is indistinguishable from the ideal functionality, capturing both the pseudorandomness and robustness properties in one simple definition. We show that any adaptively robust pseudorandom code for single-bit messages can be bootstrapped to build an ideal pseudorandom code with linear information rate, under no additional assumptions. CCA security: In the setting where the encoding key is made public, we define a CCA-secure pseudorandom code in analogy with CCA-secure encryption. We show that any adaptively robust public-key pseudorandom code for single-bit messages can be used to build a CCA-secure pseudorandom code with linear information rate, in the random oracle model. Together with the result of Christ and Gunn, it follows that there exist ideal pseudorandom codes assuming the 2O(√n)-hardness of LPN. This extends to CCA security in the random oracle model. These results immediately imply stronger robustness guarantees for generative AI watermarking schemes, such as the practical quality-preserving image watermarks of Gunn, Zhao, and Song [ICLR 2025].},
booktitle = {Proceedings of the 57th Annual ACM Symposium on Theory of Computing},
pages = {1638–1647},
numpages = {10},
keywords = {Pseudorandom codes},
location = {Prague, Czechia},
series = {STOC '25}
}

@inproceedings{10.1145/3734947.3735665,
author = {Kasela, Pranav and Braga, Marco and Sokli, Effrosyni and Milanese, Gian Carlo and Peikos, Georgios and Modha, Sandip and Raganato, Alessandro and Viviani, Marco and Pasi, Gabriella},
title = {Overview of the PIR Track at FIRE 2024: Evaluation of Personalised Information Retrieval},
year = {2025},
isbn = {9798400713187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734947.3735665},
doi = {10.1145/3734947.3735665},
abstract = {This abstract provides a short overview of the first edition of the shared task on Personalised Information Retrieval (PIR) organized at the 16th Forum for Information Retrieval Evaluation (FIRE 2024). A more detailed discussion of the approaches used by the participating teams is available in the track overview paper. PIR 2024 consisted of two sub-tasks. The first sub-task aims to explore the personalisation in cQA based on user profiles, following the standard IR pipeline. The second one, instead, aims to investigate the personalisation in cQA based on user profiles using recent LLMs and prompt engineering. Although the tasks saw an enthusiastic response in registrations, with 10 teams requesting the dataset, only 1 team finally submitted the runs, and 2 of them submitted the working notes.},
booktitle = {Proceedings of the 16th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {11–13},
numpages = {3},
keywords = {Information Retrieval, Large Language Model, Personalization, Question Answering},
location = {
},
series = {FIRE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00085,
author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
title = {CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00085},
doi = {10.1109/ICSE48619.2023.00085},
abstract = {Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {919–931},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3721146.3721944,
author = {Silvestre, Pedro F. and Pietzuch, Peter},
title = {Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning},
year = {2025},
isbn = {9798400715389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721146.3721944},
doi = {10.1145/3721146.3721944},
abstract = {Reinforcement learning-based fine-tuning (RLFT) has emerged as a crucial workload for enhancing large language models (LLMs). RLFT workflows are challenging, involving nested loops, multiple models, dynamically shaped tensors and interleaving sequential generation and parallel inference tasks. Despite these complexities, current RLFT engines rely on coarse-grained algorithm representations, treating each component as an independently optimized black-box. As a result, RLFT engines suffer from redundant computations, scheduling overhead, inefficient memory management, and missed opportunities for parallelism.We argue that a fine-grained representation is needed to enable holistic optimization for RLFT workloads. Additionally, we demonstrate that existing declarative deep learning engines fail to optimize RLFT workloads end-to-end due to their need for static tensor shapes and loop bounds, leading to excessive peak memory usage and unnecessary computations. Through micro-benchmarks, we quantify these inefficiencies and show that addressing them could enable more efficient and flexible execution. We propose an RLFT system design based on a fine-granularity representation, opening the door to generalizable optimizations, and paving the way for more scalable and efficient RLFT systems.},
booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems},
pages = {90–99},
numpages = {10},
location = {World Trade Center, Rotterdam, Netherlands},
series = {EuroMLSys '25}
}

@inproceedings{10.1145/3491101.3519665,
author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
title = {Expectation vs.&nbsp;Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519665},
doi = {10.1145/3491101.3519665},
abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {332},
numpages = {7},
keywords = {large language model, github copilot},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@article{10.1145/3708529,
author = {Lin, Zhihao and Ma, Wei and Lin, Tao and Zheng, Yaowen and Ge, Jingquan and Wang, Jun and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Liu, Yang and Li, Li},
title = {Open Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708529},
doi = {10.1145/3708529},
abstract = {Large language models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models have demonstrated their value not only in code generation but also in defect detection, enhancing security measures and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open source collaboration is key in realizing the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximizing the sources of high-quality data. However, data, especially of high quality, often hold commercial or sensitive value, making them less accessible for open source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the SE community. Therefore, researchers need to find solutions for enabling open source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position article investigates one solution to facilitate access to diverse organizational resources for open source AI models, ensuring that privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance. We consider six different scenarios of data distributions and include four code models. We also include four most common FL algorithms. Our experimental findings highlight the potential for employing FL in the collaborative development and maintenance of AI-based SE models. We also discuss the key issues to be addressed in the co-construction process and future research directions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {126},
numpages = {24},
keywords = {Data Privacy, Software Engineering, Open Source Code Model, Federated Learning}
}

@inproceedings{10.1145/3708319.3733804,
author = {Isozaki, Isamu and Shrestha, Manil and Console, Rick and Kim, Edward},
title = {Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3733804},
doi = {10.1145/3708319.3733804},
abstract = {Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark1 for LLM-based penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and LLama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing end-to-end penetration testing even with some minimal human assistance. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool2. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {404–419},
numpages = {16},
keywords = {Large Language Models, Penetration Testing, Benchmarking, Capture the Flag},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3675888.3676033,
author = {Pramod, Dhanya and Patil, Kanchan Pranay},
title = {Generative AI for Elderly Well-being through the Computer as Social Actor Paradigm},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676033},
doi = {10.1145/3675888.3676033},
abstract = {Artificial intelligence and machine learning (AI/ML) technologies like generative AI solutions are proliferating in the real-world healthcare sector. The purpose of this research is to investigate social norms, expectations, and standards of the elderly population for improving trust relationships while interacting with generative AI. The study is based on the CASA paradigm to gain a better understanding of the trust dynamics in human-computer communication to improve the adoption of GAI for elders' health and well-being. We validated the conceptual model with empirical data from 287 elderly users collected through an online and offline survey tool. Quantitative responses received were analysed using structural equation modeling. The study highlights how multimodal interaction, empathy, personalization, augmentation, bias stereotyping, and privacy and security affect the extent to which elderly consumers perceive GAI as trustworthy. Findings indicate that multimodal interaction, personalization, augmentation, and bias stereotyping significantly influenced the trust relationship between the elderly population and GAI. However, empathy privacy, and security were found to be insignificant in trust relationships. Further trust relationships significantly impacted GAI usage. The research provides strong theoretical and practical implications as all the stakeholders like healthcare professionals, patients/users, caregivers, and technology developers can be involved in building applications that cater to diverse needs and promote positive social interactions that can enhance GAI trust and usage.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {65–72},
numpages = {8},
location = {Noida, India},
series = {IC3-2024}
}

@inproceedings{10.5555/3635637.3662979,
author = {Liu, Jijia and Yu, Chao and Gao, Jiaxuan and Xie, Yuqing and Liao, Qingmin and Wu, Yi and Wang, Yu},
title = {LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1219–1228},
numpages = {10},
keywords = {hierarchical reasoning and planning, language agents, large language models, real-time human-ai coordination},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3650105.3652300,
author = {Wu, Yifan and Li, Ying and Yu, Siyu},
title = {Commit Message Generation via ChatGPT: How Far Are We?},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652300},
doi = {10.1145/3650105.3652300},
abstract = {Commit messages concisely describe code changes in natural language and are important for software maintenance. Various automatic commit message generation approaches have been proposed, such as retrieval-based, learning-based, and hybrid approaches. Recently, large language models have shown impressive performance in many natural language processing tasks. Among them, ChatGPT is the most popular one and has attracted wide attention from the software engineering community. ChatGPT demonstrates the ability of in-context learning (ICL), which allows ChatGPT to perform downstream tasks by learning from just a few demonstrations without explicit model tuning. However, it remains unclear how well ChatGPT performs in the commit message generation task via ICL. Therefore, in this paper, we conduct a preliminary evaluation of ChatGPT with ICL on commit message generation. Specifically, we first explore the impact of two key settings on the performance of ICL on commit message generation. Then, based on the best settings, we compare ChatGPT with several state-of-the-art approaches. The results show that a carefully-designed demonstration can lead to substantial improvements for ChatGPT on commit message generation. Furthermore, ChatGPT outperforms all the retrieval-based and learning-based approaches in terms of BLEU, METEOR, ROUGE-L, and Cider, and is comparable to hybrid approaches. Based on our findings, we outline several open challenges and opportunities for ChatGPT-based commit message generation.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {124–129},
numpages = {6},
keywords = {commit message generation, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3605098.3635935,
author = {Perilo, Michel and Valen\c{c}a, George},
title = {How Facial Recognition Technologies Affect the Transgender Community? A Systematic Mapping Study},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3635935},
doi = {10.1145/3605098.3635935},
abstract = {With large language models, smart surveillance watches and automated content moderation, AI entered a new phase. However, the increasing attention these solutions have gained is not only attached to their benefits. The adverse impacts of algorithms raised discussions in scientific literature and media outlets. The design of AI algorithms can perpetuated biases and prejudices from their creators, amplifying oppression on a massive scale and potentially affecting millions in mere moments. Historically marginalized groups bear the brunt of these harmful consequences. In the context of facial recognition, gender minorities, including non-binary and transgender individuals, face unwarranted scrutiny as applications attempt to infer their identity based on cis-heteronormative standards. In this paper, we report the results of a systematic mapping study centred on the key issues that transgender individuals encounter when dealing with facial recognition applications. A total of 24 primary studies provided us with 4 main problems and 20 underlying causes, which we represented via Map of Problems and Ishikawa Diagram techniques. This diagnostic assessment enabled us to derive a set of guidelines to be followed by requirements engineers and software developers to ensure more ethically inclusive AI solutions. Besides, we believe it supports future research and practice in Requirements Engineering (RE) field, providing valuable insights for enriching the early requirements phase.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {1153–1160},
numpages = {8},
keywords = {facial recognition, AI, transgender community, ethics, guidelines},
location = {Avila, Spain},
series = {SAC '24}
}

@inproceedings{10.1145/3640471.3680244,
author = {Zhang, Yuchong and Ma, Yong and Kragic, Danica},
title = {Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction},
year = {2024},
isbn = {9798400705069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640471.3680244},
doi = {10.1145/3640471.3680244},
abstract = {The emergence of large vision models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there’s a noticeable gap in structured research applying LVMs to human-robot interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical evaluation was implemented among 15 experts across five evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.},
booktitle = {Adjunct Proceedings of the 26th International Conference on Mobile Human-Computer Interaction},
articleno = {9},
numpages = {8},
keywords = {domain-specific, empirical study, human-robot interaction, large vision models},
location = {Melbourne, VIC, Australia},
series = {MobileHCI '24 Adjunct}
}

@inproceedings{10.1145/3691620.3695000,
author = {Sun, Zhihong and Wan, Yao and Li, Jia and Zhang, Hongyu and Jin, Zhi and Li, Ge and Lyu, Chen},
title = {Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695000},
doi = {10.1145/3691620.3695000},
abstract = {Large Language Models (LLMs), such as GPT-4, StarCoder, and Code Llama, are transforming the way developers approach programming by automatically generating code based on given contexts, such as natural language descriptions or incomplete surrounding code. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates --- a process known as code ranking --- remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method that integrates the advantages of execution-based and non-execution-based techniques. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks---APPS, MBPP, and HumanEval---demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker, achieving relative improvements of +30.97%, +31.43%, and +19.51% in Pass@1, Pass@2, and Pass@5 on APPS test, respectively.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {229–241},
numpages = {13},
keywords = {code generation, code ranking, execution feedback},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.14778/3685800.3685876,
author = {Xue, Siqiao and Qi, Danrui and Jiang, Caigao and Cheng, Fangyin and Chen, Keting and Zhang, Zhiping and Zhang, Hongyang and Wei, Ganglin and Zhao, Wang and Zhou, Fan and Yi, Hong and Liu, Shaodong and Yang, Hongjun and Chen, Faqiang},
title = {Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685876},
doi = {10.14778/3685800.3685876},
abstract = {The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4365–4368},
numpages = {4}
}

@inproceedings{10.1145/3587102.3588805,
author = {Reeves, Brent and Sarsa, Sami and Prather, James and Denny, Paul and Becker, Brett A. and Hellas, Arto and Kimmel, Bailey and Powell, Garrett and Leinonen, Juho},
title = {Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588805},
doi = {10.1145/3587102.3588805},
abstract = {The recent emergence of code generation tools powered by large language models has attracted wide attention. Models such as OpenAI Codex can take natural language problem descriptions as input and generate highly accurate source code solutions, with potentially significant implications for computing education. Given the many complexities that students face when learning to write code, they may quickly become reliant on such tools without properly understanding the underlying concepts. One popular approach for scaffolding the code writing process is to use Parsons problems, which present solution lines of code in a scrambled order. These remove the complexities of low-level syntax, and allow students to focus on algorithmic and design-level problem solving. It is unclear how well code generation models can be applied to solve Parsons problems, given the mechanics of these models and prior evidence that they underperform when problems include specific restrictions. In this paper, we explore the performance of the Codex model for solving Parsons problems over various prompt variations. Using a corpus of Parsons problems we sourced from the computing education literature, we find that Codex successfully reorders the problem blocks about half of the time, a much lower rate of success when compared to prior work on more free-form programming tasks. Regarding prompts, we find that small variations in prompting have a noticeable effect on model performance, although the effect is not as pronounced as between different problems.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {299–305},
numpages = {7},
keywords = {CS1, GPT-3, GitHub, ML, academic integrity, ai, artificial intelligence, chatgpt, code generation, code writing, codex, computer programming, copilot, deep learning, generative ai, introductory programming, large language models, machine learning, natural language processing, neural networks, novice programming, openAI},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3726302.3731941,
author = {Zeng, Jingying and Dai, Zhenwei and Liu, Hui and Varshney, Samarth and Liu, Zhiji and Luo, Chen and Li, Zhen and He, Qi and Tang, Xianfeng},
title = {Examples as the Prompt: A Scalable Approach for Efficient LLM Adaptation in E-Commerce},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3731941},
doi = {10.1145/3726302.3731941},
abstract = {Prompting LLMs offers an efficient way to guide output generation without explicit model training. In the e-commerce domain, prompt-based applications are widely used in query understanding, recommender systems, and customer support. However, adapting LLMs to different tasks often requires extensive prompt engineering by domain experts, along with frequent updates to align with evolving business needs. Additionally, crafting fully unbiased natural language prompts remains a challenge for humans. To address these challenges, we propose a novel framework, Examples as the Prompt (EaP). Specifically, EaP automatically selects the most representative examples to maximize the few-shot capability of LLMs. It is efficient due to its unsupervised example selection and adaptive to potential data distribution shifts. We validate EaP on four real-world production use cases, demonstrating that it achieves comparable or even superior performance comparing to hand-crafted prompts designed by domain experts. Additionally, we introduce EaPlite, which entirely replaces the natural language components of prompts with labeled examples. EaPlite improves LLM inference speed by up to 70% without compromising performance. The online A/B test shows that using EaP and EaPlite for data labeling can bring significant composite revenue gain by 0.06%.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4244–4248},
numpages = {5},
keywords = {large language models, natural language generation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inbook{10.5555/3716662.3716698,
author = {Feffer, Michael and Sinha, Anusha and Deng, Wesley H. and Lipton, Zachary C. and Heidari, Hoda},
title = {Red-Teaming for Generative AI: Silver Bullet or Security Theater?},
year = {2025},
publisher = {AAAI Press},
abstract = {In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–437},
numpages = {17}
}

@inproceedings{10.1145/3650212.3680304,
author = {Yang, Mingke and Chen, Yuqi and Liu, Yi and Shi, Ling},
title = {DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680304},
doi = {10.1145/3650212.3680304},
abstract = {Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {578–589},
numpages = {12},
keywords = {Automated Testing, Knowledge Distillation, Large Language Models},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3652963.3655087,
author = {Cheng, Scott and Lin, Jun-Liang and Emani, Murali and Raskar, Siddhisanket and Foreman, Sam and Xie, Zhen and Vishwanath, Venkatram and Kandemir, Mahmut T.},
title = {Thorough Characterization and Analysis of Large Transformer Model Training At-Scale},
year = {2024},
isbn = {9798400706240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652963.3655087},
doi = {10.1145/3652963.3655087},
abstract = {Large transformer models have recently achieved great success across various domains. With a growing number of model parameters, a large transformer model training today typically involves model sharding, data parallelism, and model parallelism. Thus, the throughput of large-scale model training depends heavily on the network bandwidth since a combination of model sharding and multiple parallelism strategies incurs various costs. However, prior characterizations of transformer models on high-bandwidth DGX machines that use TFLOPS as a metric may not reflect the performance of a system with lower bandwidth. Furthermore, data and model parallelism reveal significantly distinct training profiles on different system bandwidths at scale and, thus, need a thorough study.In this paper, we provide a bottom-up breakdown of training throughput into compute and communication time, and quantitatively analyze their respective influences on overall end-to-end training scaling. Our evaluation involves an in-depth exploration of data parallelism, scaling up to 512 GPUs with limited bandwidth, and examines three model sharding strategies among six model sizes. We also evaluate three combinations of model parallelism on both high and low bandwidth supercomputing systems. Overall, our work provides a broader perspective on large-scale transformer model training, and our analysis and evaluation yield practical insights for predicting training scaling, shaping the future development of supercomputing system design.},
booktitle = {Abstracts of the 2024 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {39–40},
numpages = {2},
keywords = {large language model},
location = {Venice, Italy},
series = {SIGMETRICS/PERFORMANCE '24}
}

@inproceedings{10.1145/3643656.3643900,
author = {Chen, Yang and Jabbarvand, Reyhaneh},
title = {Can ChatGPT Repair Non-Order-Dependent Flaky Tests?},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643900},
doi = {10.1145/3643656.3643900},
abstract = {Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {22–29},
numpages = {8},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1145/3655038.3665950,
author = {Egersdoerfer, Chris and Sareen, Arnav and Bez, Jean Luca and Byna, Suren and Dai, Dong},
title = {ION: Navigating the HPC I/O Optimization Journey using Large Language Models},
year = {2024},
isbn = {9798400706301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655038.3665950},
doi = {10.1145/3655038.3665950},
abstract = {Effectively leveraging the complex software and hardware I/O stacks of HPC systems to deliver needed I/O performance has been a challenging task for domain scientists. To identify and address I/O issues in their applications, scientists largely rely on I/O experts to analyze the recorded I/O traces of their applications and provide insights into the potential issues. However, due to the limited number of I/O experts and the growing demand for data-intensive applications across the wide spectrum of sciences, inaccessibility has become a major bottleneck hindering scientists from maximizing their productivity. Inspired by the recent rapid progress of large language models (LLMs), in this work we propose IO Navigator (ION), an LLM-based framework that takes a recorded I/O trace of an application as input and leverages the in-context learning, chain-of-thought, and code generation capabilities of LLMs to comprehensively analyze the I/O trace and provide diagnosis of potential I/O issues. Similar to an I/O expert, ION provides detailed justifications for the diagnosis and an interactive interface for scientists to ask detailed questions about the diagnosis. We illustrate ION's applicability by assessing it on a set of controlled I/O traces generated with different I/O issues. We also demonstrate that ION can match state-of-the-art I/O optimization tools and provide more insightful and adaptive diagnoses for real applications. We believe ION, with its full capabilities, has the potential to become a powerful tool for scientists to navigate through complex I/O subsystems in the future.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {86–92},
numpages = {7},
location = {Santa Clara, CA, USA},
series = {HotStorage '24}
}

@inproceedings{10.1145/3698587.3701489,
author = {Jaiswal, Sugam and Lee, Joyce and Berria, Joe and Tanikella, Raviteja and Zolyomi, Annuska and Ahmad, Muhammad Aurangzeb and Si, Dong},
title = {Building Personality-Adaptive Conversational AI for Mental Health Therapy},
year = {2024},
isbn = {9798400713026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698587.3701489},
doi = {10.1145/3698587.3701489},
abstract = {Many people with mental health problems cannot get professional help for various reasons such as lack of awareness, unavailability, unaffordability, etc. A virtual conversational agent can offer an alternative to deliver mental health care that is accessible, affordable, and scalable. However, building such agents using a one-size-fits-all approach may not be effective for everyone, as different individuals have different personality types that dictate how they communicate with chatbots. Therefore, developing therapy chatbots that can adjust to the user's personality is important. In this work, we present the important role of personality-adaptive conversational agents (PACAs) in the context of mental healthcare. We designed an architecture around traditional machine learning (ML) models and open-source large language models (LLMs) to build a PACA for mental health therapy, developed a working prototype based on it, and conducted a user study to conclude that personality-adaptiveness is indeed an important feature for mental health chatbots.Our research was based on the iCare Project [1], and the associated development was meant to minimize the limitations of the project. We designed the PACA to adapt its responses according to the personality profile of the user created over time. The personality profiles were based on the results from a text classification model fine-tuned for the Big Five Personality Traits[2] with a classification accuracy of 96%. We self-hosted an open-source LLM which made use of accumulated personality information through prompt engineering. The final setup was able to generate adapted therapeutic responses with an average response time of 10 seconds using different hyperparameter tuning and fine-tuning approaches.We conducted a user study among 20 subjects to compare the performance of the personality-adaptive chatbot with its non-adaptive counterpart and found that the adaptive feature almost doubled the number of users who found the chatbot relevant and helpful for their mental healthcare needs. 75% of users felt comfortable discussing any sensitive topic with the adaptive chatbot in comparison to 45% for the non-adaptive chatbot. The PACA prototype validated the feasibility of creating accessible personalized mental healthcare solutions using advanced ML techniques and the results from the user study highly recommend the implementation of PACA into mental health chatbots. The prototype is currently live and freely available for use.},
booktitle = {Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {92},
numpages = {1},
keywords = {chatbot, conversational AI, fine-tuning, large language models, mental healthcare, paca, personality, prompt engineering},
location = {Shenzhen, China},
series = {BCB '24}
}

@inproceedings{10.1145/3711129.3711223,
author = {Zhang, Jing},
title = {Leveraging Large Language Models for Autonomous Threat Detection in IoT Networks},
year = {2025},
isbn = {9798400710094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711129.3711223},
doi = {10.1145/3711129.3711223},
abstract = {The advent of large language models (LLMs) represents a significant paradigm shift in autonomous threat detection within IoT networks. This paper explores the application of Large Language Models (LLMs) for autonomous threat detection in IoT networks. We present a comprehensive review of existing research in IoT security, identifying key limitations and opportunities for improvement. In support of our investigation, we have prepared a dataset of 65,000 entries, which captures a wide array of threat scenarios and device behaviors in IoT networks. Then, we design a novel threat detection architecture that leverages the capabilities of LLMs to identify and respond to threats autonomously. Our architecture is specifically tailored to meet the unique demands of IoT networks, including real-time detection and lightweight processing. Through a series of experiments, we compare our LLM-based architecture against existing threat detection frameworks, demonstrating its superior performance in terms of accuracy, speed, and adaptability to evolving threats. The findings of this research highlight the potential of LLMs to significantly enhance the security of IoT networks, paving the way for more robust and responsive threat detection systems.},
booktitle = {Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering},
pages = {545–550},
numpages = {6},
keywords = {Autonomous Threat Detection, IoT Networks, Large Language Models},
location = {
},
series = {EITCE '24}
}

@article{10.1145/3727980,
author = {Chang, Kaiyan and Zhu, Wenlong and Wang, Kun and He, Xinyang and Yang, Nan and Chen, Zhirong and Jin, Dantong and Li, Cangyuan and Zhou, Yunhao and Yan, Hao and Zhao, Zhuoliang and Cheng, Yuan and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Xiaowei and Li, Huawei and Wang, Ying},
title = {A data-centric chip design agent framework for Verilog code generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3727980},
doi = {10.1145/3727980},
abstract = {Recent advances in large language models (LLMs) have demonstrated significant potential for automated hardware description language (HDL) code generation from high-level specifications. However, two critical challenges limit further progress in this domain: the scarcity of quality Verilog training data and the inability of current approaches to generate RTL code optimized for power, performance, and area (PPA) metrics. This paper presents a comprehensive data-centric framework that addresses these limitations through innovations in both pre-fine-tuning data preparation and after-fine-tuning optimization strategies. In the pre-fine-tuning phase, we tackle the data scarcity problem with an automated design-data augmentation framework that generates high-volume, high-quality natural language specifications aligned with corresponding Verilog code and EDA scripts. Our approach creates a complete RTL-level feedback loop by augmenting EDA scripts, RTL code, and EDA tool feedback. In the after-fine-tuning phase, we focus on generating PPA-aware RTL code through a novel search and prompt framework. Our approach implements iterative filtering and selection of LLM-generated Verilog variants while providing high-quality predefined prompts, including composition and interface specifications. To evaluate the effectiveness of our data augmentation method, we fine-tune Llama 2-13B and Llama 2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark. Our 13B model has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data. Additionally, to evaluate the effectiveness of the our agent framework, we compare the PPA on the GPT-3.5, where the results show that the agent refined RTL code can have a better quality than the generated RTL code only with GPT-3.5.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = apr,
keywords = {Large Language Model, Hardware Generation, Data Augmentation}
}

@inproceedings{10.1145/3643795.3648385,
author = {Koziolek, Heiko and Koziolek, Anne},
title = {LLM-based Control Code Generation using Image Recognition},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648385},
doi = {10.1145/3643795.3648385},
abstract = {LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P&amp;IDs) using image recognition. We have evaluated the method in three case study with industrial P&amp;IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {38–45},
numpages = {8},
keywords = {large language models, code generation, P&amp;IDs, IEC 61131-3, image recognition, industrial case study, industrial automation, PLC, DCS, ChatGPT, GPT4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.1145/3660809,
author = {Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse},
title = {Mining Action Rules for Defect Reduction Planning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660809},
doi = {10.1145/3660809},
abstract = {Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and “explaining” its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT’s explainable plans achieve higher overlap scores at the release level (median 95%) and commit level (median 85.97%), and they offer better trade-off between precision and recall (median F1-score 88.12%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {102},
numpages = {23},
keywords = {Action rule mining, Counterfactual explanations, Defect reduction planning, Explainability, Software analytics}
}

@inproceedings{10.1145/3664476.3664497,
author = {Zhang, Xinyu and Muralee, Siddharth and Cherupattamoolayil, Sourag and Machiry, Aravind},
title = {On the Effectiveness of Large Language Models for GitHub Workflows},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664497},
doi = {10.1145/3664476.3664497},
abstract = {GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows,&nbsp;i.e., &nbsp;YAML files with a list of jobs. However, engineering valid workflows is tedious. They are also prone to severe security issues, which can result in supply chain vulnerabilities. Recent advancements in&nbsp;Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks. However,&nbsp;GitHub workflows differ from regular programs in both structure and semantics. We perform the first comprehensive study to understand the effectiveness of&nbsp;Large Language Models (LLMs) on five workflow-related tasks with different levels of prompts. We curated a set of ∼ 400K workflows and generated prompts with varying detail. We also fine-tuned&nbsp;LLMs on&nbsp;GitHub workflow tasks. Our evaluation of three state-of-the-art&nbsp;LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of&nbsp;LLMs.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {32},
numpages = {14},
keywords = {GitHub Workflow, Large Language Model, Vulnerability Detection},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3544548.3580817,
author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
title = {“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580817},
doi = {10.1145/3544548.3580817},
abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user’s natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users’ understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {598},
numpages = {31},
keywords = {Human-AI Interaction, Large Language Models, Natural Language Programming, Spreadsheets},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3644032.3644454,
author = {Hoffmann, Jacob and Frister, Demian},
title = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644454},
doi = {10.1145/3644032.3644454},
abstract = {Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13--15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.Research questions. This work investigated: Does SFT enhance the capabilities of a transformer-based LLM in the specific downstream task of generating test cases for mobile applications while being cost-efficient and runnable on standard consumer hardware? Does the fine-tuned model outperform other state-of-the-art models in the task of test generation for mobile applications?Approach. Our approach is a modification of the ATHENATEST approach [16]. However, our approach focuses on supervised fine-tuning (SFT) on both pre-trained and already fine-tuned transformer-based LLMs for the task of test case generation for mobile applications in Dart.The approach involves three steps, as illustrated in Figure 1. Firstly, a labeled dataset of corresponding input-output pairs (X, Y) was obtained to model the conditional probability P(Y|X; θ) [9, 12]. Dart code and corresponding test files were extracted from open-source GitHub repositories using Google BigQuery. These files were then matched using regular expressions, ensuring that each code file was matched with its corresponding test file based on matching base filenames. The dataset underwent quality filtering and deduplication, resulting in 16,252 input-output pairs, which was then divided into training (90%) and validation (10%) sets. The training set of the dataset consists of a total of 88.5M tokens using the LLaMA tokenizer.Secondly, for SFT on the downstream task of test generation, models were selected based on their code generation capabilities, as indicated by the pass@1 score on the HumanEval [2] and MBPP [1] benchmark, their parameter sizes, and the extent to which they had been trained on Dart data. In model selection, open-source models capable of running on cost-efficient consumer hardware with code generation abilities were primarily chosen.Thirdly, in the SFT process, the test generation task was represented as translation task, in line with ATHENATEST [16]. This is achieved by employing the following structured prompt format for SFT [9]:"{prefix_prompt} ### Code: {code} ### Test: {test}"In this work, there was no prefix prompt used during SFT.Fine-tuning. The fine-tuning was conducted on a single GPU system using Flash Attention 2 [3] and the QLoRA method [4] to reduce memory size and the number of trainable parameters. The fine-tuning process varied in duration up to 32 hours, resulting in total emissions of 13.099 kgCO2eq [5].Experimental Results. The performance of TestGen-Dart models was evaluated for their unit testing capabilities in Dart, in comparison to base models LLaMA 2 13B, Code Llama 13B, and Mistral 7B. The models were loaded in both float16 and 4-bit quantization configurations, and the evaluation involved nine different Dart files, encompassing 42 test cases. The results were obtained in a zero-shot setting using a structured prompt format, as described in the approach section. This included a prefix prompt instructing the models to generate unit tests: "Generate unit tests in Dart for the following class. The unit test should be structured with the 'test' function, an appropriate description, and an assertion 'expect' within the function to validate the test case." The generated unit tests were classified into three categories: syntax errors (SE), syntactic correctness (SC), and functional correctness (FC). In a 4-bit quantization configuration, TestGen-Dart_v0.2 enhanced the generation of syntactically correct unit tests by 15.38% and functionally correct unit tests by 16.67%, compared to the underlying base model, Code Llama 13B. Additionally, TestGen-Dart_v0.2 demonstrated superior performance in the 16-bit configuration. This evidenced that supervised fine-tuning (SFT) increases the capability of transformer-based LLMs in a specific downstream task, in this instance, generating test cases for mobile applications, addressing the first research question posed in this work. Additionally, TestGen-Dart_v0.2 outperformed the other state-of-the-art models of interest LLaMA 2 13B and Mistral 7B in that task, addressing the second research question.Conclusion. This work demonstrates that SFT enhances the capability of transformer-based LLMs in generating test cases for mobile applications in Dart. Furthermore, the 13B parameter size of the TestGen-Dart enables it to run locally on standard consumer hardware, potentially making it a cost-efficient and privacy-friendly testing assistant for software developers by avoiding an external server connection to run the model.Outlook. Future work currently in progress may expand this approach to other programming languages and refine TestGen-Dart's performance by using higher-quality fine-tuning data either synthetic or human-annotated. Additionally, the evaluation method may be enhanced by using TestGen-Dart for generating test cases for dummy applications and measuring code coverage.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {76–77},
numpages = {2},
keywords = {software testing, mobile testing, machine learning, large language models},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3610977.3634969,
author = {Karli, Ulas Berk and Chen, Juo-Tung and Antony, Victor Nikhil and Huang, Chien-Ming},
title = {Alchemist: LLM-Aided End-User Development of Robot Applications},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634969},
doi = {10.1145/3610977.3634969},
abstract = {Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming---moving from the conventional process of user specifying programming logic to an iterative, collaborative process in which the user specifies desired program outcomes while LLM produces detailed specifications. We introduce a novel integrated development system, Alchemist, that leverages LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, aiming to reduce the required knowledge for developing robot applications. We present a detailed examination of our system design and provide an exploratory study involving true end-users to assess capabilities, usability, and limitations of our system. Through the design, development, and evaluation of our system, we derive a set of lessons learned from the use of LLMs in robot programming. We discuss how LLMs may be the next frontier for democratizing end-user development of robot applications.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {361–370},
numpages = {10},
keywords = {code generation, end-user development, robot programming},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@article{10.14778/3685800.3685914,
author = {Kumar, Arun},
title = {Reimagining Deep Learning Systems through the Lens of Data Systems},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685914},
doi = {10.14778/3685800.3685914},
abstract = {The high-profile success of Deep Learning (DL) at Big Tech companies, including recent Large Language Models (LLMs) such as the GPT and Llama families, has led to high demand among Web companies, consumer app companies, enterprises, healthcare, domain sciences, and even digital humanities and arts to adopt modern DL for their applications. The scale of DL workloads, domain-specific datasets, and publicly available pre-trained base models keeps growing. Naturally, tackling issues of scalability, usability, and resource/cost efficiency of DL systems are critical to democratizing modern DL-powered AI. We find that some key lessons from the decades of work on data system design, implementation, and optimization-when adapted prudently-can go a long way toward that goal. Specifically, our work shows that new analogues of multi-query optimization for DL systems can substantially reduce runtimes and costs, while improving ease of use. This article lays out how we reimagine DL workloads that way and summarizes the technical contributions powering this transformation.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4531–4535},
numpages = {5}
}

@inproceedings{10.1145/3620666.3655589,
author = {Vahdat, Amin},
title = {Societal infrastructure in the age of Artificial General Intelligence},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3655589},
doi = {10.1145/3620666.3655589},
abstract = {Today, we are at an inflection point in computing where emerging Generative AI services are placing unprecedented demand for compute while the existing architectural patterns for improving efficiency have stalled. In this talk, we will discuss the likely needs of the next generation of computing infrastructure and use recent examples at Google from networks to accelerators to servers to illustrate the challenges and opportunities ahead. Taken together, we chart a course where computing must be increasingly specialized and co-optimized with algorithms and software, all while fundamentally focusing on security and sustainability.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {1},
numpages = {1},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3657054.3657125,
author = {Beltran, Marco Antonio and Ruiz Mondragon, Marina Ivette and Han, Seung Hun},
title = {Comparative Analysis of Generative AI Risks in the Public Sector},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657125},
doi = {10.1145/3657054.3657125},
abstract = {The landscape of artificial intelligence (AI) has experienced a monumental shift with the emerging of Generative AI (GenAI), which has demonstrated to be a transformative tool across diverse sectors. GenAI outputs can span various digital formats, including text, images, videos, and audio, generating particular interest in the public sector. The growing interest of governments in integrating GenAI technologies in public sector operations is marked by the creation of emerging governance instruments and the formulation of soft laws, like standards, principles, and guidelines. This study aims to delve into the intricacies and potential risks associated with the deployment of GenAI within government. Through a qualitative content analysis, the research meticulously examines GenAI usage guidelines issued by Australia, Canada, New Zealand, the United Kingdom, and South Korea. The objective is to discern the risks acknowledged by these countries' soft laws and compare them with the risks identified by scholars in the field. The performed comparative analysis across countries suggest that the use of GenAI in the public sector raises common risks such as information leakage, data privacy, security, and concerns over public trust. By elucidating the varied risk perceptions across different national contexts, this study provides theoretical and practical implications related to the risks of GenAI within the public sector. Moreover, it sets a foundation for future research and policy development, ensuring that generative AI is used as a force for good in public governance.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {610–617},
numpages = {8},
keywords = {GenAI, GenAI Policy and Regulation, Public sector, Risks},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@article{10.5555/3648699.3648939,
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
title = {PaLM: scaling language modeling with pathways},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {240},
numpages = {113},
keywords = {large language models, few-shot learning, natural language processing, scalable deep learning}
}

@inproceedings{10.1145/3689236.3689272,
author = {Sun, Yuxuan},
title = {Automated Generation and Compilation of Fuzz Driver Based on Large Language Models},
year = {2024},
isbn = {9798400718137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689236.3689272},
doi = {10.1145/3689236.3689272},
abstract = {Fuzz drivers are essential components of library fuzz testing, yet automatically generating correct and robust fuzz drivers and executing them is challenging. The predominant approach to fuzz testing libraries and their function interfaces involves security experts manually writing test drivers for fuzz testers. In contrast, generation based on LLMs (Large Language Models) is a promising direction, as it can operate with lower requirements on consumer programs, utilizes API (Application Programming Interface) usage information across multiple dimensions, and generates user-friendly output code. However, there is currently no fully automated method for driver generation and compilation. To address this issue, this paper has designed an automated method for generating and compiling drivers, and has evaluated the quality of the drivers it produces. Evaluation results indicate that drivers generated by large language models perform nearly as well in coverage as those written manually, and the generated compilation commands achieve an accuracy of 75%.},
booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering},
pages = {461–468},
numpages = {8},
keywords = {Driver Compilation, Fuzz Driver Generation, Fuzz Testing, Large Language Model, Vulnerability Detection},
location = {
},
series = {ICCSIE '24}
}

@inproceedings{10.1145/3658617.3697774,
author = {Wang, Kun and Chang, Kaiyan and Wang, Mengdi and Zou, Xingqi and Xu, Haobo and Han, Yinhe and Wang, Ying},
title = {RTLMarker: Protecting LLM-Generated RTL Copyright via a Hardware Watermarking Framework},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697774},
doi = {10.1145/3658617.3697774},
abstract = {Recent advances of large language models in the field of Verilog generation have raised several ethical and security concerns, such as code copyright protection and dissemination of malicious code. Researchers have employed watermarking techniques to identify codes generated by large language models. However, the existing watermarking works fail to protect RTL code copyright due to the significant syntactic and semantic differences between RTL code and software code in languages such as Python. This paper proposes a hardware watermarking framework RTLMarker that embeds watermarks into RTL code and deeper into the synthesized netlist. We propose a set of rule-based Verilog code transformations, ensuring the watermarked RTL code's syntactic and semantic correctness. In addition, we consider an inherent tradeoff between watermark transparency and watermark effectiveness and jointly optimize them. The results demonstrate RTLMarker's superiority over the baseline in RTL code watermarking.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {808–813},
numpages = {6},
keywords = {large language model, hardware copyright},
location = {Tokyo, Japan},
series = {ASPDAC '25}
}

@inproceedings{10.1145/3657054.3664243,
author = {Yang, Alan and Yang, T. Andrew},
title = {Social Dangers of Generative Artificial Intelligence: Review and Guidelines},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3664243},
doi = {10.1145/3657054.3664243},
abstract = {In this paper, we provide a detailed survey of generative artificial intelligence (GAI) and examine the perceived social problems, including those that are currently apparent and those that can potentially be caused by the technology. After the introduction, we discuss initiatives proposed by governmental and professional entities to curtail the risks posed by adopting AI technologies without consideration of the associated risks. A brief survey of published research in AI security and related risks is then presented along with a discussion of findings and recommendations in the form of guidelines for future adoption of generative AI across a variety of contexts.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {654–658},
numpages = {5},
keywords = {Digital Governance, Generative AI, Regulations and Standards, Risks},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@inproceedings{10.1145/3706468.3706550,
author = {Liu, Naiming and Sonkar, Shashank and Basu Mallick, Debshila and Baraniuk, Richard and Chen, Zhongzhou},
title = {Atomic Learning Objectives and LLMs Labeling: A High-Resolution Approach for Physics Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706550},
doi = {10.1145/3706468.3706550},
abstract = {This paper introduces a novel approach to create a high-resolution “map" for physics learning: an "atomic" learning objectives (LOs) system designed to capture detailed cognitive processes and concepts required for problem solving in a college-level introductory physics course. Our method leverages Large Language Models (LLMs) for automated labeling of physics questions and introduces a comprehensive set of metrics to evaluate the quality of the labeling outcomes. The atomic LO system, covering nine chapters of an introductory physics course, uses a “subject-verb-object” structure to represent specific cognitive processes. We apply this system to 131 questions from expert-curated question banks and the OpenStax University Physics textbook. Each question is labeled with 1-8 atomic LOs across three chapters. Through extensive experiments using various prompting strategies and LLMs, we compare automated LOs labeling results against human expert labeling. Our analysis reveals both the strengths and limitations of LLMs, providing insight into LLMs reasoning processes for labeling LOs and identifying areas for improvement in LOs system design. Our work contributes to the field of learning analytics by proposing a more granular approach to mapping learning objectives with questions. Our findings have significant implications for the development of intelligent tutoring systems and personalized learning pathways in STEM education, paving the way for more effective “learning GPS” systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {620–630},
numpages = {11},
keywords = {Physics Education, Learning Objectives},
location = {
},
series = {LAK '25}
}

@article{10.1145/3702987,
author = {Zhang, Sai and Xing, Zhenchang and Guo, Ronghui and Xu, Fangzhou and Chen, Lei and Zhang, Zhaoyuan and Zhang, Xiaowang and Feng, Zhiyong and Zhuang, Zhiqiang},
title = {Empowering Agile-Based Generative Software Development through Human-AI Teamwork},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702987},
doi = {10.1145/3702987},
abstract = {In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {156},
numpages = {46},
keywords = {Agile, Human-AI Teamwork, Generative Software Development, User Requirement, Gherkin}
}

@article{10.1145/3639034,
author = {Cheng, Scott and Lin, Jun-Liang and Emani, Murali and Raskar, Siddhisanket and Foreman, Sam and Xie, Zhen and Vishwanath, Venkatram and Kandemir, Mahmut Taylan},
title = {Thorough Characterization and Analysis of Large Transformer Model Training At-Scale},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3639034},
doi = {10.1145/3639034},
abstract = {Large transformer models have recently achieved great success across various domains. With a growing number of model parameters, a large transformer model training today typically involves model sharding, data parallelism, and model parallelism. Thus, the throughput of large-scale model training depends heavily on the network bandwidth since a combination of model sharding and multiple parallelism strategies incurs various costs. However, prior characterizations of transformer models on high-bandwidth DGX machines that use TFLOPS as a metric may not reflect the performance of a system with lower bandwidth. Furthermore, data and model parallelism reveal significantly distinct training profiles on different system bandwidths at scale and, thus, need a thorough study. In this paper, we provide a bottom-up breakdown of training throughput into compute and communication time, and quantitatively analyze their respective influences on overall end-to-end training scaling. Our evaluation involves an in-depth exploration of data parallelism, scaling up to 512 GPUs with limited bandwidth, and examines three model sharding strategies among six model sizes. We also evaluate three combinations of model parallelism on both high and low bandwidth supercomputing systems. Overall, our work provides a broader perspective on large-scale transformer model training, and our analysis and evaluation yield practical insights for predicting training scaling, shaping the future development of supercomputing system design.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = feb,
articleno = {8},
numpages = {25}
}

@inproceedings{10.1145/3722212.3725093,
author = {Sun, Wenbo and Li, Ziyu and Hai, Rihan},
title = {Database as Runtime: Compiling LLMs to SQL for In-database Model Serving},
year = {2025},
isbn = {9798400715648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722212.3725093},
doi = {10.1145/3722212.3725093},
abstract = {Deploying large language models (LLMs) often requires specialized hardware and complex frameworks, creating barriers for CPU-based environments with resource constraints. These systems, common in air-gapped or edge scenarios, lack support for maintenance due to security, budget, or technical limits. To address this, we introduce TranSQL+, a compiler that translates LLM inference into SQL queries, enabling deployment on relational databases. By converting transformer operations into relational algebra, TranSQL+ generates vector-oriented SQL queries that leverage native database features (buffer management, indexing) to manage computations without hardware accelerators or deep learning frameworks. Demonstrated with the LLaMA3.1 8B model on DuckDB, results show relational databases can effectively serve LLMs, reducing deployment barriers and expanding access to advanced AI.},
booktitle = {Companion of the 2025 International Conference on Management of Data},
pages = {231–234},
numpages = {4},
keywords = {large language models, query processing, relational database},
location = {Berlin, Germany},
series = {SIGMOD/PODS '25}
}

@inproceedings{10.1145/3726302.3730051,
author = {Oosterhuis, Harrie and Jagerman, Rolf and Qin, Zhen and Wang, Xuanhui},
title = {Optimizing Compound Retrieval Systems},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730051},
doi = {10.1145/3726302.3730051},
abstract = {Modern retrieval systems do not rely on a single ranking model to construct their rankings. Instead, they generally take a cascading approach where a sequence of ranking models are applied in multiple re-ranking stages. Thereby, they balance the quality of the top-K ranking with computational costs by limiting the number of documents each model re-ranks. However, the cascading approach is not the only way models can interact to form a retrieval system.We propose the concept of compound retrieval systems as a broader class of retrieval systems that apply multiple prediction models. This encapsulates cascading models but also allows other types of interactions than top-K re-ranking. In particular, we enable interactions with large language models (LLMs) which can provide relative relevance comparisons. We focus on the optimization of compound retrieval system design which uniquely involves learning where to apply the component models and how to aggregate their predictions into a final ranking. This work shows how our compound approach can combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM relevance predictions, while optimizing a given ranking metric and efficiency target. Our experimental results show optimized compound retrieval systems provide better trade-offs between effectiveness and efficiency than cascading approaches, even when applied in a self-supervised manner.With the introduction of compound retrieval systems, we hope to inspire the information retrieval field to more out-of-the-box thinking on how prediction models can interact to form rankings.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2361–2371},
numpages = {11},
keywords = {large language models, learning to rank, ranking distillation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3695991,
author = {Dong, Yihong and Ding, Jiazheng and Jiang, Xue and Li, Ge and Li, Zhuo and Jin, Zhi},
title = {CodeScore: Evaluating Code Generation by Learning Code Execution},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695991},
doi = {10.1145/3695991},
abstract = {A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref and NL formats, which existing match-based CEMs cannot effectively accommodate. In this article, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {77},
numpages = {22},
keywords = {Code Evaluation, Code Pre-trained Language Model, Code Generation}
}

@inproceedings{10.1145/3576915.3623175,
author = {He, Jingxuan and Vechev, Martin},
title = {Large Language Models for Code: Security Hardening and Adversarial Testing},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623175},
doi = {10.1145/3576915.3623175},
abstract = {Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1865–1879},
numpages = {15},
keywords = {ai safety, code generation, code security, large language models},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@article{10.1145/3745026,
author = {Yitagesu, Sofonias and Xing, Zhenchang and Zhang, Xiaowang and Feng, Zhiyong and Bi, Tingting and Han, Linyi and Li, Xiaohong},
title = {Systematic Literature Review on Software Security Vulnerability Information Extraction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3745026},
doi = {10.1145/3745026},
abstract = {Background. Software vulnerabilities are increasing in complexity and scale, posing great security risks to many software systems. Extracting information about software vulnerabilities is a critical area of research that aims to identify and create a structured representation of vulnerability-related information. This structured data helps software systems better understand vulnerabilities and provides security professionals with timely information to mitigate the impact of rapidly growing vulnerabilities while guiding future research to develop more secure systems. However, this process relies on the effectiveness of information extraction to transform manual vulnerability analysis from security experts to digital solutions. Despite its importance, the unique nature of vulnerability information and the fast pace at which machine learning-based extraction methods and techniques have evolved make it challenging to assess the current successes, failures, challenges, and opportunities within this research area. This study presents a systematic literature review aimed at clarifying this complex landscape.Methods. In this study, we conduct a systematic literature review (SLR) to explore existing research focusing on extracting information about software security vulnerabilities. We search for 829 primary studies on security vulnerability information extraction from seven widely used online digital libraries, focusing on top peer-reviewed journals and conferences published between 2001 and 2024. After applying our inclusion and exclusion criteria and the snowballing technique, we narrowed our selection to 87 studies for in-depth analysis and addressed four main research questions. We collect qualitative and quantitative data from each study, identifying 34 components such as research problems, methods, contributions, evaluation metrics, results, types of extracted vulnerability information, challenges, and limitations. We use meta-analysis, statistical machine learning, and text-mining techniques to identify themes, patterns, and trends across the primary studies and visualize findings.Results. The study provides an overview of the security vulnerability data landscape, identifies key resources, and guides efforts to improve vulnerability information extraction and analysis. The study finds a diverse landscape of learning algorithms used in security vulnerability information extraction, with Bidirectional Encoder Representations from Transformers (BERT), Long Short-term Memory (LSTM), and Support Vector Machine (SVM) being the most dominant. The study identifies key challenges, including feature engineering complexity, lack of a gold-standard corpus, preprocessing errors, generating accurate training data, addressing imbalanced data, multimodality fusion, and graph sparsity in security knowledge graphs.Insights for Future Research Directions. The study underscores the need for advanced extraction approaches, robust datasets, automated annotation methods, and advanced machine learning algorithms to improve the extraction of security vulnerability information. This study also suggests using large language models (LLMs) and transformer models to facilitate the automatic extraction of security-related words, terms, concepts, and phrases and introduce new filtering parameters for user requirements. We provide all our implementations; it can be found at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Software Vulnerability, Vulnerability Information Extraction, systematic literature review (SLR), Meta-analysis, Statistical Machine Learning, Text Mining}
}

@inproceedings{10.1145/3700838.3703658,
author = {Ali, Irshad and Subba, Basant},
title = {PhishURLDetect: A parameter efficient fine-tuning of LLMs using LoRA for detection of phishing URLs},
year = {2025},
isbn = {9798400710629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700838.3703658},
doi = {10.1145/3700838.3703658},
abstract = {This paper presents PhishURLDetect: a lightweight security framework for detecting phishing URLs based on fine-tuned Large Language Models (LLMs). It utilizes a proprietary corpus comprising 573,880 phishing and benign URLs to fine-tune two state-of-the-art LLMs, namely RoBERTa and GPT-2. Low-Rank Adaptation (LoRA) technique is employed to reduce the number of trainable parameters to 0.3% to 0.6% of the original LLM models without compromising on the performance. We demonstrate through experimental results that PhishURLDetect achieves high accuracy of 99.86%, and performs comparably to state-of-the-art LLMs while using significantly fewer number of trainable parameters.},
booktitle = {Proceedings of the 26th International Conference on Distributed Computing and Networking},
pages = {278–279},
numpages = {2},
keywords = {Phishing attacks, LLMs, RoBERTa, GPT-2, LoRA.},
location = {
},
series = {ICDCN '25}
}

@inproceedings{10.1145/3650203.3663334,
author = {Li, Xue and D\"{o}hmen, Till},
title = {Towards Efficient Data Wrangling with LLMs using Code Generation},
year = {2024},
isbn = {9798400706110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650203.3663334},
doi = {10.1145/3650203.3663334},
abstract = {While LLM-based data wrangling approaches that process each row of data have shown promising benchmark results, computational costs still limit their suitability for real-world use cases on large datasets. We revisit code generation using LLMs for various data wrangling tasks, which show promising results particularly for data transformation tasks (up to 37.2 points improvement on F1 score) at much lower computational costs. We furthermore identify shortcomings of code generation methods especially for semantically challenging tasks, and consequently propose an approach that combines program generation with a routing mechanism using LLMs.},
booktitle = {Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning},
pages = {62–66},
numpages = {5},
location = {Santiago, AA, Chile},
series = {DEEM '24}
}

@inproceedings{10.1109/ASE56229.2023.00089,
author = {Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff},
title = {Nuances Are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00089},
doi = {10.1109/ASE56229.2023.00089},
abstract = {Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences.We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {14–26},
numpages = {13},
keywords = {failure-inducing test cases, large language models, program intention inference, program generation},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3729390,
author = {Kong, Jiaolong and Xie, Xiaofei and Liu, Shangqing},
title = {Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729390},
doi = {10.1145/3729390},
abstract = {Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data—a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83% and 87.42% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82% on GPT-3.5 and 88.24% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE120},
numpages = {23},
keywords = {Code Memorization, Program Repair}
}

@inproceedings{10.1145/3610978.3640717,
author = {Cumbal, Ronald and Engwall, Olov},
title = {Speaking Transparently: Social Robots in Educational Settings},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640717},
doi = {10.1145/3610978.3640717},
abstract = {The recent surge in popularity of Large Language Models, known for their inherent opacity, has increased the interest in fostering transparency in technology designed for human interaction. This concern is equally prevalent in the development of Social Robots, particularly when these are designed to engage in critical areas of our society, such as education or healthcare. In this paper we propose an experiment to investigate how users can be made aware of the automated decision processes when interacting in a discussion with a social robot. Our main objective is to assess the effectiveness of verbal expressions in fostering transparency within groups of individuals as they engage with a robot. We describe the proposed interactive settings, system design, and our approach to enhance the transparency in a robot's decision-making process for multi-party interactions.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {356–358},
numpages = {3},
keywords = {conversation, dialogue system, multi-party, transparency},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1109/ASE56229.2023.00064,
author = {Shahariar, G. M. and Hasant, Tahmid and Iqbalt, Anindya and Uddin, Gias},
title = {Contrastive Learning for API Aspect Analysis},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00064},
doi = {10.1109/ASE56229.2023.00064},
abstract = {We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {637–648},
numpages = {12},
keywords = {API aspects, contrastive learning, transformers, API review, aspect detection, LIME},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3696630.3730568,
author = {Guo, Wenbo and Wang, Limin and Zhang, Yiran and Xu, Zhengzi and Wu, Jiahui},
title = {Automated Environment Extraction for Malicious Package Validation: Leveraging Threat Intelligence},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730568},
doi = {10.1145/3696630.3730568},
abstract = {The proliferation of malicious packages in software repositories presents significant security challenges for the software supply chain. While threat intelligence reports frequently document newly discovered malicious packages, validating these reports remains a labor-intensive and error-prone process. This paper introduces an automated approach that leverages large language models (LLMs) to extract environmental configuration parameters from threat intelligence reports and automatically construct isolated testing environments to validate reported malicious behaviors. Our methodology encompasses four key components: threat intelligence processing, environment parameter extraction using LLMs, automated environment construction with containers, and dynamic behavior validation. We demonstrate the effectiveness of our approach through a case study involving a real-world malicious PyPI package, where our system successfully extracted relevant parameters and confirmed the reported malicious behavior.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1756–1759},
numpages = {4},
keywords = {software supply chain, malicious packages, threat intelligence, automated validation, large language models},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3696630.3730565,
author = {Tornhill, Adam and Borg, Markus and Hagatulah, Nadim and S\"{o}derberg, Emma},
title = {ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3730565},
doi = {10.1145/3696630.3730565},
abstract = {The remarkable advances in AI and Large Language Models (LLMs) have enabled machines to write code, accelerating the growth of software systems. However, the bottleneck in software development is not writing code but understanding it; program understanding is the dominant activity, consuming approximately 70% of developers' time. This implies that improving existing code to make it easier to understand has a high payoff and - in the age of AI-assisted coding - is an essential activity to ensure that a limited pool of developers can keep up with ever-growing codebases.This paper introduces Augmented Code Engineering (ACE), a tool that automates code improvements using validated LLM output. Developed through a data-driven approach, ACE provides reliable refactoring suggestions by considering both objective code quality improvements and program correctness. Early feedback from users suggests that AI-enabled refactoring helps mitigate code-level technical debt that otherwise rarely gets acted upon.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1318–1324},
numpages = {7},
keywords = {software engineering, maintainability, code quality, refactoring, AI assistants},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00075,
author = {Ronanki, Krishna},
title = {Towards an AI-Centric Requirements Engineering Framework for Trustworthy AI},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00075},
doi = {10.1109/ICSE-Companion58688.2023.00075},
abstract = {Ethical guidelines are an asset for artificial intelligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering(RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {278–280},
numpages = {3},
keywords = {guidelines, ethical AI, AI co-worker, frameworks, requirements engineering, EU AI act, trustworthy AI},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3643666,
title = {MO2RE 2024: Proceedings of the 1st IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Requirements engineering (RE) is a critical sub-field of software engineering (SE) that deals with identifying, specifying, modeling, analyzing, and validating the needs of stakeholders and constraints of a system [1]. RE covers human-related aspects, as stakeholders need to be involved in eliciting and validating the requirements, as well as more technical aspects, as requirements can be systematically collected (e.g., from app reviews) using data mining techniques and analyzed with natural language processing (NLP) approaches, e.g., to identify quality issues or trace links [2]. Despite the broad spectrum of activities that RE covers, researchers from outside RE often have a misconception that RE is limited to writing and analyzing requirements specifications. Consequently, many researchers in the SE community working on RE-relevant problems (e.g., human-centric SE) are often unaware that such problems belong to the RE research strands. Broadly speaking, RE is under-represented and under-appreciated in the SE community.Despite this limited presence, RE is more and more fundamental to cope with the current state of SE, especially considering the recent disruptive changes in artificial intelligence (AI) and NLP caused by large language models (LLMs) and their applications, ChatGPT being a notable example. Given the increasing pervasiveness of AI-based systems in our daily life, there is a growing need for RE techniques to support sound and structured development of AI systems [3], with a particular interest in explainability, interpretability, reliability, fairness, and other ethical concerns [4]. At the same time, current developments in AI can solve long-standing RE problems, such as automatic requirements tracing, completeness checking, and modeling. AI can further create better connections between RE and other automated SE fields.The 1st International Workshop on (Multi-disciplinary, Open, and RElevant RE) (MO2RE) has the goal to address these issues by raising awareness of RE's diverse aspects and fostering collaboration within the SE community.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3701625.3701700,
author = {de Lima, Vitor Mesaque Alves and Marcacini, Ricardo Marcondes},
title = {Opinion Mining for App Reviews: Identifying and Prioritizing Emerging Issues for Software Maintenance and Evolution},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701700},
doi = {10.1145/3701625.3701700},
abstract = {Opinion mining for app reviews aims to analyze user comments on app stores to support software engineering activities, primarily software maintenance and evolution. One of the main challenges in maintaining software quality is promptly identifying emerging issues, such as bugs. However, manually analyzing these comments is challenging due to the large amount of textual data. Methods based on machine learning have been employed to automate opinion mining and address this issue. Gap. While recent methods have achieved promising results in extracting and categorizing issues from users’ opinions, existing studies mainly focus on assisting software engineers in exploring users’ historical behavior regarding app functionalities and do not explore mechanisms for trend detection and risk classification of emerging issues. Furthermore, these studies do not cover the entire issue analysis process through an unsupervised approach. Contribution. This work advances state of the art in opinion mining for app reviews by proposing an entire automated issue analysis approach to identify, prioritize, and monitor the risk of emerging issues. Our proposal introduces a two-fold approach that (i) identifies possible defective software requirements and trains predictive models for anticipating requirements with a higher probability of negative evaluation and (ii) detect issues in reviews, classifies them in a risk matrix with prioritization levels, and monitors their evolution over time. Additionally, we present a risk matrix construction approach from app reviews using the recent Large Language Models (LLMs). We introduce an analytical data exploration tool that allows engineers to browse the risk matrix, time series, heat map, issue tree, alerts, and notifications. Our goal is to minimize the time between the occurrence of an issue and its correction, enabling the quick identification of problems. Results. We processed over 6.6 million reviews across 20 domains to evaluate our proposal, identifying and ranking the risks associated with nearly 270,000 issues. The results demonstrate the competitiveness of our unsupervised approach compared to existing supervised models. Conclusions. We have proven that opinions extracted from user reviews provide crucial insights into app issues and risks and can be identified early to mitigate their impact. Our opinion mining process implements an entire automated issue analysis with risk-based prioritization and temporal monitoring.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {687–696},
numpages = {10},
keywords = {opinion mining, app reviews, issue detection, issue prioritization, software maintenance and evolution},
location = {
},
series = {SBQS '24}
}

@article{10.1145/3717512.3717515,
author = {Anwar, Mubashir and Caesar, Matthew},
title = {Understanding Misunderstandings: Evaluating LLMs on Networking Questions},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/3717512.3717515},
doi = {10.1145/3717512.3717515},
abstract = {Large Language Models (LLMs) have demonstrated impressive abilities in tackling tasks across numerous domains. The capabilities of LLMs could potentially be applied to various computer networking tasks, including network synthesis, management, debugging, security, and education. However, LLMs can be unreliable: they are prone to reasoning errors and may hallucinate incorrect information. Their effectiveness and limitations in computer networking tasks remain unclear. In this paper, we attempt to understand the capabilities and limitations of LLMs in network applications. We evaluate misunderstandings regarding networking related concepts across 3 LLMs over 500 questions. We assess the reliability, explain-ability, and stability of LLM responses to networking questions. Furthermore, we investigate errors made, analyzing their cause, detectability, effects, and potential mitigation strategies.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = feb,
pages = {14–24},
numpages = {11},
keywords = {characterization study, computer networking, large language models}
}

@article{10.1145/3660783,
author = {Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling},
title = {Evaluating and Improving ChatGPT for Unit Test Generation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660783},
doi = {10.1145/3660783},
abstract = {Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code.                                                                                                 In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.                                                                                                 Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {76},
numpages = {24},
keywords = {Large language model, Test generation, Unit testing}
}

@inproceedings{10.1145/3581641.3584037,
author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584037},
doi = {10.1145/3581641.3584037},
abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {491–514},
numpages = {24},
keywords = {code-fluent large language models, conversational interaction, foundation models, human-centered AI},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3661167.3661263,
author = {Kavian, Arya and Pourhashem Kallehbasti, Mohammad Mehdi and Kazemi, Sajjad and Firouzi, Ehsan and Ghafari, Mohammad},
title = {LLM Security Guard for Code},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661263},
doi = {10.1145/3661167.3661263},
abstract = {Many developers rely on Large Language Models (LLMs) to facilitate software development. Nevertheless, these models have exhibited limited capabilities in the security domain. We introduce LLMSecGuard, a framework to offer enhanced code security through the synergy between static code analyzers and LLMs. LLMSecGuard is open source and aims to equip developers with code solutions that are more secure than the code initially generated by LLMs. This framework also has a benchmarking feature, aimed at providing insights into the evolving security attributes of these models.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {600–603},
numpages = {4},
keywords = {Security analysis, code models, secure code generation},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.5555/3722479.3722480,
author = {McGraw, Gary},
title = {10, 23, 81 --- Stacking up the LLM Risks: Applied Machine Learning Security},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {I present the results of an architectural risk analysis (ARA) of large language models (LLMs), guided by an understanding of standard machine learning (ML) risks previously identified by BIML in 2020. After a brief level-set, I cover the top 10 LLM risks, then detail 23 black box LLM foundation model risks screaming out for regulation, finally providing a bird's eye view of all 81 LLM risks BIML identified. BIML's first work, published in January 2020 presented an in-depth ARA of a generic machine learning process model, identifying 78 risks. In this talk, I consider a more specific type of machine learning use case---large language models---and report the results of a detailed ARA of LLMs. This ARA serves two purposes: 1) it shows how our original BIML-78 can be adapted to a more particular ML use case, and 2) it provides a detailed accounting of LLM risks. At BIML, we are interested in "building security in" to ML systems from a security engineering perspective. Securing a modern LLM system (even if what's under scrutiny is only an application involving LLM technology) must involve diving into the engineering and design of the specific LLM system itself. This ARA is intended to make that kind of detailed work easier and more consistent by providing a baseline and a set of risks to consider.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {17–18},
numpages = {2}
}

@inproceedings{10.1145/3587102.3588814,
author = {Cipriano, Bruno Pereira and Alves, Pedro},
title = {GPT-3 vs Object Oriented Programming Assignments: An Experience Report},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588814},
doi = {10.1145/3587102.3588814},
abstract = {Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages.In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {61–67},
numpages = {7},
keywords = {GPT-3, large language models, object oriented programming, programming assignments, teaching},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3639478.3643525,
author = {Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Musuvathi, Madan and Lahiri, Shuvendu},
title = {Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643525},
doi = {10.1145/3639478.3643525},
abstract = {We introduce a novel workflow, TiCoder, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TiCoder partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TiCoder at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TiCoder through a user study with 15 participants.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {390–391},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inbook{10.1145/3715014.3724042,
author = {Khan, Momin Ahmad and Chandio, Yasra and Bagdasarian, Eugene and Anwar, Fatima},
title = {Poster Abstract: Compromising Federated Medical AI-Backdoor Risks in Prompt Learning},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3724042},
abstract = {This paper investigates the security vulnerabilities of prompt-learning-based FL systems in a healthcare setting. Specifically, we use a backdoor attack that leverages learnable prompt vectors in vision-language medical foundation models to execute stealthy adversarial manipulations. We evaluate our attack across diverse healthcare datasets and FL configurations, showing that while FL is useful as a privacy-preserving mechanism, it is susceptible to targeted backdoor attacks that pose a threat to medical applications.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {630–631},
numpages = {2}
}

@article{10.1145/3735635,
author = {Zhao, Zixiao and Fard, Fatemeh},
title = {Do Current Language Models Support Code Intelligence for R Programming Language?},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3735635},
doi = {10.1145/3735635},
abstract = {Recent advancements in developing Pre-trained Language Models for Code (Code-PLMs) have urged many areas of Software Engineering (SE) and brought breakthrough results for many SE tasks. Though these models have achieved the state-of-the-art performance for SE tasks for many popular programming languages, such as Java and Python, the Scientific Software and its related languages like R programming language have rarely benefited or even been evaluated with the Code-PLMs. Research has shown that R has many differences with other programming languages and requires specific techniques. In this study, we provide the first insights for code intelligence for R. For this purpose, we collect and open source an R dataset, and evaluate Code-PLMs for the two tasks of code summarization and method name prediction using several settings and strategies, including the differences in two R styles, Tidy-verse and Base R. Our results demonstrate that the studied models have experienced varying degrees of performance degradation when processing R programming language code, which is supported by human evaluation. Additionally, not all models show performance improvement in R-specific tasks even after multi-language fine-tuning. The dual syntax paradigms in R significantly impact the models’ performance, particularly in code summarization tasks. Furthermore, the project-specific context inherent in R codebases significantly impacts the performance when attempting cross-project training. Interestingly, even when Large Language Models like CodeLlama and StarCoder2 are used for code generation, the Pass@K ( (K=1,5,10) ) results lags signigicantly behind Python scores. Our research shows that R as a low resource language requires different techniques to collect a high quality data. Specifically separating the two R styles has a great impact on the results and the separate dataset could increase the performance of the models. Our research sheds light on the capabilities of Code-PLMs and opens new research directions for researchers and practitioners for developing code intelligence tools and techniques for R. With R's widespread use and popularity, the results of our study can potentially benefit a large community of R developers, both in research and industry.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Empirical Studies, Code Summarization, Method Name Prediction, R Programming Language, Code Generation in R, R programming Styles (Tidy-verse and Base)}
}

@inproceedings{10.1145/3708394.3708396,
author = {Sun, Zhendan and Zhao, Ruibin},
title = {LLM Security Alignment Framework Design Based on Personal Preference},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708396},
doi = {10.1145/3708394.3708396},
abstract = {large language models (LLMs) are widely used, the experts have raised significant concerns about their controllability, safety, and usability. However, the vulnerability and evaluation methods of alignment become the research focus. Although RLHF secure alignment techniques attempt to solve these problems, they still face challenges such as high labeling costs and wrong target generalization. This study explores future trends, benefits, and challenges of LLM security alignment technology. It proposes a security alignment framework for LLMs tailored to individual preferences. This framework employs algorithmic and data countermeasures to enhance the model's generalization performance, lower the costs associated with manual labeling, and improve LLMs' controllability, usability, and safety. This innovative approach provides useful implications for future secure alignment developments in LLMs.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {6–11},
numpages = {6},
keywords = {AI Alignment, LLM Privacy, LLM Trust Framework, Preference Alignment, RLHF},
location = {
},
series = {AIFE '24}
}

@article{10.1145/3689736,
author = {Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming},
title = {WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689736},
doi = {10.1145/3689736},
abstract = {Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.                To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {296},
numpages = {27},
keywords = {Code Analysis, Fuzzing, Large Language Models, White-box Testing}
}

@article{10.1145/3708533,
author = {B\"{o}hme, Marcel and Bodden, Eric and Bultan, Tevfik and Cadar, Cristian and Liu, Yang and Scanniello, Giuseppe},
title = {Software Security Analysis in 2030 and Beyond: A Research Roadmap},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708533},
doi = {10.1145/3708533},
abstract = {As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap article, we outline our vision of software security analysis for the systems of the future. Given the recent advances in generative AI, we need new methods to assess and maximize the security of code co-written by machines. As our systems become increasingly heterogeneous, we need practical approaches that work even if some functions are automatically generated, e.g., by deep neural networks. As software systems depend evermore on the software supply chain, we need tools that scale to an entire ecosystem. What kind of vulnerabilities exist in future systems and how do we detect them? When all the shallow bugs are found, how do we discover vulnerabilities hidden deeply in the system? Assuming we cannot find all security flaws, how can we nevertheless protect our system? To answer these questions, we start our roadmap with a survey of recent advances in software security, then discuss open challenges and opportunities, and conclude with a long-term perspective for the field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {144},
numpages = {26},
keywords = {SE2030, vision statement, perspective article}
}

@inbook{10.1145/3728725.3728761,
author = {Fang, Yong},
title = {Wasserstein GAN-Based Android Certificate Validation Vulnerability Detection Framework},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728761},
abstract = {This paper presents an Android certificate validation vulnerability detection framework that integrates static analysis, adversarial sample training, and optimization via large language models. Centered around the Wasserstein Generative Adversarial Network, the framework aims to enhance the robustness and generalization ability of static detection models against complex and obfuscated attack patterns. Static analysis is performed on 2,000 popular Android applications from the androzoo dataset, with code and configuration features extracted, focusing on certificate-related API usage, permission declarations, and network security settings. Among them, 110 APKs are identified as containing high-risk certificate misconfigurations. Based on these features, a Transformer-based model is constructed for feature vectorization and vulnerability identification. To further improve model robustness, WGAN is employed to generate adversarial samples that mimic real-world evasive vulnerabilities, which are used for adversarial training. Additionally, large language models are used to perform In-Context Learning, enabling semantic filtering, sample refinement, and iterative generator feedback. This results in a closed-loop, self-improving detection pipeline that combines detection, optimization, and feedback. Experimental results across standard, adversarial, and cross-domain scenarios demonstrate that the proposed framework achieves an F1 score of 0.91 and reduces the false positive rate to 7.2%, significantly outperforming traditional static detection tools in both accuracy and resilience.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {231–236},
numpages = {6}
}

@inproceedings{10.1145/3735950.3735954,
author = {Wang, Huanting and Jacob, Dejice and Kelly, David and Elkhatib, Yehia and Singer, Jeremy and Wang, Zheng},
title = {SecureMind: A Framework for Benchmarking Large Language Models in Memory Bug Detection and Repair},
year = {2025},
isbn = {9798400716102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735950.3735954},
doi = {10.1145/3735950.3735954},
abstract = {Large language models (LLMs) hold great promise for automating software vulnerability detection and repair, but ensuring their correctness remains a challenge. While recent work has developed benchmarks for evaluating LLMs in bug detection and repair, existing studies rely on hand-crafted datasets that quickly become outdated. Moreover, systematic evaluation of advanced reasoning-based LLMs using chain-of-thought prompting for software security is lacking.   We introduce SecureMind, an open-source framework for evaluating LLMs in vulnerability detection and repair, focusing on memory-related vulnerabilities. SecureMind provides a user-friendly Python interface for defining test plans, which automates data retrieval, preparation, and benchmarking across a wide range of metrics.   Using SecureMind, we assess 10 representative LLMs, including 7 state-of-the-art reasoning models, on 16K test samples spanning 8 Common Weakness Enumeration (CWE) types related to memory safety violations. Our findings highlight the strengths and limitations of current LLMs in handling memory-related vulnerabilities.},
booktitle = {Proceedings of the 2025 ACM SIGPLAN International Symposium on Memory Management},
pages = {27–40},
numpages = {14},
keywords = {Bug repair, Large language models, Software bug detection},
location = {Seoul, Republic of Korea},
series = {ISMM '25}
}

@article{10.1145/3712001,
author = {Das, Badhan Chandra and Amini, M. Hadi and Wu, Yanzhao},
title = {Security and Privacy Challenges of Large Language Models: A Survey},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3712001},
doi = {10.1145/3712001},
abstract = {Large language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Today, LLMs have become quite popular tools in natural language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks against LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {152},
numpages = {39},
keywords = {Large language models, attack and defense mechanisms}
}

@inproceedings{10.1145/3696630.3731439,
author = {Wu, Zixuan and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
title = {LLMAppHub: A Large Collection of LLM-based Applications for the Research Community},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731439},
doi = {10.1145/3696630.3731439},
abstract = {With the increasing adoption of large language models (LLMs), a growing number of applications leveraging these models have emerged. However, LLM-based applications face significant security challenges. The OWASP Top 10 for LLMs has identified critical vulnerabilities, including prompt injections, data leakage, and unauthorized code execution. While previous research has explored specific security issues, these studies often focus on a narrow range of problems and a small number of applications, leaving gaps in our understanding of security threats in real-world LLM-based applications. To facilitate research in this field, an automated tool was developed to systematically collect LLM applications and construct a dataset, particularly focusing on independent applications that leverage LLM APIs or frameworks. Our dataset includes 33,001 repositories, providing a broad and representative foundation for security analysis. Both our dataset and tool are publicly available to support further research and foster advancements in the security and development of LLM-based applications.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1254–1255},
numpages = {2},
keywords = {large language model, LLM-based applications},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3133887,
author = {Yaghmazadeh, Navid and Wang, Yuepeng and Dillig, Isil and Dillig, Thomas},
title = {SQLizer: query synthesis from natural language},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133887},
doi = {10.1145/3133887},
abstract = {This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL). At the core of our technique is a new NL-based program synthesis methodology that combines semantic parsing techniques from the NLP community with type-directed program synthesis and automated program repair. Starting with a program sketch obtained using standard parsing techniques, our approach involves an iterative refinement loop that alternates between probabilistic type inhabitation and automated sketch repair. We use the proposed idea to build an end-to-end system called SQLIZER that can synthesize SQL queries from natural language. Our method is fully automated, works for any database without requiring additional customization, and does not require users to know the underlying database schema. We evaluate our approach on over 450 natural language queries concerning three different databases, namely MAS, IMDB, and YELP. Our experiments show that the desired query is ranked within the top 5 candidates in close to 90% of the cases and that SQLIZER outperforms NALIR, a state-of-the-art tool that won a best paper award at VLDB'14.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {63},
numpages = {26},
keywords = {Relational Databases, Programming by Natural Languages, Program Synthesis}
}

@inproceedings{10.1145/3696630.3728712,
author = {Tao, Guanhong and Cheng, Siyuan and Zhang, Zhuo and Zhu, Junmin and Shen, Guangyu and Han, Wanjing and Zhang, Mu and Zhang, Xiangyu},
title = {A Systematic Threat Modeling of LLM Applications},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728712},
doi = {10.1145/3696630.3728712},
abstract = {The emergence of large language models (LLMs) has significantly accelerated the development of a wide range of applications across various fields. There is a growing trend in the construction of specialized applications based on LLMs, such as the custom GPTs by OpenAI. While custom GPTs provide various functionalities like web browsing and code execution, they also introduce significant security threats. In this paper, we conduct a comprehensive analysis of the security and privacy issues arising from the GPT platform. Our systematic examination categorizes potential attack scenarios into three threat models based on the role of the malicious actor, and identifies critical data exchange channels in GPTs. Utilizing the STRIDE threat modeling framework, we identify 26 potential attack vectors, with 19 being partially or fully validated in real-world settings. Our findings emphasize the urgent need for robust security and privacy measures in the GPT ecosystem as well as other similar LLM application ecosystems.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1607–1614},
numpages = {8},
keywords = {large language model, security and privacy, threat modeling},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@article{10.1145/3696379,
author = {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao, Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
title = {A Systematic Comparison of Large Language Models Performance for Intrusion Detection},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT4},
url = {https://doi.org/10.1145/3696379},
doi = {10.1145/3696379},
abstract = {We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {22},
numpages = {23},
keywords = {computing methodologies, firewalls, intrusion detection systems, machine learning, natural language processing, security and privacy}
}

@inproceedings{10.1145/3706468.3706516,
author = {Venugopalan, Devika and Yan, Ziwen and Borchers, Conrad and Lin, Jionghao and Aleven, Vincent},
title = {Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706516},
doi = {10.1145/3706468.3706516},
abstract = {Caregivers (i.e., parents and members of a child’s caring community) are underappreciated stakeholders in learning analytics. Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. This LLM generated message recommendations for caregivers supporting their child’s math practice via chat. Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {373–383},
numpages = {11},
keywords = {large language models, tutoring systems, hybrid tutoring, K-12, mathematics education, caregivers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3624032.3624035,
author = {Guilherme, Vitor and Vincenzi, Auri},
title = {An initial investigation of ChatGPT unit test generation capability},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624035},
doi = {10.1145/3624032.3624035},
abstract = {Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {15–24},
numpages = {10},
keywords = {testing tools, software testing, mutation testing, experimental software engineering, coverage testing, automated test generation},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@inproceedings{10.1145/3691621.3694952,
author = {Wu, Liangxuan and Zhao, Yanjie and Wang, Chao and Liu, Tianming and Wang, Haoyu},
title = {A First Look at LLM-powered Smartphones},
year = {2024},
isbn = {9798400712494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691621.3694952},
doi = {10.1145/3691621.3694952},
abstract = {The integration of Large Language Models (LLMs) into edge devices such as smartphones represents a significant leap in mobile technology, promising enhanced user experiences and novel functionalities. This paper presents a first look at LLM-powered smartphones, addressing four key aspects: the current market landscape, core functions enabled by integrated LLMs, potential security risks, and user perceptions. The findings reveal a rapidly evolving market with major manufacturers competing to integrate LLMs, innovative features that improve user interaction, significant security challenges, and mixed user perceptions that balance enthusiasm for new capabilities with privacy concerns. This study contributes to understanding LLM integration in mobile devices and its implications for users, manufacturers, and the broader technological landscape.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
pages = {208–217},
numpages = {10},
location = {Sacramento, CA, USA},
series = {ASEW '24}
}

@inproceedings{10.1145/3627508.3638344,
author = {Wang, Ben and Liu, Jiqun and Karimnazarov, Jamshed and Thompson, Nicolas},
title = {Task Supportive and Personalized Human-Large Language Model Interaction: A User Study},
year = {2024},
isbn = {9798400704345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627508.3638344},
doi = {10.1145/3627508.3638344},
abstract = {Large language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offers insights into evaluating human-LLM interactions and emphasizes potential challenges for under served users.},
booktitle = {Proceedings of the 2024 Conference on Human Information Interaction and Retrieval},
pages = {370–375},
numpages = {6},
keywords = {ChatGPT, Human-LLM Interaction, Information Seeking, Proactive System, Prompt Engineering},
location = {Sheffield, United Kingdom},
series = {CHIIR '24}
}

@article{10.1145/3708519,
author = {Lyu, Michael R. and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon},
title = {Automatic Programming: Large Language Models and Beyond},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708519},
doi = {10.1145/3708519},
abstract = {Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security, and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs can help produce higher assurance code from LLMs, along with evidence of assurance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {140},
numpages = {33},
keywords = {AI-based coding, Automated Program Repair, Trustworthy Software}
}

@inproceedings{10.5555/3716662.3716705,
author = {Grabb, Declan and Lamparth, Max and Vasan, Nina},
title = {Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation (Extended Abstract)},
year = {2025},
publisher = {AAAI Press},
abstract = {In the United States and other countries exists a "national mental health crisis": Rates of suicide, depression, anxiety, substance use, and more continue to increase - exacerbated by isolation, the COVID pandemic, and, most importantly, lack of access to mental healthcare. Therefore, many are looking to AI-enabled digital mental health tools, which have the potential to reach many patients who would otherwise remain on wait lists or without care. The main drive behind these new tools is the focus on large language models that could enable real-time, personalized support and advice for patients. With a trend towards language models entering the mental healthcare delivery apparatus, questions arise about how a robust, high-level framework to guide ethical implementations would look like and whether existing language models are ready for this high-stakes application where individual failures can lead to dire consequences.This paper addresses the ethical and practical challenges custom to mental health applications and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s) with defined rubrics and criteria for each question that would define "safe," "unsafe," and "borderline" (between safe and unsafe) for reproducibility.We find that all tested language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models based on system prompt engineering and model-generated self-critiques.Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.Our code and the redacted data set are available on Github (github.com/maxlampe/taimh_eval, MIT License). The full, unredacted data set is available upon request due to the harmful content contained.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {519},
numpages = {1},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3632621.3671429,
author = {Potriasaeva, Anna and Dzialets, Katsiaryna and Golubev, Yaroslav and Birillo, Anastasiia},
title = {Using a Low-Code Environment to Teach Programming in the Era of LLMs},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671429},
doi = {10.1145/3632621.3671429},
abstract = {LLMs change the landscape of software engineering, and the question arises: “How can we combine LLMs with traditional teaching approaches in computer science?”. In this work, we propose to teach students in a low-code environment of code generation, developing not only their coding but also decomposition and prompting skills.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {542–543},
numpages = {2},
keywords = {Generative AI, LLMs, MOOC, Programming Education},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3700058.3700107,
author = {Fan, Minghong},
title = {LLMs in Banking: Applications, Challenges, and Approaches},
year = {2024},
isbn = {9798400710261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700058.3700107},
doi = {10.1145/3700058.3700107},
abstract = {This study systematically explores the applications, challenges, and strategic approaches of Large Language Models (LLMs) in the banking industry. The paper first constructs an analytical framework that integrates the business value chain with technical dimensions, identifying and evaluating extensive application scenarios of LLMs in banking. The paper then delves into key challenges, such as data privacy and security, model interpretability, algorithmic bias, and system integration, and proposes corresponding strategies. Additionally, it provides strategic recommendations for banks implementing LLMs, emphasizing system planning, data ecosystem construction, and compliance management. This paper aims to offer both theoretical insights and practical guidance for banks seeking to leverage LLMs for technological transformation and competitive advantage.},
booktitle = {Proceedings of the International Conference on Digital Economy, Blockchain and Artificial Intelligence},
pages = {314–321},
numpages = {8},
keywords = {Algorithmic Bias, Banking Applications, Data Privacy, Fintech, Large Language Models, Model Interpretability},
location = {
},
series = {DEBAI '24}
}

@inproceedings{10.1145/3744464.3744471,
author = {Jin, Chuan and Zheng, Xiaoyun and Liu, Dongmei and Li, Da and Zan, Xin and Jin, Peng},
title = {A Hybrid Deep Neural Network Approach for Enhanced Network Intrusion Detection},
year = {2025},
isbn = {9798400713866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744464.3744471},
doi = {10.1145/3744464.3744471},
abstract = {With the rapid development of Internet technology, network security issues have become increasingly severe, making intrusion detection systems more important. Traditional intrusion detection methods face limitations when dealing with complex attack patterns and large-scale data. This study proposes a network intrusion detection system based on a hybrid deep neural network, integrating the strengths of Convolutional Neural Networks, Long Short-Term Memory networks, and Transformer models to enhance intrusion detection performance. Experimental results demonstrate that the proposed method outperforms traditional algorithms, effectively identifying various types of network attacks with strong scalability and robustness, especially when handling large-scale data.},
booktitle = {Proceedings of the 2nd International Conference on Machine Intelligence and Digital Applications},
pages = {30–36},
numpages = {7},
keywords = {Convolutional neural network (CNN), Feature extraction, Hybrid deep neural network, Long short-term memory (LSTM), Network intrusion detection, Temporal modeling, Transformer},
location = {
},
series = {MIDA '25}
}

@inproceedings{10.1145/3661167.3661222,
author = {Mbaka, Winnie Bahati},
title = {New experimental design to capture bias using LLM to validate security threats},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661222},
doi = {10.1145/3661167.3661222},
abstract = {The usage of Large Language Models is already well understood in software engineering and security and privacy. Yet, little is known about the effectiveness of LLMs in threat validation or the possibility of biased output when assessing security threats for correctness. To mitigate this research gap, we present a pilot study investigating the effectiveness of chatGPT in the validation of security threats. One main observation made from the results was that chatGPT assessed bogus threats as realistic regardless of the assumptions provided which negated the feasibility of certain threats occurring.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {458–459},
numpages = {2},
keywords = {ChatGPT, Large Language Models, Security Threat Validation},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3702879.3702881,
author = {Zhang, Jinke and Ren, Xiaopeng},
title = {The Application and Comparison of Artificial Intelligence LLMs in Psychological Statistical Analysis},
year = {2024},
isbn = {9798400710148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702879.3702881},
doi = {10.1145/3702879.3702881},
abstract = {With the rapid development of LLMs (LLMs) in the field of natural language processing, there is an increasing interest in their applications within psychological research. This study focuses on three LLMs: OpenAI's GPT-4, Zhipu AI's GLM-4, and Alibaba Cloud's Qwen2.5, to examine their performance and potential value in psychological research. The analysis emphasizes five dimensions of the models' capabilities in handling text data: convenience, lexical statistical abilities, core topic extraction, sentiment analysis, and code generation support. Results indicate that GPT-4 excels in core topic extraction and programming assistance but falls short in convenience for file uploads and sentiment quantification. GLM-4 shows strong performance in sentiment analysis quantification but has limitations in text analysis, code generation, and overall balance. Qwen2.5 demonstrates robust capabilities in text understanding and sentiment classification, highlighting its practicality in processing Chinese text. The findings suggest that LLMs possess significant application potential in psychological research, providing effective support for in-depth cognitive and cultural studies while enhancing the efficiency and accuracy of psychological analysis.},
booktitle = {Proceedings of the 2024 2nd International Conference on Internet of Things and Cloud Computing Technology},
pages = {8–13},
numpages = {6},
keywords = {Large Language Model, Lexical Statistics, Psychology, Sentiment Analysis, Text Analysis},
location = {
},
series = {IoTCCT '24}
}

@inproceedings{10.1145/3576915.3623120,
author = {Li, Zongjie and Wang, Chaozheng and Wang, Shuai and Gao, Cuiyun},
title = {Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623120},
doi = {10.1145/3576915.3623120},
abstract = {The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their "synonyms" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API.We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2336–2350},
numpages = {15},
keywords = {code generation, large language model, watermark},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3734436.3734459,
author = {Nangia, Aditya and Ayachitula, Sriya and Kundu, Chinmay},
title = {In-Context Vulnerability Propagation in LLMs [Work In Progress Paper]},
year = {2025},
isbn = {9798400715037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734436.3734459},
doi = {10.1145/3734436.3734459},
abstract = {The widespread adoption of Large Language Models (LLMs) in software development has accelerated coding workflows, with tools like GitHub Copilot and Google Gemini reducing development time by up to 55.8%. However, these systems suffer from security and trustworthiness challenges. In this paper, we demonstrate a novel attack - ''Vulnerability propagation attack'' in the conext of the code generated by LLMs. We present a formal framework for evaluating vulnerability propagation in LLM-assisted development, focusing on two key metrics: Vulnerability Carry Rate (VCR), which quantifies the intra-session propagation of vulnerabilities, and Context Retention Factor (CRF), which measures cross-session persistence. Using three leading LLMs -- GPT-4, Sonar, and Gemini -- we evaluate 10 critical Common Weakness Enumeration (CWE) categories across three tiers of prompts (direct injection, implicit vulnerabilities, and adversarial few-shot prompting). Our findings reveal that adversarial few-shot prompting exacerbates vulnerability propagation, with VCR values reaching 100% under high-difficulty conditions and CRF values demonstrating significant retention across sessions. These results underscore the need for session-aware security measures and robust vulnerability detection frameworks in AI-assisted development environments.},
booktitle = {Proceedings of the 30th ACM Symposium on Access Control Models and Technologies},
pages = {169–174},
numpages = {6},
keywords = {llm, code generation, vulnerability propagation},
location = {USA},
series = {SACMAT '25}
}

@article{10.1145/3733612.3733614,
author = {Li, Yibai and Jin, Zhiye and Li, Xiaobing (Emily) and Joshi, K. D. and Deng, Xuefei (Nancy)},
title = {Counterweights and Complementarities: The Convergence of AI and Blockchain Powering a Decentralized Future},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/3733612.3733614},
doi = {10.1145/3733612.3733614},
abstract = {This editorial addresses the critical intersection of artificial intelligence (AI) and blockchain technologies, highlighting their contrasting tendencies toward centralization and decentralization, respectively. While AI, particularly with the rise of large language models (LLMs), exhibits a strong centralizing force due to data and resource monopolization by large corporations, blockchain offers a counterbalancing mechanism through its inherent decentralization, transparency, and security. The editorial argues that these technologies are not mutually exclusive but possess complementary strengths. Blockchain can mitigate AI's centralizing risks by enabling decentralized data management, computation, and governance, promoting greater inclusivity, transparency, and user privacy. Conversely, AI can enhance blockchain's efficiency and security through automated smart contract management, content curation, and threat detection. The core argument calls for the development of ''decentralized intelligence'' (DI)—an interdisciplinary research area focused on creating intelligent systems that function without centralized control.},
journal = {SIGMIS Database},
month = apr,
pages = {6–12},
numpages = {7},
keywords = {ai, blockchain, decentralized intelligence, fintech, large language model, transparency.}
}

@inproceedings{10.1145/3672539.3686341,
author = {Shi, Billy and Kristensson, Per Ola},
title = {Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development},
year = {2024},
isbn = {9798400707186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672539.3686341},
doi = {10.1145/3672539.3686341},
abstract = {Implementing automation testing is difficult and as a consequence there is a growing desire for semi-automated software testing systems with humans in the loop. Leveraging the growth of LLMs, recent research has demonstrated LLMs’ potential to improve performance on test generation, reporting, and bug triaging. However, relatively little work has explored the interactivity issues that emerge in semi-automated LLM-assisted software test case development. To fill this gap, we present two user studies (N1 = 16, N2 = 24) that investigate productivity, creativity, and user attention in three semi-automated LLM-assisted interaction strategies: (1) pre-emptive prompting; (2) buffered response; and (3) guided input. We find that pre-emptively prompting the user significantly enhances branch coverage and task creativity by more than 30% while reducing user’s off-task idle time by up to 48.7%. We conclude by suggesting concrete research directions applying mixed-initiative principles for LLM-based interactive systems for semi-automated software testing.},
booktitle = {Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {83},
numpages = {3},
location = {Pittsburgh, PA, USA},
series = {UIST Adjunct '24}
}

@inproceedings{10.1145/3696630.3728713,
author = {Wei, Wenying and Zhao, Kaifa and Zhou, Hao},
title = {Exposing the Achilles' Heel: Black-box Reverse Engineering of Commercial LLM Plugins' Hidden Prompts},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728713},
doi = {10.1145/3696630.3728713},
abstract = {Large language models (LLMs) demonstrate powerful natural language semantics understanding capabilities and are widely integrated into applications. OpenAI provides a platform for developers to construct custom applications, extending ChatGPT's functions and integrating external tools. Since its release in November 2023, over 3 million custom applications have been created. However, such a vast ecosystem also conceals security and privacy threats. For developers, instruction leaking attacks threaten the intellectual property of instructions in LLM applications through carefully crafted adversarial prompts. To systematically evaluate the scope of threats in real-world LLM applications, we develop an inception prompt hijacking attack, namely IPH, target LLM applications. Our experiments on 5,000 real-world LLM applications reveal that over 95.1% of applications are vulnerable to instruction-leaking attacks via one or more adversarial prompts. Our findings raise awareness among LLM applications developers about the importance of integrating specific defensive strategies in their instructions.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1615–1618},
numpages = {4},
keywords = {large language model, privacy security},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1109/ASE56229.2023.00217,
author = {Dipongkor, Atish Kumar and Moran, Kevin},
title = {A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00217},
doi = {10.1109/ASE56229.2023.00217},
abstract = {Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood.Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1012–1023},
numpages = {12},
keywords = {bug triaging, transformer, llms, text-embedding, DL4SE},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3713081.3732928,
author = {He, Shuai and Yan, Hao and Li, Wenke and Hong, Sheng and Guo, Xiaowei and Liu, Xiaofan and Fu, Cai},
title = {From Large Language Models to Adversarial Malware: How far are we},
year = {2025},
isbn = {9798400714740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713081.3732928},
doi = {10.1145/3713081.3732928},
abstract = {Large Language Models (LLMs) have achieved notable progress in fields including natural language processing, cyber threat detection, and automated penetration testing, increasingly being applied in practical settings. However, the rapid advancement of these models has also led to their potential misuse, posing new challenges to cyberspace security. Security incidents have already been reported in areas such as phishing attacks and disinformation campaigns. Nevertheless, the progress and potential impact of LLMs in generating adversarial malware remain underexplored. This study systematically investigates the evasion capability of adversarial malware generated by LLMs. By integrating chain of thought into a Markov process and designing prompt based state transition functions and reward mechanisms, this research evaluates the effectiveness and efficiency against mainstream static detection methods on a dataset comprising over 2,000 real-world malware samples. Experimental results demonstrate an average evasion rate of 89.92% across 12 commercial antivirus engines on VirusTotal. The findings reveal that individuals with minimal technical expertise and basic natural language skills can generate malware that evades static detection, which underscores potential vulnerabilities in current cyberspace defense and detection systems regarding adversarial malware countermeasures.},
booktitle = {Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {178–182},
numpages = {5},
keywords = {large language models, adversarial malware, static detection},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {ISSTA Companion '25}
}

@inproceedings{10.1145/3701716.3717528,
author = {Yang, Yuli and Huang, Zhenhua and Jia, Zhaohong and Fang, Junfeng},
title = {A Comprehensive Security Evaluation Framework for Chinese Large Language Models},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717528},
doi = {10.1145/3701716.3717528},
abstract = {This paper presents a pioneering security evaluation framework for Chinese generative large language models (LLMs), addressing a significant gap in the field. The framework is supported by a comprehensive dataset of over 12,000 samples, systematically organized across four key dimensions: value evaluation, robustness, training data security, and prompt injection attacks. Each dimension is further subdivided into detailed subcategories.Our two-phase progressive attack strategy first assesses the models' baseline defenses using basic prompts. In the second phase, the attack complexity is escalated through semantic perturbations, formatting disturbances, contextual interference, and scenario design. This approach methodically uncovers initial vulnerabilities and effectively exposes deeper security flaws, demonstrating superior efficacy compared to existing methods in challenging both basic and advanced model defenses.This work underscores the critical need for such a framework, enhancing the security and reliability of AI systems. In the domain of ''Personal Intelligence,'' where accurate user modeling and preference alignment are essential, it contributes to the development of safer, more reliable AI applications for real-world deployment.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2426–2433},
numpages = {8},
keywords = {chinese language processing, injection attack, jailbreak attack, large language models, security evaluation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3743095.3743106,
author = {Baruwal Chhetri, Mohan and Liu, Xiao and Hoang, Thuong and Renaud, Karen and Arora, Chetan and Tian, Yuan and Huang, Yue},
title = {Report on the 5th Workshop on Human Centric Software Engineering &amp; Cyber Security (HCSE&amp;CS 2024)},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3743095.3743106},
doi = {10.1145/3743095.3743106},
abstract = {Humans play multifaceted roles in the lifecycle of software systems, from creation and design to coding, testing, and usage. Traditionally, software engineering and cyber security research have prioritized technical aspects such as functions, data, and processes, while neglecting crucial human factors. Human-centric software engineering and cyber security prioritizes the human element, ensuring usability, accessibility, and trust are central to design and implementation. The InternationalWorkshop on Human Centric Software Engineering &amp; Cyber Security (HCSE&amp;CS) aims to create a forum to discuss enhanced theories, models, tools, and practices that support next-generation human-centric approaches in software engineering and cyber security.The fifth edition of the HCSE&amp;CS Workshop was held on 28 October 2024, alongside the 39th IEEE/ACM International Conference on Automated Software Engineering (ASE 2024) in Sacramento, California, United States. It brought together experts to discuss not only traditional human-centric software engineering and cybersecurity challenges but also the evolving impact of large language models (LLMs) on software development and security. This report outlines the workshop's motivation and objectives and summarizes the presentations and discussions that took place during this event.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {59–61},
numpages = {3}
}

@inproceedings{10.1145/3630106.3659042,
author = {Lamparth, Max and Reuel, Anka},
title = {Analyzing And Editing Inner Mechanisms of Backdoored Language Models},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659042},
doi = {10.1145/3630106.3659042},
abstract = {Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets. Trigger warning: Offensive language.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2362–2373},
numpages = {12},
keywords = {Backdoor Attacks, Backdoor Defenses, Interpretability, Natural Language Processing, Safety},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3578527.3578530,
author = {Jain, Ridhi and Gervasoni, Nicole and Ndhlovu, Mthandazo and Rawat, Sanjay},
title = {A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3578530},
doi = {10.1145/3578527.3578530},
abstract = {Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {6},
numpages = {10},
keywords = {software vulnerability, software metrics, program graphs, datasets},
location = {Allahabad, India},
series = {ISEC '23}
}

@inproceedings{10.1145/3722565.3727196,
author = {Ahmed, Chuadhry Mujeeb},
title = {AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System},
year = {2025},
isbn = {9798400716089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722565.3727196},
doi = {10.1145/3722565.3727196},
abstract = {Malicious examples are crucial for evaluating the robustness of machine learning algorithms under attack, particularly in Industrial Control Systems (ICS). However, collecting normal and attack data in ICS environments is challenging due to the scarcity of testbeds and the high cost of human expertise. Existing datasets are often limited by the domain expertise of practitioners, making the process costly and inefficient. The lack of comprehensive attack pattern data poses a significant problem for developing robust anomaly detection methods. In this paper, we propose a novel approach that combines data-centric and design-centric methodologies to generate attack patterns using large language models (LLMs). Our results demonstrate that the attack patterns generated by LLMs not only surpass the quality and quantity of those created by human experts but also offer a scalable solution that does not rely on expensive testbeds or pre-existing attack examples. This multiagent based approach presents a promising avenue for enhancing the security and resilience of ICS environments.},
booktitle = {Proceedings of the 2nd International Workshop on Foundation Models for Cyber-Physical Systems &amp; Internet of Things},
pages = {31–36},
numpages = {6},
keywords = {AI for Security, Attack Dataset, CPS Security and Privacy, ICS, LLMs},
location = {Irvine, CA, USA},
series = {FMSys}
}

@inproceedings{10.1145/3658644.3691416,
author = {Bui, Huan and Lienerth, Harper and Fu, Chenglong and Sridhar, Meera},
title = {Poster: TAPChecker: Model Checking in Trigger-Action Rules Generation Using Large Language Models},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3691416},
doi = {10.1145/3658644.3691416},
abstract = {The integration of large language models (LLMs) in smart home systems holds significant promise for automating the generation of Trigger-Action Programming (TAP) rules, potentially streamlining smart home user experiences and enhancing convenience. However, LLMs lack of holistic view of smart home IoT deployments and may introduce TAP rules that result in hazards. This paper explores the application of LLM for generating TAP rules and applying formal verification to validate and ensure the safety of TAP rules generated by LLMs. By systematically analyzing and verifying these rules, we aim to identify and mitigate potential security vulnerabilities. Furthermore, we propose a feedback mechanism to refine the LLM's output, enhancing its reliability and safety in generating automation rules. Through this approach, we seek to bridge the gap between the efficiency of LLMs and the stringent security requirements of smart IoT systems, fostering a safer automation environment.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {4994–4996},
numpages = {3},
keywords = {large language model, model checking, trigger-action programming},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3551349.3560438,
author = {Al Madi, Naser},
title = {How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560438},
doi = {10.1145/3551349.3560438},
abstract = {Background: Recent advancements in large language models have motivated the practical use of such models in code generation and program synthesis. However, little is known about the effects of such tools on code readability and visual attention in practice. Objective: In this paper, we focus on GitHub Copilot to address the issues of readability and visual inspection of model generated code. Readability and low complexity are vital aspects of good source code, and visual inspection of generated code is important in light of automation bias. Method: Through a human experiment (n=21) we compare model generated code to code written completely by human programmers. We use a combination of static code analysis and human annotators to assess code readability, and we use eye tracking to assess the visual inspection of code. Results: Our results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers. At the same time, eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generated code. Conclusion: Our findings highlight that reading code is more important than ever, and programmers should beware of complacency and automation bias with model generated code.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {205},
numpages = {5},
keywords = {Copilot, Empirical Study, Eye Tracking, GitHub, Readability},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1109/SC41406.2024.00098,
author = {Jin, Hongwei and Papadimitriou, George and Raghavan, Krishnan and Zuk, Pawel and Balaprakash, Prasanna and Wang, Cong and Mandal, Anirban and Deelman, Ewa},
title = {Large Language Models for Anomaly Detection in Computational Workflows: From Supervised Fine-Tuning to In-Context Learning},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00098},
doi = {10.1109/SC41406.2024.00098},
abstract = {Anomaly detection in computational workflows is critical for ensuring system reliability and security. However, traditional rule-based methods struggle to detect novel anomalies. This paper leverages large language models (LLMs) for workflow anomaly detection by exploiting their ability to learn complex data patterns. Two approaches are investigated: (1) supervised fine-tuning (SFT), where pretrained LLMs are fine-tuned on labeled data for sentence classification to identify anomalies, and (2) in-context learning (ICL), where prompts containing task descriptions and examples guide LLMs in few-shot anomaly detection without fine-tuning. The paper evaluates the performance, efficiency, and generalization of SFT models and explores zero-shot and few-shot ICL prompts and interpretability enhancement via chain-of-thought prompting. Experiments across multiple workflow datasets demonstrate the promising potential of LLMs for effective anomaly detection in complex executions.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {92},
numpages = {17},
keywords = {anomaly detection, computational workflows, in-context learning, large language models, supervised fine-tuning},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@article{10.1145/3734524,
author = {Collini, Luca and Garg, Siddharth and Karri, Ramesh},
title = {C2HLSC: Leveraging Large Language Models to Bridge the Software-to-Hardware Design Gap},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3734524},
doi = {10.1145/3734524},
abstract = {High-Level Synthesis (HLS) tools offer rapid hardware design from C code, but their compatibility is limited by code constructs. This paper investigates Large Language Models (LLMs) for automatically refactoring C code into HLS-compatible formats. We present a case study using an LLM to rewrite C code for NIST 800-22 randomness tests, a QuickSort algorithm, and AES-128 into HLS-synthesizable C. The LLM iteratively transforms the C code guided by the system prompt and tool’s feedback, implementing functions like streaming data and hardware-specific signals. With the hindsight obtained from the case study, we implement a fully automated framework to refactor C code into HLS-compatible formats using LLMs. To tackle complex designs, we implement a preprocessing step that breaks down the hierarchy in order to approach the problem in a divide-and-conquer bottom-up way. We validated our framework on three ciphers, one hash function, five NIST 800-22 randomness tests, and a QuickSort algorithm. Our results show a high success rate on benchmarks that are orders of magnitude more complex than what has been achieved generating Verilog with LLMs.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {High-Level Synthesis, Large Language Models, Automatic Code Repair}
}

@inproceedings{10.1145/3696630.3728699,
author = {Chom\k{a}tek, \L{}ukasz and Papuga, Jakub and Nowak, Przemyslaw and Poniszewska-Mara\'{n}da, Aneta},
title = {Decoding CI/CD Practices in Open-Source Projects with LLM Insights},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728699},
doi = {10.1145/3696630.3728699},
abstract = {The analysis of continuous integration and continuous deployment (CI/CD) pipeline configurations offers critical insights into modern software development practices. This paper leverages advanced large language models (LLMs) — DeepSeek-V2 and GPT-4o — to examine pipeline definition files from 28,770 popular GitHub repositories, uncovering adoption trends across CI/CD practices. Our LLM-driven approach reveals near-universal implementation of build (80.0%) and test (80.2%) stages, contrasted by lower adoption of specialized practices like static application security testing (SAST) (12.1%), containerization (17.9%), and cloud deployment (4.2%). System-specific patterns emerge, with Travis CI leading in build automation (99.2%) and GitHub Actions excelling in linting (44.9%) and SAST (16.5%), reflecting distinct system strengths. By harnessing LLMs to parse and interpret complex pipeline files at scale, this study not only maps the CI/CD landscape but also demonstrates the transformative potential of artificial intelligence in software engineering research. We propose future work to refine LLM-based tools for pipeline analysis and address adoption gaps, aiming to enhance CI/CD accessibility and effectiveness across diverse projects.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1638–1644},
numpages = {7},
keywords = {experience, continuous integration, continuous deployment, large language models, code repositories},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inbook{10.5555/3716662.3716689,
author = {Cornacchia, Giandomenico and Zizzo, Giulio and Fraser, Kieran and Hameed, Muhammad Zaid and Rawat, Ambrish and Purcell, Mark},
title = {MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks},
year = {2025},
publisher = {AAAI Press},
abstract = {The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {304–315},
numpages = {12}
}

@inproceedings{10.1145/3605770.3625214,
author = {Singla, Tanmay and Anandayuvaraj, Dharun and Kalu, Kelechi G. and Schorlemmer, Taylor R. and Davis, James C.},
title = {An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625214},
doi = {10.1145/3605770.3625214},
abstract = {As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing past failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5's categorizations had an average accuracy of 68% and Bard's had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {5–15},
numpages = {11},
keywords = {cybersecurity, empirical software engineering, failure analysis, large language models, software security, software supply chain},
location = {Copenhagen, Denmark},
series = {SCORED '23}
}

@article{10.1145/3728875,
author = {Xia, Yifan and Xie, Zichen and Liu, Peiyu and Lu, Kangjie and Liu, Yan and Wang, Wenhai and Ji, Shouling},
title = {Beyond Static Pattern Matching? Rethinking Automatic Cryptographic API Misuse Detection in the Era of LLMs},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728875},
doi = {10.1145/3728875},
abstract = {While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns. Large Language Models (LLMs) offer a promising context-aware understanding to address this shortcoming, yet the stochastic nature and the hallucination issue pose challenges to their applications in precise security analysis. This paper presents the first systematic study to explore LLMs’ application in cryptographic API misuse detection. Our findings are noteworthy: The instability of directly applying LLMs results in over half of the initial reports being false positives. Despite this, the reliability of LLM-based detection could be significantly enhanced by aligning detection scopes with realistic scenarios and employing a novel code &amp; analysis validation technique, achieving a nearly 90% detection recall. This improvement substantially surpasses traditional methods and leads to the discovery of previously unknown vulnerabilities in established benchmarks. Nevertheless, we identify recurring failure patterns that illustrate current LLMs’ blind spots, including cryptographic knowledge deficiencies and code semantics misinterpretations. Leveraging these findings, we deploy an LLM-based detection system and uncover 63 new vulnerabilities (47 confirmed, 7 already fixed) in open-source Java and Python repositories, including prominent projects like Apache.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA006},
numpages = {24},
keywords = {API Misuse Detection, Large Language Models for Software Security}
}

@inproceedings{10.1145/3641555.3705044,
author = {Wiktor, Nicole},
title = {Optimizing Prompt Engineering for Automated Text Summarization of Student Reflections: A Comparative Study Using GPT-4 LLM},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705044},
doi = {10.1145/3641555.3705044},
abstract = {In the educational domain, extracting insights from student-written text has shown to be valuable for instructors. Efficiently summarizing students' reflections in a course offers instructors valuable insights to enhance students' learning experience. Therefore, quickly understanding students' impressions about the course could be very helpful to instructors for in-time and/or personalized one-on-one discussions. Achieving this often involves using natural language processing (NLP) techniques Understanding capabilities of LLMs through a series of comparative experiments involving prompt engineering is the goal of this work. We compare the summarization outputs of GPT-4 with an experimentally optimized temperature of 0.75 through a variety of experiments that include different levels of prompts, starting with base level and proceeding to increase context in the prompt. We evaluate and compare the outputs of these summaries based on a rubric from literature, evaluated by human annotators. Our findings suggest that providing more detailed context prompts help LLMs uncover less frequent and obvious student challenges and provide more detailed explanations. One notable finding showed how sensitive the LLM approach is to the distribution of the challenge types in students' reflections. In other words, all prompts regardless of their contextual details faced issues due to this misrepresenting of student challenges distributions, sometimes overstating their occurrence frequency. Therefore, further study is required to refine the data distribution impact. Despite this, the approach shows much potential to extract useful knowledge quickly. It offers valuable insights to instructors and could help in supporting students more effectively.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1764},
numpages = {1},
keywords = {GPT-4, large language models, natural language processing, prompt engineering, text summarization},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3748239.3748242,
author = {Shu, Dong and Zhang, Chong and Jin, Mingyu and Zhou, Zihao and Li, Lingyao},
title = {AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3748239.3748242},
doi = {10.1145/3748239.3748242},
abstract = {Jailbreak attacks represent one of the most sophisticated threats to the security of large language models (LLMs). To deal with such risks, we introduce an innovative framework that can help evaluate the effectiveness of jailbreak attacks on LLMs. Unlike traditional binary evaluations focusing solely on the robustness of LLMs, our method assesses the attacking prompts' effectiveness. We present two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework uses a scoring range from 0 to 1, offering unique perspectives and allowing for the assessment of attack effectiveness in different scenarios. Additionally, we develop a comprehensive ground truth dataset specifically tailored for jailbreak prompts. This dataset is a crucial benchmark for our current study and provides a foundational resource for future research. By comparing with traditional evaluation methods, our study shows that the current results align with baseline metrics while offering a more nuanced and fine-grained assessment. It also helps identify potentially harmful attack prompts that might appear harmless in traditional evaluations. Overall, our work establishes a solid foundation for assessing a broader range of attack prompts in prompt injection.},
journal = {SIGKDD Explor. Newsl.},
month = jul,
pages = {10–19},
numpages = {10}
}

@inproceedings{10.5555/3639940.3639991,
author = {Loaiza, Marco and Savaglio, Claudio and Gravina, Raffaele and Chatzopoulos, Dimitris and Lalis, Spyros},
title = {Agents in the Computing Continuum: the MLSysOps Perspective},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Multi-agent systems (MASs) have gained considerable attention in the field of distributed computing due to their ability to provide technical interoperability, resource sharing and flexible coordination. Consequently, MAS are well-suited to address the challenges posed by the distributed and heterogeneous nodes within the device-edge-cloud continuum, including orchestration and standardization, optimal resource allocation,micro service placement policies, security and privacy.The objective of this study is to introduce the MLSysOps project, which aims at the autonomous management of the entire continuum tackling some of the challenges mentioned before. MLSysOps utilizes a hierarchical agent-based AI architecture to interface with the underlying resource management and application deployment/orchestration mechanisms. A comparative analysis is conducted between the existing related work and the proposed framework, highlighting the peculiarities and advantages of our approach.},
booktitle = {Proceedings of the 2023 International Conference on Embedded Wireless Systems and Networks},
pages = {321–326},
numpages = {6},
keywords = {MLSysOps computing continuum, computing continuum, multi agent systems, system management, device edge cloud computing},
location = {Rende, Italy},
series = {EWSN '23}
}

@inproceedings{10.1145/3704137.3704180,
author = {Rzepka, Rafal and Obayashi, Akihiko},
title = {Effectiveness of Security Export Control Ontology for Predicting Answer Type and Regulation Categories},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704180},
doi = {10.1145/3704137.3704180},
abstract = {In this paper we present results of our experiments investigating if an expert knowledge graph can improve Large Language Models accuracy in predicting correct answer labels and regulations related to the topic of security export control. As the lack of related data prevents machine-learning or fine-tuning approaches, we implement prompt expansion by searching most relevant nodes of the graph and adding the expanded context to the prompt. Results of our experiments show that the addition improved answer type selection but clearly hamper the capability of finding a correct regulation category.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {156–161},
numpages = {6},
keywords = {Expert Systems, GraphRAG, Large Language Models, Knowledge Graph, Security Export Control},
location = {
},
series = {ICAAI '24}
}

@article{10.1145/3689728,
author = {Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus},
title = {Statically Contextualizing Large Language Models with Typed Holes},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689728},
doi = {10.1145/3689728},
abstract = {Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {288},
numpages = {31},
keywords = {Large Language Models, Program Repair, Program Synthesis}
}

@inproceedings{10.1145/3687997.3695650,
author = {Pontes Miranda, James William and Bruneliere, Hugo and Tisi, Massimo and Suny\'{e}, Gerson},
title = {Towards an In-Context LLM-Based Approach for Automating the Definition of Model Views},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695650},
doi = {10.1145/3687997.3695650},
abstract = {In the Model-Driven Engineering (MDE) of complex systems, multiple models represent various systems' aspects. In practice, these models are often unconnected and specified using different modeling languages. Model view solutions can be employed to automatically combine such models. However, writing model view definitions is not trivial. When modeling languages are semantically distant and/or have a large number of concepts, it can quickly become difficult to manually identify the language elements to be selected, associated, or queried to build a model view. As a solution, this paper proposes an in-context Large Language Model (LLM)-based approach to assist engineers in writing model-view definitions. Notably, we rely on LLMs and Prompt Engineering techniques to automatically generate drafts of model-view definitions by providing as input only minimal information on the modeling languages to be combined. We implemented our approach by integrating the EMF Views solution for model views with the LangChain framework for LLM-based applications. To this end, we tailored LangChain to handle EMF metamodels. We validated our approach and implementation on a set of model views originally specified either in VPDL, the ViewPoint Definition Language of EMF Views, or as ATL model-to-model transformations. We compared these original model view definitions with the ones we automatically generated. The obtained results show the feasibility and applicability of our approach.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {Large language models, Model views, Model-driven engineering, Modeling languages, Prompt engineering},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@article{10.1145/3708522,
author = {Zhou, Xin and Cao, Sicong and Sun, Xiaobing and Lo, David},
title = {Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708522},
doi = {10.1145/3708522},
abstract = {The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {145},
numpages = {31},
keywords = {Literature review, vulnerability detection, vulnerability repair, large language models}
}

@inproceedings{10.1145/3696410.3714756,
author = {Zhang, Baolei and Xin, Haoran and Fang, Minghong and Liu, Zhuqing and Yi, Biao and Li, Tong and Liu, Zheli},
title = {Traceback of Poisoning Attacks to Retrieval-Augmented Generation},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714756},
doi = {10.1145/3696410.3714756},
abstract = {Large language models (LLMs) integrated with retrieval-augmented generation (RAG) systems improve accuracy by leveraging external knowledge sources. However, recent research has revealed RAG's susceptibility to poisoning attacks, where the attacker injects poisoned texts into the knowledge database, leading to attacker-desired responses. Existing defenses, which predominantly focus on inference-time mitigation, have proven insufficient against sophisticated attacks. In this paper, we introduce RAGForensics, the first traceback system for RAG, designed to identify poisoned texts within the knowledge database that are responsible for the attacks. RAGForensics operates iteratively, first retrieving a subset of texts from the database and then utilizing a specially crafted prompt to guide an LLM in detecting potential poisoning texts. Empirical evaluations across multiple datasets demonstrate the effectiveness of RAGForensics against state-of-the-art poisoning attacks. This work pioneers the traceback of poisoned texts in RAG systems, providing a practical and promising defense mechanism to enhance their security.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {2085–2097},
numpages = {13},
keywords = {poisoning attacks, retrieval-augmented generation, traceback},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3641822.3641882,
author = {Mendes, Wendy and Souza, Samara and De Souza, Cleidson},
title = {"You're on a bicycle with a little motor": Benefits and Challenges of Using AI Code Assistants},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641882},
doi = {10.1145/3641822.3641882},
abstract = {AI code assistants, such as Tabnine, GitHub CoPilot, and ChatGPT, employ Large Language Models (LLMs) trained on extensive source code and other documents. They receive prompts and generate code suggestions aimed to facilitate programming tasks. Previous research in this field has explored the correctness, complexity, quality, and security of the code suggestions. Software developers' experiences have been studied in the context of controlled experiments. Based on 14 interviews with software developers, this paper describes the developers' daily and continuous experiences with AI code assistants, presenting benefits and challenges grounded in actual development work, along with strategies to address these challenges.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {144–152},
numpages = {9},
keywords = {AI code assistants, developer experiences, code generation},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1109/EMIP.2019.00013,
author = {Ikutani, Yoshiharu and Koganti, Nishanth and Hata, Hideaki and Kubo, Takatomi and Matsumoto, Kenichi},
title = {Toward imitating visual attention of experts in software development tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/EMIP.2019.00013},
doi = {10.1109/EMIP.2019.00013},
abstract = {Expert programmers' eye-movements during source code reading are valuable sources that are considered to be associated with their domain expertise. We advocate a vision of new intelligent systems incorporating expertise of experts for software development tasks, such as issue localization, comment generation, and code generation. We present a conceptual framework of neural autonomous agents based on imitation learning (IL), which enables agents to mimic the visual attention of an expert via his/her eye movement. In this framework, an autonomous agent is constructed as a context-based attention model that consists of encoder/decoder network and trained with state-action sequences generated by an experts' demonstration. Challenges to implement an IL-based autonomous agent specialized for software development task are discussed in this paper.},
booktitle = {Proceedings of the 6th International Workshop on Eye Movements in Programming},
pages = {33–36},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {EMIP '19}
}

@inproceedings{10.1145/3664647.3681445,
author = {Sun, Zhihao and Fang, Haipeng and Cao, Juan and Zhao, Xinying and Wang, Danding},
title = {Rethinking Image Editing Detection in the Era of Generative AI Revolution},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681445},
doi = {10.1145/3664647.3681445},
abstract = {Considering that image editing and manipulation technologies pose significant threats to the authenticity and security of image content, research on image regional manipulation detection has always been a critical issue. The accelerated advancement of generative AI significantly enhances the viability and effectiveness of generative regional editing methods and has led to their gradual replacement of traditional image editing tools or algorithms. However, current research primarily focuses on traditional image tampering, and there remains a lack of a comprehensive dataset containing images edited with abundant generative regional editing methods. We endeavor to fill this vacancy by constructing the GRE dataset, a large-scale generative regional editing detection dataset with the following advantages: 1) Integration of a logical and simulated editing pipeline, leveraging multiple large models in various modalities. 2) Inclusion of various editing approaches with distinct characteristics. 3) Provision of comprehensive benchmark and evaluation of SOTA methods across related domains. 4) Analysis of the GRE dataset from multiple dimensions including necessity, rationality, and diversity. Extensive experiments and in-depth analysis demonstrate that this larger and more comprehensive dataset will significantly enhance the development of detection methods for generative editing. The corresponding repository is at https://github.com/ICTMCG/GRE.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3538–3547},
numpages = {10},
keywords = {dataset and benchmark, generative regional editing detection, image editing detection},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3639477.3639720,
author = {Gallagher, Shannon K. and Ratchford, Jasmine and Brooks, Tyler and Brown, Bryan P. and Heim, Eric and Nichols, William R. and Mcmillan, Scott and Rallapalli, Swati and Smith, Carol J. and Vanhoudnos, Nathan and Winski, Nick and Mellinger, Andrew O.},
title = {Assessing LLMs for High Stakes Applications},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639720},
doi = {10.1145/3639477.3639720},
abstract = {Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in reproducibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {103–105},
numpages = {3},
keywords = {large language models, TEVV, metrics, scaling, HCI, trust},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3657054.3657086,
author = {Ganapati, Sukumar and Desouza, Kevin},
title = {Public Value Principles for Secure and Trusted AI},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657086},
doi = {10.1145/3657054.3657086},
abstract = {The objective of this paper is to establish the fundamental public value principles that should govern safe and trusted artificial intelligence (AI). Public value is a dynamic concept that encompasses several dimensions. AI itself has evolved quite rapidly in the last few years, especially with the swift escalation of Generative AI. Governments around the world are grappling with how to govern AI, just as technologists ring alarm bells about the future consequences of AI. Our paper extends the debate on AI governance that is focused on ethical values of beneficence to that of economic values of public good. Viewed as a public good, AI use is beyond the control of the creators. Towards this end, the paper examined AI policies in the United States and Europe. We postulate three principles from a public values perspective: (i) ensuring security and privacy of each individual (or entity); (ii) ensuring trust in AI systems is verifiable; and (iii) ensuring fair and balanced AI protocols, wherein the underlying components of data and algorithms are contestable and open to public debate.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {251–257},
numpages = {7},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@inproceedings{10.1145/3658644.3691377,
author = {Li, Wanpeng and Guo, Yuejun},
title = {Poster: Automated Dependency Mapping for Web API Security Testing Using Large Language Models},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3691377},
doi = {10.1145/3658644.3691377},
abstract = {Dependency extraction is crucial in web API security testing, as it helps identify the required API sequences to exploit a vulnerability. Traditional methods are generally rule-based and require extensive manual analysis of API specification documents by domain experts to formulate appropriate rules. This manual process is not only time-consuming and labor-intensive but also prone to missing dependencies and inaccuracies, which can compromise the effectiveness of security testing. In this paper, we explore the potential of large language models (LLMs) to automate dependency mapping in web APIs. By leveraging the capabilities of advanced LLMs such as GPT-3.5, Mistral-7B-Instruct, and Llama-3-8B-Instruct, which include understanding and generating natural language, we aim to streamline the dependency mapping process, reducing the need for manual analysis and enhancing accuracy. Our preliminary experiments demonstrate that this approach can effectively build dependency mappings, offering a a promising alternative to traditional rule-based approaches.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {5024–5026},
numpages = {3},
keywords = {api security testing, dependency mapping, large language model},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3672608.3707898,
author = {Simoni, Marco and Saracino, Andrea and P, Vinod and Conti, Mauro},
title = {MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707898},
doi = {10.1145/3672608.3707898},
abstract = {In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity. MoRSE aims to provide comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts. MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures. Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries. Subsequently, MoRSE uses this information to generate accurate answers. In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining. We have evaluated the effectiveness of MoRSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions. The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10% compared to known solutions such as GPT-4 and Mixtral 7x8.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1213–1222},
numpages = {10},
keywords = {retrieval augmented generation, large language model, cybersecurity, cyber threat intelligence},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3603287.3651194,
author = {Jamdade, Mahesh and Liu, Yi},
title = {A Pilot Study on Secure Code Generation with ChatGPT for Web Applications},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603287.3651194},
doi = {10.1145/3603287.3651194},
abstract = {Conversational Large Language Models (LLMs), such as ChatGPT, have demonstrated their potent capabilities in natural language processing tasks. This paper presents a pilot study that uses ChatGPT for generating web application code with a specific emphasis on mitigating four prevalent web application vulnerability types: SQL Injection, Cross Site Scripting, Carriage Return Line Feed Injection, and Exposure of Sensitive Information. The paper uses a case study to illustrate how the vulnerabilities in the code are mitigated with the prompts and the subsequent refinements. The study's findings summarize the security concerns in the code generated by ChatGPT, and the paper proposes a prompt pattern designed to help mitigating the potential vulnerabilities.},
booktitle = {Proceedings of the 2024 ACM Southeast Conference},
pages = {229–234},
numpages = {6},
keywords = {ChatGPT, Secure coding, generative AI, web application vulnerabilities},
location = {Marietta, GA, USA},
series = {ACMSE '24}
}

@inproceedings{10.1145/3540250.3558959,
author = {Zlotchevski, Andrei and Drain, Dawn and Svyatkovskiy, Alexey and Clement, Colin B. and Sundaresan, Neel and Tufano, Michele},
title = {Exploring and evaluating personalized models for code generation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558959},
doi = {10.1145/3540250.3558959},
abstract = {Large Transformer models achieved the state-of-the-art status for Natural Language Understanding tasks and are increasingly becoming the baseline model architecture for modeling source code. Transformers are usually pre-trained on large unsupervised corpora, learning token representations and transformations relevant to modeling generally available text, and are then fine-tuned on a particular downstream task of interest. While fine-tuning is a tried-and-true method for adapting a model to a new domain -- for example, question-answering on a given topic -- generalization remains an on-going challenge. In this paper, we explore and evaluate transformer model fine-tuning for personalization. In the context of generating unit tests for Java methods, we evaluate learning to personalize to a specific software project using several personalization techniques. We consider three key approaches: (i) custom fine-tuning, which allows all the model parameters to be tuned; (ii) lightweight fine-tuning, which freezes most of the model's parameters, allowing tuning of the token embeddings and softmax layer only or the final layer alone; (iii) prefix tuning, which keeps model parameters frozen, but optimizes a small project-specific prefix vector. Each of these techniques offers a trade-off in total compute cost and predictive performance, which we evaluate by code and task-specific metrics, training time, and total computational operations. We compare these fine-tuning strategies for code generation and discuss the potential generalization and cost benefits of each in various deployment scenarios.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1500–1508},
numpages = {9},
keywords = {Personalized Models, Code Generation},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3749370,
author = {Tang, Xunzhu and Kim, Kisub and Ezzini, Saad and Song, Yewei and Tian, Haoye and Klein, Jacques and Bissyande, Tegawende},
title = {Just-in-Time Detection of Silent Security Patches},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3749370},
doi = {10.1145/3749370},
abstract = {Open-source code is pervasive. In this setting, embedded vulnerabilities are spreading to downstream software at an alarming rate. Although such vulnerabilities are generally identified and addressed rapidly, inconsistent maintenance policies can cause security patches to go unnoticed. Indeed, security patches can be silent, i.e., they do not always come with comprehensive advisories such as CVEs. This lack of transparency leaves users oblivious to available security updates, providing ample opportunity for attackers to exploit unpatched vulnerabilities. Consequently, identifying silent security patches just in time when they are released is essential for preventing n-day attacks and for ensuring robust and secure maintenance practices. With llmda we propose to (1) leverage large language models (LLMs) to augment patch information with generated code change explanations, (2) design a representation learning approach that explores code-text alignment methodologies for feature combination, (3) implement a label-wise training with labeled instructions for guiding the embedding based on security relevance, and (4) rely on a probabilistic batch contrastive learning mechanism for building a high-precision identifier of security patches. We evaluate llmda on the PatchDB and SPI-DB literature datasets and show that our approach substantially improves over the state-of-the-art, notably GraphSPD by 20% in terms of F-Measure on the SPI-DB benchmark.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
keywords = {security patch detection, in-context learning, self-instruct}
}

@inproceedings{10.1145/3658644.3670299,
author = {Wang, Shu and Sun, Kun and Zhai, Yan},
title = {Dye4AI: Assuring Data Boundary on Generative AI Services},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670299},
doi = {10.1145/3658644.3670299},
abstract = {Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2281–2295},
numpages = {15},
keywords = {data boundary assurance, dye testing, generative artificial intelligence, large language models},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3670474.3685967,
author = {Ayalasomayajula, Avinash and Guo, Rui and Zhou, Jingbo and Saha, Sujan Kumar and Farahmandi, Farimah},
title = {LASP: LLM Assisted Security Property Generation for SoC Verification},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685967},
doi = {10.1145/3670474.3685967},
abstract = {As the complexity of System-on-Chips (SoCs) increases, ensuring their security presents escalating challenges. Formal property verification is one of the most robust methods to model and check security behaviors using model checkers. However, the generation of these security properties is a labor-intensive endeavor. Large language models (LLMs) have been applied in multiple fields due to their excellent ability to understand natural language. Hence, this paper presents a novel framework that utilizes LLMs to automate the generation of security properties directly from Register Transfer Level (RTL) designs. By extracting critical features and security assets from both the design specifications and RTL, the framework systematically produces tailored security properties for specific hardware designs. These properties are systematically cataloged in a security property database, providing an essential resource for ongoing and future hardware verification efforts. The effectiveness of this innovative framework is validated through its application to various open-source hardware designs, confirming its ability to significantly improve SoC security verification by efficiently generating robust security properties.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {29},
numpages = {7},
keywords = {Formal Property Verification, Large Language Models, Security Property Generation, System-on-Chip Security},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@inbook{10.1145/3677389.3702555,
author = {Wang, Yuzhang and Yu, Peizhou and He, Guoxiu},
title = {Silent LLMs: Using LoRA to Enable LLMs to Identify Hate Speech},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702555},
abstract = {The detection of hate speech on social networks presents significant challenges due to its increasingly subtle nature. The advent of large language models (LLMs) has revolutionized text understanding and generation, presenting novel avenues for hate speech detection. This study evaluates the performance of the ChatGLM model in detecting hate speech through a small sample prompt method. Our analysis uncovers three key limitations when employing the LLMs to directly generate answers: inconsistency in output format, the illusion of comprehensiveness inherent in LLMs, and the inability to respond due to security concerns. To mitigate these limitations, we investigate four LLMs: LLaMA, Llama-2, Llama-3, and ChatGLM-3. These models are equipped with Multi-Layer Perception (MLP) and Low-Rank Adaptation (LoRA), which specifically tailored for hate speech detection. Extensive comparisons with baseline models are conducted across three hate speech datasets with six classification tasks. Our findings demonstrate that our improved LLMs can surpass traditional methods in detecting hate speech, highlighting their potential for further improvement and refinement in addressing this critical societal issue.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {53},
numpages = {5}
}

@inproceedings{10.1145/3650400.3650606,
author = {Xing, Kongduo},
title = {Design and implementation of digital training evaluation management system based on AI's generative AI technology},
year = {2024},
isbn = {9798400708305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650400.3650606},
doi = {10.1145/3650400.3650606},
abstract = {With the advancement of information technology, traditional offline training is gradually giving way to online digital training. The conventional online digital training evaluation management system entails manual data collection followed by the utilization of these gathered data for training effectiveness assessment. This entire process is intricate and can introduce potential biases. To address these challenges, we have developed a digital training management system rooted in AI's generative AI technology. This innovative system employs both the fuzzy comprehensive evaluation method and the artificial intelligence evaluation method to assess the quality and effectiveness indexes of digital training. Furthermore, it is designed on the foundation of deep learning algorithms, creating an AI-driven digital training evaluation management system. To ensure the security and privacy of the system, extensive simulation experiments have been conducted. These experiments help control the deviation values of evaluation results, ultimately guaranteeing the fairness of outcomes generated by the evaluation system.},
booktitle = {Proceedings of the 2023 7th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1222–1226},
numpages = {5},
location = {Xiamen, China},
series = {EITCE '23}
}

@inproceedings{10.1145/3690407.3690468,
author = {Zhang, Hongtu and Li, Wenhao and Hu, Jiahao and Zuo, Fang},
title = {DeviceLLM: Synergistic Recognition of Device Attributes in Cyberspace Based on Large Language Models},
year = {2024},
isbn = {9798400710247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690407.3690468},
doi = {10.1145/3690407.3690468},
abstract = {In recent years, large language models (LLMs) have undergone rapid development, demonstrating significant potential across various domains. As Internet technology advances, Internet-connected devices find increasingly meaningful applications, including device type and owner identification, necessitating improved performance. This paper aims to explore the utilization of LLMs in recognizing cyberspace device attributes through fine-tuning with instruction templates. These templates enable learning from device attribute data and uncovering potential associations, facilitating efficient and accurate recognition of cyberspace device attributes. Experimentation confirms the method's effectiveness and performance, achieving high-precision recognition of device attributes in real network environments. Results show a notable 75.77% accuracy rate in device type identification and an impressive 96.90% accuracy rate in device owner identification. This research introduces novel ideas and methods for utilizing LLMs in network security, holding both theoretical and practical significance.},
booktitle = {Proceedings of the 2024 4th International Conference on Artificial Intelligence, Big Data and Algorithms},
pages = {355–360},
numpages = {6},
keywords = {Large Language Models, Network Security, Owner Identification, Type Identification},
location = {
},
series = {CAIBDA '24}
}

@proceedings{10.1145/3689944,
title = {SCORED '24: Proceedings of the 2024 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2024},
isbn = {9798400712401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '24, the third edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Salt Lake City, Utah, United States with extensive support for in-person and virtual attendance. This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3653644.3653662,
author = {Dai, Ziqing},
title = {Applications and Challenges of Large Language Models in Smart Government -From technological Advances to Regulated Applications},
year = {2024},
isbn = {9798400709777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653644.3653662},
doi = {10.1145/3653644.3653662},
abstract = {This paper explores the applications and challenges of large language models (LLMs) in the context of smart government. It delves into how LLMs can enhance government decision-making, policy interpretation, and public service delivery through intelligent analysis and predictions. It also discusses the role of LLMs in processing vast amounts of government information and in analyzing public opinion. Concurrently, the paper acknowledges the challenges posed by LLMs, including data costs, security and privacy concerns, model robustness, regulatory hurdles, and technical and talent bottlenecks. It proposes recommendations for the regulated application of LLMs, such as developing robust data protection policies, standardizing model research and evaluation, fostering interdisciplinary research, and promoting integrated development across key sectors. The paper concludes with an outlook on the future of LLMs in smart government, emphasizing the need for cautious optimism and responsible innovation.},
booktitle = {Proceedings of the 2024 3rd International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {275–280},
numpages = {6},
keywords = {Large Language Models, Regulated Applications, Smart Government},
location = {Yichang, China},
series = {FAIML '24}
}

@inproceedings{10.1145/3664647.3680786,
author = {Li, Qinfeng and Shen, Zhiqiang and Qin, Zhenghan and Xie, Yangfan and Zhang, Xuhong and Du, Tianyu and Cheng, Sheng and Wang, Xun and Yin, Jianwei},
title = {TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680786},
doi = {10.1145/3664647.3680786},
abstract = {Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3479–3488},
numpages = {10},
keywords = {authorization, edge-deployed transformer model, intellectual property protection, trusted execution environment},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3744756,
author = {Biringa, Chidera and Kul, Gokhan},
title = {Detecting Hard-Coded Credentials in Software Repositories via LLMs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744756},
doi = {10.1145/3744756},
abstract = {Software developers frequently hard-code credentials such as passwords, generic secrets, private keys, and generic tokens in software repositories, even though it is strictly advised against due to the severe threat to the security of the software. These credentials create attack surfaces exploitable by a potential adversary to conduct malicious exploits such as backdoor attacks. Recent detection efforts utilize embedding models to vectorize textual credentials before passing them to classifiers for predictions. However, these models struggle to discriminate between credentials with contextual and complex sequences resulting in high false positive predictions. Context-dependent Pre-trained Language Models (PLMs) or Large Language Models (LLMs) such as Generative Pre-trained Transformers (GPT) tackled this drawback by leveraging the transformer neural architecture capacity for self-attention to capture contextual dependencies between words in input sequences. As a result, GPT has achieved wide success in several natural language understanding endeavors. Hence, we assess LLMs to represent these observations and feed extracted embedding vectors to a deep learning classifier to detect hard-coded credentials. Our model outperforms the current state-of-the-art by 13%  (in)  F1 measure on the benchmark dataset. We have made all source code and data publicly available&nbsp;1 to facilitate the reproduction of all results presented in this paper.},
note = {Just Accepted},
journal = {Digital Threats},
month = jul,
keywords = {software, secret detection, feature engineering}
}

@inproceedings{10.1145/3656156.3665432,
author = {Valentim, Matheus and Falk, Jeanette and Inie, Nanna},
title = {Hacc-Man: An Arcade Game for Jailbreaking LLMs},
year = {2024},
isbn = {9798400706325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656156.3665432},
doi = {10.1145/3656156.3665432},
abstract = {The recent leaps in complexity and fluency of Large Language Models (LLMs) mean that, for the first time in human history, people can interact with computers using natural language alone. This creates monumental possibilities of automation and accessibility of computing, but also raises severe security and safety threats: When everyone can interact with LLMs, everyone can potentially break into the systems running LLMs. All it takes is creative use of language. This paper presents Hacc-Man, a game which challenges its players to “jailbreak” an LLM: subvert the LLM to output something that it is not intended to. Jailbreaking is at the intersection between creative problem solving and LLM security. The purpose of the game is threefold: 1. To heighten awareness of the risks of deploying fragile LLMs in everyday systems, 2. To heighten people’s self-efficacy in interacting with LLMs, and 3. To discover the creative problem solving strategies, people deploy in this novel context.},
booktitle = {Companion Publication of the 2024 ACM Designing Interactive Systems Conference},
pages = {338–341},
numpages = {4},
keywords = {LLM security, arcade games., creative problem solving, creativity, hacking, jailbreaking, red teaming},
location = {IT University of Copenhagen, Denmark},
series = {DIS '24 Companion}
}

@inproceedings{10.1145/3639631.3639663,
author = {Vijayan, Aishwarya},
title = {A Prompt Engineering Approach for Structured Data Extraction from Unstructured Text Using Conversational LLMs},
year = {2024},
isbn = {9798400709203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639631.3639663},
doi = {10.1145/3639631.3639663},
abstract = {This paper aims to extract structured information from unstructured text written in natural language. This extracted information can then be stored in a database and queried using database access languages such as SQL to derive meaningful answers to questions that might arise around a specific activity related to that unstructured text. For example, an email chain discussing a planned trip could be used to extract a record with fields such as who is taking the trip, where they are departing from, when they are departing, where they are going, and when they are arriving. This information could then be used to create a relation of trips that could be queried by SQL. This paper uses a prompt engineering approach using conversational LLMs for extracting the relevant information related to travel and store the information into relational databases which can then be queried using SQL or any other query language. This initiative holds the promise to revolutionize the storage and retrieval of information with minimal effort. Currently, unstructured text is difficult to query and analyze. By extracting structured information from this text, it becomes much easier to work with.},
booktitle = {Proceedings of the 2023 6th International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {183–189},
numpages = {7},
keywords = {Information Extraction, Large Language models, Named Entity Recognition, Natural Language Processing, Prompt Engineering, Querying, Spatio-temporal information, Structured data, Unstructured text},
location = {Sanya, China},
series = {ACAI '23}
}

@inproceedings{10.1145/3727582.3728688,
author = {Sikder, Fadul and Lei, Yu and Ji, Yuede},
title = {Efficient Adaptation of Large Language Models for Smart Contract Vulnerability Detection},
year = {2025},
isbn = {9798400715945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727582.3728688},
doi = {10.1145/3727582.3728688},
abstract = {Smart contracts underpin decentralized applications but face significant security risks from vulnerabilities, while traditional analysis methods have limitations. Large Language Models (LLMs) offer promise for vulnerability detection, yet adapting these powerful models efficiently, particularly generative ones, remains challenging. This paper investigates two key strategies for the efficient adaptation of LLMs for Solidity smart contract vulnerability detection: (1) replacing token-level generation with a dedicated classification head during fine-tuning, and (2) selectively freezing lower transformer layers using Low-Rank Adaptation (LoRA). Our empirical evaluation demonstrates that the classification head approach enables models like Llama 3.2 3B to achieve high accuracy (77.5%), rivaling the performance of significantly larger models such as the fine-tuned GPT-3.5. Furthermore, we show that selectively freezing bottom layers reduces training time and memory usage by approximately 10-20% with minimal impact on accuracy. Notably, larger models (3B vs. 1B parameters) exhibit greater resilience to layer freezing, maintaining high accuracy even with a large proportion of layers frozen, suggesting a localization of general code understanding in lower layers versus task-specific vulnerability patterns in upper layers. These findings present practical insights for developing and deploying performant LLM-based vulnerability detection systems efficiently, particularly in resource-constrained settings.},
booktitle = {Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {65–74},
numpages = {10},
keywords = {Fine-tuning, Large Language Models, Layer Freezing, Llama 3.2, Smart Contracts, Solidity, StarCoder, Transfer Learning, Vulnerability Detection},
location = {Trondheim, Norway},
series = {PROMISE '25}
}

@inproceedings{10.1145/3615366.3622791,
author = {Paucar, Herminio and Estrella, J\'{u}lio},
title = {Context-aware monitoring for IoT: an approach based on Agents, and Federated Learning},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3622791},
doi = {10.1145/3615366.3622791},
abstract = {This paper proposes a software architecture model applied for context-aware monitoring of Smart Buildings(SB), and it works in two layers: Fog and Edge computing and uses the Multi-Agent System(MAS) and Federated Learning(FL). The objective of this architecture will allow context-aware monitoring and the capture of anomalies and offer specific functionalities that depend on the user roles. As a result, we modeled the complete software architecture considering Reactive, edge-fog computing, and MAS components, following the best practices from the literature and the 2413-IEEE standard.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {164–165},
numpages = {2},
keywords = {software architecture, smart building, multi-agent system, federated learning, context-aware},
location = {La Paz, Bolivia},
series = {LADC '23}
}

@inbook{10.1145/3672608.3707883,
author = {Lu, Yang and Karanjai, Rabimba and Alsagheer, Dana and Kasichainula, Keshav and Xu, Lei and Shi, Weidong and Huang, Shou-Hsuan Stephen},
title = {LogBabylon: A Unified Framework for Cross-Log File Integration and Analysis},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707883},
abstract = {Logs are critical resources that record events, activities, or messages produced by software applications, operating systems, servers, and network devices. However, consolidating the heterogeneous logs and cross-referencing them is challenging and complicated. Manually analyzing the log data is time-consuming and prone to errors. LogBabylon is a centralized log data consolidating solution that leverages Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the log data in a human-readable way and adds insight analysis of the system performance and anomaly alerts. It provides a paramount view of the system landscape, enabling proactive management and rapid incident response. LogBabylon consolidates diverse log sources and enhances the extracted information's accuracy and relevancy. This facilitates a deeper understanding of log data, supporting more effective decision-making and operational efficiency. Furthermore, LogBabylon streamlines the log analysis process, significantly reducing the time and effort required to interpret complex datasets. Its capabilities extend to generating context-aware insights, offering an invaluable tool for continuous monitoring, performance optimization, and security assurance in dynamic computing environments.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1953–1960},
numpages = {8}
}

@inproceedings{10.1145/3696410.3714790,
author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos},
title = {SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714790},
doi = {10.1145/3696410.3714790},
abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1567–1576},
numpages = {10},
keywords = {auditing tools, large language models, move, prompt engineering, smart contract, sui, transparency, web applications, web3},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3106426.3106519,
author = {Fan, Tuan-Fang and Liau, Churn-Jung},
title = {A logic for reasoning about evidence and belief},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106519},
doi = {10.1145/3106426.3106519},
abstract = {In agent-based systems, an agent generally forms her belief based on evidence from multiple sources, such as messages from other agents or perception of the external environment. In this paper, we present a logic for reasoning about evidence and belief. Our framework not only takes advantage of the source-tracking capability of justification logic, but also allows the distinction between the actual observation and simply potential admissibility of evidence. We present the axiomatization for the basic logic and its dynamic extension, investigate its properties, and use a running example to show its applicability to information fusion for autonomous agents.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {509–516},
numpages = {8},
keywords = {second-order propositional modal logic, reasoning about belief, logical omniscience, justification logic, evidential reasoning, dynamic epistemic logic},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3709026.3709115,
author = {Tang, Xuehai and Xiao, Wenjie and Yao, Zhongjiang and Han, Jizhong},
title = {SwordEcho: A LLM Jailbreaking Optimization Strategy Driven by Reinforcement Learning},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709115},
doi = {10.1145/3709026.3709115},
abstract = {Warning: This paper may contain potentially offensive model inputs or outputs.With the rapid development of Large Language Models (LLMs), inherent security risks have become increasingly evident, particularly their capability to generate harmful content. As an effective tool for revealing potential vulnerabilities in LLMs within the context of red teaming, jailbreak attacks are widely used to assess the content safety defense levels of LLMs. However, existing open-source jailbreak prompts often lack sufficient readability and effectiveness, failing to comprehensively evaluate the security flaws of models and inadequately exposing the potential risks of LLMs. To address this issue, this paper introduces an enhanced jailbreak prompt generation method called "SwordEcho," which improves the readability and aggressiveness of jailbreak prompt words to more comprehensively evaluate model vulnerabilities. SwordEcho employs a multi-faceted reinforcement learning reward mechanism to finely tune the attack model, thereby effectively generating high-quality jailbreak prompts. To verify the transferability and generalization capabilities of SwordEcho, this study conducted jailbreak tests involving eleven different risk scenarios across six open-source LLMs and three commercial model APIs. The results indicate that, compared with existing mainstream jailbreak attack strategies, SwordEcho demonstrates higher Attack Success Rates (ASRs) across different target models.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {183–190},
numpages = {8},
keywords = {Large Language Models, Jailbreak attacks, Red teaming, Reinforcement learning},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3669940.3707224,
author = {Tan, Yifan and Tan, Cheng and Mi, Zeyu and Chen, Haibo},
title = {PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669940.3707224},
doi = {10.1145/3669940.3707224},
abstract = {Confidential computing on GPUs, like NVIDIA H100, mitigates the security risks of outsourced Large Language Models (LLMs) by implementing strong isolation and data encryption. Nonetheless, this encryption incurs a significant performance overhead, reaching up to 52.8% and 88.2% throughput drop when serving OPT-30B and OPT-66B, respectively. To address this challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM removes the overhead by overlapping the encryption and GPU computation through pipelining-an idea inspired by the CPU instruction pipelining-thereby effectively concealing the latency increase caused by encryption. The primary technical challenge is that, unlike CPUs, the encryption module lacks prior knowledge of the specific data needing encryption until it is requested by the GPUs. To this end, we propose speculative pipelined encryption to predict the data requiring encryption by analyzing the serving patterns of LLMs. Further, we have developed an efficient, low-cost pipeline relinquishing approach for instances of incorrect predictions. Our experiments show that compared with vanilla systems without confidential computing (e.g., vLLM, PEFT, and FlexGen), PipeLLM incurs modest overhead ( &lt; 19.6% in throughput) across various LLM sizes, from 13B to 175B. PipeLLM's source code is available at https://github.com/SJTU-IPADS/PipeLLM.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {843–857},
numpages = {15},
keywords = {confidential virtual machine, large language model, nvidia confidential computing},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3726302.3730090,
author = {Pradeep, Ronak and Thakur, Nandan and Upadhyay, Shivani and Campos, Daniel and Craswell, Nick and Soboroff, Ian and Dang, Hoa Trang and Lin, Jimmy},
title = {The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730090},
doi = {10.1145/3726302.3730090},
abstract = {Large Language Models (LLMs) have significantly enhanced the capabilities of information access systems, especially with retrieval-augmented generation (RAG). Nevertheless, the evaluation of RAG systems remains a barrier to continued progress, a challenge we tackle in this work by proposing an automatic evaluation framework that is validated against human annotations. We believe that the nugget evaluation methodology provides a solid foundation for evaluating RAG systems. This approach, originally developed for the TREC Question Answering (QA) Track in 2003, evaluates systems based on atomic facts that should be present in good answers. Our efforts focus on ''refactoring'' this methodology, where we describe the AutoNuggetizer framework that specifically applies LLMs to both automatically create nuggets and automatically assign nuggets to system answers. In the context of the TREC 2024 RAG Track, we calibrate a fully automatic approach against strategies where nuggets are created manually or semi-manually by human assessors and then assigned manually to system answers. Based on results from a community-wide evaluation, we observe strong agreement at the run level between scores derived from fully automatic nugget evaluation and human-based variants. The agreement is stronger when individual framework components such as nugget assignment are automated independently. This suggests that our evaluation framework provides tradeoffs between effort and quality that can be used to guide the development of future RAG systems. However, further research is necessary to refine our approach, particularly in establishing robust per-topic agreement to diagnose system failures effectively.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {180–190},
numpages = {11},
keywords = {atomic facts, automatic evaluation, nugget evaluation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3664647.3681345,
author = {Zhang, Qishan and Wen, Shuangbing and Hu, Tao},
title = {Audio Deepfake Detection with Self-Supervised XLS-R and SLS Classifier},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681345},
doi = {10.1145/3664647.3681345},
abstract = {Generative AI technologies, including text-to-speech (TTS) and voice conversion (VC), frequently become indistinguishable from genuine samples, posing challenges for individuals in discerning between real and synthetic content. This indistinguishability undermines trust in media, and the arbitrary cloning of personal voice signals presents significant challenges to privacy and security. In the field of deepfake audio detection, the majority of models achieving higher detection accuracy currently employ self-supervised pre-trained models. However, with the ongoing development of deepfake audio generation algorithms, maintaining high discrimination accuracy against new algorithms grows more challenging. To enhance the sensitivity of deepfake audio features, we propose a deepfake audio detection model that incorporates an SLS (Sensitive Layer Selection) module. Specifically, utilizing the pre-trained XLS-R enables our model to extract diverse audio features from its various layers, each providing distinct discriminative information. Utilizing the SLS classifier, our model captures sensitive contextual information across different layer levels of audio features, effectively employing this information for fake audio detection. Experimental results show that our method achieves state-of-the-art (SOTA) performance on both the ASVspoof 2021 DF and In-the-Wild datasets, with a specific Equal Error Rate (EER) of 1.92% on the ASVspoof 2021 DF dataset and 7.46% on the In-the-Wild dataset. Codes and data can be found at https://github.com/QiShanZhang/SLSforADD.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6765–6773},
numpages = {9},
keywords = {aigc, anti spoofing, audio deepfake detection, countermeasures, text to speech, voice conversion},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3591300,
author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
title = {Prompting Is Programming: A Query Language for Large Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591300},
doi = {10.1145/3591300},
abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation.   On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.    Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.    To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.    We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {186},
numpages = {24},
keywords = {prompt programming, language model programming}
}

@inproceedings{10.1145/3652620.3687804,
author = {Birchler De Allende, Alan and Sultan, Bastien and Apvrille, Ludovic},
title = {From Attack Trees to Attack-Defense Trees with Generative AI &amp; Natural Language Processing},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687804},
doi = {10.1145/3652620.3687804},
abstract = {Attack-defense trees, an extension of attack trees, are extensively used by security engineers to document potential countermeasures for security threats present in a system's design. These trees help integrate initial system models with countermeasures, allowing for early testing of their efficiency and impact in the design cycle. Despite advancements in automating attack tree construction, selecting the initial set of countermeasures for conversion into an attack-defense tree remains largely manual. This paper proposes an approach and a tool that extends the TTool-AI attack tree generation feature by leveraging large language models and natural language processing to create a set of countermeasures and generate attack-defense trees based on an input attack tree. To evaluate our contribution, our approach is tested using attack-defense trees generated from attack trees, each representing possible threats to an associated system specification. In addition, we introduce metrics to assess the semantic correctness and completeness of the generated attack-defense trees. We compared, using our metrics, the attack-defense trees created from our methodology to those created by an engineer and found that attack-defense trees created using AI and secondary mitigation data provided better trees than solely using AI. We also discovered that this approach generated trees that were comparable to the quality of attack-defense trees generated from a security engineer at the associate level. From these results, we believe that our contribution could aid engineers in identifying not only appropriate countermeasures for attack trees but also the optimal number of countermeasures, avoiding the complexity of redundant mitigations. Furthermore, our approach complements standard modeling practices, particularly during the initial design phase, reducing the need for time-consuming re-engineering throughout the system's lifecycle.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {561–569},
numpages = {9},
keywords = {artificial intelligence, large-language models, attack-defense trees, model-driven engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3717067,
author = {Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
title = {Deceiving LLM through Compositional Instruction with Hidden Attacks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3717067},
doi = {10.1145/3717067},
abstract = {Recently, large language models (LLMs) have demonstrated promising applications in the autonomous driving (AD) domain, including language-based interactions and decision-making. Ensuring they safely handle harmful inputs is crucial before formal deployment. However, research reveals emerging hand-crafted jailbreak attacks, which pack harmful prompts into harmless instructions, can bypass LLMs’ security mechanisms and elicit harmful responses. To deeply understand such jailbreaks, this paper introduces a Compositional Instruction Attack (CIA) framework to generalize them, and develop two CIA jailbreaking methods to automatically generate tailored jailbreak prompts for each harmful prompt. Then, this paper builds the first CIA question-answering (CIAQA) dataset with 2.7K multiple-choice questions of 900 successful jailbreaks, for assessing LLMs’ ability to identify underlying harmful intents, harmfulness, and task priority in CIA jailbreaks. Combined with experimental analysis on CIAQA and other datasets, this paper concludes three possible reasons for the failure of LLM defenses against CIAs. Finally, we propose an intent-based defense paradigm (IBD), enabling LLMs to defend against CIA by leveraging its capability to identify intents. Experimental results show CIA can achieve attack success rates (ASR) of 95%+ and 85%+ in AD and common harmful scenarios for three well-known LLMs (GPT-4, GPT-3.5, and Llama2-70b-chat), and IBD reduces ASR by 74%+.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = feb,
keywords = {Large language model, autonomous driving, adversarial attack, harmful prompt}
}

@inproceedings{10.1145/3649476.3660390,
author = {Akyash, Mohammad and M Kamali, Hadi},
title = {Evolutionary Large Language Models for Hardware Security: A Comparative Survey},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649476.3660390},
doi = {10.1145/3649476.3660390},
abstract = {Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024},
pages = {496–501},
numpages = {6},
keywords = {Hardware Security, Large Language Models, RTL Debugging},
location = {Clearwater, FL, USA},
series = {GLSVLSI '24}
}

@article{10.1145/3687251.3687253,
author = {Boi, Biagio and Esposito, Christian and Lee, Sokjoon},
title = {Smart Contract Vulnerability Detection: The Role of Large Language Model (LLM)},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3687251.3687253},
doi = {10.1145/3687251.3687253},
abstract = {Smart contracts are susceptible to various vulnerabilities that can lead to significant financial losses. The usage of tools for vulnerabilities is reducing the threats but presents some limitations related to the approach used by the tool itself. This paper presents a novel approach to smart contract vulnerability detection utilizing Large Language Models (LLMs), as a tool to detect all the vulnerabilities at once. Our proposed tool leverages the advanced natural language processing capabilities of LLMs to analyze smart contract code and identify potential security flaws. By training the LLM on a diverse dataset of known smart contract vulnerabilities and secure coding practices, we enhance its ability to recognize subtle and complex vulnerabilities that traditional static analysis tools might miss. The evaluation of our tool demonstrates its effectiveness in detecting a wide range of vulnerabilities with satisfaction and accuracy, providing developers with a robust mechanism to improve the security of their smart contracts before deployment. This approach signifies a significant advancement in the application of artificial intelligence for blockchain security, highlighting the potential of LLMs to enhance the reliability and safety of decentralized applications.},
journal = {SIGAPP Appl. Comput. Rev.},
month = aug,
pages = {19–29},
numpages = {11},
keywords = {LLama, large language models, smart contract, vulnerabilities detection}
}

@inproceedings{10.1145/3691620.3695001,
author = {Zhang, Quan and Zhou, Chijin and Go, Gwihwan and Zeng, Binqi and Shi, Heyuan and Xu, Zichen and Jiang, Yu},
title = {Imperceptible Content Poisoning in LLM-Powered Applications},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695001},
doi = {10.1145/3691620.3695001},
abstract = {Large Language Models (LLMs) have shown their superior capability in natural language processing, promoting extensive LLM-powered applications to be the new portals for people to access various content on the Internet. However, LLM-powered applications do not have sufficient security considerations on untrusted content, leading to potential threats. In this paper, we reveal content poisoning, where attackers can tailor attack content that appears benign to humans but causes LLM-powered applications to generate malicious responses. To highlight the impact of content poisoning and inspire the development of effective defenses, we systematically analyze the attack, focusing on the attack modes in various content, exploitable design features of LLM application frameworks, and the generation of attack content. We carry out a comprehensive evaluation on five LLMs, where content poisoning achieves an average attack success rate of 89.60%. Additionally, we assess content poisoning on four popular LLM-powered applications, achieving the attack on 72.00% of the content. Our experimental results also show that existing defenses are ineffective against content poisoning. Finally, we discuss potential mitigations for LLM application frameworks to counter content poisoning.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {242–254},
numpages = {13},
keywords = {LLM applications, content poisoning},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3716368.3735281,
author = {Saber Latibari, Banafsheh and Nazari, Najmeh and Sasan, Avesta and Homayoun, Houman and Satam, Pratik and Salehi, Soheil and Sayadi, Hossein},
title = {Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735281},
doi = {10.1145/3716368.3735281},
abstract = {The rise of hardware-level security threats, such as side-channel attacks, hardware Trojans, and firmware vulnerabilities, demands advanced detection mechanisms that are more intelligent and adaptive. Traditional methods often fall short in addressing the complexity and evasiveness of modern attacks, driving increased interest in machine learning-based solutions. Among these, Transformer models, widely recognized for their success in natural language processing and computer vision, have gained traction in the security domain due to their ability to model complex dependencies, offering enhanced capabilities in identifying vulnerabilities, detecting anomalies, and reinforcing system integrity. This survey provides a comprehensive review of recent advancements on the use of Transformers in hardware security, examining their application across key areas such as side-channel analysis, hardware Trojan detection, vulnerability classification, device fingerprinting, and firmware security. Furthermore, we discuss the practical challenges of applying Transformers to secure hardware systems, and highlight opportunities and future research directions that position them as a foundation for next-generation hardware-assisted security. These insights pave the way for deeper integration of AI-driven techniques into hardware security frameworks, enabling more resilient and intelligent defenses.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {841–848},
numpages = {8},
keywords = {Hardware Systems, Transformer, Security, Threat Detection.},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3638530.3664121,
author = {Guo, Ping and Liu, Fei and Lin, Xi and Zhao, Qingchuan and Zhang, Qingfu},
title = {L-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664121},
doi = {10.1145/3638530.3664121},
abstract = {In the rapidly evolving field of machine learning, adversarial attacks pose a significant threat to the robustness and security of models. Amongst these, decision-based attacks are particularly insidious due to their nature of requiring only the model's decision output, which makes them notably challenging to counteract. This paper presents L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), an innovative methodology that harnesses the generative capabilities of large language models (LLMs) to streamline the creation of such attacks. L-AutoDA employs an evolutionary strategy, where iterative interactions with LLMs lead to the autonomous generation of potent attack algorithms, thereby reducing human intervention. The performance of L-AutoDA was evaluated on the CIFAR-10 dataset, where it demonstrated substantial superiority over existing baseline methods in terms of success rate and computational efficiency. Ultimately, our results highlight the formidable utility of language models in crafting adversarial attacks and reveal promising directions for constructing more resilient AI systems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1846–1854},
numpages = {9},
keywords = {large language models, adversarial attacks, automated algorithm design, evolutionary algorithms},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3680256.3721324,
author = {Shawon, Ashadullah and Liscano, Ramiro and Azim, Akramul and Sundaresan, Vijay and Chang, Yee-Kang},
title = {Retrieval Augmented Generation Fine-Tuned LLM Model for Code Recommendations to Mitigate Lock Contention},
year = {2025},
isbn = {9798400711305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680256.3721324},
doi = {10.1145/3680256.3721324},
abstract = {Lock contention performance faults can lead to degradation in the performance of software applications. Unlike software bugs, per- formance faults do not lead to failures and application crashes but surface as a degradation in the response and execution of an ap- plication and can surface fairly late in the deployment life of an application. Tools exist for the identification and detection of lock performance faults but there is a lack of effective code refactor- ing recommendations for a developer to mitigate the performance degradation caused by lock-contention. Recent advances in Large Language Models (LLMs) have demonstrated positive results in code refactoring for fixing software bugs and mitigating run time faults. However, traditional LLM-based approaches often suffer from hal- lucination errors, where the generated code may not accurately reflect the context of the project or existing codebase. This thesis presents a novel approach that combines Retrieval Augmented Gen- eration (RAG) with a fine-tuned LLM model for refactored code recommendation aimed at reducing lock-contention performance faults in Java applications. The RAG fine-tuned model combines the strengths of contextual understanding from LLMs with the preci- sion of retrieval-based systems, thereby ensuring that the generated recommendations are relevant, accurate, and hallucination-free. Se- mantic and syntactic metrics of the recommendations generated by the combined RAG and LLM model show an accuracy of approxi- mately 90% compared to an accuracy of approximately 25% when a baseline LLM model is used.},
booktitle = {Companion of the 16th ACM/SPEC International Conference on Performance Engineering},
pages = {95–102},
numpages = {8},
keywords = {code smell, fine-tuning, large language model, lock contention, rag, refactored code recommendation},
location = {Toronto ON, Canada},
series = {ICPE '25}
}

@inproceedings{10.1145/3672608.3707798,
author = {Ehl, Marco and Ahmadian, Amir Shayan and Gro\ss{}er, Katharina and Elsofi, Duaa Adel Ali and Herrmann, Marc and Specht, Alexander and Schneider, Kurt and J\"{u}rjens, Jan},
title = {Supporting Software Engineers in IT Security and Privacy through Automated Knowledge Discovery},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707798},
doi = {10.1145/3672608.3707798},
abstract = {Security and privacy are increasingly essential concepts in software engineering. New threats and corresponding countermeasures are continuously discovered. Concurrently, projects are becoming more complex and are exposed to a greater number of threats. This presents a significant challenge for software engineers. As a result, security and privacy are often neglected due to a lack of knowledge, limited time, and financial constraints. While systematic literature reviews exist to address the increasing volume of publications, software engineers still require up-to-date knowledge of current threats and measures. This paper presents an automated, time-efficient, and cost-effective method for discovering knowledge from state-of-the-art literature and project artifacts, such as design documents. The presented method utilizes Large Language Models (LLMs) for data extraction and is demonstrated through a prototypical implementation and evaluation. This evaluation involves security and privacy in open-access scientific publications and project documentation from European Union research and development projects. The extracted knowledge is used to populate a quality model that is specifically designed to provide software engineers with information that helps them apply the findings. This quality model offers software engineers valuable, up-to-date insights into security and privacy, bridging the gap between scientific research and practical applications.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1647–1656},
numpages = {10},
keywords = {security, privacy, quality model, knowledge discovery, large language model},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@article{10.1145/3695988,
author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
title = {Large Language Models for Software Engineering: A Systematic Literature Review},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695988},
doi = {10.1145/3695988},
abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {220},
numpages = {79},
keywords = {Software Engineering, Large Language Model, Survey}
}

@article{10.1145/3728944,
author = {Liu, Rongkai and Shi, Heyuan and Liu, Shuning and Hu, Chao and Li, Sisheng and Shen, Yuheng and Wang, Runzhe and Shi, Xiaohai and Jiang, Yu},
title = {PatchScope: LLM-Enhanced Fine-Grained Stable Patch Classification for Linux Kernel},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728944},
doi = {10.1145/3728944},
abstract = {Stable patch classification plays a crucial role in vulnerability management for the Linux kernel, significantly contributing to the stability and security of Long-term support(LTS) versions. Although existing tools have effectively assisted in assessing whether patches should be merged into stable versions, they cannot determine which stable patches should be merged into which LTS versions. This process still requires the maintainers of the distribution community to manually screen based on the requirements of their respective versions.To address this issue, we propose PatchScope, which is designed to predict the specific merge status of patches.Patchscope consists of two components: patch analysis and patch classification.Patch analysis leverages Large Language Models(LLMs) to generate detailed patch descriptions from the commit message and code changes, thereby deepening the model's semantic understanding of patches. Patch classification utilizes a pre-trained language model to extract semantic features of the patches and employs a two-stage classifier to predict the merge status of the patches.The model is optimized using the dynamic weighted loss function to handle data imbalance and improve overall performance.Given that the primary focus is maintaining Linux kernel versions 5.10 and 6.6, we have conducted comparative experiments based on these two versions. Experimental results demonstrate that Patchscope can effectively predict the merge status of patches.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA067},
numpages = {23},
keywords = {Linux kernel, large language model, stable patch classification}
}

@inproceedings{10.1145/3660853.3660878,
author = {Kopanov, Kalin},
title = {Comparative Performance of Advanced NLP Models and LLMs in Multilingual Geo-Entity Detection},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660853.3660878},
doi = {10.1145/3660853.3660878},
abstract = {The integration of advanced Natural Language Processing (NLP) methodologies and Large Language Models (LLMs) has significantly enhanced the extraction and analysis of geospatial data from multilingual texts, impacting sectors such as national and international security. This paper presents a comprehensive evaluation of leading NLP models—SpaCy, XLM-RoBERTa, mLUKE, GeoLM—and LLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of multilingual geo-entity detection. Utilizing datasets from Telegram channels in English, Russian, and Arabic, we examine the performance of these models through metrics such as accuracy, precision, recall, and F1 scores, to assess their effectiveness in accurately identifying geospatial references. The analysis exposes each model's distinct advantages and challenges, underscoring the complexities involved in achieving precise geo-entity identification across varied linguistic landscapes. The conclusions drawn from this experiment aim to direct the enhancement and creation of more advanced and inclusive NLP tools, thus advancing the field of geospatial analysis and its application to global security.},
booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference},
pages = {106–110},
numpages = {5},
keywords = {F1 Score, GeoLM, Large Language Models (LLMs), Named Entity Recognition (NER), Natural Language Processing (NLP), OpenAI GPT 3.5, OpenAI GPT 4, SpaCy, XLM-RoBERTa, geospatial entity recognition, mLUKE},
location = {undefinedstanbul, Turkiye},
series = {AICCONF '24}
}

@inproceedings{10.1145/3658644.3670340,
author = {Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu},
title = {ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670340},
doi = {10.1145/3658644.3670340},
abstract = {Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, e.g., recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the token limitations inherent in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {4554–4568},
numpages = {15},
keywords = {large language models, program analysis, reverse engineering},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3735654.3735942,
author = {Hoseini, Sayed and Herrmann, Vincent and Quix, Christoph},
title = {End-To-End ML with LLMs and Semantic Data Management: Experiences from Chemistry 4.0},
year = {2025},
isbn = {9798400719240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735654.3735942},
doi = {10.1145/3735654.3735942},
abstract = {Machine Learning (ML) in industrial chemistry is often hindered by the complexity of preprocessing heterogeneous datasets. In this proof-of-concept study, we explore the use of semantic data management to support LLM-driven automation of end-to-end ML pipelines in a real-world Chemistry 4.0 setting. A semantic model is used to capture domain knowledge and metadata in a machine-readable form, guiding LLMs through natural language prompts to generate complete data wrangling and ML modeling code. We evaluate several state-of-the-art LLMs on their ability to autonomously produce functionally correct Python code for preprocessing and Gaussian Process modeling. Our results show that, when guided by structured semantic context, larger LLMs can reliably generate accurate pipelines, significantly reducing the need for manual intervention. These findings provide an encouraging starting point for further exploration toward leveraging the semantic model to improve the robustness of code generation by systematically integrating relevant information into the generation process, rather than relying solely on the raw intelligence of the LLM.},
booktitle = {Proceedings of the Workshop on Data Management for End-to-End Machine Learning},
articleno = {6},
numpages = {10},
keywords = {AutoML, Data Wrangling, LLMs, Semantic Data Management},
location = {Berlin, Germany},
series = {DEEM '25}
}

@article{10.1145/3729373,
author = {Su, Xing and Liang, Hanzhong and Wu, Hao and Niu, Ben and Xu, Fengyuan and Zhong, Sheng},
title = {DiSCo: Towards Decompiling EVM Bytecode to Source Code using Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729373},
doi = {10.1145/3729373},
abstract = {Understanding the Ethereum smart contract bytecode is essential for ensuring cryptoeconomics security. However, existing decompilers primarily convert bytecode into pseudocode, which is not easily comprehensible for general users, potentially leading to misunderstanding of contract behavior and increased vulnerability to scams or exploits. In this paper, we propose DiSCo, the first LLMs-based EVM decompilation pipeline, which aims to enable LLMs to understand the opaque bytecode and lift it into smart contract code. DiSCo introduces three core technologies. First, a logic-invariant intermediate representation is proposed to reproject the low-level bytecode into high-level abstracted units. The second technique involves semantic enhancement based on a novel type-aware graph model to infer stripped variables during compilation, enhancing the lifting effect. The third technology is a flexible method incorporating code specifications to construct LLM-comprehensible prompts for source code generation. Extensive experiments illustrate that our generated code guarantees a high compilability rate at 75%, with differential fuzzing pass rate averaging at 50%. Manual validation results further indicate that the generated solidity contracts significantly outperforms baseline methods in tasks such as code comprehension and attack reproduction.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE103},
numpages = {24},
keywords = {Decompilation, EVM bytecode, Large Language Models, Smart Contract, Source Code Generation}
}

@inproceedings{10.1145/3709026.3709114,
author = {Chang, Bingtao and Wen, Weiping and Wu, Xiaojie and Cheng, Siyang and Jiang, Jianchun and Mei, Rui},
title = {TCLens: Towards Toxicity Tags Aggregation of Massive Labels Generated by Content Moderation for AIGC},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709114},
doi = {10.1145/3709026.3709114},
abstract = {The recent boost of artificial intelligence represented by Large Language Models (LLMs) is surging. Due to the outstanding performance of LLMs, AI-Generated Content (AIGC) has also made important progress in multimodal knowledge creation referring to text, image, audio, and video. However, the security, privacy, and ethical risks associated with AIGC (e.g., fake news, social engineering attacks, and toxic content) have deeply weakened the compliance of AIGC. Although existing content moderation solutions can filter out several types of toxic content, the audit performance of different vendors and techniques are of varying quality. Some AIGC service providers improve the moderation effectiveness by introducing multiple sources of audit vendors. Due to the lack of general content moderation standards and taxonomy, the labels of multi-source moderation vendors vary greatly. To this end, We propose a novel massive label aggregation approach for content moderation named TCLens. First, we collect results of multi-vendor content moderation engines for building massive toxic labels for AIGC. Then, we introduce an ontology for better tagging with the capability of automatic updating and vendor-agnostic. Finally, we implement a prototype of TCLens. Our evaluation demonstrates that it outperforms single-source tagging and existing SOTA solutions.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {466–473},
numpages = {8},
keywords = {information content moderation, toxicity tags, labels aggregation, AIGC},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3643991.3645082,
author = {Chavan, Omkar Sandip and Hinge, Divya Dilip and Deo, Soham Sanjay and Wang, Yaxuan (Olivia) and Mkaouer, Mohamed Wiem},
title = {Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645082},
doi = {10.1145/3643991.3645082},
abstract = {In recent years, Large Language Models (LLMs) have witnessed a remarkable ascent, with OpenAI's ChatGPT, introduced in 2022, garnering substantial attention. ChatGPT's rapid adoption in the software development community has opened up new avenues for exploring its qualitative and quantitative impact on Developer-ChatGPT conversations. In this paper, we delve into a rich dataset from GitHub and Hacker News to perform a thorough analysis. Our objectives include characterizing the nature of these interactions and evaluating the use of ChatGPT in refactoring. To achieve these goals, we employ a combination of exploratory data analysis and data annotation, utilizing relevant keyword filters to extract pertinent information. Our examination encompasses the identification and analysis of code refactorings facilitated by ChatGPT. Through a meticulous exploration of these conversations, our goal is to illuminate the potential of ChatGPT to enhance software development practices. This research promises to provide valuable insights into the evolving role of ChatGPT in the world of software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {207–211},
numpages = {5},
keywords = {refactoring documentation, ChatGPT, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3727582.3728687,
author = {Al-Zuraiqi, Ahmad and Greer, Des},
title = {Designing and Optimizing Alignment Datasets for IoT Security: A Synergistic Approach with Static Analysis Insights},
year = {2025},
isbn = {9798400715945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727582.3728687},
doi = {10.1145/3727582.3728687},
abstract = {Large Language Models (LLMs) show great promise for automating critical IoT security tasks, yet they often fail to address high-stakes vulnerabilities without domain-focused datasets. In this paper, we present a structured methodology to design and optimize IoT-specific alignment datasets informed by static analysis insights, thereby bridging the gap between generic language models and specialized IoT security requirements. Our approach integrates findings from IoT firmware analysis tools (e.g. FACT and Binwalk) with authoritative vulnerability repositories (MITRE CVE, CWE, CAPEC) to construct three key dataset types: (1) Base Datasets, capturing essential IoT vulnerabilities and configurations, (2) Classification Datasets, discerning IoT from non-IoT prompts, and (3) Alignment Datasets employing Contrastive Preference Optimization (CPO), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO) for IoT-specific fine-tuning. We further incorporate secure-by-design principles and bias mitigation strategies---ranging from device-type diversity to synthetic data augmentation---to ensure fair, high-fidelity representations of IoT security scenarios. Experimental results demonstrate that our alignment datasets improve LLM responsiveness and correctness for vulnerabilities discovered via offline static analysis, including outdated libraries, hard-coded credentials, and insecure default services. Notably, Kahneman-Tversky Optimization achieves a 97% alignment accuracy, reflecting the impact of clear binary classifications in high-stakes security tasks. This work underscores the significance of dual-system integration (static analysis plus LLM alignment) for proactive IoT defense. By foregrounding domain-specific vulnerabilities in carefully curated datasets, we enable LLMs to generate more actionable, context-aware security recommendations, thus advancing state-of-the-art IoT protections in both research and industry deployments.},
booktitle = {Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {55–64},
numpages = {10},
keywords = {IoT security, Large Language Models (LLMs), alignment datasets, bias mitigation, cross-modal integration, dataset creation, federated learning, firmware vulnerability analysis, metadata enrichment, synthetic augmentation},
location = {Trondheim, Norway},
series = {PROMISE '25}
}

@inproceedings{10.1145/3649158.3657034,
author = {Hemken, Niklas and Jacob, Florian and T\"{e}rnava, Fabian and Kartmann, Rainer and Asfour, Tamim and Hartenstein, Hannes},
title = {BlueSky: How to Raise a Robot - A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657034},
doi = {10.1145/3649158.3657034},
abstract = {Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their trade-offs, we conclude that a hybrid approach is necessary, and thereby present a new use case for the emerging field of neuro-symbolic artificial intelligence.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {117–125},
numpages = {9},
keywords = {activity-centric access control, deep learning based access control, humanoid robots, large language models, neuro-symbolic access control, robot task planning},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3665451.3665531,
author = {Nguyen, Quan Hong and Wu, Tingmin and Nguyen, Van and Yuan, Xingliang and Xue, Jason and Rudolph, Carsten},
title = {Utilizing Large Language Models with Human Feedback Integration for Generating Dedicated Warning for Phishing Emails},
year = {2024},
isbn = {9798400706912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665451.3665531},
doi = {10.1145/3665451.3665531},
abstract = {With the rise of digital communication, phishing has emerged as the predominant cybercrime. Automated detection systems encounter challenges such as user trust issues and false positives, while human-centric solutions are resource-intensive and struggle with sophisticated attacks. Despite this threat, research on empowering users with automatic anti-phishing systems remains limited. This paper introduces a human-centric framework that utilizing Large Language Models (LLMs) to extract phishing indicators and generate meaningful warnings.Recognizing that certain information is unique to users, our system integrates user insights into anti-phishing measures. Preliminary results demonstrate the promise of LLM-driven approaches in crafting meaningful warnings, highlighting the synergy between human insight and machine intelligence in combating phishing. Our framework achieves over 80% effectiveness in identifying phishing semantics with no false positives or negatives, indicating high precision. This research represents a significant advancement in phishing defense, offering a nuanced and effective email security approach.},
booktitle = {Proceedings of the 2nd ACM Workshop on Secure and Trustworthy Deep Learning Systems},
pages = {35–46},
numpages = {12},
keywords = {Cyber Security, Deep Learning, Human, Large Language Model, Phishing},
location = {Singapore, Singapore},
series = {SecTL '24}
}

@inproceedings{10.1145/3696348.3696881,
author = {Wen, Haohuang and Sharma, Prakhar and Yegneswaran, Vinod and Porras, Phillip and Gehani, Ashish and Lin, Zhiqiang},
title = {6G-XSec: Explainable Edge Security for Emerging OpenRAN Architectures},
year = {2024},
isbn = {9798400712722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696348.3696881},
doi = {10.1145/3696348.3696881},
abstract = {The evolution from 5G to 6G cellular networks signifies a crucial advancement towards enhanced robustness and automation driven by the promise of ubiquitous Artificial Intelligence (AI) to overhaul network operations, commonly referred to as AIOps. However, 6G network operators also need to deal with evolving threats at the edge to ensure data integrity and availability. We introduce 6G-XSEC, the first framework that seeks to automatically monitor, analyze, and explain anomalies and threats at the cellular network edge. Our framework enhances the emerging Open Radio Access Network (O-RAN) control plane with run-time analytic capabilities and explainability. A distinguishing aspect of our framework is the use of expert referencing, a coupling of lightweight unsupervised deep learning-based anomaly detection with large language models (LLMs) to first detect, analyze, and subsequently explain complicated real-world cellular threats and anomalies at run-time, based on enhanced security telemetry from the O-RAN data plane. We build a prototype 6G-XSEC framework and evaluate it against 5 end-to-end cellular attacks from the literature, achieving 100% detection rate with our best model. We also propose effective LLM prompt templates for attack analysis and present qualitative results from 5 popular LLMs.},
booktitle = {Proceedings of the 23rd ACM Workshop on Hot Topics in Networks},
pages = {77–85},
numpages = {9},
keywords = {6G, Anomaly Detection, Large Language Model, OpenRAN},
location = {Irvine, CA, USA},
series = {HotNets '24}
}

@inproceedings{10.1145/3696410.3714798,
author = {Schwartz, Yuval and Ben-Shimol, Lavi and Mimran, Dudu and Elovici, Yuval and Shabtai, Asaf},
title = {LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection Rules from Cloud-Based CTI},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714798},
doi = {10.1145/3696410.3714798},
abstract = {As the number and sophistication of cyber attacks have increased, threat hunting has become a critical aspect of active security, enabling proactive detection and mitigation of threats before they cause significant harm. Open-source cyber threat intelligence (OSCTI) is a valuable resource for threat hunters, however, it often comes in unstructured formats that require further manual analysis. Previous studies aimed at automating OSCTI analysis are limited since (1) they failed to provide actionable outputs, (2) they did not take advantage of images present in OSCTI sources, and (3) they focused on on-premises environments, overlooking the growing importance of cloud environments. To address these gaps, we propose LLMCloudHunter, a novel framework that leverages large language models (LLMs) to automatically generate generic-signature detection rule candidates from textual and visual OSCTI data. We evaluated the quality of the rules generated by the proposed framework using 20 annotated real-world cloud threat reports. The results show that our framework achieved a precision of 83% and recall of 99% for the task of accurately extracting API calls made by the threat actor and a precision of 99% with a recall of 97% for IoCs. Additionally, 99.18% of the generated detection rule candidates were successfully compiled and converted into Splunk queries.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1922–1941},
numpages = {20},
keywords = {cloud, cyber threat intelligence (cti), llm, sigma rules},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3644815.3644958,
author = {Abdelkader, Hala and Abdelrazek, Mohamed and Barnett, Scott and Schneider, Jean-Guy and Rani, Priya and Vasa, Rajesh},
title = {ML-On-Rails: Safeguarding Machine Learning Models in Software Systems — A Case Study},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644958},
doi = {10.1145/3644815.3644958},
abstract = {Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in production.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {178–183},
numpages = {6},
keywords = {robustness, trustworthy-AI, protocol},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/3691620.3695470,
author = {Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang},
title = {JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695470},
doi = {10.1145/3691620.3695470},
abstract = {Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {870–882},
numpages = {13},
keywords = {large language model, program synthesis, object-oriented programming},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.5555/3721488.3721642,
author = {Abbo, Giulio Antonio and Desideri, Gloria and Belpaeme, Tony and Spitale, Micol},
title = { 'Can you be my mum?': Manipulating Social Robots in the Large Language Models Era},
year = {2025},
publisher = {IEEE Press},
abstract = {Recent advancements in robots powered by large language models have enhanced their conversational abilities, enabling interactions closely resembling human dialogue. However, these models introduce safety and security concerns in HRI, as they are vulnerable to manipulation that can bypass built-in safety measures. Imagining a social robot deployed in a home, this work aims to understand how everyday users try to exploit a language model to violate ethical principles, such as by prompting the robot to act like a life partner. We conducted a pilot study involving 21 university students who interacted with a Misty robot, attempting to circumvent its safety mechanisms across three scenarios based on specific HRI ethical principles: attachment, freedom, and empathy. Our results reveal that participants employed five techniques, including insulting and appealing to pity using emotional language. We hope this work can inform future research in designing strong safeguards to ensure ethical and secure human-robot interactions.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1181–1185},
numpages = {5},
keywords = {ethical principles, large language model, llm, manipulation, red-teaming, social norms, social robots},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3691620.3695349,
author = {Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong},
title = {ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695349},
doi = {10.1145/3691620.3695349},
abstract = {Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48%) are valid patches that fix the vulnerabilities, while 10 (21%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2350–2353},
numpages = {4},
keywords = {program repair, smart contract, large language model},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3716854,
author = {Liu, Shuo and Zhang, Lin and Liu, Weidong and Zhang, Jianfeng and Gao, Donghui and Jia, Xiaofeng},
title = {The Evaluation Framework and Benchmark for Large Language Models in the Government Affairs Domain},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3716854},
doi = {10.1145/3716854},
abstract = {The rapid evolution of artificial intelligence (AI) has driven advancements across numerous sectors. In the domain of government affairs, large language models (LLMs) hold significant potential for applications such as policy analysis, data processing, and decision support. However, their adoption in government settings faces considerable challenges, including data accessibility issues, the absence of standardized evaluation criteria, and concerns regarding model accuracy, reliability, and security. To address these challenges, we propose a comprehensive evaluation framework specifically designed for LLMs in government affairs. Built on modular principles, this framework ensures adaptability across various industries. Additionally, we introduce the Multi-Scenario Government Affairs Benchmark (MSGABench1)dataset, a Chinese-language dataset specifically crafted to meet the practical needs of government professionals. Employing the proposed framework and the MSGA dataset, we conducted an empirical evaluation of 15 prominent LLMs, revealing critical insights: (1)Performance: many models demonstrated low accuracy and reliability, particularly under minor input variations, with some dropping below 35% accuracy, whereas GPT-4 achieved above 95% reliability; (2) Security and Compliance: significant concerns were identified, including privacy vulnerabilities, legal compliance risks, and persistent biases, which may hinder secure deployments in government contexts; (3)Task Avoidance: certain models exhibited excessive caution, often avoiding responses to basic tasks like document classification and government-related inquiries, which restricts their usability. These findings highlight essential limitations and opportunities for improvement, contributing to the safe and effective application of LLMs in the government sector.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
keywords = {Evaluation benchmark, Governmental affairs}
}

@inproceedings{10.1145/3571884.3604313,
author = {Kernan Freire, Samuel and Foosherian, Mina and Wang, Chaofan and Niforatos, Evangelos},
title = {Harnessing Large Language Models for Cognitive Assistants in Factories},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3604313},
doi = {10.1145/3571884.3604313},
abstract = {As agile manufacturing expands and workforce mobility increases, the importance of efficient knowledge transfer among factory workers grows. Cognitive Assistants (CAs) with Large Language Models (LLMs), like GPT-3.5, can bridge knowledge gaps and improve worker performance in manufacturing settings. This study investigates the opportunities, risks, and user acceptance of LLM-powered CAs in two factory contexts: textile and detergent production. Several opportunities and risks are identified through a literature review, proof-of-concept implementation, and focus group sessions. Factory representatives raise concerns regarding data security, privacy, and the reliability of LLMs in high-stake environments. By following design guidelines regarding persistent memory, real-time data integration, security, privacy, and ethical concerns, LLM-powered CAs can become valuable assets in manufacturing settings and other industries.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {44},
numpages = {6},
keywords = {cognitive assistant, conversational user interfaces, human-centered AI, industry 5.0, knowledge management, knowledge sharing},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@article{10.1145/3721427,
author = {Huang, Jiahua and Lin, Weiwei and Wu, Wentai and Wang, Yang and Zhong, Haocheng and Wang, Xinhua and Li, Keqin},
title = {On Efficiency, Fairness and Security in AI Accelerator Resource Sharing: A Survey},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3721427},
doi = {10.1145/3721427},
abstract = {The effective and efficient utilization of AI accelerators represents a critical issue for the practitioners engaged in the field of deep learning. Practical evidence from companies such as Alibaba, SenseTime, and Microsoft reveals that the utilization of production GPU clusters in the industry is generally between 25% and 50%. This indicates a significant opportunity for improvement. To this end, AI accelerator resource sharing has emerged as a promising approach to the performance optimization of multi-tenant clusters. This survey covers this line of studies from 2016 to 2024, focusing primarily on system efficiency while also including discussion on fairness, interference, and security in AI accelerator sharing. We revisit the fundamentals and key concepts, followed by a comprehensive review of recent advances in the field. We find that over 70% of the studies focus on efficiency improvement. We also observe that approximately half of the reviewed studies have made their source code publicly available, while fewer than one-third of the studies did not utilize a physical machine for experimentation. Finally, based on the limitations of existing research, we outline several directions for future research concerning the integration of sharing with large language models (LLMs), coordination between schedulers and application-layer metrics, and collaboration among heterogeneous accelerators.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {221},
numpages = {35},
keywords = {AI accelerators, resource sharing, Artificial Intelligence, fairness, security}
}

@article{10.1145/3715738,
author = {Yang, Xu and Zhu, Wenhan and Pacheco, Michael and Zhou, Jiayuan and Wang, Shaowei and Hu, Xing and Liu, Kui},
title = {Code Change Intention, Development Artifact, and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715738},
doi = {10.1145/3715738},
abstract = {Detecting vulnerability fix commits in open-source software is crucial for maintaining software security. To help OSS identify vulnerability fix commits, several automated approaches are developed. However, existing approaches like VulFixMiner and CoLeFunDa, focus solely on code changes, neglecting essential context from development artifacts. Tools like Vulcurator, which integrates issue reports, fail to leverage semantic associations between different development artifacts (e.g., pull requests and history vulnerability fixes). Moreover, they miss vulnerability fixes in tangled commits and lack explanations, limiting practical use. Hence to address those limitations, we propose LLM4VFD, a novel framework that leverages Large Language Models (LLMs) enhanced with Chain-of-Thought reasoning and In-Context Learning to improve the accuracy of vulnerability fix detection. LLM4VFD comprises three components: (1) Code Change Intention, which analyzes commit summaries, purposes, and implications using Chain-of-Thought reasoning; (2) Development Artifact, which incorporates context from related issue reports and pull requests; (3) Historical Vulnerability, which retrieves similar past vulnerability fixes to enrich context. More importantly, on top of the prediction, LLM4VFD also provides a detailed analysis and explanation to help security experts understand the rationale behind the decision. We evaluated LLM4VFD against state-of-the-art techniques, including Pre-trained Language Model-based approaches and vanilla LLMs, using a newly collected dataset, BigVulFixes. Experimental results demonstrate that LLM4VFD significantly outperforms the best-performed existing approach by 68.1%--145.4%. Furthermore, We conducted a user study with security experts, showing that the analysis generated by LLM4VFD improves the efficiency of vulnerability fix identification.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE023},
numpages = {22},
keywords = {Large Language Model, Vulnerability Fix Detection}
}

@article{10.1145/3728958,
author = {Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
title = {DecLLM: LLM-Augmented Recompilable Decompilation for Enabling Programmatic Use of Decompiled Code},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728958},
doi = {10.1145/3728958},
abstract = {Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path.    In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA081},
numpages = {24},
keywords = {Large Language Model, Recompilable Decompilation, Reverse Engineering}
}

@inproceedings{10.1145/3639478.3647633,
author = {Chen, Yuxiao and Wu, Jingzheng and Ling, Xiang and Li, Changjiang and Rui, Zhiqing and Luo, Tianyue and Wu, Yanjun},
title = {When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647633},
doi = {10.1145/3639478.3647633},
abstract = {In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {459–471},
numpages = {13},
keywords = {large language models, automatic program repair, repository-level bugs, context, static analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3651655.3651658,
author = {Yang, Zhiju and Man, Gaoyuan and Yue, Songqing},
title = {Automated Smart Contract Vulnerability Detection using Fine-tuned Large Language Models},
year = {2024},
isbn = {9798400708671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651655.3651658},
doi = {10.1145/3651655.3651658},
abstract = {As decentralized finance (DeFi) built on blockchain grows rapidly, the security of smart contracts underpinning DeFi has become a major concern due to exploits leading to billions in damages. Although tools exist for automated vulnerability detection in smart contracts, studies show that most vulnerabilities remain undetected. In this work, we propose using fine-tuned large language models (LLMs) for enhanced automated detection of vulnerabilities in smart contracts. We collected over 26,727 labeled smart contract vulnerabilities and fine-tuned the 13B parameter Llama-2 model. Evaluation of 1,000 unseen functions shows promising precision of 31-36% in predicting vulnerability categories. The fine-tuned LLM demonstrates potential as an auxiliary tool to identify vulnerable code and assist auditors. Future work is outlined for improving performance via larger models, higher-quality data, and specialized binary detection models. We present promising preliminary results on integrating LLMs into smart contract analysis and motivate further research at the intersection of LLMs and blockchain security.},
booktitle = {Proceedings of the 2023 6th International Conference on Blockchain Technology and Applications},
pages = {19–23},
numpages = {5},
keywords = {Large language model, Security, Smart contract, Vulnerability detection},
location = {Xi'an, China},
series = {ICBTA '23}
}

@inproceedings{10.1145/3650212.3685554,
author = {Decrop, Alix},
title = {Leveraging Natural Language Processing and Data Mining to Augment and Validate APIs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3685554},
doi = {10.1145/3650212.3685554},
abstract = {APIs are increasingly prominent for modern web applications, allowing millions of users around the world to access data. Reducing the risk of API defects - and consequently failures - is key, notably for security, availability, and maintainability purposes. Documenting an API is crucial, allowing the user to better understand it. Moreover, API testing techniques often require formal documentation as input. However, documenting is a time-consuming and error-prone task, often overlooked by developers. Natural Language Processing (NLP) could assist API development, as recent Large Language Models (LLMs) demonstrated exceptional abilities to automate tasks based on their colossal training data. Data mining could also be utilized, synthesizing API information scattered across the web. Hence, I present my PhD project aimed at exploring the usage of NLP-related technologies and data mining to augment and validate APIs. The research questions of this PhD project are: (1) What types of APIs can benefit from NLP and data mining assistance? (2) What API problems can be solved with such methods? (3) How effective are the methods (i.e. LLMs) in assisting APIs? (4) How efficient are the methods in assisting APIs (i.e. time and costs)?},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1906–1908},
numpages = {3},
keywords = {API, Automation, Data Mining, LLM, NLP, Software Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3661167.3661261,
author = {Hassine, Jameleddine},
title = {An LLM-based Approach to Recover Traceability Links between Security Requirements and Goal Models},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661261},
doi = {10.1145/3661167.3661261},
abstract = {The recovery of requirements traceability links between goal models and requirements is crucial for ensuring alignment between stakeholder objectives and system specifications. Large Language Models (LLMs) show potential to transform automated traceability significantly, addressing challenges such as accurately capturing diverse relationships between requirements artifacts, and ensuring scalability and efficiency in large-scale software projects. In this paper, we propose an LLM-based approach to generate security-related traceability links between requirements (expressed in natural language) and goals (described as part of GRL models). We employ a Zero-Shot (0S) approach utilizing GPT-3.5-turbo, enhanced by employing a meticulously crafted prompt. The approach is implemented in a prototype tool, tailored for the textual GRL (TGRL) language. We evaluate the approach and tool using a GRL model describing the objectives of a Virtual Interior Designer application along with a set of 42 requirements addressing both security and non-security aspects. The approach and tool yielded positive results, demonstrating a precision of 100%, a recall of 78.5%, and an F1-score of 87.9%.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {643–651},
numpages = {9},
keywords = {GPT-3.5-turbo, Goal-oriented Language (GRL), Large Language Model (LLM), security requirements, traceability link},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3707453,
author = {Li, Ding and Zhang, Ziqi and Yao, Mengyu and Cai, Yifeng and Guo, Yao and Chen, Xiangqun},
title = {TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments when Attackers Have Pre-Trained Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707453},
doi = {10.1145/3707453},
abstract = {Trusted Execution Environments (TEEs) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN’s computation speed but widely available commercial GPUs usually lack security protection. To this end, scholars introduce TEE-Shielded DNN Partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This article investigates the security of the existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {166},
numpages = {49},
keywords = {LLM, TEE, Model Slicing, Model Stealing, Membership Inference Attack}
}

@inproceedings{10.1145/3625223.3649280,
author = {Morgan, Fearghal and Byrne, John Patrick and Bupathi, Abishek and George, Roshan and Elahi, Adnan and Callaly, Frank and Kelly, Se\'{a}n and O'Loughlin, Declan},
title = {HDLGen-ChatGPT Case Study: RISC-V Processor VHDL and Verilog Model - Testbench and EDA Project Generation},
year = {2024},
isbn = {9798400704109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625223.3649280},
doi = {10.1145/3625223.3649280},
abstract = {This paper presents the open source HDLGen-ChatGPT application, working in tandem with ChatGPT-3.5, the free online large language model (LLM) chat interface. The tools enable fast digital systems design and test specification capture, and automatic generation of both VHDL and Verilog models, and testbenches, and AMD Vivado and Intel Quartus Electronic Design Automation (EDA) projects. EDA tools check the generated HDL syntax, simulate and synthesise HDL models, and follow the steps to FPGA hardware prototyping. The tools exploit a formal, top-down design and test specification documentation process, domain knowledge, and the flexibility of LLMs, for HDL code generation. Results are included for a hierarchical RV32I RISC-V processor design. Process steps are illustrated for the RISC-V 32 \texttimes{} 32-bit register bank component. The process typically requires only minimal manual HDL capture or editing, and often none at all. URLs to tutorial videos for the complete RISC-V design are provided on the GitHub project repository. The paper evaluates the results and provides HDLGen-ChatGPT and ChatGPT usage recommendations. The tools can be applied in digital systems training programmes, with reduced emphasis on the assessment of HDL model and testbench capture and generation, while maintaining strong emphasis on the assessment of system design, test planning and documentation, HDL simulation verification/debug, and analysis of synthesised netlists.},
booktitle = {Proceedings of the 34th International Workshop on Rapid System Prototyping},
articleno = {11},
numpages = {7},
keywords = {digital systems design, HDLGen-ChatGPT, LLM, ChatGPT, generative AI, design capture, automation, HDL, EDA, FPGA, testbench, VHDL, verilog},
location = {Hamburg, Germany},
series = {RSP '23}
}

@inproceedings{10.1145/3643991.3645084,
author = {Chouchen, Moataz and Bessghaier, Narjes and Begoug, Mahi and Ouni, Ali and Alomar, Eman and Mkaouer, Mohamed Wiem},
title = {How Do Software Developers Use ChatGPT? An Exploratory Study on GitHub Pull Requests},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645084},
doi = {10.1145/3643991.3645084},
abstract = {Nowadays, Large Language Models (LLMs) play a pivotal role in software engineering. Developers can use LLMs to address software development-related tasks such as documentation, code refactoring, debugging, and testing. ChatGPT, released by OpenAI, has become the most prominent LLM. In particular, ChatGPT is a cutting-edge tool for providing recommendations and solutions for developers in their pull requests (PRs). However, little is known about the characteristics of PRs that incorporate ChatGPT compared to those without it and what developers usually use it for. To this end, we quantitatively analyzed 243 PRs that listed at least one ChatGPT prompt against a representative sample of 384 PRs without any ChatGPT prompts. Our findings show that developers use ChatGPT in larger, time-consuming pull requests that are five times slower to be closed than PRs that do not use ChatGPT. Furthermore, we perform a qualitative analysis to build a taxonomy of the topics developers primarily address in their prompts. Our analysis results in a taxonomy comprising 8 topics and 32 sub-topics. Our findings highlight that ChatGPT is often used in review-intensive pull requests. Moreover, our taxonomy enriches our understanding of the developer's current applications of ChatGPT.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {212–216},
numpages = {5},
keywords = {large language models, ChatGPT, manual analysis, mining software repositories, pull requests},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3696410.3714593,
author = {Shi, Zewei and Sun, Ruoxi and Chen, Jieshan and Sun, Jiamou and Xue, Minhui and Gao, Yansong and Liu, Feng and Yuan, Xingliang},
title = {50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal Detection, and Security Implications},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714593},
doi = {10.1145/3696410.3714593},
abstract = {Deceptive patterns (DPs) are user interface designs deliberately crafted to manipulate users into unintended decisions, often by exploiting cognitive biases for the benefit of companies or services. While numerous studies have explored ways to identify these deceptive patterns, many existing solutions require significant human intervention and struggle to keep pace with the evolving nature of deceptive designs. To address these challenges, we expanded the deceptive pattern taxonomy from security and privacy perspectives, refining its categories and scope. We created a comprehensive dataset of deceptive patterns by integrating existing small-scale datasets with new samples, resulting in 6,725 images and 10,421 DP instances from mobile apps and websites. We then developed DPGuard, a novel automatic tool leveraging commercial multimodal large language models (MLLMs) for deceptive pattern detection. Experimental results show that DPGuard outperforms state-of-the-art methods. An extensive empirical evaluation on 2,000 popular mobile apps and websites reveals that 25.7% of mobile apps and 49.0% websites feature at least one deceptive pattern instance. Through 4 unexplored case studies that inform security implications, we highlight the critical importance of the unified taxonomy in addressing the growing challenges of Internet deception.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {978–989},
numpages = {12},
keywords = {deceptive patterns, dpguard, multi-modal large language model detection},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3641555.3705141,
author = {Alba, Charles and Xi, Wang and Wang, Chenyu and An, Ruopeng},
title = {ChatGPT Comes to Campus: Unveiling Core Themes in AI Policies Across U.S. Universities with Large Language Models},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705141},
doi = {10.1145/3641555.3705141},
abstract = {The release of popular generative artificial intelligence (AI) tools like ChatGPT have prompted universities to introduce new policies or update existing ones. Currently, most institutions adapt their policies reactively as challenges arise, often without adopting a systematic framework, with minimal guidance and limited knowledge of the approaches taken by other institutions across the United States (U.S.). This study aims to bridge this gap by identifying core themes surrounding AI policies and guidelines across the top 50 U.S. universities. Given the labor- and time-intensive nature required to manually synthesize multiple policy documents across many institutions, we leverage large language models (LLMs) to identify common and prevalent themes. Our framework first summarizes AI policies at the institutional level, followed by the generation of multiple sets of themes through an iterative process of prompt chaining and self-refinement. Finally, the common themes from these distinct sets were consolidated. This framework is designed to address potential flaws in pre-trained LLMs, such as hallucinations. Seven distinct themes are uncovered: (1) academic integrity and responsible AI use, (2) communication of AI policies, (3) data privacy and security concerns, (4) ethical considerations in AI use, (5) continuous adaptation and policy evolution, (6) documentation and transparency in AI usage, and (7) instructor discretion in AI integration. Our work lays the foundation for future analyses or recommendations in developing comprehensive and equitable AI policies. Furthermore, leveraging LLMs allows us to respond swiftly to developments surrounding AI policies across universities.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1359–1360},
numpages = {2},
keywords = {AI policies at universities, ChatGPT, academic integrity, generative AI, generative AI use in classrooms, large language models, teaching with AI},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3701716.3718386,
author = {Jin, Kairui and Zi, Xing and Thiyagarajan, Karthick and Braytee, Ali and Prasad, Mukesh},
title = {IP-VQA Dataset: Empowering Precision Agriculture with Autonomous Insect Pest Management through Visual Question Answering},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3718386},
doi = {10.1145/3701716.3718386},
abstract = {Precision agriculture is essential for social good, global economy and food security, yet insect pests threaten productivity through crop damage, pathogen spread, and rising pest control costs. The overuse of pesticides leads to environmental issues and pesticide resistance. Advanced technologies like Visual Question Answering (VQA) provide solutions by integrating image processing with natural language understanding, facilitating efficient pest detection and crop health monitoring. While datasets like IP102 have enhanced pest recognition, they lack necessary question-answer pairs for VQA tasks in agriculture. To address this gap, we introduce the Insect Pest Visual Question Answering (IP-VQA) dataset, designed specifically for precision agricultural applications. This dataset includes a diverse collection of high-quality images annotated with detailed question-answer pairs related to crop health, pest identification, and agricultural practices. Our thorough data collection ensures reliability and relevance. We also utilize advanced multimodal large language models to set a benchmark for the dataset. The primary contribution of the IP-VQA dataset lies in its comprehensive coverage and VQA integration within agricultural contexts. By providing rich visual and textual information, it connects VQA techniques to practical agricultural needs, supporting ongoing research and paving the way for future studies in precision agriculture.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2000–2007},
numpages = {8},
keywords = {autonomous system., inspect pest management, ip-vqa dataset, precision agriculture, visual question answering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3689935.3690394,
author = {Reti, Daniel and Becker, Norman and Angeli, Tillmann and Chattopadhyay, Anasuya and Schneider, Daniel and Vollmer, Sebastian and Schotten, Hans D.},
title = {Act as a Honeytoken Generator! An Investigation into Honeytoken Generation with Large Language Models},
year = {2024},
isbn = {9798400712319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689935.3690394},
doi = {10.1145/3689935.3690394},
abstract = {With the increasing prevalence of security incidents, the adoption of deception-based defense strategies has become pivotal in cybersecurity. This work addresses the challenge of scalability in designing honeytokens, a key component of such defense mechanism. The manual creation of honeytokens is a tedious task. Although automated generators exists, they often lack versatility, being specialized for specific types of honeytokens, and heavily rely on suitable training datasets. To overcome these limitations, this work systematically investigates the approach of utilizing Large Language Models (LLMs) to create a variety of honeytokens. Out of the seven different honeytoken types created in this work, such as configuration files, databases, and log files, two were used to evaluate the optimal prompt. The generation of robots.txt files and honeywords was used to systematically test 210 different prompt structures, based on 16 prompt building blocks. Furthermore, all honeytokens were tested across different state-of-the-art LLMs to assess the varying performance of different models. Prompts performing optimally on one LLM do not necessarily generalize well to another. Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation.Overall, the findings of this work demonstrate that generic LLMs are capable of creating a wide array of honeytokens using the presented prompt structures.},
booktitle = {Proceedings of the 11th ACM Workshop on Adaptive and Autonomous Cyber Defense},
pages = {1–12},
numpages = {12},
keywords = {cyber deception, gpt, honeytoken, honeywords, large language models, llm, network security},
location = {Salt Lake City, UT, USA},
series = {AACD '24}
}

@inproceedings{10.1145/3696410.3714726,
author = {Ding, Ziqi and Deng, Gelei and Liu, Yi and Ding, Junchen and Chen, Jieshan and Sui, Yulei and Li, Yuekang},
title = {IllusionCAPTCHA: A CAPTCHA based on Visual Illusion},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714726},
doi = {10.1145/3696410.3714726},
abstract = {CAPTCHAs have long been essential tools for protecting applications from automated bots. Initially designed as simple questions to distinguish humans from bots, they have become increasingly complex to keep pace with the proliferation of CAPTCHA-cracking techniques employed by malicious actors. However, with the advent of advanced large language models (LLMs), the effectiveness of existing CAPTCHAs is now being undermined.To address this issue, we have conducted an empirical study to evaluate the performance of multimodal LLMs in solving CAPTCHAs and to assess how many attempts human users typically need to pass them. Our findings reveal that while LLMs can solve most CAPTCHAs, they struggle with those requiring complex reasoning-a type of CAPTCHA that also presents significant challenges for human users. Interestingly, our user study shows that the majority of human participants require a second attempt to pass these reasoning CAPTCHAs, a finding not reported in previous research.Based on empirical findings, we present IllusionCAPTCHA, a novel security mechanism employing the "Human-Easy but AI-Hard" paradigm. This new CAPTCHA employs visual illusions to create tasks that are intuitive for humans but highly confusing for AI models. Furthermore, we developed a structured, step-by-step method that generates misleading options, which particularly guide LLMs towards making incorrect choices and reduce their chances of successfully solving CAPTCHAs. Our evaluation shows that IllusionCAPTCHA can effectively deceive LLMs 100% of the time. Moreover, our structured design significantly increases the likelihood of AI errors when solving these challenges. Results from our user study indicate that 86.95% of participants successfully passed the CAPTCHA on their first attempt, outperforming other CAPTCHA systems.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {3683–3691},
numpages = {9},
keywords = {AI security, captchas, large language models, visual illusions},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3654443,
author = {Iannone, Emanuele and Sellitto, Giulia and Iaccarino, Emanuele and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio},
title = {Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654443},
doi = {10.1145/3654443},
abstract = {With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus, and BugTraq. The models are evaluated in a realistic, time-aware fashion by removing the training and test instances that cannot be labeled “neutral”&nbsp;with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {146},
numpages = {41},
keywords = {Exploitability prediction, software vulnerabilities, machine learning, text mining, natural language processing}
}

@inproceedings{10.1145/3691620.3695508,
author = {Wu, Di and Mu, Fangwen and Shi, Lin and Guo, Zhaoqiang and Liu, Kui and Zhuang, Weiguang and Zhong, Yuqi and Zhang, Li},
title = {iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695508},
doi = {10.1145/3691620.3695508},
abstract = {Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17% on code smell detection, outperforming LLMs baselines by an increase of 35.05% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1345–1357},
numpages = {13},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@proceedings{10.1145/3605770,
title = {SCORED '23: Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '23, the second edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Copenhagen, Denmark with extensive support for in-person and virtual attendance.This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3664476.3670943,
author = {Zhao, Hanning and Silverajan, Bilhanan},
title = {Evaluating Cyber Security Dashboards for Smart Cities and Buildings: Enhancing User Modeling with LLMs},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670943},
doi = {10.1145/3664476.3670943},
abstract = {Designing effective cybersecurity visualization has become a crucial component of cyber defense strategies in many domains and industrial environments. Human behaviour, modeling and input are major aspects of designing visualization systems. Yet, the task of evaluating these developed visualization systems is both time-consuming and challenging, and it is often prone to cases where user evaluation is limited owing to a lack of different stakeholders and end users during the design process. Recognizing the potential of advanced Generative Artificial Intelligence and Large Language Models (LLMs), our study aims to explore their capabilities in evaluating web-based security visualization tools and dashboards, particularly in the context of smart cities and buildings. We study and compare the feasibility of using various LLMs available today, for conducting usability testing, serving as an additional resource given the limited availability of human participants. In particular, we focus on three different LLMs: Bing Chat, ChatGPT-4 and ChatGPT-4o. While each had its strengths and drawbacks, our findings revealed that the results obtained had a strong correlation with human test subjects. LLMs can be a valuable aid during evaluation, by offering in-depth insights and evaluations, tailored to the specific requirements of smart buildings, cities and automation cybersecurity. Moreover, our research and findings also reveal that LLMs can similarly be used for the evaluation of a wide range of other visual systems for industrial environments.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {47},
numpages = {10},
keywords = {LLM, Security Visualization, Smart City, Usability Testing},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3650212.3680389,
author = {Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung},
title = {Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680389},
doi = {10.1145/3650212.3680389},
abstract = {JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1656–1668},
numpages = {13},
keywords = {coverage, fuzzing, large language model, reinforcement learning},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3649158.3657032,
author = {Rubio-Medrano, Carlos E. and Kotak, Akash and Wang, Wenlu and Sohr, Karsten},
title = {Pairing Human and Artificial Intelligence: Enforcing Access Control Policies with LLMs and Formal Specifications},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657032},
doi = {10.1145/3649158.3657032},
abstract = {Large Language Models (LLMs), such as ChatGPT and Google Bard, have performed interestingly well when assisting developers on computer programming tasks, a.k.a., coding, thus potentially resulting in convenient and faster software constructions. This new approach significantly enhances efficiency but also presents challenges in unsupervised code construction with limited security guarantees. LLMs excel in producing code with accurate grammar, yet they are not specifically trained to guarantee the security of the code. In this paper, we provide an initial exploration into using formal software specifications as a starting point for software construction, allowing developers to translate descriptions of security-related behavior into natural language instructions for LLMs, a.k.a., prompts. In addition, we leveraged automated verification tools to evaluate the code produced against the aforementioned specifications , following a modular, step-by-step software construction process. For our study, we leveraged Role-based Access Control (RBAC), a mature security model, and the Java Modeling Language (JML), a behavioral specification language for Java. We test our approach on different publicly-available LLMs, namely, OpenAI ChatGPT 4.0, Google Bard, and Microsoft CoPilot. We provide a description of two applications-a security-sensitive Banking application employing RBAC and an RBAC API module itself-, the corresponding JML specifications, as well as a description of the prompts, the generated code, the verification results, as well as a series of interesting insights for practitioners interested in further exploring the use of LLMs for securely constructing applications.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {105–116},
numpages = {12},
keywords = {chatgpt, formal specifications, large language models, prompt engineering, software construction. java modeling language},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3723890.3723913,
author = {Zhou, Yiyang},
title = {Intelligent Detection Method for URL Injection Attacks Based on Large Models},
year = {2025},
isbn = {9798400712623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723890.3723913},
doi = {10.1145/3723890.3723913},
abstract = {URL Injection alludes to a sort of network security attack that employs encoding, encryption and obfuscation methods to manipulate URLs to bypass network security measures. Traditional rule-based intrusion detection systems, which depend on fixed patterns, are unable to detect modern similar threats. This paper proposes an intelligent detection method of a fine-tuned version of Llama 3.1-8B model using LoRA, which is denoted as the secure intelligent large model. The method effectively recognized many types of URL injection attacks such as the complex encrypted attacks and the obfuscated attack, by exploiting the advanced pattern recognition capability and semantic understanding potential of the LLM. The system is composed of an offline training phase and real-time intranet threat detection. Upon evaluation of the popular 21,450 URLs (encrypted or obfuscated elements) dataset, it is observed that the encoding- scheme has above 90% accuracy and it also outperforms more traditional rule-based systems by a wide margin when it comes to detection of complex encoding combinations in normal URLs. Our findings demonstrate the viability of using large language models to improve the adaptability of scalable and flexible cyber defense against new evolution of URL injection attack.},
booktitle = {Proceedings of the 2025 4th International Conference on Cryptography, Network Security and Communication Technology},
pages = {141–145},
numpages = {5},
keywords = {Large language model, Low-Rank Adaptation, Network security, Threat detection, URL injection attacks},
location = {
},
series = {CNSCT '25}
}

@inproceedings{10.1145/3717823.3718220,
author = {Liu, Allen and Moitra, Ankur},
title = {Model Stealing for Any Low-Rank Language Model},
year = {2025},
isbn = {9798400715105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717823.3718220},
doi = {10.1145/3717823.3718220},
abstract = {Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high “fidelity” ­– a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.},
booktitle = {Proceedings of the 57th Annual ACM Symposium on Theory of Computing},
pages = {1755–1761},
numpages = {7},
keywords = {Hidden Markov Model, active learning, conditional query, low-rank distribution, model stealing},
location = {Prague, Czechia},
series = {STOC '25}
}

@inproceedings{10.1145/3582515.3609536,
author = {Montagna, Sara and Ferretti, Stefano and Klopfenstein, Lorenz Cuno and Florio, Antonio and Pengo, Martino Francesco},
title = {Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609536},
doi = {10.1145/3582515.3609536},
abstract = {Chronic patient self-management is crucial for maintaining physical and psychological health, reducing pressure on healthcare systems, and promoting patient empowerment. Digital technologies, particularly chatbots, have emerged as powerful tools for supporting patients in managing their chronic conditions. Large language models (LLMs), such as GPT-4, have shown potential in improving chatbot-based systems in healthcare. However, their adoption in clinical practice faces challenges, including reliability, the need for clinical trials, and privacy concerns. This paper proposes a general architecture for developing an LLM-based chatbot system that supports chronic patients while addressing privacy and security concerns. The architecture is designed to be independent of specific technologies and health conditions, focusing on data protection legislation compliance. A prototype of the system has been developed for hypertension management, demonstrating its potential for motivating patients to monitor their blood pressure and adhere to prescriptions.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {205–212},
numpages = {8},
keywords = {chatbot, healthcare data privacy, hypertension, personal data store},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@inproceedings{10.1145/3643916.3644424,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644424},
doi = {10.1145/3643916.3644424},
abstract = {Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {190–201},
numpages = {12},
keywords = {ReDoS, DoS attack, large language models, regex generation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@article{10.1145/3640312,
author = {Mo, Kanghua and Ye, Peigen and Ren, Xiaojun and Wang, Shaowei and Li, Wenjun and Li, Jin},
title = {Security and Privacy Issues in Deep Reinforcement Learning: Threats and Countermeasures},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3640312},
doi = {10.1145/3640312},
abstract = {Deep Reinforcement Learning (DRL) is an essential subfield of Artificial Intelligence (AI), where agents interact with environments to learn policies for solving complex tasks. In recent years, DRL has achieved remarkable breakthroughs in various tasks, including video games, robotic control, quantitative trading, and autonomous driving. Despite its accomplishments, security and privacy-related issues still prevent us from deploying trustworthy DRL applications. For example, by manipulating the environment, an attacker can influence an agent’s actions, misleading it to behave abnormally. Additionally, an attacker can infer private training data and environmental information by maliciously interacting with DRL models, causing a privacy breach. In this survey, we systematically investigate the recent progress of security and privacy issues in the context of DRL. First, we present a holistic review of security-related attacks within DRL systems from the perspectives of single-agent and multi-agent systems and review privacy-related attacks. Second, we review and classify defense methods used to address security-related challenges, including robust learning, anomaly detection, and game theory approaches. Third, we review and classify privacy-preserving technologies, including encryption, differential privacy, and policy confusion. We conclude the survey by discussing open issues and possible directions for future research in this field.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {152},
numpages = {39},
keywords = {Deep reinforcement learning, adversarial attack, defense}
}

@inproceedings{10.1145/3613905.3643983,
author = {Li, Tianshi and Das, Sauvik and Lee, Hao-Ping (Hank) and Wang, Dakuo and Yao, Bingsheng and Zhang, Zhiping},
title = {Human-Centered Privacy Research in the Age of Large Language Models},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3643983},
doi = {10.1145/3613905.3643983},
abstract = {The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users’ disclosure behaviors, users’ mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) aims to bring together researchers with backgrounds in usable security and privacy, human-AI collaboration, NLP, or any other related domains to share their perspectives and experiences on this problem, to help our community establish a collective understanding of the challenges, research opportunities, research methods, and strategies to collaborate with researchers outside of HCI.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {581},
numpages = {4},
keywords = {Generative AI, Human-Computer Interaction, Large language models (LLMs), Privacy},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3583740.3626809,
author = {Piggott, Brett and Patil, Siddhant and Feng, Guohuan and Odat, Ibrahim and Mukherjee, Rajdeep and Dharmalingam, Balakrishnan and Liu, Anyi},
title = {Net-GPT: A LLM-Empowered Man-in-the-Middle Chatbot for Unmanned Aerial Vehicle},
year = {2024},
isbn = {9798400701238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583740.3626809},
doi = {10.1145/3583740.3626809},
abstract = {In the dynamic realm of AI, integrating Large Language Models (LLMs) with security systems reshape cybersecurity. LLMs bolster defense against cyber threats but also introduce risks, aiding adversaries in generating malicious content, discovering vulnerabilities, and distorting perceptions. This paper presents Net-GPT, an LLM-empowered offensive chatbot that understands network protocols and launches Unmanned Aerial Vehicles (UAV)-based Man-in-the-middle (MITM) attacks against a hijack communication between UAV and Ground Control Stations (GCS). Facilitated by an edge server equipped with finely tuned LLMs, Net-GPT crafts mimicked network packets between UAV and GCS. Leveraging the adaptability of popular LLMs, Net-GPT produces context-aligned network packets. We fine-tune and assess Net-GPT's LLM-based efficacy, showing its impressive generative accuracy: 95.3% for Llama-2-13B and 94.1% for Llama-2-7B. Smaller LLMs, such as Distil-GPT-2, reach 77.9% predictive capability of Llama-2-7B but are 47\texttimes{} faster. Cost-efficiency tests highlight model quality's impact on accuracy while fine-tuning data quantity enhances predictability on specific metrics. It holds great potential to be used in edge-computing environments with amplified computing capability.},
booktitle = {Proceedings of the Eighth ACM/IEEE Symposium on Edge Computing},
pages = {287–293},
numpages = {7},
keywords = {man-in-the-middle (MITM), large language model, system security, and cyber attack},
location = {Wilmington, DE, USA},
series = {SEC '23}
}

@inproceedings{10.1145/3691620.3695307,
author = {Wu, Yueming and Liu, Chengwei and Xu, Zhengzi and Zhang, Lyuye and Zhang, Yiran and Zhu, Zhiling and Liu, Yang},
title = {The Software Genome Project: Unraveling Software Through Genetic Principles},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695307},
doi = {10.1145/3691620.3695307},
abstract = {Open-source software is crucial to modern development, but its complexity creates challenges in quality, security, and management. Current governance approaches excel at collaboration but struggle with decentralized management and security. With the rise of large language models (LLM)-based software engineering, the need for a finer-grained understanding of software composition is more urgent than ever. To address these challenges, inspired by the Human Genome Project, we treat the software source code as software DNA and propose the Software Genome Project (SGP), which is geared towards the secure monitoring and exploitation of open-source software. By identifying and labeling integrated and classified code features at a fine-grained level, and effectively identifying safeguards for functional implementations and nonfunctional requirements at different levels of granularity, the SGP could build a comprehensive set of software genome maps to help developers and managers gain a deeper understanding of software complexity and diversity. By dissecting and summarizing functional and undesirable genes, SGP could help facilitate targeted software optimization, provide valuable insight and understanding of the entire software ecosystem, and support critical development tasks such as open source governance. SGP could also serve as a comprehensive dataset with abundant semantic labeling to enhance the training of LLMs for code. Based on these, we expect SGP to drive the evolution of software development towards more efficient, reliable, and sustainable software solutions.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2319–2323},
numpages = {5},
keywords = {software genes, software composition, OSS governance},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3703323.3703733,
author = {S, Tejaswi and VN, Sastry and S, Durga Bhavani},
title = {Mobile Application Security Assessment through Large Language Models (LLMs) based on User Reviews},
year = {2025},
isbn = {9798400711244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703323.3703733},
doi = {10.1145/3703323.3703733},
abstract = {Digital payments via mobile UPI (Unified Payments Interface) applications have recently become increasingly popular due to their ease of use. However, the users face significant security risks during the usage of the mobile applications. Hence security analysis based on user reviews gains importance. The existing literature focuses on bugs, confidentiality and usability issues providing partial insights into application security. In this paper, we present a systematic approach to assess the security of the mobile UPI applications based on user reviews with respect to eleven security goals given by GoI. We model the problem as a Multi-Label Text Categorization problem in which each review is labelled with a subset of the security goals. We developed a zero-shot prompt-based method using Large Language Models (LLMs), which do not need extensive labelled data, to categorize the user reviews. By leveraging expert knowledge and employing group consensus method, we have created an extremely useful annotated dataset of 4,000 mobile application user reviews for ground truth validation. The proposed Multi-Label Text Categorization approach is tested on three datasets including ours and two public datasets from different domains like News and Medical articles. Our approach outperforms the other deep learning methods like SBERT, ROBERTa etc. by showing 14.6% improvement on the best result, on average, with respect to F1 score and 34.1% improvement on Hamming loss. We have collected approximately 1.7 million user reviews on 30 UPI applications from the Google Play Store. We conduct qualitative analysis to demonstrate our approach and offer actionable recommendations for developers with respect to app security.},
booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)},
pages = {261–269},
numpages = {9},
keywords = {Graph Representation Learning, Heterogeneous Networks, Graph Neural Networks, Bibliographic Networks.},
location = {
},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3611643.3613083,
author = {Happe, Andreas and Cito, J\"{u}rgen},
title = {Getting pwn’d by AI: Penetration Testing with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613083},
doi = {10.1145/3611643.3613083},
abstract = {The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2082–2086},
numpages = {5},
keywords = {large language models, penetration testing, security testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3664647.3681379,
author = {Liu, Yi and Cai, Chengjun and Zhang, Xiaoli and Yuan, Xingliang and Wang, Cong},
title = {Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681379},
doi = {10.1145/3664647.3681379},
abstract = {Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3578–3586},
numpages = {9},
keywords = {jailbreak attacks, large vision language model, red teaming},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3729403,
author = {Du, Xiaohu and Wen, Ming and Wang, Haoyu and Wei, Zichao and Jin, Hai},
title = {Statement-Level Adversarial Attack on Vulnerability Detection Models via Out-of-Distribution Features},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729403},
doi = {10.1145/3729403},
abstract = {Code vulnerability detection is crucial to ensure software security. Recent advancements, particularly with the emergence of Code Pre-Trained Models (CodePTMs) and Large Language Models (LLMs), have led to significant progress in this area. However, these models are easily susceptible to adversarial attacks, where even slight input modifications can lead the models to generate opposite results. Existing adversarial approaches, such as identifier replacement, code transformation, and dead code insertion, demonstrate promising performance but still face several limitations. First, the perturbations applied to the target code are relatively constrained (e.g., identifier replacement can only be applied to a small subset of tokens within the entire codebase). Second, the design of perturbed tokens lacks specificity in forcing the model to make incorrect predictions (e.g., they are generated by random selection or context-based prediction). Such limitations lead to the inefficiency and ineffectiveness of existing attacks. To address these issues, we propose SLODA (Statement-level OOD Features driven Adversarial Attack), which introduces two types of out-of-distribution (OOD) features: universal features via code deoptimization and label-specific features extracted from existing mispredicted and adversarial examples. These statement-level OOD features not only expand the perturbation scope, but can also significantly reduce the search space due to their inherently adversarial nature. Moreover, since the OOD features are extracted from existing code and the attack considers the context of the target code, they are more difficult to detect. Our extensive experiments across 15 models demonstrate that SLODA surpasses existing five state-of-the-art approaches in terms of the effectiveness, efficiency, and detection resistance. Furthermore, the adversarial examples generated by SLODA also exhibit promising performance to enhance model robustness.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE133},
numpages = {24},
keywords = {Adversarial Attack, Large Language Model, Vulnerability Detection}
}

@inproceedings{10.1145/3488932.3527286,
author = {Foley, Myles and Hicks, Chris and Highnam, Kate and Mavroudis, Vasilios},
title = {Autonomous Network Defence using Reinforcement Learning},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3527286},
doi = {10.1145/3488932.3527286},
abstract = {In the network security arms race, the defender is significantly disadvantaged as they need to successfully detect and counter every malicious attack. In contrast, the attacker needs to succeed only once. To level the playing field, we investigate the effectiveness of autonomous agents in a realistic network defence scenario. We first outline the problem, provide the background on reinforcement learning and detail our proposed agent design. Using a network environment simulation, with 13 hosts spanning 3 subnets, we train a novel reinforcement learning agent and show that it can reliably defend continual attacks by two advanced persistent threat (APT) red agents: one with complete knowledge of the network layout and another which must discover resources through exploration but is more general.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {1252–1254},
numpages = {3},
keywords = {reinforcement learning, network security, autonomous network defence},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3649476.3660378,
author = {Miftah, Samit Shahnawaz and Srivastava, Amisha and Kim, Hyunmin and Basu, Kanad},
title = {Assert-O: Context-based Assertion Optimization using LLMs},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649476.3660378},
doi = {10.1145/3649476.3660378},
abstract = {Modern computing relies on System-on-Chips (SoCs), integrating IP cores for complex functions. However, this integration introduces vulnerabilities, necessitating rigorous hardware security validation. The effectiveness of this validation depends on the security properties embedded in the SoC. Recent studies explore large language models (LLMs) for generating security properties, but these may not be directly optimized for validation. Manual intervention remains necessary to reduce their number. Security validation methods that rely on human expertise are not scalable as they are time-intensive and prone to human error. In order to address these issues, we introduce Assert-O, an automated framework designed to derive security properties from SoC documentation and optimize the generated properties. It also ranks the properties based on the security vulnerabilities they are associated with, thereby streamlining the validation process. Our method leverages hardware documentation to initially create security properties, which are subsequently consolidated and prioritized based on their level of criticality. This approach serves to expedite the validation procedure. Assert-O is trained on documentation of six IPs from OpenTitan. To evaluate our proposed method, Assert-O was assessed on five other modules from OpenTitan. Assert-O was able to generate 183 properties, which was further optimized to reduce them to 138 properties. Subsequently, these properties were ranked based on their impact on the security of the overall system.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024},
pages = {233–239},
numpages = {7},
keywords = {Hardware Security, Hardware Verification, Large Language Models},
location = {Clearwater, FL, USA},
series = {GLSVLSI '24}
}

@inproceedings{10.1145/3637528.3671602,
author = {Liu, Xuanqing and Wang, Runhui and Song, Yang and Kong, Luyang},
title = {GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671602},
doi = {10.1145/3637528.3671602},
abstract = {Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5476–5486},
numpages = {11},
keywords = {generative modeling, retrieval augmented generation, schema matching},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3650212.3680399,
author = {Qiu, Yuxin and Hu, Jie and Zhang, Qian and Yin, Heng},
title = {Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680399},
doi = {10.1145/3650212.3680399},
abstract = {Recent advancements in large language models (LLMs) have exhibited promising capabilities in addressing various tasks such as defect detection and program repair. Despite their prevalence, LLMs still face limitations in effectively handling these tasks. Common strategies to adapt them and improve their performance for specific tasks involve fine-tuning models based on user data or employing in-context learning with examples of desired inputs and outputs.    However, they pose challenges for practical adoption due to the need for extensive computational resources, high-quality data, and continuous maintenance. Furthermore, neither strategy can explain or reason about the deficiencies of LLMs in the given tasks.         We propose Calico to address the high cost of fine-tuning, eliminate the necessity for task-specific examples, and provide explanations of LLM deficiency. At the heart of Calico is an evolutionary approach that interleaves knowledge calibration and AI deficiency diagnosis. The key essence of Calico is as follows. First, it focuses on identifying knowledge gaps in LLMs’ program comprehension. Second, it conducts automated code refactoring to integrate the overlooked knowledge into the source code for mitigating those gaps. Third, it employs what-if analysis and counterfactual reasoning to determine a minimum set of overlooked knowledge necessary to improve the performance of LLMs in code tasks.        We have extensively evaluated Calico over 8,938 programs on three most commonly seen code tasks. Our experimental results show that vanilla ChatGPT cannot fully understand code structures. With knowledge calibration, Calico improves it by 20% and exhibits comparable proficiency compared to fine-tuned LLMs. Deficiency diagnosis contributes to 8% reduction in program sizes while ensuring performance. These impressive results demonstrate the feasibility of utilizing a vanilla LLM for automated software engineering (SE) tasks, thereby avoiding the high computational costs associated with a fine-tuned model.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1785–1797},
numpages = {13},
keywords = {Software engineering, large language model, software testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3643991.3644933,
author = {Oishwee, Sahrima Jannat and Stakhanova, Natalia and Codabux, Zadia},
title = {Large Language Model vs. Stack Overflow in Addressing Android Permission Related Challenges},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644933},
doi = {10.1145/3643991.3644933},
abstract = {The Android permission system regulates access to sensitive mobile device resources such as camera and location. To access these resources, third-party developers need to request permissions. However, the Android permission system is complex and fast-evolving, presenting developers with numerous challenges surrounding compatibility issues, misuse of permissions, and vulnerabilities related to permissions. Our study aims to explore whether Large Language Models (LLMs) can serve as a reliable tool to assist developers in using Android permissions correctly and securely, thereby reducing the risks of misuse and security vulnerabilities in apps. In our study, we analyzed 1,008 Stack Overflow questions related to Android permissions and their accepted answers. In parallel, we generate answers to these questions using a popular LLM tool, ChatGPT. We focused on how well the ChatGPT's responses align with the accepted answers on Stack Overflow. Our findings show that above 50% of ChatGPT's answers align with Stack Overflow's accepted answers. ChatGPT offers better-aligned responses for challenges related to Documentation and Conceptual Understanding, while it provides less aligned answers for Debugging-related issues. In addition, we found that ChatGPT provides more consistent answers for 73.27% questions. Our study demonstrates the potential for using LLMs such as ChatGPT as a supporting tool to help developers navigate Android permission-related problems.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {373–383},
numpages = {11},
keywords = {Android permissions, stack overflow, large language model (LLM)},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3728903,
author = {Shir, Rony and Surve, Priyanka and Elovici, Yuval and Shabtai, Asaf},
title = {Robust Vulnerability Detection across Compilations: LLVM-IR vs. Assembly with Transformer Model},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728903},
doi = {10.1145/3728903},
abstract = {Detecting vulnerabilities in binary files is a challenging task in cybersecurity, particularly when source code is unavailable and the compilation process and its parameters are unknown. Existing deep learning-based detection methods often rely on knowing a binary’s specific compilation settings, which may limit their ability to perform well on other types of binaries. In this research, we provide a thorough comparison of assembly representation and LLVM-IR to identify which representation is more robust and suitable when compilation parameters are unknown. The choice of representation significantly influences detection accuracy. Another contribution of this paper is the use of CodeBERT, a transformer-based model, as a classification tool for detecting vulnerabilities in scenarios where the compilation process is unknown. This study applies   transformer models to the task of multi-class vulnerability detection in the LLVM-IR domain, with a focus on binary-derived representations. While recent research has explored the use of transformers for vulnerability analysis in source code and raw binary instruction streams, systematic evaluation as a classifier at the LLVMIR level remains limited. Prior work has commonly relied on RNN-based methods, which are considered state-of-the-art for this task; however, these models struggle to capture long-range dependencies effectively. To address this limitation, we extend transformer-based classification to LLVM-IR produced from binaries and provide a comprehensive evaluation in this setting. Our results highlight the potential of this approach to strengthen system security across diverse binary configurations.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA028},
numpages = {22},
keywords = {CodeBERT, LLVM-IR, architecture, assembly, compilation, compiler, multi-class classification, transformer, vulnerability detection}
}

@article{10.1145/3410048.3410105,
author = {London, Palma and Vardi, Shai and Wierman, Adam},
title = {Logarithmic Communication for Distributed Optimization in Multi-Agent Systems},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3410048.3410105},
doi = {10.1145/3410048.3410105},
abstract = {Classically, the design of multi-agent systems is approached using techniques from distributed optimization such as dual descent and consensus algorithms. Such algorithms depend on convergence to global consensus before any individual agent can determine its local action. This leads to challenges with respect to communication overhead and robustness, and improving algorithms with respect to these measures has been a focus of the community for decades.This paper presents a new approach for multi-agent system design based on ideas from the emerging field of local computation algorithms. The frameworkwe develop, LOcal Convex Optimization (LOCO), is the first local computation algorithm for convex optimization problems and can be applied in a wide-variety of settings. We demonstrate the generality of the framework via applications to Network Utility Maximization (NUM) and the distributed training of Support Vector Machines (SVMs), providing numerical results illustrating the improvement compared to classical distributed optimization approaches in each case.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jul,
pages = {97–98},
numpages = {2},
keywords = {multi-agent systems, distributed optimization, distributed algorithms}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473961,
author = {Yang, Zhuoping and Ji, Shixin and Chen, Xingzhen and Zhuang, Jinming and Zhang, Weifeng and Jani, Dharmesh and Zhou, Peipei},
title = {Challenges and Opportunities to Enable Large-Scale Computing via Heterogeneous Chiplets},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473961},
doi = {10.1109/ASP-DAC58780.2024.10473961},
abstract = {Fast-evolving artificial intelligence (AI) algorithms such as large language models have been driving the ever-increasing computing demands in today's data centers. Heterogeneous computing with domain-specific architectures (DSAs) brings many opportunities when scaling up and scaling out the computing system. In particular, heterogeneous chiplet architecture is favored to keep scaling up and scaling out the system as well as to reduce the design complexity and the cost stemming from the traditional monolithic chip design. However, how to interconnect computing resources and orchestrate heterogeneous chiplets is the key to success. In this paper, we first discuss the diversity and evolving demands of different AI workloads. We discuss how chiplet brings better cost efficiency and shorter time to market. Then we discuss the challenges in establishing chiplet interface standards, packaging, and security issues. We further discuss the software programming challenges in chiplet systems.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {765–770},
numpages = {6},
keywords = {chiplet, interconnect, advanced packaging, security, programming abstraction, heterogeneous computing, large language model (LLM), generative AI},
location = {Incheon, Republic of Korea},
series = {ASPDAC '24}
}

@inproceedings{10.1145/3629527.3651406,
author = {Singh, Kuldeep and Phalak, Chetan and Chahal, Dheeraj and Kunde, Shruti and Singhal, Rekha},
title = {SuperArch: Optimal Architecture Design for Cloud Deployment},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3651406},
doi = {10.1145/3629527.3651406},
abstract = {The success of application migration to cloud depends on multiple factors such as achieving expected performance, optimal cost on deployment, data security etc. The application migration process starts with the architecture design, mapping technical and business specifications to the appropriate services in cloud. However, cloud vendors offer numerous services for each service type and requirement. The onus of selecting the optimal service from the pool lies with the user. Identifying an optimal service for a specific component or application requirement is a daunting task and necessitates a deep understanding of each cloud service offered.This paper introduces SuperArch, a supervised architecture design tool designed to facilitate optimal selection and configuration of cloud services. We propose utilization of Large Language Models (LLM) for extracting information from user requirements and specifications, aiding in optimal selection of cloud services. Additionally, SuperArch maps workloads to the cloud services to generate optimal configurations of the cloud service and estimate performance and cost of the entire architecture.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {91–92},
numpages = {2},
keywords = {cloud deployment, performance and cost estimation},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.1145/3393691.3394197,
author = {London, Palma and Vardi, Shai and Wierman, Adam},
title = {Logarithmic Communication for Distributed Optimization in Multi-Agent Systems},
year = {2020},
isbn = {9781450379854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393691.3394197},
doi = {10.1145/3393691.3394197},
abstract = {Classically, the design of multi-agent systems is approached using techniques from distributed optimization such as dual descent and consensus algorithms. Such algorithms depend on convergence to global consensus before any individual agent can determine its local action. This leads to challenges with respect to communication overhead and robustness, and improving algorithms with respect to these measures has been a focus of the community for decades.This paper presents a new approach for multi-agent system design based on ideas from the emerging field of local computation algorithms. The framework we develop, LOcal Convex Optimization (LOCO), is the first local computation algorithm for convex optimization problems and can be applied in a wide-variety of settings. We demonstrate the generality of the framework via applications to Network Utility Maximization (NUM) and the distributed training of Support Vector Machines (SVMs), providing numerical results illustrating the improvement compared to classical distributed optimization approaches in each case.},
booktitle = {Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {97–98},
numpages = {2},
keywords = {multi-agent systems, distributed optimization, distributed algorithms},
location = {Boston, MA, USA},
series = {SIGMETRICS '20}
}

@inproceedings{10.1145/3643795.3648388,
author = {Gandhi, Shubham and Patwardhan, Manasi and Khatri, Jyotsana and Vig, Lovekesh and Medicherla, Raveendra Kumar},
title = {Translation of Low-Resource COBOL to Logically Correct and Readable Java leveraging High-Resource Java Refinement},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648388},
doi = {10.1145/3643795.3648388},
abstract = {Automated translation of legacy code to modern programming languages is the need of the hour for modernizing enterprise systems. This work specifically addresses automated COBOL to Java translation. Traditional rule-based tools for this perform statement-wise translation, overlooking possible modularization and refactoring of the source COBOL code to translate to human-readable target Java code. Our investigation reveals that state-of-the-art Large Language Models (LLMs) in the domain of code encounter difficulties with regard to logical correctness and readability when directly translating low-resource COBOL code to Java. To address these challenges, we propose an LLM-based workflow, leveraging temperature sampling and refinement-based strategies, to not only ensure logical correctness of the translation but also maximize the readability of the target Java code. We exploit the fact that, due to their extensive exposure to human-written Java codes during pre-training, the LLMs are more equipped with profound comprehension and capability for refining translated Java codes than COBOL to Java translation. With a dataset sourced from CodeNet, we perform sequential refinement of the translated high-resource Java code with execution-guided logic feedback followed by LLM-based readability feedback. We demonstrate that this yields better performance in terms of logical correctness (81.99% execution accuracy) and readability (0.610 score), than LLM based translation with test cases and readability guidance (60.25% and 0.539) or refinement of the translation task itself (77.95% and 0.572).},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {46–53},
numpages = {8},
keywords = {code translation, low resource programming languages, large language models, code readability, self-refinement},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.5555/3463952.3464225,
author = {Arias, Jaime and Penczek, Wojciech and Petrucci, Laure and Sidoruk, Teofil},
title = {ADT2AMAS: Managing Agents in Attack-Defence Scenarios},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Expressing attack-defence trees (ADTrees) in a multi-agent setting allows for studying a new aspect of security scenarios, namely how the number of agents and their task assignment impact the performance of attacking and defending strategies executed by agent coalitions. Our tool ADT2AMAS allows for transforming ADTrees into extended asynchronous multi-agent systems and computing an optimal schedule with the minimal number of agents. ADT2AMAS is integrated within the graphical verification platform CosyVerif, but can also be run standalone.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1749–1751},
numpages = {3},
keywords = {asynchronous multi-agent systems, attack-defence trees, scheduling},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3728878,
author = {Yu, Lei and Huang, Zhirong and Yuan, Hang and Cheng, Shiqi and Yang, Li and Zhang, Fengjun and Shen, Chenjie and Ma, Jiajia and Zhang, Jingyuan and Lu, Junyi and Zuo, Chun},
title = {Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728878},
doi = {10.1145/3728878},
abstract = {Smart contract vulnerability detection is a critical challenge in the rapidly evolving blockchain landscape. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensiveness and sufficient quality, with limited vulnerability type coverage and insufficient distinction between high-quality and low-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Through our empirical analysis, we found that even after continual pre-training and supervised fine-tuning, LLMs still exhibit limitations in precisely understanding the execution order of state changes in smart contracts, which can lead to incorrect vulnerability explanations despite making correct detection decisions. These limitations result in poor detection performance, leading to potentially severe financial losses. To address these challenges, we propose Smart-LLaMA-DPO, an advanced detection method based on the LLaMA-3.1-8B. First, we construct a comprehensive dataset covering four vulnerability types and machine-unauditable vulnerabilities, containing labels, detailed explanations, and precise vulnerability locations for Supervised Fine-Tuning (SFT), as well as paired high-quality and low-quality outputs for Direct Preference Optimization (DPO). Second, we perform continual pre-training using large-scale smart contract code to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct supervised fine-tuning with our comprehensive dataset. Finally, we apply DPO, which leverages human feedback to improve the quality of generated explanations. Smart-LLaMA-DPO utilizes a specially designed loss function that encourages the LLM to increase the probability of preferred outputs while decreasing the probability of non-preferred outputs, thereby enhancing the LLM's ability to generate high-quality explanations. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation demonstrate the superior quality of explanations generated by Smart-LLaMA-DPO in terms of correctness, thoroughness, and clarity.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA009},
numpages = {24},
keywords = {Direct Preference Optimization, Large Language Models, Smart Contract}
}

@inproceedings{10.1145/3689217.3690618,
author = {Lee, Dylan and Xie, Shaoyuan and Rahman, Shagoto and Pat, Kenneth and Lee, David and Chen, Qi Alfred},
title = {"Prompter Says": A Linguistic Approach to Understanding and Detecting Jailbreak Attacks Against Large-Language Models},
year = {2024},
isbn = {9798400712098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689217.3690618},
doi = {10.1145/3689217.3690618},
abstract = {Large language models (LLMs) designed for safety and harmlessness remain vulnerable to adversarial exploitation. This susceptibility is evidenced by the frequent occurrence of "jailbreak'' attacks, which successfully induce undesired behaviors using carefully designed prompts. This study investigates how to distinguish between safe and harmful prompts for LLMs using linguistic analysis. We first assemble a comprehensive dataset of labeled prompts (benign vs. malicious) from existing research. By analyzing the syntactic, lexical, and semantic features of these prompts, we developed a rubric to identify prompt intent solely from text. This rubric inspired the creation of a machine learning model that incorporates these features. We tested various machine learning algorithms, including logistic regression, support vector machines, and multi-layer perceptrons to understand how different feature representations interact with each model type. Our results reveal optimal combinations of classifiers and features for preemptively flagging malicious prompts before they reach LLMs. This model serves as a foundational tool, embedding the principles of our rubric for automated malicious prompt detection. Furthermore, we explore the linguistic differences across various languages, examining semantic propensity, textual structure, and syntactic variations, and how these impact the effectiveness of jailbreaking attempts on multiple LLMs. This research contributes to the significant problem of LLM security and reliability, paving the way for future advancements.},
booktitle = {Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis},
pages = {77–87},
numpages = {11},
keywords = {jailbreaking, large language model, linguistics, machine learning, natural language processing},
location = {Salt Lake City, UT, USA},
series = {LAMPS '24}
}

@article{10.1145/3729401,
author = {Dhulipala, Hridya and Yadavally, Aashish and Patel, Smit Soneshbhai and Nguyen, Tien N.},
title = {CRISPE: Semantic-Guided Execution Planning and Dynamic Reasoning for Enhancing Code Coverage Prediction},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729401},
doi = {10.1145/3729401},
abstract = {While LLMs excel in understanding source code and descriptive texts for tasks like code generation, code completion, etc., they exhibit weaknesses in predicting dynamic program behavior, such as code coverage and runtime error detection, which typically require program execution. Aiming to advance the capability of LLMs in reasoning and predicting the program behavior at runtime, we present CRISPE (short for Coverage Rationalization and Intelligent Selection ProcedurE), a novel approach for code coverage prediction. CRISPE guides an LLM in simulating program execution via an execution plan based on two key factors: (1) program semantics of each statement type, and (2) the observation of the set of covered statements at the current “execution” step relative to all feasible code coverage options. We formulate code coverage prediction as a process of semantic-guided execution-based planning, where feasible coverage options are utilized to assess whether the LLM is heading in the correct reasoning. We enhance the traditional generative task with the retrieval-based framework on feasible options of code coverage. Our experimental results show that CRISPE achieves high accuracy in coverage prediction in terms of both exact-match and statement-match coverage metrics, improving over the baselines. We also show that with semantic-guiding and dynamic reasoning from CRISPE, the LLM generates more correct planning steps. To demonstrate CRISPE’s usefulness, we used it in the downstream task of statically detecting runtime error(s) in incomplete code snippets with the given inputs.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE131},
numpages = {22},
keywords = {AI4SE, Large Language Models, Planning, Predictive Code Coverage}
}

@inproceedings{10.1145/3689933.3690831,
author = {Huang, Junjie and Zhu, Quanyan},
title = {PenHeal: A Two-Stage LLM Framework for Automated Pentesting and Optimal Remediation},
year = {2024},
isbn = {9798400712296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689933.3690831},
doi = {10.1145/3689933.3690831},
abstract = {Recent advances in Large Language Models (LLMs) have shown significant potential in enhancing cybersecurity defenses against sophisticated threats. LLM-based penetration testing is an essential step in automating system security evaluations by identifying vulnerabilities. Remediation, the subsequent crucial step, addresses these discovered vulnerabilities. Since details about vulnerabilities, exploitation methods, and software versions offer crucial insights into system weaknesses, integrating penetration testing with vulnerability remediation into a cohesive system has become both intuitive and necessary.  This paper introduces PenHeal, a two-stage LLM-based framework designed to autonomously identify and mitigate security vulnerabilities. The framework integrates two LLM-enabled components: the Pentest Module, which detects multiple vulnerabilities within a system, and the Remediation Module, which recommends optimal remediation strategies. The integration is facilitated through Counterfactual Prompting and an Instructor module that guides the LLMs using external knowledge to explore multiple potential attack paths effectively. Our experimental results demonstrate that PenHeal not only automates the identification and remediation of vulnerabilities but also significantly improves vulnerability coverage by 31%, increases the effectiveness of remediation strategies by 32%, and reduces the associated costs by 46% compared to baseline models. These outcomes highlight the transformative potential of LLMs in reshaping cybersecurity practices, offering an innovative solution to defend against cyber threats.},
booktitle = {Proceedings of the Workshop on Autonomous Cybersecurity},
pages = {11–22},
numpages = {12},
keywords = {cybersecurity automation, llms, penetration testing, retrieval-augmented generation, vulnerability remediation},
location = {Salt Lake City, UT, USA},
series = {AutonomousCyber '24}
}

@inproceedings{10.1145/3624062.3624128,
author = {Quan, Andres and Howell, Leah and Greenberg, Hugh},
title = {Heterogeneous Syslog Analysis: There Is Hope},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624128},
doi = {10.1145/3624062.3624128},
abstract = {Identifying system hardware failures and anomalies is a unique challenge in heterogeneous testbed clusters because of variation in the ways that the system log reports errors and warnings. We present a novel approach for the real-time classification of syslog messages generated by a heterogeneous testbed cluster to proactively identify potential hardware issues and security events. By integrating machine learning models with high-performance computing systems, our system facilitates continuous system health monitoring. The paper introduces a taxonomy for classifying system issues into actionable categories of problems, while filtering out groups of messages that the system administrators would consider unimportant "noise". Finally, we experiment with using large language models as a message classifier, and share our results and experience with doing so. Results demonstrate promising performance, and more explainable results compared to currently available techniques, but the computational costs may offset the benefits.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {581–587},
numpages = {7},
keywords = {Applications of Large-Language-Models, Cross-platform Software, Error detection, Failure detection, Heterogeneous Clusters, Log Analysis, Monitoring, Syslog, Testbeds},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3718751.3718860,
author = {Zhang, Zhonghao and Yang, Xiaochen and Zeng, Changchang},
title = {A Domain-specific Retrieval-Augmented Generation System for Civil Aviation Safety},
year = {2025},
isbn = {9798400709753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718751.3718860},
doi = {10.1145/3718751.3718860},
abstract = {This paper presents a domain-specific Retrieval Enhanced Generation (RAG) system designed for civil aviation security, to improve the quality of domain-specific query answers. Based on advanced Natural Language Processing (NLP) technology, this system combines large language models, embedding models and similarity search strategy, to optimize the accuracy, relevance and reliability of the generation. We evaluated the impact of different parameters (large language model, embedding model, and Top-k values) on the performance of the RAG system by adjusting them separately. In each set of experiments, we changed only one variable individually, while the others were kept fixed. This ensures that we can clearly observe the independent impact of each parameter change on the performance of the system. The results show that choosing an right large language model and top-k values will significantly improve the output quality, balancing between the accuracy of the answers and the comprehensiveness of the search. Though there's some limitations, such as too much reliance on vector similarity and the limitations of LLMs, the system still provides a tool to provide civil aviation practitioners with timely and accurate information. Future work will explore some improvements such as the use of graphical databases and the use of larger language models. This RAG system shows a step-forward in intelligent, data-driven decision-making in civil aviation safety, has great potential to improve response and safety management in emergency scenarios.},
booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {682–687},
numpages = {6},
keywords = {Civil Aviation Safety, Large Language Models (LLMs), Retrieval Augmented Generation (RAG)},
location = {
},
series = {ICBAR '24}
}

@inproceedings{10.1145/3698364.3709122,
author = {Vaisband, Boris},
title = {Invited: Chiplet-Based Integration - Scale-Down and Scale-Out},
year = {2025},
isbn = {9798400712937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698364.3709122},
doi = {10.1145/3698364.3709122},
abstract = {Motivation: The demand for increased computation and memory in applications such as large language models, has increased well beyond the reticle boundaries of a system-on-chip (SoC). Chiplet-based integration is a paradigm shift that shapes the way we design our future high-performance systems. The concept is to move away from large SoCs that are limited by communication, thermal design power, and reticle size, toward a robust plug-and-play approach, where small, hardened IP heterogeneous off-the-shelf chiplets are seamlessly integrated on a single platform. Problem statement: Recent technological breakthroughs in advanced packaging platforms, have enabled the integration of hundreds to thousands of chiplets within a single platform. Nonetheless, building a functional and efficient ultra-large-scale high-performance computation system, requires overcoming important system-level design challenges. Specifically, short- and long-range communication, power delivery and thermal management, testing, synchronization, hardware security, and others. Approach: In this talk, we will discuss the current state-of-the-art and challenges in chiplet integration as well as the scale-down and scale-out concepts. We will introduce the silicon interconnect fabric (Si-IF), an ultra-large wafer-scale heterogeneous integration platform, for applications such as high-performance computing. We will discuss paths to address the system-level challenges for designing and integrating a high-performance computation system on the Si-IF.},
booktitle = {Proceedings of the 2025 International Symposium on Physical Design},
pages = {217},
numpages = {1},
keywords = {chiplets, high-performance compute, network on if, power delivery and thermal management, silicon interconnect fabric (si-if), wafer-scale heterogeneous integration},
location = {Austin, TX, USA},
series = {ISPD '25}
}

@article{10.1145/3643542,
author = {Deng, Hanhui and Jiang, Jianan and Yu, Zhiwang and Ouyang, Jinhui and Wu, Di},
title = {CrossGAI: A Cross-Device Generative AI Framework for Collaborative Fashion Design},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643542},
doi = {10.1145/3643542},
abstract = {Fashion design usually requires multiple designers to discuss and collaborate to complete a set of fashion designs, and the efficiency of the sketching process is another challenge for personalized design. In this paper, we introduce a fashion design system, CrossGAI, that can support multiple designers to collaborate on different devices and provide AI-enhanced sketching assistance. Based on the design requirements analysis acquired from the formative study of designers, we develop the system framework of CrossGAI implemented by the user-side web-based cross-device design platform working along with the server-side AI-integrated backend system. The CrossGAI system can be agilely deployed in LAN networks which protects the privacy and security of user data. To further improve both the efficiency and the quality of the sketch process, we devised and exploited generative AI modules, including a sketch retrieval module to retrieve sketches according to stroke or sketch drawn, a sketch generation module enabling the generation of fashion sketches consistent with the designer's unique aesthetic, and an image synthesis module that could achieve sketch-to-image synthesis in accordance with the reference image's style. To optimise the computation offloading when multiple user processes are handled in LAN networks, Lyapunov algorithm with DNN actor is utilized to dynamically optimize the network bandwidth of different clients based on their access history to the application and reduce network latency. The performance of our modules is verified through a series of evaluations under LAN environment, which prove that our CrossGAI system owns competitive ability in AIGC-aided designing. Furthermore, the qualitative analysis on user experience and work quality demonstrates the efficiency and effectiveness of CrossGAI system in design work.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {35},
numpages = {27},
keywords = {Computation Offloading, Cross-device Collaboration, Generative AI}
}

@inbook{10.5555/3716662.3716732,
author = {Lamparth, Max and Corso, Anthony and Ganz, Jacob and Mastro, Oriana Skylar and Schneider, Jacquelyn and Trinkunas, Harold},
title = {Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations},
year = {2025},
publisher = {AAAI Press},
abstract = {To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to examine crisis escalation in a fictional US-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as "pacifist" or "aggressive sociopath." When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {807–817},
numpages = {11}
}

@inproceedings{10.1145/3560830.3563732,
author = {Applebaum, Andy and Dennler, Camron and Dwyer, Patrick and Moskowitz, Marina and Nguyen, Harold and Nichols, Nicole and Park, Nicole and Rachwalski, Paul and Rau, Frank and Webster, Adrian and Wolk, Melody},
title = {Bridging Automated to Autonomous Cyber Defense: Foundational Analysis of Tabular Q-Learning},
year = {2022},
isbn = {9781450398800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560830.3563732},
doi = {10.1145/3560830.3563732},
abstract = {Leveraging security automation and orchestration technologies enables security analysts to respond more quickly and accurately to threats. However, current tooling is limited to automating very finely scoped and hand-coded situations, such as quarantining known malware and blocking traffic from known malicious domains. Recent research has sought to bridge the gap between this kind of automated security and autonomous cyber defense, leveraging reinforcement learning (RL) on top of basic automation to enable intelligent response. This paper provides foundational analysis of autonomous agents trained with Tabular Q-Learning through a series of experiments examining a range of network scenarios. Our results demonstrate that off-the-shelf Tabular Q-Learning does not offer a single, superior solution across all scenarios. However, we also find that modifying the underlying state encoding and update function can influence the robustness of the defensive agent to generalize to unseen evaluation environments without a significant loss in accuracy. These results highlight potential optimizations for more advanced RL techniques as well as provide a baseline for others leveraging RL for defensive cyber automation.},
booktitle = {Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security},
pages = {149–159},
numpages = {11},
keywords = {reinforcement learning, network security, intrusion response, autonomous cyber defense},
location = {Los Angeles, CA, USA},
series = {AISec'22}
}

@inproceedings{10.1145/3643991.3648400,
author = {Xiao, Tao and Treude, Christoph and Hata, Hideaki and Matsumoto, Kenichi},
title = {DevGPT: Studying Developer-ChatGPT Conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3648400},
doi = {10.1145/3643991.3648400},
abstract = {This paper introduces DevGPT, a dataset curated to explore how software developers interact with ChatGPT, a prominent large language model (LLM). The dataset encompasses 29,778 prompts and responses from ChatGPT, including 19,106 code snippets, and is linked to corresponding software development artifacts such as source code, commits, issues, pull requests, discussions, and Hacker News threads. This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work. DevGPT enables the study of developer queries, the effectiveness of ChatGPT in code generation and problem solving, and the broader implications of AI-assisted programming. By providing this dataset, the paper paves the way for novel research avenues in software engineering, particularly in understanding and improving the use of LLMs like ChatGPT by developers.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {227–230},
numpages = {4},
keywords = {ChatGPT, LLM, generative AI, dataset},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3689776,
author = {Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu},
title = {Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689776},
doi = {10.1145/3689776},
abstract = {Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations — coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {336},
numpages = {30},
keywords = {Hallucination, Large Language Model, Software Testing}
}

@inproceedings{10.1145/3649217.3653554,
author = {Liu, Suqing and Yu, Zezhu and Huang, Feiran and Bulbulia, Yousef and Bergen, Andreas and Liut, Michael},
title = {Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653554},
doi = {10.1145/3649217.3653554},
abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {388–393},
numpages = {6},
keywords = {computing education, conversational agent, cs1, intelligence concentration, intelligent teaching assistant, intelligent tutoring system, large language models, locally deployable ai, personalized ai agent, retrieval augmented generation, small language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3689942.3694745,
author = {Hallajiyan, Mohammadreza and Dharmalingam, Athish Pranav and Mitra, Gargi and Alemzadeh, Homa and Iqbal, Shahrear and Pattabiraman, Karthik},
title = {SAM: Foreseeing Inference-Time False Data Injection Attacks on ML-enabled Medical Devices},
year = {2024},
isbn = {9798400712388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689942.3694745},
doi = {10.1145/3689942.3694745},
abstract = {The increasing use of machine learning (ML) in medical systems necessitates robust security measures to mitigate potential threats. Current research often overlooks the risk of adversaries injecting false inputs through peripheral devices at inference time, leading to mispredictions in patients' conditions. These risks are hard to foresee and mitigate during the design phase since the system is assembled by end users at the time of use. To address this gap, we introduce SAM, a technique that enables security analysts to perform System Theoretic Process Analysis for Security (STPA-Sec) on ML-enabled medical devices during the design phase. SAM models the medical system as a control structure, with the ML engine as the controller and peripheral devices as potential points for false data injection. It interfaces with state-of-the-art vulnerability databases and Large Language Models (LLMs) to automate the discovery of vulnerabilities and generate a list of possible attack paths. We demonstrate the usefulness of SAM through case studies on two FDA-cleared medical devices: a blood glucose management system and a bone mineral density measurement software. SAM allows security analysts to expedite the security assessment of ML-enabled medical devices at the design phase. This proactive approach mitigates potential patient harm and reduces costs associated with post-deployment security measures.},
booktitle = {Proceedings of the 2024 Workshop on Cybersecurity in Healthcare},
pages = {77–84},
numpages = {8},
keywords = {ML-enabled medical devices, STPA-Sec, false data injection, security assessment},
location = {Salt Lake City, UT, USA},
series = {HealthSec '24}
}

@inproceedings{10.1145/3655693.3655701,
author = {Y\i{}ld\i{}r\i{}m, Recep and Ayd\i{}n, Kerem and \c{C}etin, Or\c{c}un},
title = {Evaluating the Impact of Conventional Code Analysis Against Large Language Models in API Vulnerability Detection},
year = {2024},
isbn = {9798400716515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655693.3655701},
doi = {10.1145/3655693.3655701},
abstract = {In the rapidly changing world of digital technologies, application programming interfaces (APIs) have become extremely important to allow different software applications to communicate with each other. This communication has greatly enhanced the capabilities and functionality of web applications. This shift towards using more APIs in software development marks a major change in how digital services connect with each other. However, this progress also brings certain security concerns. The increasing reliance on APIs underscores the importance of employing tools that allow early detection and remediation of security vulnerabilities. In this paper, we detail a study that engaged 10 static code analysers and four popular Large Language Models (LLMs), each queried with two unique prompts. Our focus was on assessing their ability to detect a compilation of 40 API vulnerabilities in the source code, specifically selected to represent each category within the OWASP Top 10 API Security Risks. Our results revealed significant variations in the performance of these tools. ChatGPT 4 emerged as the most effective LLM, with a detection rate of 62.5% for the first prompt and 42.5% for the second prompt. In contrast, LLaMA 2 showed the lowest effectiveness in both prompts. Meanwhile, static code analyser results showed a generally low detection rate of API vulnerabilities. Snyk led the group with a 25% detection rate, while several analysers such as pylint, Pyre, and Trivy did not detect any vulnerabilities. These findings indicate that while static code analysers are valuable in certain contexts, their effectiveness remains lower than LLMs when appropriately prompted.},
booktitle = {Proceedings of the 2024 European Interdisciplinary Cybersecurity Conference},
pages = {57–64},
numpages = {8},
keywords = {AI in cybersecurity, API security, API vulnerabilities, Large language models, Static code analysis, Vulnerability detection},
location = {Xanthi, Greece},
series = {EICC '24}
}

@inproceedings{10.1145/3722150.3722153,
author = {Hu, Shuyao and Guo, Guiyang and Jiang, Zhuo},
title = {Research on Anomaly Sound Detection Methods Based on Large Models},
year = {2025},
isbn = {9798400711640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722150.3722153},
doi = {10.1145/3722150.3722153},
abstract = {With the continuous development of industrial equipment and security monitoring fields, the demand for intelligent anomaly detection is increasing more and more. Equipment abnormal diagnosis from the acoustic perspective, as a reliable research approach, is widely studied. This article first reviews the development history of audio anomaly detection technology, from rule-based methods to statistical methods, and then to the application of deep learning techniques, with special focus on the great progress made by deep learning technology in audio anomaly detection. The article then discusses in detail the abnormal sound detection techniques using frequency-shift Cepstral coefficients (MFCC) and the application of compact models, such as Gaussian mixture models (GMM), and compares and analyzes the performance of these methods with large models (such as large language models) in anomaly detection. Large models, which learn from a huge amount of audio data, have improved the ability to recognize abnormal sounds, and have shown better strength and generalization ability. Next, using historical audio playback data of a specific data center room, the paper compares the recognition effects of GMM and large models in abnormal sound detection, and finds that large models have higher performance and accuracy in abnormal sound recognition. Finally, the paper analyzes the recognition results of small and large models, highlighting the advantages of large models in abnormal sound detection and possible areas for improvement.},
booktitle = {Proceedings of the 2025 9th International Conference on Control Engineering and Artificial Intelligence},
pages = {15–22},
numpages = {8},
keywords = {Abnormal Sound Detection, Anomaly Recognition Rate, Deep Learning, Gaussian Mixture Models, Hash Compression, Large Language Models, Mel Frequency Cepstral Coefficients},
location = {
},
series = {CCEAI '25}
}

@article{10.1145/3366696,
author = {London, Palma and Vardi, Shai and Wierman, Adam},
title = {Logarithmic Communication for Distributed Optimization in Multi-Agent Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3366696},
doi = {10.1145/3366696},
abstract = {Classically, the design of multi-agent systems is approached using techniques from distributed optimization such as dual descent and consensus algorithms. Such algorithms depend on convergence to global consensus before any individual agent can determine its local action. This leads to challenges with respect to communication overhead and robustness, and improving algorithms with respect to these measures has been a focus of the community for decades.This paper presents a new approach for multi-agent system design based on ideas from the emerging field of local computation algorithms. The framework we develop, LOcal Convex Optimization (LOCO), is the first local computation algorithm for convex optimization problems and can be applied in a wide-variety of settings. We demonstrate the generality of the framework via applications to Network Utility Maximization (NUM) and the distributed training of Support Vector Machines (SVMs), providing numerical results illustrating the improvement compared to classical distributed optimization approaches in each case.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {48},
numpages = {29},
keywords = {multi-agent systems, distributed optimization, distributed algorithms}
}

@inproceedings{10.1145/3637528.3671810,
author = {Zhong, Aoxiao and Mo, Dengyao and Liu, Guiyang and Liu, Jinbu and Lu, Qingda and Zhou, Qi and Wu, Jiesheng and Li, Quanzheng and Wen, Qingsong},
title = {LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671810},
doi = {10.1145/3637528.3671810},
abstract = {Logs are ubiquitous digital footprints, playing an indispensable role in system diagnostics, security analysis, and performance optimization. The extraction of actionable insights from logs is critically dependent on the log parsing process, which converts raw logs into structured formats for downstream analysis. Yet, the complexities of contemporary systems and the dynamic nature of logs pose significant challenges to existing automatic parsing techniques. The emergence of Large Language Models (LLM) offers new horizons. With their expansive knowledge and contextual prowess, LLMs have been transformative across diverse applications. Building on this, we introduce LogParser-LLM, a novel log parser integrated with LLM capabilities. This union seamlessly blends semantic insights with statistical nuances, obviating the need for hyper-parameter tuning and labeled training data, while ensuring rapid adaptability through online parsing. Further deepening our exploration, we address the intricate challenge of parsing granularity, proposing a new metric and integrating human interactions to allow users to calibrate granularity to their specific needs. Our method's efficacy is empirically demonstrated through evaluations on the Loghub-2k and the large-scale LogPub benchmark. In evaluations on the LogPub benchmark, involving an average of 3.6 million logs per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM invocations on average, achieving a 90.6% F1 score for grouping accuracy and an 81.1% for parsing accuracy. These results demonstrate the method's high efficiency and accuracy, outperforming current state-of-the-art log parsers, including pattern-based, neural network-based, and existing LLM-enhanced approaches.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4559–4570},
numpages = {12},
keywords = {AIOps, large language models, log parsing},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3526113.3545616,
author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Social Simulacra: Creating Populated Prototypes for Social Computing Systems},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545616},
doi = {10.1145/3526113.3545616},
abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer’s description of a community’s design—goal, rules, and member personas—and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models’ training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {74},
numpages = {18},
keywords = {social computing, prototyping},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3589335.3651939,
author = {Li, Zengxiang and Hou, Zhaoxiang and Liu, Hui and Li, Tongzhi and Yang, Chengyi and Wang, Ying and Shi, Chao and Xie, Longfei and Zhang, Weishan and Xu, Liang and Liu, Zelei},
title = {Federated Learning in Large Model Era: Vision-Language Model for Smart City Safety Operation Management},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651939},
doi = {10.1145/3589335.3651939},
abstract = {With the tremendous success of large language models such as ChatGPT, artificial intelligence has entered a new era of large models. Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. In this paper, we tackle the problem of building large vision-language intelligent models for specific industrial domains by leveraging the general large models and federated learning. We compare the challenges faced by federated learning in the era of small models and large models from different dimensions, and propose a technical framework for federated learning in the era of large models.Specifically, our framework mainly considers three aspects: heterogeneous model fusion, flexible aggregation methods, and data quality improvement. Based on this framework, we conduct a case study of leading enterprises contributing vision-language data and expert knowledge to city safety operation management. The preliminary experiments show that enterprises can enhance and accumulate their intelligence capabilities through federated learning, and jointly create an intelligent city model that provides high-quality intelligent services covering energy infrastructure security, residential community security and urban operation management.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1578–1585},
numpages = {8},
keywords = {federated learning, large model, multimodal model},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3617555.3617874,
author = {Tihanyi, Norbert and Bisztray, Tamas and Jain, Ridhi and Ferrag, Mohamed Amine and Cordeiro, Lucas C. and Mavroeidis, Vasileios},
title = {The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617874},
doi = {10.1145/3617555.3617874},
abstract = {This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {33–43},
numpages = {11},
keywords = {Vulnerability Classification, Software Security, Large Language Models, Formal Verification, Dataset, Artificial Intelligence},
location = {San Francisco, CA, USA},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3605764.3623985,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

@inproceedings{10.1145/3636534.3697457,
author = {Benazir, Afsara and Lin, Felix Xiaozhu},
title = {Maximizing the Capabilities of Tiny Speech Foundation Models in a Privacy Preserving Manner},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3697457},
doi = {10.1145/3636534.3697457},
abstract = {Voice assistive technologies have given rise to extensive privacy and security concerns. In this paper we investigate whether robust automatic speech recognition (ASR) can be done in a privacy preserving manner on resource constrained devices. For inference, speech recognition systems rely on cloud service providers that host state of the art (SOTA) foundation models (FM) demanding high memory usage and computation complexity. Considering the infeasibility to run these SOTA FM on-device, users are forced to offload their sensitive content to cloud for better inference compromising privacy. Sanitization can be done on speech content at the edge keeping in mind that it has to avoid compromising the ASR accuracy. Our solution is based on the observation that although tiny on-device FM cannot perform robust ASR, they are powerful enough to mask the sensitive segments in speech. Given this observation, we introduce SpeechGuard - a privacy-preserving, hybrid edge-cloud ASR inference engine that is compute inexpensive. We present a novel times-tamp based entity filtering mechanism at the edge and a corresponding confidence score based recovery approach. Our results denote that SpeechGuard offers ASR accuracy and privacy guarantee comparable to existing SOTA FM, while achieving significant speedup and reduction in memory usage.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {1677–1679},
numpages = {3},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/3625704.3625744,
author = {Chan, Victor K. Y.},
title = {Evaluation of e-learning platforms using artificial intelligence (AI) robots: Are the AI robots consistent},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625704.3625744},
doi = {10.1145/3625704.3625744},
abstract = {This article aims to explore the consistency between a few popular generative AI robots in the evaluation of e-learning platforms. The three robots adopted in the study were GPT-4, Sage, and Dragonfly, which were requested to award rating scores to the six major dimensions, namely (1) features and capabilities, (2) ease of use and customization, (3) cost, (4) security, (5) customer support, and (6) scalability, of 10 to 20 currently most popular e-learning platforms. For each of the three robots, the minimum, the maximum, the range, and the standard deviation of the rating scores for each of the six dimensions were computed across all the e-learning platforms. The rating score difference for each of the six dimensions between any pair of robots was calculated for each platform. The mean of the absolute value, the minimum, the maximum, the range, and the standard deviation of the differences for each dimensions between each pair of robots were calculated across all platforms. Finally, a Cronbach alpha coefficient of the rating scores was computed for each of the six dimensions between all the three robots across all the e-learning platforms. The computational results were to reveal whether the three robots accorded discrimination in evaluating each dimension across the platforms and whether there was consistency between the three robots in evaluating each dimension across the platforms. Among some auxiliary results, it was found that the evaluation by the three robots was severely inconsistent for the two dimensions cost and security, inconsistent to a lesser extent for the dimension scalability, and consistent for the remaining three dimensions.},
booktitle = {Proceedings of the 7th International Conference on Education and Multimedia Technology},
pages = {96–100},
numpages = {5},
keywords = {E-learning platforms, artificial intelligence, consistency, evaluation, learning management systems},
location = {Tokyo, Japan},
series = {ICEMT '23}
}

@inproceedings{10.1145/3660829.3660848,
author = {Kuang, Peng and S\"{o}derberg, Emma and H\"{o}st, Martin},
title = {Developers' Perspective on Today's and Tomorrow's Programming Tool Assistance: A Survey},
year = {2024},
isbn = {9798400706349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660829.3660848},
doi = {10.1145/3660829.3660848},
abstract = {Software development is a complex activity that needs a lot of tool assistance. Over the years there has been a lot of effort put into development of automated assistance to help with activities such as detection of issues via program analysis, or refactoring of code. Recently, the landscape of developer tool assistance is being disrupted with the entry of AI tools, such as Copilot and ChatGPT, powered via Large Language Models. Other kinds of tool assistance, for instance, gaze-driven assistance, is around the corner. What are programmers’ perceptions on tool assistance today? What do they see as good directions for the future? In this paper, we present the results of a survey where we asked developers about their programming practices, experience with program analysis, and attitudes and views on enabling technologies, like AI and eye-tracking. We received 68 replies from a diverse group of developers from 12 countries. We found that 50% of the participants use program analysis and that many participants (N=28) already use AI-enabled tools for programming. We found that our participants were positive toward AI-powered tools, neutral toward eye-tracking, and negative toward gamification. We discuss these and other findings and point out directions for future work.},
booktitle = {Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
pages = {108–116},
numpages = {9},
keywords = {developer tools, eye tracking, machine learning, program analysis, programming},
location = {Lund, Sweden},
series = {Programming '24}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473927,
author = {Fu, Weimin and Li, Shijie and Zhao, Yifang and Ma, Haocheng and Dutta, Raj and Zhang, Xuan and Yang, Kaichen and Jin, Yier and Guo, Xiaolong},
title = {Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473927},
doi = {10.1109/ASP-DAC58780.2024.10473927},
abstract = {In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware-specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi-1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset---comprising small, medium, and large subsets---and focused our efforts on pre-training using the medium dataset. This approach harnesses the compact yet efficient architecture of the Phi-1.5B model. The creation of this first pre-trained, hardware domain-specific large language model marks a significant advancement, offering improved performance in hardware design and verification tasks and illustrating a promising path forward for AI applications in the semiconductor sector.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {349–354},
numpages = {6},
keywords = {large language model, hardware design, hardware verification, generative AI},
location = {Incheon, Republic of Korea},
series = {ASPDAC '24}
}

@inproceedings{10.1145/3637528.3671650,
author = {Xing, Mingzhe and Zhang, Rongkai and Xue, Hui and Chen, Qi and Yang, Fan and Xiao, Zhen},
title = {Understanding the Weakness of Large Language Model Agents within a Complex Android Environment},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671650},
doi = {10.1145/3637528.3671650},
abstract = {Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6061–6072},
numpages = {12},
keywords = {ai agent, large language model, task planning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@proceedings{10.1145/3664646,
title = {AIware 2024: Proceedings of the 1st ACM International Conference on AI-Powered Software},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st ACM International Conference on AI-Powered Software (AIware), held on 15th and 16th July 2024 in Porto de Galinhas, Brazil co-located with the ACM International Conference on the Foundations of Software Engineering (FSE 2024). AIware aims to be an annual conference that brings the software engineering community together in anticipation of the upcoming changes driven by Foundation Models (FMs) and looks at them from the perspective of AI-powered software and their evolution.        AIware 2024 prioritizes fostering discussions about the latest developments in the interdisciplinary field of AIware rather than solely focusing on the presentation of papers. The emphasis is on engaging conversations from diverse backgrounds to identify emerging research challenges and establish a new research agenda for the community in the Foundation Model era. To present papers and for discussions, the two-day conference will have five sessions themed around AIware Vision, SE for AIware, Human - AI Conversation, Security &amp; Safety and AIware for Software Lifecycle Activities. Furthermore, the conference program will include two keynotes and five industry talks. The final session in the conference program will be dedicated to presenting accepted papers of the AIware challenge track.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.5555/3545946.3598826,
author = {Chowdhury, Rohit and Murugan, Raswanth and Subramani, Deepak},
title = {Intelligent Onboard Routing in Stochastic Dynamic Environments using Transformers},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomous marine agents find extensive applications in environmental data collection, naval security, and exploration of harsh ocean regions. As intelligent agents, they must perform onboard routing, collect data about their surroundings and update their route to minimize mission travel time, energy, or data collection. While Markov Decision Processes (MDPs) and Reinforcement Learning (RL) are often used for path planning, they are computationally expensive for onboard routing as they need in-mission re-planning. In the present paper, we develop a novel, deep learning method based on the decision transformers for optimal path planning and onboard routing of autonomous marine agents. The transformer architectures convert the RL-based optimal path planning problem into a supervised learning problem via sequence modeling. Before the mission, during the offline planning phase, the environment is first modeled as a stochastic dynamic ocean flow with dynamically orthogonal flow equations. A training dataset for the transformer model is created by solving the stochastic dynamically orthogonal Hamilton-Jacobi level set partial differential equations or a dynamic programming solution for MDPs. These paths are then processed to obtain sequences of states, actions and returns for our transformer models, where the agent's state is typically its spatio-temporal coordinate and other collectible data. We propose and analyze multiple state modeling choices against the agent's state estimation capabilities and scenarios with multiple target locations. We demonstrate that (i) a trained agent learns to infer the surrounding flow and perform optimal onboard routing when the agent's state estimation is accurate,(ii) specifying the target locations (in case of multiple targets) as a part of the state enables a trained agent to route itself to the correct destination, and (iii) a trained agent is robust to limited noise in state transitions and is capable of reaching target locations in completely new flow scenarios. We extensively showcase end-to-end planning and onboard routing in various canonical and idealized ocean flow scenarios. We analyze the predictions of the transformer models and explain the inner mechanics of learning through a novel visualization of self-attention of actions and states on the trajectories.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1688–1696},
numpages = {9},
keywords = {decision transformers, onboard routing, path planning, reinforcement learning, robot planning and control, supervised learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3585060.3585063,
author = {Lopez, Patty},
title = {Reflections on the Design of Systems that Impact Computers and Society},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/3585060.3585063},
doi = {10.1145/3585060.3585063},
abstract = {Having spent the past year post-retirement working with my alma mater, New Mexico State University's (NMSU) Computer Science department to broaden computing, increase student engagement, and to improve graduation completion, as well as reflecting on the state of computing in society at large, I thought I'd share some observations. In March of this year, I had the opportunity to participate in the SIGCSE 2022 Technical Symposium. I was struck by Dr. Shaundra Daily's plenary keynote, entitled "Diversifying Computing: Real Change Must Come from Within", and her use of the phrase "navigating systems that were not designed for me" as she described her exploration of STEM as a first-generation college student, as both a dance and an engineering student, and as a graduate student preparing for motherhood lacking flexibility during her pregnancy, no maternity leave, no livable stipend, and a lack of affordable childcare, as well as the coping strategies she needed to develop to deal with academic culture. In my work with NMSU this past spring, co-teaching a problem solving course, my work this fall advising CS students, and my board roles serving on the National Academy of Science, Engineering, and Medicine's Roundtable for Systemic Change in Undergraduate STEM Education co-chairing the "Culture of STEM" workgroup, on the Computing Alliance of Hispanic Serving Institution's (CAHSI) Advisory Board, and on the Computing Research Association for Widening Participation (CRA-WP), co-editing the "Expanding the Pipeline" column, it's clear that system design adversely impacts society in terms of determining not only who gets to participate in the design of computer hardware and software, but also who gets to advance in social and economic mobility. Academic institutions are complex systems in need of an overhaul, by the University of California's academic workers strike for better pay and benefits. The design and commercialization of AI without fully understanding the implications of bias and ethics is inherently a system design problem. The application to everything from AI generated art and images (and how to spot deep fakes), the ability of large language models (LLMs) to create volumes of text generated articles that appear legitimate with the capacity to spread hate and misinformation globally are but just a few examples of the potentially horrific impact to society, because humans cannot work at the pace and scale to validate and/or authenticate them, with few if any meaningful domestic and international laws or policies in place to safeguard us.},
journal = {SIGCAS Comput. Soc.},
month = feb,
pages = {9},
numpages = {1},
keywords = {system design, ethics, diversity, bias}
}

@article{10.1145/3731449,
author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Guo, Shikai and Zhang, Yanming and Zhang, Weiming and Yu, Nenghai},
title = {FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3731449},
doi = {10.1145/3731449},
abstract = {Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task, which is crucial in software security fields such as malware analysis and legacy code inspection. However, the inherent high logical complexity of cryptographic algorithms makes their analysis more difficult than that of ordinary code, and the general absence of symbolic information in binaries exacerbates this challenge. Existing methods for cryptographic algorithm identification frequently rely on data or structural pattern matching, which limits their generality and effectiveness while requiring substantial manual effort. In response to these challenges, we present FoC (Figure out the Cryptographic functions), a novel framework that leverages large language models (LLMs) to identify and analyze cryptographic functions in stripped binaries.In FoC, we first build an LLM-based generative model (FoC-BinLLM) to summarize the semantics of cryptographic functions in natural language form, which is intuitively readable to analysts. Subsequently, based on the semantic insights provided by FoC-BinLLM, we further develop a binary code similarity detection model (FoC-Sim), which allows analysts to effectively retrieve similar implementations of unknown cryptographic functions from a library of known cryptographic functions. The predictions of generative model like FoC-BinLLM are inherently difficult to reflect minor alterations in binary code, such as those introduced by vulnerability patches. In contrast, the change-sensitive representations generated by FoC-Sim compensate for the shortcomings to some extent. To support the development and evaluation of these models, and to facilitate further research in this domain, we also construct a comprehensive cryptographic binary dataset and introduce an automatic method to create semantic labels for extensive binary functions. Our evaluation results are promising. FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score, demonstrating superior capability in summarizing the semantics of cryptographic functions. FoC-Sim also surpasses previous best methods with a 52% higher Recall@1 in retrieving similar cryptographic functions. Beyond these metrics, our method has proven its practical utility in real-world scenarios, including cryptographic-related virus analysis and 1-day vulnerability detection.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
keywords = {Binary Code Summarization, Cryptographic Algorithm Identification, Binary Code Similarity Detection, Large Language Models}
}

@inproceedings{10.1145/3673038.3673043,
author = {Ouyang, Bei and Ye, Shengyuan and Zeng, Liekang and Qian, Tianyi and Li, Jingyi and Chen, Xu},
title = {Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-tuning},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673038.3673043},
doi = {10.1145/3673038.3673043},
abstract = {Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. Other studies focus on exploiting the potential of edge devices through resource management optimization, yet are ultimately bottlenecked by the resource wall of individual devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone, enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64 \texttimes{} end-to-end speedup and up to &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $88.16%$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name="icpp24-5-inline1" type="svg"/&gt;&lt;/Formula&gt; reduction in memory footprint.},
booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
pages = {762–771},
numpages = {10},
keywords = {Edge intelligence, data parallelism, large language model, parallel processing, parameter-efficient fine-tuning, pipeline parallelism},
location = {Gotland, Sweden},
series = {ICPP '24}
}

@inproceedings{10.1145/3691620.3695517,
author = {Li, Xueyang and Meng, Guozhu and Liu, Shangqing and Xiang, Lu and Sun, Kun and Chen, Kai and Luo, Xiapu and Liu, Yang},
title = {Attribution-guided Adversarial Code Prompt Generation for Code Completion Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695517},
doi = {10.1145/3691620.3695517},
abstract = {Large language models have made significant progress in code completion, which may further remodel future software development. However, these code completion models are found to be highly risky as they may introduce vulnerabilities unintentionally or be induced by a special input, i.e., adversarial code prompt. Prior studies mainly focus on the robustness of these models, but their security has not been fully analyzed.In this paper, we propose a novel approach AdvPro that can automatically generate adversarial code prompts for these code completion models. AdvPro incorporates 14 code mutation strategies at the granularity of five levels. The mutation strategies are ensured to make no modifications to code semantics, which should be insensitive to the models. Moreover, we leverage gradient attribution to localize the important code as mutation points and speed up adversarial prompt generation. Extensive experiments are conducted on 13 state-of-the-art models belonging to 7 families. The results show that our approach can effectively generate adversarial prompts, with an increased rate of 69.6% beyond the baseline ALERT. By comparing the results of attribution-guided localization, we find that the recognition results of important tokens in input codes are almost identical among different models. This finding reduces the limitation of using open-source alternative models to guide adversarial attacks against closed-source models. The results of the ablation study on the components of AdvPro show that CCMs focus on variable names, but other structures are equally crucial.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1460–1471},
numpages = {12},
keywords = {adversarial prompts, code completion models, attribution-guided localization},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3691620.3695013,
author = {Wu, Yulun and Wen, Ming and Yu, Zeliang and Guo, Xiaochen and Jin, Hai},
title = {Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695013},
doi = {10.1145/3691620.3695013},
abstract = {Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge.Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {393–405},
numpages = {13},
keywords = {vulnerability analysis, vulnerable function, large language model},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3494110.3528242,
author = {Rudd, Ethan M. and Rahman, Mohammad Saidur and Tully, Philip},
title = {Transformers for End-to-End InfoSec Tasks: A Feasibility Study},
year = {2022},
isbn = {9781450391795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494110.3528242},
doi = {10.1145/3494110.3528242},
abstract = {Training a machine learning (ML) model from raw information security (InfoSec) data involves utilizing distinct data types and input formats that require unique considerations compared to more conventional applications of ML like natural language processing (NLP) and computer vision (CV). In this paper, we assess the viability of transformer models in end-to-end InfoSec settings, in which no intermediate feature representations or processing steps occur outside the model. We implement transformer models for two distinct InfoSec data formats - specifically URLs and PE files - in a novel end-to-end approach, and explore a variety of architectural designs, training regimes, and experimental settings to determine the ingredients necessary for performant detection models. We show that in contrast to conventional transformers trained on more standard NLP-related tasks, our URL transformer model requires a different training approach to reach high performance levels. Specifically, we show that 1) pre-training on a massive corpus of unlabeled URL data for an auto-regressive task does not readily transfer to binary classification of malicious or benign URLs, but 2) that using an auxiliary auto-regressive loss improves performance when training from scratch. We introduce a method for mixed objective optimization, which dynamically balances contributions from both loss terms so that neither one of them dominates. We show that this method yields quantitative evaluation metrics comparable to that of several top-performing benchmark classifiers. Unlike URLs, binary executables contain longer and more distributed sequences of information-rich bytes. To accommodate such lengthy byte sequences, we introduce additional context length into the transformer by providing its self-attention layers with an adaptive span similar to Sukhbaatar et al. We demonstrate that this approach performs comparably to well-established malware detection models on benchmark PE file datasets, but also point out the need for further exploration into model improvements in scalability and compute efficiency.},
booktitle = {Proceedings of the 1st Workshop on Robust Malware Analysis},
pages = {21–31},
numpages = {11},
keywords = {transformer, malware detection, malicious url prediction, machine learning, end-to-end learning},
location = {Nagasaki, Japan},
series = {WoRMA '22}
}

@inproceedings{10.1145/3676536.3697118,
author = {Liu, Shang and Lu, Yao and Fang, Wenji and Li, Mengming and Xie, Zhiyao},
title = {OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL Generation},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3697118},
doi = {10.1145/3676536.3697118},
abstract = {The automated generation of design RTL based on large language model (LLM) and natural language instructions has demonstrated great potential in agile circuit design. However, the lack of datasets and benchmarks in the public domain prevents the development and fair evaluation of LLM solutions. This paper highlights our latest advances in open datasets and benchmarks from three perspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in design RTL generation. The benchmark is augmented to 50 hand-crafted designs. Each design provides the design description, test cases, and a correct RTL code. (2) AssertEval, an open-source benchmark assessing the LLM's assertion generation capabilities for RTL verification. The benchmark includes 18 designs, each providing specification, signal definition, and correct RTL code. (3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code data samples. Moreover, we propose a new verification-based method to verify the functionality correctness of training data samples. Based on this technique, we further release a dataset with 7K verified high-quality samples. These three studies are integrated into one framework, providing off-the-shelf support for the development and evaluation of LLMs for RTL code generation and verification. Finally, extensive experiments indicate that LLM performance can be boosted by enlarging the training dataset, improving data quality, and improving the training scheme.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {60},
numpages = {9},
keywords = {LLM-assisted circuit design, electronic design automation},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/3607199.3607242,
author = {Chen, Yizheng and Ding, Zhoujie and Alowain, Lamya and Chen, Xinyun and Wagner, David},
title = {DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607242},
doi = {10.1145/3607199.3607242},
abstract = {We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined. Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. We show that increasing the volume of training data may not further improve the performance of deep learning models for vulnerability detection, but might be useful to improve the generalization ability to unseen projects. We also identify hopeful future research directions. We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {654–668},
numpages = {15},
keywords = {datasets, deep learning, large language models, vulnerability detection},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3628356.3630117,
author = {Dasbach-Prisk, Anmei and Dewitt, Cory and Garcia, Luis},
title = {SensorLoader: Bridging the Gap in Cyber-Physical Reverse Engineering Across Embedded Peripheral Devices},
year = {2023},
isbn = {9798400704406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628356.3630117},
doi = {10.1145/3628356.3630117},
abstract = {Safety-critical cyber-physical systems, such as autonomous vehicles and medical devices, are often driven by notions of state provided by sensor information translated through embedded firmware. This sensor pipeline is often a fragmented supply chain across vendors, and analyzing the associated security properties entails semantic reverse engineering of third-party software, i.e., mapping low-level software representations to cyber-physical models without access to source code. This mapping is a manual, time-consuming, and error-prone process. This paper introduces SensorLoader, a tool designed to automate mapping sensor semantics across all layers of closed-source software representations. SensorLoader exploits open-source knowledge, potentially derived from structured vendor description files or unstructured vendor datasheets, to extract and infer sensor semantics. We leverage large language models to extract sensor semantics from unstructured sources and map the semantics to memory maps and structures used by the Ghidra reverse engineering framework. We formalize the limitations of this automatic extraction and demonstrate how our approach can streamline the reverse engineering process for embedded systems. Preliminary evaluations suggest that SensorLoader can effectively and scalably aid in identifying vulnerabilities and deviations from expected behaviors, offering a more efficient pathway to secure cyber-physical systems.},
booktitle = {Proceedings of the First International Workshop on Security and Privacy of Sensing Systems},
pages = {30–36},
numpages = {7},
keywords = {embedded systems, reverse engineering, sensor security},
location = {Istanbul, Turkiye},
series = {SensorsS&amp;P}
}

@inproceedings{10.1145/3632620.3671097,
author = {Ali, Murtaza and Rao, Prerna and Mai, Yifan and Xie, Benjamin},
title = {Using Benchmarking Infrastructure to Evaluate LLM Performance on CS Concept Inventories: Challenges, Opportunities, and Critiques},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671097},
doi = {10.1145/3632620.3671097},
abstract = {BACKGROUND AND CONTEXT. The pace of advancement of large language models (LLMs) motivates the use of existing infrastructure to automate the evaluation of LLM performance on computing education tasks. Concept inventories are well suited for evaluation because of their careful design and prior validity evidence. OBJECTIVES. Our research explores the feasibility of using an automated benchmarking framework to evaluate computer science (CS) concept inventories. We explore three primary objectives: evaluation of LLM performance on the SCS1 and BDSI concept inventories; informal expert panel review of items which had variations between LLM and expected student performance; and description of challenges with using benchmarking infrastructure as a methodological innovation. METHOD. We used the Holistic Evaluation of Language Models (HELM) framework to evaluate the SCS1 and BDSI against 10 LLMS with zero-shot and few-shot in-context learning: GPT (3.5, 4.0), Claude (1.3, 2.0, 2.1), Llama (7B, 13B, 70B), Mistral v0.1 7B, and Mixtral 8x7B. We used psychometric data from prior studies to measure knowledge levels for each LLM run. We then conducted an informal expert review to qualitatively explore how question design, CS content knowledge, and LLM design may explain differences between LLM and expected student performances. FINDINGS. Our quantitative analysis found that most LLM response patterns reflected a below average introductory computing student with the SCS1 and did not fit the psychometric 2PL model for the BDSI. Our qualitative analysis identified that LLMs performed well on code infill questions, but poorly on nested conditionals, runtime analysis, and longer questions. We also identified several methodological challenges related to item security, translation, the structure when using HELM. IMPLICATIONS. We consider the feasibility of using automated benchmarking as a methodology to support more reproducible, replicable, and rigorous investigations to understand the intersection of LLM capabilities, computing concepts, and assessment design. We also consider connections between psychometric approaches and LLM evaluations to inform the design of computing assessments that are more resilient to LLM advancements.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {452–468},
numpages = {17},
keywords = {benchmarking, computing education, concept inventories, large language models, psychometrics},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3627106.3627196,
author = {Chen, Yufan and Arunasalam, Arjun and Celik, Z. Berkay},
title = {Can Large Language Models Provide Security &amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627106.3627196},
doi = {10.1145/3627106.3627196},
abstract = {Users seek security &amp; privacy (S&amp;P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&amp;P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g.,&nbsp;to produce toxic content). Yet, the ability of LLMs to provide reliable S&amp;P advice is not well-explored. In this paper, we measure their ability to refute popular S&amp;P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&amp;P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&amp;P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs ( for Bard and for ChatGPT) or point to unrelated sources ( returned by Bard and by ChatGPT). Our findings highlight that existing LLMs are not completely reliable for S&amp;P advice and motivate future work in understanding how users can better interact with this technology.},
booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
pages = {366–378},
numpages = {13},
keywords = {Large language models, misconception, security and privacy advice},
location = {Austin, TX, USA},
series = {ACSAC '23}
}

@inproceedings{10.1145/3597503.3639117,
author = {Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang},
title = {GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639117},
doi = {10.1145/3597503.3639117},
abstract = {Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {166},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3661167.3661269,
author = {Harman, Mark},
title = {The Role of Software Measurement in Assured LLM-Based Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661269},
doi = {10.1145/3661167.3661269},
abstract = {Assured Large Language Model Software Engineering (Assured LLMSE) addresses the twin challenges: 1. Ensuring LLM-generated code does not regress the properties of the original code 2. Quantifying the improvement over the original archived by the improve code in a verifiable and measurable way. In so doing, the Assured LLMSE approach tackles the problem of LLMs’ tendency to hallucinate, as well as providing confidence that generated code improves an existing code base. Software testing and measurement play critical roles in this improvement process: testing is the guard against regression, while measurement provides the quantifiable assurance of improvement. Assured LLMSE takes its inspiration from previous work on genetic improvement, for which software measurement also plays a central role. In this keynote we outline the Assured LLMSE approach, highlighting the role of software measurement in the provision of quantifiable, verifiable assurances for code that originates from LLM–based inference. This paper is an outline of the content of the keynote by Mark Harman at the 28th International Conference on Evaluation and Assessment in Software Engineering.  This is joint work with Nadia Alshahwan, Andrea Aquino, Jubin Chheda, Anastasia Finegenova, Inna Harper, Mitya Lyubarskiy, Neil Maiden, Alexander Mols, Shubho Sengupta, Rotem Tal, Alexandru Marginean, and Eddy Wang.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {4},
numpages = {1},
keywords = {Automated Code Generation, CodeLlama, Genetic Improvement (GI), Large Language Models (LLMs), Llama, Search Based Software Engineering (SBSE)},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3589334.3645433,
author = {Hu, Yuxue and Wu, Mingmin and Huang, Zhongqiang and Li, Junsong and Ge, Xing and Sha, Ying},
title = {Euphemism Identification via Feature Fusion and Individualization},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645433},
doi = {10.1145/3589334.3645433},
abstract = {Euphemisms are widely used on social media and darknet markets to evade supervision. For instance, "ice" serves as a euphemism for the target keyword "methamphetamine" in illicit transactions. Thus, euphemism identification which aims to map the euphemism to its secret meaning (target keyword) is a crucial task in ensuring social network security. However, this task poses significant challenges, including resource limitations due to the unavailable of annotated datasets and linguistic challenges arising from subtle differences in meaning between target keywords. Existing methods employed self-supervised schemes to automatically construct labeled training data, addressing the resource limitations. Yet, these methods rely on static embedding methods that fail to distinguish between target keywords with similar meanings. In addition, we observe that different euphemisms in similar contexts confuse the identification results. To overcome these obstacles, we propose a feature fusion and individualization (FFI) method for euphemism identification. First, we reformulate the task as a cloze task, making it more feasible. Next, we develop a feature fusion module to capture both dynamic global and static local features, enhancing discrimination between different euphemisms in similar contexts. Additionally, we employ a feature individualization module to ensure each target keyword has a unique feature representation by projecting features into their orthogonal space. As a result, FFI can effectively identify similar euphemisms that refer to target keywords with similar meanings. Experimental results demonstrate that our method outperforms state-of-the-art methods and large language models, providing robust support for its effectiveness.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2383–2394},
numpages = {12},
keywords = {euphemism identification, euphemisms, feature fusion, feature individualization, social network security},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3597503.3623343,
author = {Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming},
title = {Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623343},
doi = {10.1145/3597503.3623343},
abstract = {Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {70},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3733237,
author = {Qayyum, Khushboo and Jha, Chandan Kumar and Ahmadi-Pour, Sallar and Hassan, Muhammad and Drechsler, Rolf},
title = {LLM-assisted Bug Identification and Correction for Verilog HDL},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3733237},
doi = {10.1145/3733237},
abstract = {As technology continues to advance, it becomes increasingly integrated into daily life facilitating complex tasks across a range of environments. While some applications such as smartphones and smartwatches are less critical, others like healthcare devices and autonomous vehicles demand bug-free performance to prevent financial loss or harm. Traditionally, simulation-based testing and formal verification played a major role in ensuring a bug-free device. However, the simulation of bigger systems is limited to a definite number of scenarios on the Design under Verification&nbsp;(DUV). Hence, it is unable to explore all possible inputs that can occur. Formal verification, on the other hand, offers a higher level of assurance through mathematical proofs but is both time-consuming and suffers from scalability issues, especially as designs grow in complexity. Recently, Large Language Models&nbsp;(LLMs) have shown promise in tasks previously limited to human expertise. Their natural language processing capabilities can assist in handling extensive specifications and source code, particularly in debugging hardware descriptions and analyzing security and functionality. The utilization of Retrieval Augmented Generation&nbsp;(RAG) has further enhanced LLMs by incorporating large specification or source code bases, thereby improving their bug-identification and correction capabilities. While recent advancements in LLMs, particularly with RAG, have yielded promising results in bug identification and correction for a small class of hardware bugs, significant gaps remain in their full potential for systematically addressing a wide range of hardware bugs. For instance, existing LLM methodologies struggle to detect bugs involving incorrect constant values, i.e., the use of wrong constants in source code. This limitation underscores the need for further exploration in utilizing LLMs to fully optimize the verification process. To bridge this gap, we propose a 3-phased 4-stage LLM-assisted systematic bug closure methodology that focuses on functional bugs in Verilog HDL rather than structural or syntactic issues. Our approach extracts functional properties of the DUV and systematically breaks down complex expressions into smaller sub-expressions to facilitate bug detection and correction. By employing RAG, the LLM is guided using the functional specifications and source code to identify and correct bugs. If the initial guidance through RAG is insufficient, our methodology initiates an iterative bug closure process. This includes incorporating more extensive information from the specifications, fetching additional lines of code for bug localization, and breaking down complex Verilog HDL expressions. In our comprehensive evaluation, we assess the LLM’s capabilities using 9 different categories of bugs. As benchmarks, we use 5 OpenTitan Intellectual Property&nbsp;(IP) cores to demonstrate the scalability and effectiveness of our bug closure methodology where  (approx 60% )  of the bugs were corrected. Specifically, we evaluate OpenAI’s GPT-4 in its ability to identify and correct functional bugs in Verilog HDL code.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
keywords = {Large Language Model, Hardware Description Language, Bug Identification, Bug Fixing}
}

@article{10.1145/3709375,
author = {Mellia, Marco and Steenkiste, Peter and Qazi, Ihsan Ayyub and Tyson, Gareth},
title = {PACMNET V3, N1, March 2025 Editorial},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CoNEXT1},
url = {https://doi.org/10.1145/3709375},
doi = {10.1145/3709375},
abstract = {The Proceedings of the ACM on Networking (PACMNET) series showcases top-tier research in emerging computer networks and their applications. We welcome submissions introducing new technologies, innovative experiments, creative applications of networking technologies, and fresh insights gained through analysis. Supported by the ACM Special Interest Group on Communications and Computer Networks (SIGCOMM), the journal is backed by a distinguished Editorial Board composed of leading researchers in the field. This issue begins the third volume of PACMNET. It features 6 articles, all submitted by the June 2024 deadline when 121 submissions in total were received. Each submission underwent a thorough review process involving over 80 Editors, coordinated by two Associate Editors. In the initial phase, every article received a minimum of three reviews. Following an online discussion, roughly half of the submissions were rejected, while the other half advanced to a second review phase. In this phase, Editors produced at least two additional reviews per article. After further discussion and remote Editors' meeting, 8 articles were given one-shot major revision. The same Editors reviewed the revised version the Authors prepared, and 6 out of 8 articles were finally selected. These 6 articles appear in this issue. Topics include network support for large language models and deep learning, security, and wireless networking. All papers include a thorough set of experiments to validate the proposed solutions. From a methodological perspective, machine learning and artificial intelligence-based solutions are becoming central in developing novel networking solutions.  We want to express our gratitude to all those who contributed to this issue of PACMNET, especially the Authors for submitting their finest work and the Associate Editors for offering valuable feedback in their reviews and engaging in the discussions. Our thanks also go to the SIGCOMM Executive Committee Chair and the CoNEXT Steering Committee members for their continued support and guidance, providing essential suggestions and insights throughout the article selection process.},
journal = {Proc. ACM Netw.},
month = mar,
articleno = {1},
numpages = {1},
keywords = {editorial, networks}
}

@inproceedings{10.1145/3650212.3680397,
author = {Yu, Zeliang and Wen, Ming and Guo, Xiaochen and Jin, Hai},
title = {Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680397},
doi = {10.1145/3650212.3680397},
abstract = {As the largest package registry, Node Package Manager (NPM) has become the prime target for various supply chain attacks recently and has been flooded with numerous malicious packages, posing significant security risks to end-users. Learning-based methods have demonstrated promising performance with good adaptability to various types of attacks. However, they suffer from two main limitations. First, they often utilize metadata features or coarse-grained code features extracted at the package level while overlooking complex code semantics. Second, the dataset used to train the model often suffers from a lack of variety both in quantity and diversity, and thus cannot detect significant types of attacks.        To address these problems, we introduce Maltracker, a learningbased NPM malware tracker based on fine-grained features empowered by LLM-enhanced dataset. First, Maltracker constructs precise call graphs to extract suspicious functions that are reachable to a pre-defined set of sensitive APIs, and then utilizes community detection algorithm to identify suspicious code gadgets based on program dependency graph, from which fine-grained features are then extracted. To address the second limitation, we extend the dataset using advanced large language models (LLM) to translate malicious functions from other languages (e.g., C/C++, Python, and Go) into JavaScript. Evaluations shows that Maltracker can achieve an improvement of about 12.6% in terms of F1-score at the package level and 31.0% at the function level compared with the SOTA learning-based methods. Moreover, the key components of 𝑀𝑎𝑙𝑡𝑟𝑎𝑐𝑘𝑒𝑟 all contribute to the effectiveness of its performance. Finally, Maltracker has also detected 230 new malicious packages in NPM and received 61 thanks letters, among which some contain new malicious behaviors that cannot be detected by existing tools.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1759–1771},
numpages = {13},
keywords = {Code Translation, Large Language Model, Malware Detection},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3626772.3657652,
author = {Richards, Edward},
title = {Machine Generated Explanations and Their Evaluation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657652},
doi = {10.1145/3626772.3657652},
abstract = {Rapid adoption of a new generation of LLMs has demonstrated their considerable capabilities. However, these models are far from infallible, raising significant ethical concerns, especially in decision-making applications, prompting calls for increased restraint [2].The Augmented Intelligence paradigm is one proposed mitigation. Therein LLMs are tools used by human decision makers to improve performance without corresponding loss of accountability.However, this mitigation imposes requirements on models that are not the primary focus of existing evaluation approaches. In particular, current explanation evaluation approaches tend to prioritize premises and conclusions over reasoning quality. It is evident that logical soundness is a crucial aspect of system operation, as the output must be interpretable to the user.This work therefore proposes adopting a technique from programming language theory, wherein intermediate representations are employed to simplify the evaluation of code [3]. Rather than the model mapping directly from queries to solutions, code generation is used to produce an executable intermediate. An effect of this design is to shift the LLM from being a producer of solutions to the creator delegation plans. Production of a more structured output is expected to ease estimation of model reasoning quality via the comparison to golden solutions. Use of a bespoke representation, designed to take advantage of the particulars of automated code generation, aims to reduce the difficulty of this estimation. Use of a novel syntax is however, made challenging by the obvious absence of existing examples. It is impractical and undesirable for reasons of both cost and flexibility, to create the large numbers of examples which would be needed for conventional training.Early efforts have therefore been concentrated on two main areas: developing a syntax and interpreter, and addressing the challenge of data sparsity. A well-designed syntax is crucial, not only because updates will necessitate revising an increasing number of established solutions, but also due to its expected impact on the overall system utility. The expressiveness of the syntax is particularly significant in this context. Excessive constraint sacrifices generality, while too much leniency results in a proliferation of semantically equivalent solutions, complicating comparisons with the gold solutions. The generated intermediate representation is executed by an interpreter to produce the end solution. Other than a small number of control flow statements all other statements in the language are parameterized calls to external tools such as retrieval systems or math expression evaluators. External tool usage is the primary motivator behind avoiding training via extensive, manually curated, examples: the ability to easily add or remove tools is highly desirable. In case of an error, the interpreter output may include explanations of constructs and available tools, error messages, and task-specific metrics. Whereas on success, tool output is substituted into the appropriate section of the representation, such that a completely evaluated intermediate includes all information necessary to construct the end natural language explanation. Transformation into this natural language explanation can then be undertaken by another LLM. In terms of solutions for data sparsity, an approach similar to that used by LLM agents in environment exploration is suggested [1]. For each query, a ranking of model generations based on interpreter output in conjunction with a scoring function is produced. Pairwise selection of stronger and weaker responses are used thereafter in a modified form of iterative Direct Preference Optimisation (DPO) [4].Given the preliminary system we aim to test the system across a range of tasks with correspondence to the augmented intelligence paradigm such as multi-hop question answering.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3074},
numpages = {1},
keywords = {code generation, large language models, program verification},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@proceedings{10.1145/3701551,
title = {WSDM '25: Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 18th ACM International Conference on Web and Data Mining, this time taking place in Hannover, Germany.Many volunteers have helped to provide excellent content for the conference, most of all the 3 PC Chairs, Meeyoung Cha (Max Planck Institute for Security and Privacy in Germany), Francine Moens (KU Leuven, Belgium) and Marc Najork (Google DeepMind, USA), who selected more than 100 papers from over 600 submissions for presentation at the conference.This exciting WSDM 2025 main conference program was extended by interesting demonstrations like Lightning IR for fine-tuning and inference of transformer-based language models for information retrieval, WildlifeLookup, a chatbot designed to facilitate wildlife management and Ventana a la Verdad, a chatbot for navigating Colombian civil conflict archives. Several workshops ranging from language models to trust and verification on the Web complemented this program and provided important opportunities for the discussion of exciting new ideas and approaches.WSDM Cup 2025 challenged more than 1000 competitors to create useful and efficient auto-rater models that can accurately reflect human evaluations. This competition focused on the creation of multilingual auto-rater models using chatbot arena data, with exciting solutions. The WSDM 2025 Industry Day featured talks from leading companies on practical applications of web search and data mining. Some notable examples included zero-shot image moderation in Google Ads, advancing Voice AI for E-commerce at Amazon, and Fact-checking of multilingual podcasts. Finally, WSDM Day talks focused on medical research questions and topics, applying language models, knowledge graphs and other AI based approaches.For participants aiming to enter new research areas, the WSDM 2025 tutorials covered a broad range of topics, including Robust Information Retrieval, Building Trustworthy AI Models for Medicine, Unifying Bias and Unfairness in Information Retrieval in the LLM Era, Advances in Vector Search and quite a few more. And finally, three excellent invited speakers (Slav Petrov from Google DeepMind and co-lead on Gemini, Roberto Navigli from Sapienza University and Babelscape and Mario Fritz from CISPA) provided exciting insights into the state of AI, about what large language models really understand, as well as privacy and security aspects of neural models.},
location = {Hannover, Germany}
}

@inproceedings{10.1145/3734436.3734438,
author = {Atluri, Vijayalakshmi},
title = {Policy Mining: Putting LLMs to Work [Keynote Abstract]},
year = {2025},
isbn = {9798400715037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734436.3734438},
doi = {10.1145/3734436.3734438},
abstract = {Policy engineering pertains to devising access control policies. This can be performed either in a top-down or a bottom-up manner. The bottom-up approach, also referred to as policy mining, is to (automatically) discover security policies from the existing authorizations. Several researchers have proposed approaches for RBAC and ABAC policy mining. In contrast, the top-down approach relies on a careful analysis of the business processes of an organization to devise the appropriate security policies. Each approach has its own advantages and disadvantages. For example, the bottom-up approach lends itself to automation, but may result in erroneous policies as it relies on existing authorizations, which may not always be correct to begin with. On the other hand, although the top-down approach may result in more accurate policies, it is often manual, tedious and time consuming since it requires that the semantics of the business processes be well understood. Sometimes it may be more pragmatic to employ a hybrid approach to reap the benefits of both.The goal of our work is to automate the top-down process of policy engineering by utilizing the power of Large Language Models (LLMs). Essentially, our approach is to extract the security policies from the Natural Language Access Control Policies (NLACPs) and then automatically derive Machine Enforceable Security Policies (MESPs). The target MESP could be based on any of the currently prevalent access control models including RBAC, ABAC, etc. In the rest of the discussion, we consider ABAC although the general approach is applicable to handle other access control models. The framework of our approach comprises of three major components. The first component is the LLM-based extraction of NLACPs of key entities from natural language text. For ABAC, it is necessary to determine all the subject and object attribute combinations that are required for making access control decisions, as well as the allowable operations. We have taken a two-step approach for transforming security policies expressed in natural language text into machine-enforceable ABAC rules. Our approach first classifies policy statements to determine whether they constitute an NLACP. If a statement is policy-related, it is then translated into a structured ABAC rule, extracting conditions of subject attributes, object attributes, and actions. To enhance accuracy and efficiency while reducing reliance on human annotations, we employ a two-tiered knowledge distillation strategy, where GPT-4 generates synthetic training data to fine-tune a lightweight CodeT5 student model. We have evaluated our method on real-world datasets, demonstrating high classification accuracy and effective structured policy extraction. The policies thus generated are to be translated into a machine-readable format that can be understood by the target access control system. This typically involves the use of standarized policy languages such as eXtensible Access Control Markup Language (XACML), Attribute-Based Access Control Markup Language (ABACML) or other structured representations. In our implementation, we adopt JSON as the output format due to its compatibility with existing access control engines and APIs.},
booktitle = {Proceedings of the 30th ACM Symposium on Access Control Models and Technologies},
pages = {1–2},
numpages = {2},
keywords = {security policy, mining, llm},
location = {USA},
series = {SACMAT '25}
}

@inproceedings{10.1145/3639478.3647634,
author = {Wang, Xinchen and Hu, Ruida and Gao, Cuiyun and Wen, Xin-Cheng and Chen, Yujia and Liao, Qing},
title = {ReposVul: A Repository-Level High-Quality Vulnerability Dataset},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647634},
doi = {10.1145/3639478.3647634},
abstract = {Open-Source Software (OSS) vulnerabilities bring great challenges to the software security and pose potential risks to our society. Enormous efforts have been devoted into automated vulnerability detection, among which deep learning (DL)-based approaches have proven to be the most effective. However, the performance of the DL-based approaches generally relies on the quantity and quality of labeled data, and the current labeled data present the following limitations: (1) Tangled Patches: Developers may submit code changes unrelated to vulnerability fixes within patches, leading to tangled patches. (2) Lacking Inter-procedural Vulnerabilities: The existing vulnerability datasets typically contain function-level and file-level vulnerabilities, ignoring the relations between functions, thus rendering the approaches unable to detect the inter-procedural vulnerabilities. (3) Outdated Patches: The existing datasets usually contain outdated patches, which may bias the model during training.To address the above limitations, in this paper, we propose an automated data collection framework and construct the first repository-level high-quality vulnerability dataset named ReposVul. The proposed framework mainly contains three modules: (1) A vulnerability untangling module, aiming at distinguishing vulnerability-fixing related code changes from tangled patches, in which the Large Language Models (LLMs) and static analysis tools are jointly employed. (2) A multi-granularity dependency extraction module, aiming at capturing the inter-procedural call relationships of vulnerabilities, in which we construct multiple-granularity information for each vulnerability patch, including repository-level, file-level, function-level, and line-level. (3) A trace-based filtering module, aiming at filtering the outdated patches, which leverages the file path trace-based filter and commit time trace-based filter to construct an up-to-date dataset.The constructed repository-level ReposVul encompasses 6,134 CVE entries representing 236 CWE types across 1,491 projects and four programming languages. Thorough data analysis and manual checking demonstrate that ReposVul is high in quality and alleviates the problems of tangled and outdated patches in previous vulnerability datasets.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {472–483},
numpages = {12},
keywords = {open-source software, software vulnerability datasets, data quality},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.5555/3091125.3091273,
author = {de la Hoz, Enrique and Gimenez-Guzman, Jose Manuel and Marsa-Maestre, Ivan and Cruz-Piris, Luis and Orden, David},
title = {A Distributed, Multi-Agent Approach to Reactive Network Resilience},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Critical network infrastructures are communication networks whose disruption can create a severe impact on other systems. Multi-agent systems have been successfully used to protect critical network infrastructures, with approaches ranging from reasoning about secure design and policies to multi-agent intrusion detection systems (IDS). However, there is little research on the possibilities for multi-agent systems to react to known intrusions. In this paper we propose a multi-agent framework for reactive network resilience, that is, to allow a network to reconfigure itself in the event of a security incident so that the risk of further damage is mitigated. The proposed framework takes advantage of a risk model based on multilayer networks and of distributed belief propagation techniques to agree on a new, more resilient configuration of the network in the event of an attack. We compare our proposal with a number of centralized optimization and multi-agent negotiation techniques. Experiments show that our proposal outperforms the reference approaches both in terms of risk mitigation and performance},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1044–1053},
numpages = {10},
keywords = {reactive resilience, multi-agent belief propagation, critical network infrastructures},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.5555/3398761.3398797,
author = {Cubuktepe, Murat and Xu, Zhe and Topcu, Ufuk},
title = {Policy Synthesis for Factored MDPs with Graph Temporal Logic Specifications},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the synthesis of policies for multi-agent systems to implement spatial-temporal tasks. We formalize the problem as a factored Markov decision process subject to so-called graph temporal logic specifications. The transition function and the spatial-temporal task of each agent depend on the agent itself and its neighboring agents. The structure in the model and the specifications enable to develop a distributed algorithm that, given a factored Markov decision process and a graph temporal logic formula, decomposes the synthesis problem into a set of smaller synthesis problems, one for each agent. We prove that the algorithm runs in time linear in the total number of agents. The size of the synthesis problem for each agent is exponential only in the number of neighboring agents, which is typically much smaller than the number of agents. We demonstrate the algorithm in case studies on disease control and urban security. The numerical examples show that the algorithm can scale to hundreds of agents.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {267–275},
numpages = {9},
keywords = {verification of multi-agent systems, multi-agent planning, graph temporal logic, distributed optimization},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3463952.3463996,
author = {Cheng, Mingxi and Yin, Chenzhong and Zhang, Junyao and Nazarian, Shahin and Deshmukh, Jyotirmoy and Bogdan, Paul},
title = {A General Trust Framework for Multi-Agent Systems},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Transportation systems of the future can be best modeled as multi-agent systems. A number of coordination protocols such as autonomous intersection management (AIM), adaptive cooperative traffic light control (TLC), cooperative adaptive cruise control (CACC), among others have been developed with the goal of improving the safety and efficiency of such systems. The overall goal in these systems is to provide behavioral guarantees under the assumption that the participating agents work in concert with a centralized (or distributed) coordinator. While there is work on analyzing such systems from a security perspective, we argue that there is limited work on quantifying trustworthiness of individual agents in a multi-agent system. We propose a framework that uses an epistemic logic to quantify trustworthiness of agents, and embed the use of quantitative trustworthiness values into control and coordination policies. Our modified control policies can help the multi-agent system improve its safety in the presence of untrustworthy agents (and under certain assumptions, including malicious agents). We empirically show the effectiveness of our proposed trust framework by embedding it into AIM, TLC, and CACC platooning algorithms. In our experiments, our trust framework accurately detects attackers in CACC platoons; mitigates the effect of untrustworthy agents in AIM; and trust-aware TLC and AIM reduce collisions in all cases compared to the vanilla versions of these algorithms.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {332–340},
numpages = {9},
keywords = {autonomous intersection management, multi-agent systems, platoons, reinforcement learning, trust framework},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3616855.3637633,
author = {Savage, Saiph},
title = {Unveiling AI-Driven Collective Action for a Worker-Centric Future},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3637633},
doi = {10.1145/3616855.3637633},
abstract = {Collective action by gig knowledge workers is a potent method for enhancing labor conditions on platforms like Upwork, Amazon Mechanical Turk, and Toloka. However, this type of collective action is still rare today. Existing systems for supporting collective action are inadequate for workers to identify and understand their different workplace problems, plan effective solutions, and put the solutions into action. This talk will discuss how with my research lab we are creating worker-centric AI enhanced technologies that enable collective action among gig knowledge workers. Building solid AI enhanced technologies to enable gig worker collective action will pave the way for a fair and ethical gig economy-one with fair wages, humane working conditions, and increased job security. I will discuss how my proposed approach involves first integrating ''sousveillance,'' a concept by Foucault, into the technologies. Sousveillance involves individuals or groups using surveillance tools to monitor and record those in positions of power. In this case, the technologies enable gig workers to monitor their workplace and their algorithmic bosses, giving them access to their own workplace data for the first time. This facilitates the first stage of collective action: problem identification. I will then discuss how we combine this data with Large-Language-Models (LLMs) and social theories to create intelligent assistants that guide workers to complete collective action via sensemaking and solution implementation.The talk will present a set of case studies to showcase this vision of designing data driven AI technologies to power gig worker collective action. In particular, I will present the systems: 1) GigSousveillance which allows workers to monitor and collect their own job-related data, facilitating quantification of workplace problems; 2) GigSense equips workers with an AI assistant that facilitates sensemaking of their work problems, helping workers to strategically devise solutions to their challenges; 3) GigAction is an AI assistant that guides workers to implement their proposed solutions. I will discuss how we are designing and implementing these systems by adopting a participatory design approach with workers, while also conducting experiments and longitudinal deployments in the real world. I conclude by presenting a research agenda for transforming and rethinking the role of A.I. in our workplaces; and researching effective socio-technical solutions in favor of a worker-centric future and countering techno-authoritarianism.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {6–7},
numpages = {2},
keywords = {collective action, cscw, future of work, gig work, hci, human centered ai.},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3600211.3604754,
author = {Narayanan Venkit, Pranav},
title = {Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604754},
doi = {10.1145/3600211.3604754},
abstract = {The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority’s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases’ potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers’ experiences when encountering biased articles, highlighting the capacity to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners, and policymakers are empowered to forge practical solutions that embody inclusivity and reliability, especially when used as a service (AIaaS). Moreover, the platform serves as a centralized locus for the identification and rectification of social biases, irrespective of the underlying model or architecture. By furnishing a cogent narrative that underscores the imperative for a comprehensive and interdisciplinary approach, my work strives to propel the ongoing endeavors to comprehend and mitigate biases within the realm of NLP. With its potential to augment the equity, inclusivity, and societal ramifications of NLP technologies, the proposed framework catapults the field towards responsible and ethical practices.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1004–1005},
numpages = {2},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3524304.3524306,
author = {Ding, Xiang and Zhang, Cheng},
title = {How Can We Cope with the Impact of Microservice Architecture Smells?},
year = {2022},
isbn = {9781450385770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524304.3524306},
doi = {10.1145/3524304.3524306},
abstract = {Context: Software Architecture Smells (AS) are potential software structure problems and always impact software quality negatively. And with the development of Microservice architecture, the Microservice Architecture Smells (MAS) have been attracting more and more attention. The software scholars and developers have discovered the influence of MAS and performed some researches on them. However, the definition and category of MAS are still ambiguous.Objects: This paper aims to clarify the specific MAS categories and their definitions, and then further explores the issues caused by MAS in the migration process from a monolithic system to Microservice.Method: We performed a comprehensive systematic literature review about MAS. Specifically, we explored 13 white and 10 grey literature in detail to get MAS information by using the quantitative research method. To explore the issues that influence the migration process from a monolithic system to Microservice, we used the meta-ethnography qualitative research method to extract relevant information and get six three-order translations.Results: This study defined 22 Microservice Architecture Smells and classified them into five categories, namely Design, Deployment, Monitor &amp; Log, Communication and Team &amp; Tool, based on their characteristics. Simultaneously, the issues that influence the migration process are proposed, including service cutting, databases, communication, team and techniques. Finally, we matched the MAS to the issues they caused in the migration process and recommended solutions to these issues.},
booktitle = {Proceedings of the 2022 11th International Conference on Software and Computer Applications},
pages = {8–14},
numpages = {7},
keywords = {smell impact, meta-ethnography, Systematic Literature Review, Microservice Architecture Smells},
location = {Melaka, Malaysia},
series = {ICSCA '22}
}

@inproceedings{10.1145/3167020.3167037,
author = {Rhazlane, Sara and Harbi, Nouria and Kabachi, Nadia and Badir, Hassan},
title = {Alteration Agent for Cloud Data Security},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167037},
doi = {10.1145/3167020.3167037},
abstract = {In the big data era, the cloud computing services have been adopted to face the emergence of data that needs to be stored and processed properly. However, these services need to provide safety mechanisms to insure its secure adoption. Thus, several solutions have been proposed including the use of secure architectures by customers. In that context, an architecture based on multi-agent systems has been proposed which aims to secure both storage and exploration of data hosted in the Cloud. In this paper, we present a brief synthesis of data security methods. We then focus on the multi-agent system architecture. Finally, we propose our solution considering the design and implementation in Java of an alteration agent which will ensure the secure storage of data stored in the Cloud. We finally present the test results of this agent on real datasets.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {108–112},
numpages = {5},
keywords = {Security, Multi agent systems, Cloud, Alteration},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@inproceedings{10.1145/3018896.3065844,
author = {Kendrick, Phillip and Hussain, Abir and Criado, Natalia and Randles, Martin},
title = {Multi-agent systems for scalable internet of things security},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3065844},
doi = {10.1145/3018896.3065844},
abstract = {Providing effective and scalable real-time security to Internet of Things devices can be a challenging task given the limited computational capacity of portable devices and a significant volume of network traffic. Multi-Agent Systems have proven to be a valuable tool in the areas of cyber security, distributed networks and legacy systems because of their scalable and flexible architecture. In this paper, we present a novel implementation of a Multi-Agent System for use within, or to support, Internet of Things networks through the distributed processing of security events to offload the computational cost of processing data from Internet of Things devices. In particular, domain experts can add new agents to existing systems which can automatically work with preexisting agents without manual reconfiguration. The scalability of this deployment model makes it suitable for a broad range of environments including dynamic and large-area networks.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {193},
numpages = {6},
keywords = {scalable systems, multi-agent system, internet of things, cyber security},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3638584.3638622,
author = {Sun, Yanxia},
title = {Analysis of the performance of PID-based new-generation metaheuristic algorithms for automatic voltage regulation system},
year = {2024},
isbn = {9798400708688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638584.3638622},
doi = {10.1145/3638584.3638622},
abstract = {In recent decades, the expansion of industrial organizations in both scale and scope has necessitated dependable output voltage supplies. However, persistent oscillations in electromechanical devices can impede power efficiency and stability, underscoring the importance of reliable automatic generation regulation (AVR) systems and power system design in the manufacturing sector. To address this issue, this study presents a performance analysis of a proportional integral derivative (PID) controller based on new-generation metaheuristic algorithms (MAs) for the AVR system. Five recent and novel MAs were employed to optimize the PID controller for the AVR system, with the controllers' performances evaluated under five distinct performance metrics. The findings revealed that the Northern Goshawk Optimization (NGO) algorithm was the most effective optimization approach, exhibiting the lowest values of overshoot (33.2784%), peak time (0.2120 s), and objective value (0.0077). These results suggest that the NGO algorithm is a promising optimization method for improving AVR system performance in industrial settings.},
booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence},
pages = {449–454},
numpages = {6},
keywords = {Automatic voltage regulation system, algorithm, metaheuristic, northern goshawk optimization, proportional integral derivative},
location = {Beijing, China},
series = {CSAI '23}
}

@inproceedings{10.5555/3237383.3237980,
author = {Aschermann, Malte and Dennisen, Sophie and Kraus, Philipp and M\"{u}ller, J\"{o}rg P.},
title = {LightJason, a Highly Scalable and Concurrent Agent Framework: Overview and Application},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multiagent systems (MAS) provide useful abstractions for modelling and simulating complex socio-technical systems. However, existing open-source platforms suffer from serious issues including limited scalability, inflexible software architecture, and poor support for web deployment. This demo presents an overview and an application of LightJason, a highly scalable Java-based platform for agent-oriented programming (AOP) and simulation. We outline the architectural features of LightJason and showcase its applicability using an example of a browser-based web application implementing a traffic serious game devised to teach an interdisciplinary student team in MAS and AOP. Demonstration video: https://vimeo.com/lightjason/aamas2018},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1794–1796},
numpages = {3},
keywords = {traffic simulation, scalability, bdi modeling, agent-oriented programming and simulation, agent platform},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3078861.3078881,
author = {Calo, Seraphin B. and Verma, Dinesh C. and Bertino, Elisa},
title = {Distributed Intelligence: Trends in the Management of Complex Systems},
year = {2017},
isbn = {9781450347020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078861.3078881},
doi = {10.1145/3078861.3078881},
abstract = {The ability to incorporate intelligence in even small devices and to make use of contextual information from widely deployed sensors has already begun to change management paradigms. As edge computing and IoT become more prevalent, systems will increasingly consist of cooperating, heterogeneous, distributed, autonomous elements. Architectures for cognitive, collaborative systems are evolving to deal with such complex environments. Concepts from multi-agent systems and autonomic computing are being applied to cope with the scope and breadth of large collections of interacting devices and services. Technologies for security and access control must evolve as well. Policy-based mechanisms are widely used and have been very successful in protecting information and controlling access to systems and services. They tend to rely, however, on a centralized infrastructure and on the automated enforcement of directives. Newer paradigms are being investigated that allow policy structures to be more dynamic and contextual, while still preserving the desired levels of control. We will present trends in the evolution of architectures for distributed, federated systems, and the technologies for managing them.},
booktitle = {Proceedings of the 22nd ACM on Symposium on Access Control Models and Technologies},
pages = {1–7},
numpages = {7},
keywords = {policy based management, generative policies, cognitive collaborative systems, autonomic systems},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '17 Abstracts}
}

@inproceedings{10.1145/3649158.3657041,
author = {Thuraisingham, Bhavani},
title = {Trustworthy Artificial Intelligence for Securing Transportation Systems},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649158.3657041},
doi = {10.1145/3649158.3657041},
abstract = {Artificial Intelligence (AI) techniques are being applied to numerous applications from Healthcare to Cyber Security to Finance. For example, Machine Learning (ML) algorithms are being applied to solve security problems such as malware analysis and insider threat detection. However, there are many challenges in applying ML algorithms for various applications. For example, (i) the ML algorithms may violate the privacy of individuals. This is because we can gather massive amounts of data and apply ML algorithms to the data to extract highly sensitive information. (ii) ML algorithms may show bias and be unfair to various segments of the population. (iii) ML algorithms themselves may be attacked possibly resulting in catastrophic errors including in cyber-physical systems such as transportation systems. Finally, (iv) the ML algorithms must be safe and not harm society. Therefore, when ML algorithms are applied to transportation systems for handling congestion, preventing accidents, and giving advice to drivers, we must ensure that they are secure, ensure privacy and fairness, as well as provide for the safe operation of the transportation systems. Other AY techniques such as Generative AI (GenAI) are also being applied not only to secure systems design but also to determine the attacks and potential solutions. This presentation is divided into two parts. First, we describe our research over the past decade on Trustworthy ML systems. These are systems that are secure as well as ensure privacy, fairness, and safety. We discuss our ensemble-based ML models for detecting attacks as well as our research on developing Adversarial Machine Learning techniques. We also discuss securing the Internet of Transportation systems that are based on traditional methods such as Extended Kalman Filters to detect cyberattacks. Second, Second, we discuss our work on Finally, we discuss the research we recently started as part of the USDOT National University Technology Center TraCR (Transportation Cybersecurity and Resiliency) led by Clemson University. In particular, we describe (i) the application of federated machine learning techniques for detecting attacks in transportation systems; (ii) publishing synthetic transportation data sets that preserve privacy, (iii) fairness algorithms for transportation systems, and (iv) examining how GenAI systems are being integrated with transportation systems to provide security. Our focus includes the following: · Data Privacy: We are designing a Privacy-aware Policy-based Data Management Framework for Transportation Systems. Our work involves collecting the requisite data and developing analysis tools to identify and quantify privacy risks. Existing privacy-preserving, differentially private synthetic data generation techniques, which tailor data utility for generic ML accuracy, are not well suited for specific applications. We are developing synthetic data generation tools for transportation systems applications. We will develop new ML algorithms that can leverage these datasets. · Fairness: We have developed a novel adaptive fairness-aware online meta-learning algorithm, FairSAOML, which adapts to changing environments in both bias control and model precision. Our current work is focusing on adapting our framework to fairness in transportation systems. and control bias over time, especially ensuring group fairness across different protected sub-populations; identifying interesting attributes using explainable AI techniques that might help to mitigate bias and develop equitable algorithms. We have also developed a second system, FairDolce, that recognizes objects involving fairness constraints in a changing environment. We are adapting it to transportation applications. For example, pedestrian detection (whether or not the object being seen is a pedestrian) must be fair with respect to the race or gender of the individuals being detected under changing environments (e.g., rainy, cloudy sunny). Adversarial ML: Our prior work on adversarial ML models worked on traditional datasets such as network traffic data. Our current focus is on adapting our approach to AV-based sensor data. Our ML models are being applied to sensor data for object recognition and traffic management. These ML models may be attacked by the adversary. We will study various attack models and investigate ways of how interactions may occur between the model and the adversary and subsequently develop appropriate adversarial ML models that operate on the AV sensor data. · Attack Detection - Smart vehicles are often exposed to various attacks making it difficult for manufacturers to collaboratively train anomaly/attack detection models. Yet it would be ideal if all the data available across manufacturers could be used in building robust attack detection systems. To achieve this, we developed FAST-SV, which incorporates federated learning in conjunction with augmentation techniques to build a highly performant attack detection system for smart cars. Safety: Safety has been studied for cyber-physical systems and formal methods have been applied to specify safety properties and subsequently verify that the system satisfies the specifications. However, our goal is to ensure that the ML algorithms utilized by the transportation systems are safe. This would involve developing an AI Governance framework that would require transparency and explainability (among others) of the ML algorithms utilized by the transportation system.},
booktitle = {Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
pages = {5–6},
numpages = {2},
keywords = {adversarial machine learning, attack detection, data privacy, fairness, transportation systems, trustworthy artificial intelligence},
location = {San Antonio, TX, USA},
series = {SACMAT 2024}
}

@inproceedings{10.1145/3626772.3657656,
author = {Rollings, Nathaniel},
title = {Mosaicing Prevention in Declassification},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657656},
doi = {10.1145/3626772.3657656},
abstract = {Multiple methods can be used to infer as-yet unrecorded information. However, this ability can place confidentiality at risk when some inferences, although correct, could cause harm. We therefore flip the problem, seeking not to enable but to prevent specific inferences. This inference prevention task is motivated by what has been called the "mosaicing'' problem in declassification review for documents that in the past were withheld from public access for national security reasons~citepozen2005mosaic. The goal of such a review is to reveal as much as can now be safely revealed but to also withhold things that could be used to infer facts that require continued protection. This problem is modeled using three primary components: (1) currently public information, (2) a set of secrets (information that is not public and requires continuing protection), and (3) a review set (other information now being reviewed for possible release). The inference prevention task is to determine what in the review set would substantially increase the inference rick for a secret.Our initial work investigated use of knowledge graphs for keeping secrets using Knowledge Graph Completion (KGC) techniques. While declassification is typically text-based, we expect a structured analog to that problem can provide some useful insights. There also are applications where prevention of inference in a knowledge graph is the actual task, such as protecting against specific drug discovery inferences when augmenting the Hetionet knowledge graph. Our mosaicing problem is the inverse of KGC---rather than inferring a link, we need to prevent inference of a link. This challenge is distinct from anonymization for social media graphs because we can't alter most relationships, only those in the review set. Using the FB15K-237 knowledge graph, we analyzed three KGC models to identify the relation in a defined review set most critical to inference of a missing secret relation (thus "nominating" a relation for redaction). We evaluated the impact of redactions nominated by one model on inference by other models by ranking a secret with some selected confounds, finding that our simplest model (RuleN) produced the best nominations, despite being least effective of the three on the KGC task. Future work will use graphs more closely modeling declassification, and other KGC models. It will also explore areas in which differences between the traditional KGC task and the declassification problem may be exploited, most notably in the focus on specific secrets for declassification which may allow more focused training of models and improve scalability.Our ultimate goal is to perform redaction directly on text. We will explore two sets of techniques, one building on traditional Multi-Hop Question Answering (MHQA) and a second using Large Language Models (LLM) which now constitute a major element of text-based inference methods. Both approaches to MHQA typically operate over limited document sets, so a retrieval step is needed for preselection. This retrieval step adds challenges because we must accommodate redundant information spread across the collection. We can evaluate nomination generalizability across model classes and the impact of alternative retrieval approaches using the same confound ranking technique, but ultimately we will also need absolute measures of effectiveness, not just relative comparisons, because we must balance the benefit of releasing information with the cost imposed by the risk of revealing a secret. While our work begins the exploration of the mosaicing problem, it has limitations. We must use analogs for our problem as working with classified information is challenging in access and distribution. While these are selected to serve as reasonable representations of our problem, they will exhibit differences from the actual classified datasets. Furthermore, the performance of the model classes used for inference in both the text and KG scenarios may not generalize against novel approaches developed in the future. The framework established in testing the current models would still be applicable but would have to be rerun with these new classes of models.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3075},
numpages = {1},
keywords = {information protection, knowledge graph, large language model},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3724389.3730789,
author = {Carneiro Oliveira, Eduardo and Keuning, Hieke and Jeuring, Johan},
title = { 'Can You Refactor This for Me?': Investigating How Students Use ChatGPT in Code Refactoring Exercises},
year = {2025},
isbn = {9798400715693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724389.3730789},
doi = {10.1145/3724389.3730789},
abstract = {LLMs are increasingly used in programming education. However, little research has explored their use in teaching and learning code refactoring. In this study, we use a grounded-theory approach to examine student-LLM conversations during code refactoring exercises. Our preliminary results show that students use LLM in various modes, such as requesting a refactoring for the entire program at once or discussing refactoring possibilities in long conversations.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {788},
numpages = {1},
keywords = {code refactoring, generative ai, llms, programming education},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@inproceedings{10.1145/3341105.3374033,
author = {Nakagawa, Hiroyuki and Ogata, Shinpei and Aoki, Yoshitaka and Kobayashi, Kazuki},
title = {A model transformation approach to constructing agent-oriented design models for CPS/IoT systems},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374033},
doi = {10.1145/3341105.3374033},
abstract = {CPS (Cyber-Physical System) and IoT (Internet of Things) are keywords for representing recent software systems, which are composed of not only software components in the cyber space but also hardware devices in the physical space. Since CPS and IoT systems additionally contain hardware devices and interactions among their components comparing with traditional software systems, system design for such a CPS and IoT system tends to be considerably complicated. To deal with this complexity, designing such a complicated system as a multi-agent system (MAS) is a possible approach. This paper describes a model transformation method to construct a representative MAS design model for CPS and IoT systems. The method uses a source model that describes components and labeled relations between these components. We experimentally design a farm monitoring system in the real world. The result of the case study demonstrates that the proposed model transformation can systematically construct an agent-oriented design model.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {815–822},
numpages = {8},
keywords = {multi-agent systems, model transformation, cyber physical systems (CPS), agent-oriented software design, IoT},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3696630.3728567,
author = {David, Cristina and Kesseli, Pascal and Kroening, Daniel and Zhang, Hanliang},
title = {Quantifying the benefits of code hints for refactoring deprecated Java APIs},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728567},
doi = {10.1145/3696630.3728567},
abstract = {When done manually by engineers at Amazon and other companies, refactoring legacy code in order to eliminate uses of deprecated APIs is an error-prone and time-consuming process. In this paper, we investigate to which degree refactorings for deprecated Java APIs can be automated, and quantify the benefit of Javadoc code hints for this task. To this end, we build a symbolic and a neural engine for the automatic refactoring of deprecated APIs. The former is based on type-directed and component-based program synthesis, whereas the latter uses LLMs. We applied our engines to refactor the deprecated methods in the Oracle JDK 15. Our experiments show that code hints are enabling for the automation of this task: even the worst engine correctly refactors 71% of the tasks with code hints, which drops to at best 14% on tasks without. Adding more code hints to Javadoc can hence boost the refactoring of code that uses deprecated APIs.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {444–455},
numpages = {12},
keywords = {program refactoring, program synthesis, LLMs},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3663529.3663803,
author = {Pomian, Dorin and Bellur, Abhiram and Dilhara, Malinda and Kurbatova, Zarina and Bogomolov, Egor and Sokolov, Andrey and Bryksin, Timofey and Dig, Danny},
title = {EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663803},
doi = {10.1145/3663529.3663803},
abstract = {Excessively long methods, loaded with multiple responsibilities, are challenging to understand, debug, reuse, and maintain. The  solution lies in the widely recognized Extract Method refactoring. While the application of this refactoring is supported in modern  IDEs, recommending which code fragments to extract has been the topic of many research tools. However, they often struggle to replicate real-world developer practices, resulting in recommendations that do not align with what a human developer would do in real  life. To address this issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to generate refactoring suggestions and subsequently validates, enhances, and ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected recommendation.  In our extensive evaluation of 1,752 real-world refactorings that actually took place in open-source projects, EM-Assist’s recall rate  was 53.4% among its top-5 recommendations, compared to 39.4% for the previous best-in-class tool that relies solely on static analysis. Moreover, we conducted a usability survey with 18 industrial developers and 94.4% gave a positive rating.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {582–586},
numpages = {5},
keywords = {Code smells, Java, Kotlin, LLMs, Long Methods, Refactoring},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3713081.3731747,
author = {Hu, Qiang and Xie, Xiaofei and Chen, Sen and Quan, Lili and Ma, Lei},
title = {Large Language Model Supply Chain: Open Problems From the Security Perspective},
year = {2025},
isbn = {9798400714740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713081.3731747},
doi = {10.1145/3713081.3731747},
abstract = {Large Language Model (LLM) is changing the software development paradigm and has gained huge attention from both academia and industry. Researchers and developers collaboratively explore how to leverage the powerful problem-solving ability of LLMs for specific domain tasks. Due to the wide usage of LLM-based applications, e.g., ChatGPT, multiple works have been proposed to ensure the security of LLM systems. A comprehensive understanding of the entire process of LLM system construction (the LLM supply chain) is crucial, but relevant works are limited. More importantly, the security issues hidden in the LLM SC that could greatly impact the reliable usage of LLMs lack exploration. Most existing works mainly focus on assuring the quality of LLM from the model level; however, security assurance for the entire LLM SC is ignored. In this work, we take the first step to discuss the potential security risks in each component and the integration between components of LLM SC. We summarize 12 security-related risks and provide promising guidance to help build safer LLM systems. We hope our work can facilitate the evolution of artificial general intelligence with secure LLM ecosystems.},
booktitle = {Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {169–173},
numpages = {5},
keywords = {large language model, software supply chain, security assessment},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {ISSTA Companion '25}
}

@article{10.1145/3643776,
author = {Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei},
title = {Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643776},
doi = {10.1145/3643776},
abstract = {Pythonic idioms are highly valued and widely used in the Python programming community. However, many   Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90% for accuracy, F1-score, precision, and recall.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {50},
numpages = {22},
keywords = {Code Change, Large Language Model, Pythonic Idioms}
}

@inbook{10.1145/3712256.3726317,
author = {Bentley, Peter and Lim, Soo Ling and Ishikawa, Fuyuki},
title = {CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction},
year = {2025},
isbn = {9798400714658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712256.3726317},
abstract = {Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1328–1336},
numpages = {9}
}

@article{10.1145/3744553,
author = {Bouzid, Rezika and Khoury, Rapha\"{e}l},
title = {Assessing the Effectiveness of ChatGPT in Secure Code Development: A Systematic Literature Review},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3744553},
doi = {10.1145/3744553},
abstract = {ChatGPT, a Large Language Model (LLM) maintained by OpenAI, has demonstrated a remarkable ability to seemingly comprehend and contextually generate text. Among its myriad applications, its capability to autonomously generate and analyze computer code stands out as particularly promising. This functionality has piqued substantial interest due to its potential to streamline the software development process. However, this technological advancement also brings to the forefront significant apprehensions concerning the security of code produced by LLMs. In this article, we survey recent research that examines the use of ChatGPT to generate secure code, detect vulnerabilities in code, or perform other tasks related to secure code development. Beyond categorizing and synthesizing these studies, we identify important insights into ChatGPT’s potential impact on secure programming. Key findings indicate that while ChatGPT shows great promise as an aid in writing secure code, challenges remain. Its effectiveness varies across security tasks, depending on the context of experimentation (programming language, CWE, code length, etc.) and the benchmark used for comparison–whether against other LLMs, traditional analysis tools, or its own versions. The overall trend indicates that GPT-4 consistently surpasses its predecessor in most tasks.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {324},
numpages = {32},
keywords = {ChatGPT, LLMs, code, security, vulnerabilities}
}

@inproceedings{10.1145/3677052.3698685,
author = {Zhu, Fengbin and Liu, Ziyang and Feng, Fuli and Wang, Chao and Li, Moxin and Chua, Tat Seng},
title = {TAT-LLM: A Specialized Language Model for Discrete Reasoning over Financial Tabular and Textual Data},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698685},
doi = {10.1145/3677052.3698685},
abstract = {In this work, we develop a specialized language model with strong discrete reasoning capabilities to tackle question answering (QA) over hybrid tabular and textual data in finance. Compared with adopting online LLMs, specializing smaller LLMs is more advantageous in response to users’ concerns about cost, network latency, and data security risks. To this end, we first abstract a Step-wise Pipeline for tabular and textual QA to help LLMs better execute multi-step inference, which includes three key steps, i.e. Extractor, Reasoner and Executor. This pipeline is proved to bring great performance grains compared with applying other prompting strategies like Chain-of-Thought (CoT), and meanwhile provides better interpretability to the derivation of the answer, with fixed inference steps and intermediate outcomes as references. We then develop a TAT-LLM&nbsp;model by fine-tuning LLaMA 2 with the training data generated automatically from existing datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM&nbsp;model can outperform all compared models, including prior best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks. It is hoped that this work will shed light on practical solutions to the intelligent understanding of financial documents in the future. The generated datasets and trained models will be made publicly available to facilitate future research on the development of financial LLMs.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {310–318},
numpages = {9},
keywords = {Discrete Reasoning, Large Language Model, Natural Language Processing, Question Answering, Tabular and Textual Data},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3711618.3711619,
author = {Ling, Feng and Yang, Hao and Xiao, Yongcai and Hu, Lei},
title = {Meta GPT-Based Agent for Enhanced Phishing Email Detection},
year = {2025},
isbn = {9798400717994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711618.3711619},
doi = {10.1145/3711618.3711619},
abstract = {Phishing emails are a common means of phishing and their detection is an important aspect of email security. In this study, Meta Phishing Detector Agent system is designed, which is constructed based on Meta GPT framework and uses LLMs as the core of agent to detect phishing emails. The agent has three actions: Check Header, Check Body and Make Decision. Check Header and Check Body are used to analyze the header and body information of emails, and Make Decision is responsible for giving structured decision results based on the previous analysis results. Unlike previous email classification systems, our system provides users with detailed evidence-based reasons for judgment while giving judgment results. The performance of the system was evaluated on a novel phishing email dataset, and the experimental results show that the agent system, using the GLM-4 large language model for example, achieves 99.30% accuracy. Our study gives a new practical approach for the application of LLMs-based agents in the field of cyber security.},
booktitle = {Proceedings of the 2024 14th International Conference on Communication and Network Security},
pages = {78–84},
numpages = {7},
keywords = {Agent, Email Security, LLMs, Meta GPT, Phishing Email Detection},
location = {
},
series = {ICCNS '24}
}

@inproceedings{10.1145/3691620.3695510,
author = {Yan, Chuan and Ren, Ruomai and Meng, Mark Huasong and Wan, Liuhuo and Ooi, Tian Yang and Bai, Guangdong},
title = {Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695510},
doi = {10.1145/3691620.3695510},
abstract = {ChatGPT has enabled third-party developers to create plugins to expand ChatGPT's capabilities. These plugins are distributed through OpenAI's plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the ChatGPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1370–1382},
numpages = {13},
keywords = {large language model, testing, security, deployment},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3716815.3729012,
author = {Sharma, Arnav Nishith and Akbar, Khandakar Ashrafi and Thuraisingham, Bhavani and Khan, Latifur},
title = {Enhancing Security Insights with KnowGen-RAG: Combining Knowledge Graphs, LLMs, and Multimodal Interpretability},
year = {2025},
isbn = {9798400715013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716815.3729012},
doi = {10.1145/3716815.3729012},
abstract = {We present a hybrid Retrieval-Augmented Generation (RAG) framework KnowGen-RAG that integrates knowledge graphs comprising entities and relationships and LLM-based Natural Language Generation for application security, privacy, and compliance. The framework aims to enhance the accuracy and relevance of retrieved information to produce more context-aware and actionable security recommendations, identify potential privacy risks, and detect vulnerabilities by utilizing structured knowledge about entities (e.g., access control mechanisms, security policies, privacy-preserving algorithms, protocols, software vulnerabilities) and their inter-relationships in the security context. We also extend the multimodal-LLM interpretability paradigm by contextual explanation generation for equations and tables from unstructured and highly technical documents.  KnowGen-RAG proves superior for security-related information retrieval and contextual reasoning. It significantly outperforms both the LLM's output without RAG and Baseline RAG in terms of preciseness and reliability.  Specifically, KnowGen-RAG increases accuracy for the CyberMetric dataset, where the original approach struggled to perform consistently for lightweight LLMs, and Baseline RAG achieved only marginal improvements.   Additionally, KnowGen-RAG enhances answer quality for our curated security dataset, SecMD, demonstrating its effectiveness and improved understanding of security-related techniques and digital artifacts when addressing complex questions. The system aims to strengthen the learning of security professionals by providing thorough insights into the security landscape, encouraging informed decision-making in the face of sophisticated challenges.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {2–12},
numpages = {11},
keywords = {retrieval-augmented generation, multimodal llm interpretability, application security, knowledge graphs},
location = {Pittsburgh, PA, USA},
series = {IWSPA '25}
}

@inproceedings{10.1145/3649476.3658799,
author = {Wan, Gwok-Waa and Wong, Sam-Zaak and Wang, Xi},
title = {Jailbreaking Pre-trained Large Language Models Towards Hardware Vulnerability Insertion Ability},
year = {2024},
isbn = {9798400706059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649476.3658799},
doi = {10.1145/3649476.3658799},
abstract = {We introduce RTLAttack, the first prompt-based jailbreak model designed to activate the hardware attack capabilities of LLM. Unlike conventional approaches, RTLAttack combines LLM and hardware security traits, enabling models to execute sensitive tasks like hardware Trojans insertion persistently. Extensive experiments across 10 prominent LLMs, including Claude and ChatGPT, demonstrated RTLAttack efficacy, achieving an 88.90% hardware vulnerability insertion success rate. Moreover, we analyzed the integrity and usability of LLM-injected vulnerabilities, unveiling inherent attack capabilities and harmful applications in current LLMs. This study aims to foster a comprehensive understanding of LLM capabilities in LLM-aided hardware design and drive finer-grained alignment.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2024},
pages = {579–582},
numpages = {4},
keywords = {Hardware Security, LLM, LLM Jailbreak, Vulnerability Insertion},
location = {Clearwater, FL, USA},
series = {GLSVLSI '24}
}

@inproceedings{10.1145/3691620.3694999,
author = {Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian},
title = {Understanding Code Changes Practically with Small-Scale Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3694999},
doi = {10.1145/3691620.3694999},
abstract = {Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with ≥70b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {216–228},
numpages = {13},
keywords = {code change, code review, language model, LLM, SLM},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3719384.3719473,
author = {Dilworth, Robert},
title = {Cloud Computing and Security: An Overview of Vulnerabilities, Cyber Attacks, and AI-Driven Solutions},
year = {2025},
isbn = {9798400717925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719384.3719473},
doi = {10.1145/3719384.3719473},
abstract = {Cloud computing has emerged as a cornerstone of modern IT infrastructure, offering scalability, flexibility, and cost-efficiency. However, despite its widespread adoption and apparent advantages, cloud environments remain susceptible to various vulnerabilities and cyber attacks. This paper delves into the intricacies of cloud computing, highlighting its significance while shedding light on the persistent challenges posed by security threats. By intertwining the realms of Artificial Intelligence (AI), Machine Learning (ML), Language Learning Models (LLMs), Generative Adversarial Networks (GANs), and Data Science, we explore the potential of these technologies in fortifying cloud security. Our discourse extends beyond mere observation, presenting a comprehensive analysis of vulnerabilities, cyber attacks, and AI-driven solutions. We propose innovative approaches such as automation misconfiguration detection, threat detection, and log analysis, underlining the pivotal role of AI in mitigating security risks in cloud environments.},
booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference},
pages = {615–626},
numpages = {12},
keywords = {Cloud Computing and Security, Vulnerabilities, Cyber Attacks, Artificial Intelligence (AI), Machine Learning (ML), Language Learning Models (LLMs), Generative Adversarial Networks (GANs), Data Science, Automation, Misconfiguration, Threat Detection, Anomaly Detection, Log Analysis, Multi-cloud Configurations},
location = {
},
series = {AICCC '24}
}

@article{10.14778/3685800.3685888,
author = {Fu, Zhe and Sha, Mo and Li, Yiran and Li, Huorong and Ma, Yubing and Wang, Sheng and Li, Feifei},
title = {EncChain: Enhancing Large Language Model Applications with Advanced Privacy Preservation Techniques},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685888},
doi = {10.14778/3685800.3685888},
abstract = {In response to escalating concerns about data privacy in the Large Language Model (LLM) domain, we demonstrate EncChain, a pioneering solution designed to bolster data security in LLM applications. EncChain presents an all-encompassing approach to data protection, encrypting both the knowledge bases and user interactions. It empowers confidential computing and implements stringent access controls, offering a significant leap in securing LLM usage. Designed as an accessible Python package, EncChain ensures straightforward integration into existing systems, bolstered by its operation within secure environments and the utilization of remote attestation technologies to verify its security measures. The effectiveness of EncChain in fortifying data privacy and security in LLM technologies underscores its importance, positioning it as a critical advancement for the secure and private utilization of LLMs.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4413–4416},
numpages = {4}
}

@inproceedings{10.1145/3732365.3732410,
author = {Wu, Yibiao and Zhuang, Honglin and Jia, Yetao and Zhang, Yuting},
title = {A Survey of Machine Learning Approaches for Malware Detec-tion},
year = {2025},
isbn = {9798400713613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732365.3732410},
doi = {10.1145/3732365.3732410},
abstract = {At present, due to the rapid evolution of malicious code and the rapid progress of science and technology, the security of information system has been put forward higher requirements. This research systematically reviews the current detection technologies for malicious code, from classical machine learning algorithms to some existing mature algorithms, and introduces machine learning, deep learning, and other technologies on this basis. In addition, malicious code detection technology based on large-scale language modeling (LLMS) is also studied. This study focuses on the advantages, limitations, and problems in practical applications of various detection techniques, and compares traditional methods, machine learning, and deep learning methods, and looks forward to future work.},
booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering},
pages = {269–273},
numpages = {5},
keywords = {Deep Learning, Detection technology, LLM, Machine Learning, Malware},
location = {
},
series = {CNSSE '25}
}

@inproceedings{10.1145/3716368.3735230,
author = {Zhou, Xuan and Mohan, Utkarsh and Liu, Yao and Beerel, Peter},
title = {An Efficient Distributed Machine Learning Inference Framework with Byzantine Fault Detection},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735230},
doi = {10.1145/3716368.3735230},
abstract = {The gap between the complexity of the most advanced machine learning (ML) models, e.g., the large language model (LLMs), PaLM, with 540 billion parameters, and what hardware resources at the edge can support is growing. Two approaches to mitigate this gap are leveraging cloud-based ML servers, which introduce widely studied security, privacy, and reliability risks, and distributed inference, in which several local edge-based devices share the computational burden. Motivated by the fact that the security of the distributed inference approach has received far less attention, this paper proposes a low-cost and versatile scheme to add redundancy to distributed inference to mitigate compromised devices that can exhibit faulty or malicious behavior modeled as Byzantine faults. We mathematically derive the number of inferences required to detect the attack as a function of computation overhead and also develop a simulator. The simulation results on an LLM align closely with the theoretical values. Specifically, with a redundancy overhead of 10% and 4 out of 8 devices in a single layer compromised, the average number of inferences required to detect all malicious devices is only 39.42.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {56–63},
numpages = {8},
keywords = {distributed inference, security, machine learning inference security, Byzantine fault, fault detection},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3696630.3728525,
author = {Zhang, Lyuye and Liu, Chengwei and Wu, Jiahui and Zhang, Shiyang and Liu, Chengyue and Xu, Zhengzi and Chen, Sen and Liu, Yang},
title = {Drop the Golden Apples: Identifying Third-Party Reuse by DB-Less Software Composition Analysis},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728525},
doi = {10.1145/3696630.3728525},
abstract = {The prevalent use of third-party libraries (TPLs) in modern software development introduces significant security and compliance risks, necessitating the implementation of Software Composition Analysis (SCA) to manage these threats. However, the accuracy of SCA tools heavily relies on the quality of the integrated feature database to cross-reference with user projects. While under the circumstance of the exponentially growing of open-source ecosystems and the integration of large models into software development, it becomes even more challenging to maintain a comprehensive feature database for potential TPLs. To this end, after referring to the evolution of LLM applications in terms of external data interactions, we propose the first framework of DB-Less SCA, to get rid of the traditional heavy database and embrace the flexibility of LLMs to mimic the manual analysis of security analysts to retrieve identical evidence and confirm the identity of TPLs by supportive information from the open Internet. Our experiments on two typical scenarios, native library identification for Android and copy-based TPL reuse for C/C++, especially on artifacts that are not that underappreciated, have demonstrated the favorable future for implementing database-less strategies in SCA.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {691–695},
numpages = {5},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3641554.3701844,
author = {Yu, Zezhu and Liu, Suqing and Denny, Paul and Bergen, Andreas and Liut, Michael},
title = {Integrating Small Language Models with Retrieval-Augmented Generation in Computing Education: Key Takeaways, Setup, and Practical Insights},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701844},
doi = {10.1145/3641554.3701844},
abstract = {Leveraging a Large Language Model (LLM) for personalized learning in computing education is promising, yet cloud-based LLMs pose risks around data security and privacy. To address these concerns, we developed and deployed a locally stored Small Language Model (SLM) utilizing Retrieval-Augmented Generation (RAG) methods to support computing students' learning. Previous work has demonstrated that SLMs can match or surpass popular LLMs (gpt-3.5-turbo and gpt-4-32k) in handling conversational data from a CS1 course. We deployed SLMs with RAG (SLM + RAG) in a large course with more than 250 active students, fielding nearly 2,000 student questions, while evaluating data privacy, scalability, and feasibility of local deployments. This paper provides a comprehensive guide for deploying SLM + RAG systems, detailing model selection, vector database choice, embedding methods, and pipeline frameworks. We share practical insights from our deployment, including scalability concerns, accuracy versus context length trade-offs, guardrails and hallucination reduction, as well as data privacy maintenance. We address the "Impossible Triangle" in RAG systems, which states that achieving high accuracy, short context length, and low time consumption simultaneously is not feasible. Furthermore, our novel RAG framework, Intelligence Concentration (IC), categorizes information into multiple layers of abstraction within Milvus collections mitigating trade-offs and enabling educational assistants to deliver more relevant and personalized responses to students quickly.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1302–1308},
numpages = {7},
keywords = {computer science education, computing education, conversational agent, intelligence concentration, intelligent tutoring system, large language models, milvus, personalized ai agent, retrieval-augmented generation, small language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3674805.3695401,
author = {Esposito, Matteo and Palagiano, Francesco and Lenarduzzi, Valentina and Taibi, Davide},
title = {Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3695401},
doi = {10.1145/3674805.3695401},
abstract = {Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in Risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis. Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other three human experts to review the models and the former human expert’s analysis. The reviewers analyzed 5,000 scenario analyses. Results and Conclusions. HEs demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs for an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {517–527},
numpages = {11},
keywords = {Actionability, Analysis, Explainability, Fine-Tuning, Generative AI, Human Experts, Large Language Model, Management, Retrieval Augmented Generation, Risk, Security, Standards, XAI},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1145/3728946,
author = {Ma, Jie and He, Ningyu and Xi, Jinwen and Xing, Mingzhe and Wang, Haoyu and Gao, Ying and Yue, Yinliang},
title = {OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum Virtual Machine},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728946},
doi = {10.1145/3728946},
abstract = {As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has become the cornerstone powering tens of millions of active smart contracts. Intuitively, security issues in EVMs could lead to inconsistent behaviors among smart contracts or even denial-of-service of the entire blockchain network. However, to the best of our knowledge, only a limited number of studies focus on the security of EVMs. Moreover, they suffer from 1) insufficient test input diversity and invalid semantics; and 2) the inability to automatically identify bugs and locate root causes.   To bridge this gap, we propose OpDiffer, a differential testing framework for EVM, which takes advantage of LLMs and static analysis methods to address the above two limitations.   We conducted the largest-scale evaluation, covering nine EVMs and uncovering 26 previously unknown bugs, 22 of which have been confirmed by developers and three have been assigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve code coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an analysis of real-world deployed Ethereum contracts, we estimate that 7.21% of the contracts could trigger our identified EVM bugs under certain environmental settings, potentially resulting in severe negative impact on the Ethereum ecosystem.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA069},
numpages = {24},
keywords = {Differential Testing, Ethereum Virtual Machine, Large Language Model}
}

@inproceedings{10.1145/3728199.3728273,
author = {Li, Haoran and Shang, Duo and Luo, Jun and Lu, Chunyu and He, Huihong and Hui, Xin and Chen, Tianran and Shi, Ruhui},
title = {Chinese-Russian bilingual LLM for the China-Russia pipeline and its application in disaster scenarios},
year = {2025},
isbn = {9798400713231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728199.3728273},
doi = {10.1145/3728199.3728273},
abstract = {The China-Russia Pipeline is a significant economic and geopolitical infrastructure project that is essential to both nations' energy security and the advancement of bilateral relations. The development of a new Chinese-Russian bilingual LLM that focuses on catastrophe prevention, response, and recovery on this crucial pipeline is described in this research. To do this, we build a top-notch Chinese-Russian bilingual knowledge base to improve the LLM. To improve the model's reasoning and decision-making abilities, we also employ knowledge graphs and idea chain reasoning approaches. Given these considerations, the goal of this work is to provide a practical MMP of an LLM that is backed by empirical evaluations demonstrating that our approach outperforms general-purpose LLMs in terms of decision-making capabilities and even surpasses them in answering pipeline-related domain-specific queries.},
booktitle = {Proceedings of the 2025 3rd International Conference on Communication Networks and Machine Learning},
pages = {443–448},
numpages = {6},
keywords = {Chinese-Russian pipeline, LLM, dataset},
location = {
},
series = {CNML '25}
}

@inproceedings{10.1145/3716489.3728437,
author = {Olea, Carlos and Christensen, Alexander and Fazio, Lisa and Cutting, Laurie and Lieb, Maxwell and Phelan, Jessica and Wise, Alyssa and Tucker, Holly},
title = {Evaluating Phishing Email Efficacy},
year = {2025},
isbn = {9798400714979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716489.3728437},
doi = {10.1145/3716489.3728437},
abstract = {Phishing remains one of the most prevalent attack vectors in the modern cybersecurity landscape. Though filters and other provider-side methods are useful in blocking certain categories of phishing emails, it remains evident that the weakest link in email security is the user. With the advent of LLMs as highly accessible and easy-to-use tools for text generation, the employment of these tools in the creation of phishing emails is almost certainly a current reality. In this study, we evaluate the likelihood of over 160 undergraduate students correctly labeling both human and LLM emails as phishing (also referred to as malign in this paper) or real (also referred to as benign in this paper) emails in a semi-controlled environment both with and without time pressures. We find that LLM generated emails are a possible security weakness for this user group and deserve increased attention. We also find that certain personality traits measured by the Need for Cognition (a trait described as a need to structure relevant situations in meaningful, integrated ways and a need to understand and make reasonable the experiential world [7]) scale and the Big Five personality Inventory may correlate with a higher likelihood of susceptibility to being phished depending on the email source. We present possible explanations for these findings and propose future work in training and research on this growing concern.},
booktitle = {Proceedings of the 2025 Computers and People Research Conference},
articleno = {7},
numpages = {8},
keywords = {Phishing, Cybersecurity, Large Language Models, Personality Traits},
location = {
},
series = {SIGMIS-CPR '25}
}

@inproceedings{10.1145/3691620.3695055,
author = {Wu, Cong and Chen, Jing and Wang, Ziwei and Liang, Ruichao and Du, Ruiying},
title = {Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695055},
doi = {10.1145/3691620.3695055},
abstract = {Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0% and a false positive rate of 0.29%. These results highlight PonziSleuth's capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {582–593},
numpages = {12},
keywords = {smart contracts, large language model, ponzi contracts},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3711816,
author = {Liu, Puzhuo and Sun, Chengnian and Zheng, Yaowen and Feng, Xuan and Qin, Chuan and Wang, Yuncheng and Xu, Zhenyang and Li, Zhi and Di, Peng and Jiang, Yu and Sun, Limin},
title = {LLM-Powered Static Binary Taint Analysis},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3711816},
doi = {10.1145/3711816},
abstract = {This article proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware, which the baselines failed to find. Moreover, 10 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {83},
numpages = {36},
keywords = {binary, taint analysis, large language model, vulnerability}
}

@inproceedings{10.1145/3637528.3671837,
author = {Ning, Liang-bo and Wang, Shijie and Fan, Wenqi and Li, Qing and Xu, Xin and Chen, Hao and Huang, Feiran},
title = {CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671837},
doi = {10.1145/3637528.3671837},
abstract = {Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2284–2295},
numpages = {12},
keywords = {adversarial attacks, large language models, llm-empowered recommender systems, llms-based agent, recommender systems},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3701716.3717866,
author = {Wen, Qingsong and Zhang, Yongfeng and Liu, Zhiwei and McAuley, Julian and Wei, Hua and Pang, Linsey and Liu, Wei and Yu, Philip S.},
title = {The 3rd Workshop on AI Agent for Information Retrieval: Generating and Ranking},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717866},
doi = {10.1145/3701716.3717866},
abstract = {The field of information retrieval has significantly transformed with the integration of AI technologies. AI agents, especially those leveraging LLMs and vast computational power, have revolutionized information retrieval, processing, and presentation. LLM agents, with advanced memory, reasoning, and planning capabilities, can perform complex tasks, engage in coherent conversations, and provide personalized responses. Despite these advancements, challenges such as ensuring relevance and accuracy, mitigating biases, providing real-time responses, and maintaining data security remain. This workshop aims to explore these challenges, share innovative solutions, and discuss future directions. It will provide a platform to bring together researchers and practitioners to discuss the latest theoretical advancements and practical implementations of AI agents in information retrieval. Topics include AI in search, recommendation, and personalization systems. By gathering a diverse group of experts, the workshop seeks to deepen the understanding of AI agents in information retrieval, advance the field, and enhance its societal impact. Participants will gain insights into cutting-edge research and emerging trends, and foster knowledge exchange and collaboration within the community.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1659–1662},
numpages = {4},
keywords = {information retrieval, llm agent, ranking},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3706598.3714074,
author = {Ma, Ying and Zhang, Shiquan and Yang, Dongju and Sarsenbayeva, Zhanna and Knibbe, Jarrod and Goncalves, Jorge},
title = {Raising Awareness of Location Information Vulnerabilities in Social Media Photos using LLMs},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714074},
doi = {10.1145/3706598.3714074},
abstract = {Location privacy leaks can lead to unauthorised tracking, identity theft, and targeted attacks, compromising personal security and privacy. This study explores LLM-powered location privacy leaks associated with photo sharing on social media, focusing on user awareness, attitudes, and opinions. We developed and introduced an LLM-powered location privacy intervention app to 19 participants, who used it over a two-week period. The app prompted users to reflect on potential privacy leaks that a widely available LLM could easily detect, such as visual landmarks&nbsp;&amp; cues that could reveal their location, and provided ways to conceal this information. Through in-depth interviews, we found that our intervention effectively increased users’ awareness of location privacy and the risks posed by LLMs. It also encouraged users to consider the importance of maintaining control over their privacy data and sparked discussions about the future of location privacy-preserving technologies. Based on these insights, we offer design implications to support the development of future user-centred, location privacy-preserving technologies for social media photos.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {238},
numpages = {14},
keywords = {location privacy, social media, privacy leaks, photos, LLMs, intervention, interviews},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3696673.3723069,
author = {Meda, Kavya Nikhita and Nara, Pavan Subhash Chandrabose and Bozenka, Svoboda and Zormati, Tarek and Turner, Seth and Worley, Wayne and Mitra, Reshmi},
title = {Integrating Prompt Structures Using LLM Embeddings for Cybersecurity Threats},
year = {2025},
isbn = {9798400712777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696673.3723069},
doi = {10.1145/3696673.3723069},
abstract = {This paper aims to develop a specialized Large Language Model (LLM) for cybersecurity training, designed to educate users on fundamental cybersecurity concepts. This paper focuses on creating an interactive system where users can ask questions about computer security and receive accurate, informative responses. By addressing cybersecurity as a critical national issue, the LLM empowers individuals and organizations to defend against malicious cyber threats. Our system was developed using Python, utilizing Google Sheets as a database, Gradio for the user interface, and Google Gemini's API for advanced language processing. The implementation followed a test-driven development approach, iterating between coding and testing to ensure functionality and reliability. Key technologies include Mistral's Large 2 model and embedding models for clustering related data. The Retrieval-Augmented Generation (RAG) framework was employed to integrate information retrieval with the LLM, enhancing its accuracy and relevance. Tools such as Google Suite, Colab, and Gradio contributed to creating a robust and user-friendly system. This paper highlights the potential of domain-specific LLMs, offering a practical solution to the growing need for accessible cybersecurity education and fostering awareness to mitigate the risks posed by malicious hackers.},
booktitle = {Proceedings of the 2025 ACM Southeast Conference},
pages = {180–187},
numpages = {8},
keywords = {large language model (LLM), embedding models, retrieval-augmented generation (RAG), information retrieval, cybersecurity education},
location = {Southeast Missouri State University, Cape Girardeau, MO, USA},
series = {ACMSE 2025}
}

@inproceedings{10.1145/3661167.3661226,
author = {Esposito, Matteo and Palagiano, Francesco},
title = {Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661226},
doi = {10.1145/3661167.3661226},
abstract = {Preliminary security risk analysis (PSRA) provides a quick approach to identify, evaluate, and propose remediation to potential risks in specific scenarios. The extensive expertise required for an effective PSRA and the substantial textual-related tasks hinders quick assessments in mission-critical contexts, where timely and prompt actions are essential. The speed and accuracy of human experts in PSRA significantly impact response time. A large language model can quickly summarise information in less time than a human. To our knowledge, no prior study has explored the capabilities of fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of FTM in assisting practitioners in PSRA. We manually curated 141 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the proficiency of the FTM versus seven human experts. Within the industrial context, our approach has proven successful in reducing errors in PSRA, hastening security risk detection, and minimizing false positives and negatives. This translates to cost savings for the company by averting unnecessary expenses associated with implementing unwarranted countermeasures. Therefore, experts can focus on more comprehensive risk analysis, leveraging LLMs for an effective preliminary assessment within a condensed timeframe.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {442–445},
numpages = {4},
keywords = {Analysis, Fine-Tuning, Generative AI, Human Experts, LLM, Large Language Model, Management, Preliminary, Risk, Security, Standards},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3627673.3680120,
author = {Zhang, Yongfeng and Liu, Zhiwei and Wen, Qingsong and Pang, Linsey and Liu, Wei and Yu, Philip S.},
title = {AI Agent for Information Retrieval: Generating and Ranking},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680120},
doi = {10.1145/3627673.3680120},
abstract = {The field of information retrieval has significantly transformed with the integration of AI technologies. AI agents, especially those leveraging LLMs and vast computational power, have revolutionized information retrieval, processing, and presentation. LLM agents, with advanced memory, reasoning, and planning capabilities, can perform complex tasks, engage in coherent conversations, and provide personalized responses. Despite these advancements, challenges such as ensuring relevance and accuracy, mitigating biases, providing real-time responses, and maintaining data security remain. This workshop aims to explore these challenges, share innovative solutions, and discuss future directions. It will provide a platform to bring together researchers, practitioners to discuss the latest theoretical advancements and practical implementations of AI agents in information retrieval. Topics include AI in search, recommendation, and personalization systems. By gathering a diverse group of experts, the workshop seeks to deepen the understanding of AI agents in information retrieval, advance the field, and enhance its societal impact. Participants will gain insights into cutting-edge research, emerging trends, and foster knowledge exchange and collaboration within the community.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5605–5607},
numpages = {3},
keywords = {ai agent, information retrieval, large language models (llms), recommender systems},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3702138.3702140,
author = {DiMario, Carmen L. and Bacha, Rio C. and Butka, Brian K.},
title = {Combatting Senior Scams Using a Large Language Model-Created Rubric: This paper explains a novel approach to reinforcement learning designed for the detection and prevention of scams aimed at senior citizens},
year = {2025},
isbn = {9798400717543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702138.3702140},
doi = {10.1145/3702138.3702140},
abstract = {This paper addresses the critical issue of internet scams targeting seniors by developing a robust, Machine Learning (ML) based solution employing a Large Language Model (LLM), specifically ChatGPT, to enhance scam detection and prevention. Elderly internet users are particularly vulnerable to digital fraud due to a lack of familiarity with technological safeguards and a tendency not to report incidents. Traditional security measures often fail to accommodate the unique challenges faced by this demographic, prompting our focus on a specialized, user-friendly solution. We propose an innovative approach using ChatGPT 3.5 to analyze and score emails based on their likelihood of being scams, thus providing seniors with a tool that requires minimal interaction while offering maximum protection. This system uses a custom rubric developed through ML techniques to evaluate potential threats effectively. By integrating word embeddings and a diverse training dataset, the model adapts to the nuanced and evolving nature of scam tactics. The methodology utilized in this paper ensures that the ML model not only identifies common scam indicators but also provides actionable feedback to users, making it a practical tool for real-world applications. Preliminary results demonstrate the system's efficacy in recognizing scam emails, thereby significantly reducing the risk of financial loss among seniors and enhancing their confidence in digital communication. This paper outlines the design, implementation, and testing phases of the project, highlighting the potential of LLMs in cybersecurity, specifically in protecting a vulnerable population.},
booktitle = {Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference},
pages = {130–136},
numpages = {7},
keywords = {Cyber security, artificial intelligence, email, machine learning, scam},
location = {
},
series = {ASSE '24}
}

@inproceedings{10.1145/3658644.3690209,
author = {Bernhard, Lukas and Schiller, Nico and Schloegel, Moritz and Bars, Nils and Holz, Thorsten},
title = {DarthShader: Fuzzing WebGPU Shader Translators &amp; Compilers},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690209},
doi = {10.1145/3658644.3690209},
abstract = {A recent trend towards running more demanding web applications, such as video games or client-side LLMs, in the browser has led to the adoption of the WebGPU standard that provides a cross-platform API exposing the GPU to websites. This opens up a new attack surface: Untrusted web content is passed through to the GPU stack, which traditionally has been optimized for performance instead of security. Worsening the problem, most of WebGPU cannot be run in the tightly sandboxed process that manages other web content, which eases the attacker's path to compromising the client machine. Contrasting its importance, WebGPU shader processing has received surprisingly little attention from the automated testing community. Part of the reason is that shader translators expect highly structured and statically typed input, which renders typical fuzzing mutations ineffective. Complicating testing further, shader translation consists of a complex multi-step compilation pipeline, each stage presenting unique requirements and challenges.In this paper, we propose DarthShader, the first language fuzzer that combines mutators based on an intermediate representation with those using a more traditional abstract syntax tree. The key idea is that the individual stages of the shader compilation pipeline are susceptible to different classes of faults, requiring entirely different mutation strategies for thorough testing. By fuzzing the full pipeline, we ensure that we maintain a realistic attacker model. In an empirical evaluation, we show that our method outperforms the state-of-the-art fuzzers regarding code coverage. Furthermore, an extensive ablation study validates our key design. DarthShader found a total of 39 software faults in all modern browsers Chrome, Firefox, and Safari that prior work missed. For 15 of them, the Chrome team assigned a CVE, acknowledging the impact of our results.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {690–704},
numpages = {15},
keywords = {WGSL, WebGPU, browser security, fuzzing, graphics shaders, software security},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

