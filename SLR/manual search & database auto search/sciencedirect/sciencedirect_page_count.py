import pandas as pd
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.service import Service

# åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼‰
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')

# æ­£ç¡®åˆå§‹åŒ–æµè§ˆå™¨çš„å†™æ³•
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel("sciencedirect_merged_results.xlsx")

# åˆå§‹åŒ–æ–°åˆ—
if "page_count" not in df.columns:
    df["page_count"] = None
if "page_fetch_status" not in df.columns:
    df["page_fetch_status"] = None

# æå–å¹²å‡€çš„ URL å‡½æ•°
def extract_clean_url(raw_url):
    if not isinstance(raw_url, str):
        return None
    match = re.search(r'https://www\.sciencedirect\.com[^\']+', raw_url)
    return match.group(0) if match else None

# æŠ“å–é¡µç æ•°
def fetch_page_count(url):
    try:
        driver.get(url)
        time.sleep(1.5)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # æŸ¥æ‰¾åŒ…å«é¡µç çš„æ ‡ç­¾
        page_span = soup.find("span", string=re.compile(r"Pages", re.IGNORECASE))
        if page_span:
            text = page_span.text.strip()
            match = re.search(r'(\d+)\s*[-â€“]\s*(\d+)', text)
            if match:
                start = int(match.group(1))
                end = int(match.group(2))
                return end - start + 1, "success"
            else:
                return None, "found span, but no match"
        else:
            return None, "page not found"
    except Exception as e:
        return None, f"error: {str(e)}"

# ä¸»å¾ªç¯
for idx, row in df.iterrows():
    raw_url = row.get("urls", None)
    url = extract_clean_url(raw_url)

    if not url:
        df.at[idx, "page_fetch_status"] = "no valid url"
        continue

    print(f"ğŸ” [{idx}] Fetching: {url}")
    count, status = fetch_page_count(url)
    df.at[idx, "page_count"] = count
    df.at[idx, "page_fetch_status"] = status
    print(f"â†’ page count: {count}, status: {status}")
    time.sleep(1.5)

# ä¿å­˜ç»“æœ
output_path = "sciencedirect_with_page_count.xlsx"
df.to_excel(output_path, index=False)
print(f"âœ… å®Œæˆï¼ç»“æœä¿å­˜ä¸º {output_path}")

# å…³é—­æµè§ˆå™¨
driver.quit()