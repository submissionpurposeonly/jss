"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction","S. Kang; J. Yoon; N. Askarbekkyzy; S. Yoo","Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea",IEEE Transactions on Software Engineering,"17 Oct 2024","2024","50","10","2677","2694","Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique Libro could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using Libro improves as LLM size increases, providing information as to which LLMs can be used with the Libro pipeline.","1939-3520","","10.1109/TSE.2024.3450837","National Research Foundation of Korea (NRF)(grant numbers:RS-2023-00208998); Engineering Research Center Program(grant numbers:2021R1A5A1021944); Institute for Information and Communications Technology Promotion(grant numbers:IITP2022-0-00995); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664637","Test generation;natural language processing;software engineering","Computer bugs;Codes;Pipelines;Large language models;Debugging;Java;Computational modeling","","7","","58","IEEE","4 Sep 2024","","","IEEE","IEEE Journals"
"Porting an LLM based Application from ChatGPT to an On-Premise Environment","T. Paloniemi; M. Setälä; T. Mikkonen","University of Jyväskylä, Jyväskylä, Finland; Solita Tampere, Finland; University of Jyväskylä, Jyväskylä, Finland",2025 IEEE/ACM 22nd International Conference on Software and Systems Reuse (ICSR),"10 Jun 2025","2025","","","78","83","Given the data-intensive nature of Machine Learning (ML) systems in general, and Large Language Models (LLM) in particular, using them in cloud based environments can become a challenge due to legislation related to privacy and security of data. Taking such aspects into consideration implies porting the LLMs to an on-premise environment, where privacy and security can be controlled. In this paper, we study this porting process of a real-life application using ChatGPT, which runs in a public cloud, to an on-premise environment. The application being ported is AIPA, a system that leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids. The main considerations in the porting process include transparency of open source models and cost of hardware, which are central design choices of the on-premise environment. In addition to presenting the porting process, we evaluate downsides and benefits associated with porting.","","979-8-3315-2617-7","10.1109/ICSR66718.2025.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024507","Porting;Large Language Models;LLMs","Procurement;Cloud computing;Privacy;Costs;Large language models;Legislation;Machine learning;Chatbots;Software;Hardware","","","","31","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"On the Evaluation of Large Language Models in Unit Test Generation","L. Yang; C. Yang; S. Gao; W. Wang; B. Wang; Q. Zhu; X. Chu; J. Zhou; G. Liang; Q. Wang; J. Chen","College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; School of Future Technology, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; School of Computer and Information Technology, Beijing Jiaotong University, China; Key Laboratory of HCST, MoE DCST, Peking University, China; Huawei Cloud Computing Co. Ltd., China; Huawei Cloud Computing Co. Ltd., China; Huawei Cloud Computing Co. Ltd., China; Huawei Cloud Computing Co. Ltd., China; College of Intelligence and Computing, Tianjin University, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1607","1619","Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs’ capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.CCS CONCEPTS • Software and its engineering → Software testing and debugging.","2643-1572","979-8-4007-1248-7","","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765033","Large Language Model;Unit Test Generation;Empirical Study","Software testing;Codes;Large language models;Training data;Writing;Syntactics;Software;Test pattern generators;Defect detection;Software engineering","","4","","94","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Stealing Watermarks of Large Language Models via Mixed Integer Programming","Z. Zhang; X. Zhang; Y. Zhang; L. Y. Zhang; C. Chen; S. Hu; A. Gill; S. Pan","University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia; University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia; Royal Melbourne Institute of Technology, Melbourne, Australia; Huazhong University of Science and Technology, Wuhan, China; University of Technology Sydney, Sydney, Australia; Griffith University, Gold Coast, Australia",2024 Annual Computer Security Applications Conference (ACSAC),"18 Mar 2025","2024","","","46","60","The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM’s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.","2576-9103","979-8-3315-2088-5","10.1109/ACSAC63791.2024.00021","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917955","Large Language Model;LLM Security;Watermarking;Security and Privacy","Integer programming;Threat modeling;Vocabulary;Privacy;Large language models;Perturbation methods;Watermarking;Detectors;Programming;Robustness","","","","34","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Dynamic Data Integration for Resilience to Sensor Attacks in Multi-Agent Systems","L. Burbano; L. F. Cómbita; N. Quijano; S. Rueda","Departamento de Ingeniería Eléctrica y Electrónica, Universidad de Los Andes, Bogotá, Colombia; Departamento de Ingeniería Eléctrica y Electrónica, Universidad de Los Andes, Bogotá, Colombia; Departamento de Ingeniería Eléctrica y Electrónica, Universidad de Los Andes, Bogotá, Colombia; Departamento de Ingeniería de Sistemas y Computación, Universidad de Los Andes, Bogotá, Colombia",IEEE Access,"26 Feb 2021","2021","9","","31236","31245","In recent years the number of security incidents affecting control systems has increased. These incidents have shown the need to develop strategies to improve system resilience to cyber-attacks. This paper presents a practical implementation of a strategy to detect cyber-attacks and mitigate their effects on sensors of a multi-agent system. The proposed approach computes, in real-time, a convex combination of measurements from main and redundant sensors, producing a trust value of the measurement and feeding it to the controller. We implemented this approach on a testbed of ground robots in formation. Experimental results to various kinds of attacks and a key performance index show that the proposed strategy reduces the effects of attacks not only on the affected agent, but also prevents the propagation of the attack over the remaining agents.","2169-3536","","10.1109/ACCESS.2021.3059560","Air Force Office of Scientific Research(grant numbers:FA9550-19-1-0014); Comisión de Estudios No. 015 de 2014 by Universidad Distrital Francisco José de Caldas; Convocatoria 727 Doctorados Nacionales 2015 by Colciencias; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354779","Cyber-physical systems;multi-agent systems;resilience to sensor attacks;sensor attacks","Sensors;Robots;Sensor systems;Robot sensing systems;Wheels;Multi-agent systems;Mobile robots","","10","","22","CCBY","16 Feb 2021","","","IEEE","IEEE Journals"
"Robust Pricing Mechanism for Resource Sustainability Under Privacy Constraint in Competitive Online Learning Multi-Agent Systems","E. Tampubolon; H. Boche","Lehrstuhl für Theoretische Informationstechnik, Technische Universität München; Lehrstuhl für Theoretische Informationstechnik, Technische Universität München","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8733","8737","We consider the problem of resource congestion control for competing online learning agents under privacy and security constraints. Based on the non-cooperative game as the model for agents' interaction and the noisy online mirror ascent as the model for the rationality of the agents, we propose a novel pricing mechanism that gives the agents incentives for sustainable use of the resources. An advantage of our method is that it is privacy-preserving in the sense that mainly the resource congestion serves as an orientation for our pricing mechanism, in place of the agents' preference and state. Moreover, our method is robust against adversary agents' feedback in the form of the noisy gradient. We present the following result of our theoretical investigation: In case that the feedback noise is persistent, and for several choices of the intrinsic parameter (the learning rate) of the agents and of the mechanism parameters (the learning rate of the price-setters, their progressivity, and the extrinsic price sensitivity of the agents), we show that the accumulative violation of the resource constraints of the resulted iterates is sub-linear w.r.t the time horizon. To support our theoretical findings, we provide some numerical simulations.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054699","Game Theory;Online Learning;Pricing Mechanism;Resource Sustainability;Multi-Agent Systems","Privacy;Pricing;Games;Signal processing;Noise measurement;Sustainable development;Speech processing","","3","","30","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Improving the Efficiency of IDPS by using Hybrid Methods from Artificial Intelligence","G. Tsochev; R. Trifonov; R. Yoshinov; S. Manolov; G. Pavlova","Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Laboratory of Telematics, Bulgarian Academy of Sciences, Sofia, Bulgaria; Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria",2019 International Conference on Information Technologies (InfoTech),"7 Oct 2019","2019","","","1","4","The present paper describes some of the results obtained in the Faculty of Computer Systems and Technology at Technical University of Sofia in the implementation of project related to the application of intelligent methods for increasing the security in computer networks. Also is made a survey about existing hybrid methods, which are using several artificial intelligent methods for cyber defense. The paper introduces a model for intrusion detection systems where multi agent systems are the bases and artificial intelligence are applicable by the means simple real-time models constructed in laboratory environment.","","978-1-7281-3274-7","10.1109/InfoTech.2019.8860895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8860895","multi-agent systems;artificial intelligence;network and information security;intrusion detection system;intrusion prevention system;reinforcement learning","Monitoring;Computational modeling;IP networks;Artificial intelligence;Optimization;Fuzzy logic;Security","","4","","12","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Systematic Literature Review of Prompt Engineering Patterns in Software Engineering","Y. Sasaki; H. Washizaki; J. Li; D. Sander; N. Yoshioka; Y. Fukazawa","Waseda University, Japan; Waseda University, Japan; Waseda University, Japan; University of Hamburg, Germany; Waseda University, Japan; Waseda University, Japan","2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)","26 Aug 2024","2024","","","670","675","Advancements in large language models (LLMs) are transforming software engineering through innovative prompt engineering strategies. By analyzing prompt-driven enhancements across key software engineering tasks, we present a sys-tematic literature review and a pioneering taxonomy elucidating the practical applications of prompt engineering in software engineering. Our taxonomy offers a foundational framework that clarifies the roles of prompt engineering and measures its impact, thereby guiding evolving AI -driven software engineering research and practices.","2836-3795","979-8-3503-7696-8","10.1109/COMPSAC61105.2024.00096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633588","prompt engineering;software engineering;sys-tematic literature review;large language model;LLM application","Technological innovation;Large language models;Taxonomy;Market research;Software;Prompt engineering;Software measurement","","4","","35","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"The Fault in our Stars: Quality Assessment of Code Generation Benchmarks","M. L. Siddiq; S. Dristi; J. Saha; J. C. S. Santos","University of Notre Dame, Notre Dame, IN, USA; University of Virginia, Charlottesville, VA, USA; University of Virginia, Charlottesville, VA, USA; University of Notre Dame, Notre Dame, IN, USA",2024 IEEE International Conference on Source Code Analysis and Manipulation (SCAM),"19 Dec 2024","2024","","","201","212","Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.","2470-6892","979-8-3315-2850-8","10.1109/SCAM63643.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795375","benchmarks;code generation;data quality;data contamination","Java;Codes;Source coding;Large language models;Documentation;Benchmark testing;Software;Quality assessment;Contamination;Python","","1","","66","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"A Binary Question Answering System for Diagnosing Mental Health Syndromes powered by Large Language Model with Custom-Built Dataset","D. Pawar; S. Phansalkar","MIT Art Design Technology University, Pune, India; MIT Art Design Technology University, Pune, India",2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Mar 2025","2024","","","1","8","With telemedicine and remote patient monitoring, the need for accessible mental health resources has become increasingly apparent. This work presents a binary question-answering system tailored for individuals experiencing depression, utilizing Large Language Models (LLMs) designed to handle the complexities of mental health inquiries. By examining existing LLMs, a system enhanced by prompt engineering on pre-trained models was developed to meet the particular necessities of those seeking help for depression. Utilizing depression inventories and dictionaries, a detailed dataset of depression symptoms was compiled, creating precise prompts to elicit comprehensive and relevant responses from the LLMs. The system was tested using zero-shot, one-shot, and few-shot learning techniques, with results compared against human-generated responses to assess efficacy. This study confirms the flexibility and accuracy of LLMs in managing sensitive mental health conversations and serves as a foundation for future developments. Further enhancement of the system is proposed through specialized fine-tuning of LLMs with a dedicated corpus of depression-related prompts. The goal is to deliver a scalable, empathetic tool that provides timely and effective support for individuals battling mental health issues, leveraging cutting-edge AI technology to address a critical gap in mental health care.","","979-8-3315-1898-1","10.1109/ICTBIG64922.2024.10911079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911079","Mental Health;Depression;Symptoms;Large Language Models;Prompt Engineering;Fine Tuning;Question Answering;Biomedical;Healthcare","Patient monitoring;Large language models;Telemedicine;Mental health;Oral communication;Medical services;Depression;Question answering (information retrieval);Prompt engineering;Tuning","","","","25","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Test-Driven Development and LLM-based Code Generation","N. S. Mathews; M. Nagappan","University of Waterloo, Canada; University of Waterloo, Canada",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1583","1594","Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.CCS CONCEPTS• Software and its engineering → Software development techniques; • Computing methodologies → Artificial intelligence.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764936","Code Generation;LLM;TDD;Testing;Software Engineering","Codes;Accuracy;Large language models;Computational modeling;Benchmark testing;Programming;Software;Mirrors;Software development management;Software engineering","","2","","31","","29 Nov 2024","","","IEEE","IEEE Conferences"
"AgriPrompt: A Method to Enhance ChatGPT for Agricultural Question Answering","T. Chen; X. Chen; Y. Qian; L. Zheng; H. Li; J. Zhao; Y. Wang","College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China",2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD),"10 Jul 2024","2024","","","2436","2441","We propose a method called AgriPrompt to enhance the agricultural question-answering capability of ChatGPT. We propose a BERT-based model, AgriParse, to extract semantic information from agricultural questions, which is filled into pre-defined prompt templates to guide ChatGPT in answering agricultural questions. We establish an evaluation metric called KQScore, which uses the TF-IDF method and the Word2vec model to evaluate the semantic accuracy of answers. The results show that the average KQScore of ChatGPT with AgriPrompt is 0.81, ChatGPT without prompt is 0.74, and ChatGPT with a static prompt is 0.72. Our method significantly improves the accuracy of ChatGPT in answering agricultural questions.","2768-1904","979-8-3503-4918-4","10.1109/CSCWD61410.2024.10580165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10580165","ChatGPT;Large Language Models;Knowledge-Based Question Answering;Prompt Engineering","Measurement;Accuracy;Federated learning;Computational modeling;Semantics;Chatbots;Question answering (information retrieval)","","","","9","IEEE","10 Jul 2024","","","IEEE","IEEE Conferences"
"Prostate Cancer Identification from Pathology Diagnosis Descriptions in Lithuanian Language","T. Griguola; G. Smailytė; L. Petkevičius","Faculty of Mathematics and Informatics, Vilnius University, Vilnius, Lithuania; National Cancer Institute, Vilnius, Lithuania; Institute of Computer Science, Vilnius University, Vilnius, Lithuania","2025 IEEE 12th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)","1 Jul 2025","2025","","","1","5","This research address the analysis on automating medical diagnosis extraction from medical diagnosis description. The methodology involved investigating several multilingual models, such as Google-Gemma-2-2B and EuroLLM-9B, to classify data based on the described cancer groups. Results showed that it is possible to classify Lithuanian pathological diagnosis with varying performance across different models. After checking all the results, the most stable model was Gemma-2-2B. When classifying T and N values the best results were obtain by Gemma-2-2B with respectively 65% and 78% accuracy. When classifying grade group value, the best results were obtained using Gemma-2-9B with 83% accuracy. This study demonstrate the cost-effective scalable approach to engineering and manual work of processing medical diagnosis.","2689-7342","978-1-6654-9688-9","10.1109/AIEEE66149.2025.11050774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050774","natural language processing;large language models;prostate cancer;prompt engineering","Pathology;Accuracy;Data models;Natural language processing;Registers;Multilingual;Medical diagnosis;Prompt engineering;Prostate cancer;Medical diagnostic imaging","","","","23","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Compositional API Recommendation for Library-Oriented Code Generation","Z. Ma; S. An; B. Xie; Z. Lin","School of Computer Science, Peking University, Key Laboratory of High Conidence Software Technologies, Ministry of Education, Beijing, China; Xi’an Jiaotong University, Xi’an, China; School of Computer Science, Peking University, Key Laboratory of High Conidence Software Technologies, Ministry of Education, Beijing, China; Microsoft Corporation, Beijing, China",2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC),"18 Jun 2024","2024","","","87","98","Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a “divide-and-conquer” strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID’s TorchdataAR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG’s Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.Ccs Concepts • Software and its engineering $\rightarrow$ Search-based software engineering; Software development techniques.","2643-7171","979-8-4007-0586-1","","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556452","API recommendation;code generation;requirements decomposition;large language model","Codes;Software libraries;Training data;Documentation;Benchmark testing;Software;Task analysis","","4","","51","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Can LLMs Understand Parallel and Distributed Machine Learning Algorithms in AIoT?","H. Du; W. Li; X. Ding; H. Huo","College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China; College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China; School of Computer Science, University of Technology Sydney, Sydney, Australia; School of Computer Science, University of Technology Sydney, Sydney, Australia",2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT),"10 Jun 2025","2024","","","501","510","The Internet of Things technology has evolved from standalone tools to open systems supporting parallel and distributed computing. In particular, federated learning (FL) has become a key solution as a distributed team of parallel training methods in the artificial intelligence of things (AIoT). Academia and industry formed a significant consensus on efficient reproducing and rapidly deploying federated learning in the AIoT. The current best practice typically resorts to three approaches: 1) looking for publicly open-source algorithmic prototypes, 2) contacting the authors to get a private prototype, and 3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes, and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor-consuming and error-prone. In this paper, we propose reproducing FL research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with an experiment in which three students with essential parallel and distributed machine learning knowledge reproduce different FL algorithms published in prominent conferences and journals by prompt engineering ChatGPT-4. Finally, our experimental results focus on the efficiency and quality of reproducing code. We report the experiment’s observations and discuss future open research questions of this paper. Additionally, we verify the better robustness of reproduced codes with different data poisoning attacks via extensive experiments. This work also raises no ethical issue.","","979-8-3315-1709-0","10.1109/ACAIT63902.2024.11022196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022196","large language models;federated learning;algorithm reproducibility;distributed machine learning","Training;Machine learning algorithms;Codes;Federated learning;Large language models;Prototypes;Transforms;Robustness;Internet of Things;Artificial intelligence","","","","30","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"LLM4Workflow: An LLM-based Automated Workflow Model Generation Tool","J. Xu; W. Du; X. Liu; X. Li","Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, School of Computer Science and Technology, Anhui University, Hefei, China; School of Information Technology, Deakin University, Geelong, Australia; Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, School of Computer Science and Technology, Anhui University, Hefei, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2394","2398","Workflows are pervasive in software systems where business processes and scientific methods are implemented as workflow models to achieve automated process execution. However, despite the benefit of no/low-code workflow automation, creating workflow models requires in-depth domain knowledge and nontrivial workflow modeling skills, which becomes a hurdle for the proliferation of workflow applications. Recently, Large language models (LLMs) have been widely applied in software code generation given their outstanding ability to understand complex instructions and generate accurate, context-aware code. Inspired by the success of LLMs in code generation, this paper aims to investigate how to use LLMs to automate workflow model generation. We present LLM4Workflow, an LLM-based automated workflow model generation tool. Using workflow descriptions as the input, LLM4Workflow can automatically embed relevant API knowledge and leverage LLM’s powerful contextual learning abilities to generate correct and executable workflow models. Its effectiveness was validated through functional verification and simulation tests on a real-world workflow system. LLM4Workflow is open sourced at https://github.com/ISEC-AHU/LLM4Workflow, and the demo video is provided at https://youtu.be/XRQ0saKkuxY.","2643-1572","979-8-4007-1248-7","","Young Scientists Fund; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764986","Automated Workflow Model Generation;Large Language Models;Low Code Development","Codes;Automation;Accuracy;Large language models;Software systems;Software engineering;Context modeling;Business","","","","12","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Neurosymbolic Architectural Reasoning: Towards Formal Analysis through Neural Software Architecture Inference","S. Herbold; C. Knieke; A. Rausch; C. Schindler","Faculty of Computer Science and Mathematics, Universität Passau, Passau, Germany; Institute for Software and Systems Engineering, Technische Universität Clausthal, Germany; Institute for Software and Systems Engineering, Technische Universität Clausthal, Germany; Institute for Software and Systems Engineering, Technische Universität Clausthal, Germany",2025 IEEE/ACM 1st International Workshop on Neuro-Symbolic Software Engineering (NSE),"19 Jun 2025","2025","","","5","10","Formal analysis to ensure adherence of software to defined architectural constraints is not yet broadly used within software development, due to the effort involved in defining formal architecture models. Within this paper, we outline neural architecture inference to solve the problem of having a formal architecture definition for subsequent symbolic reasoning over these architectures, enabling neurosymbolic architectural reasoning. We discuss how this approach works in general and outline a research agenda based on six general research question that need to be addressed, to achieve this vision.","","979-8-3315-1460-0","10.1109/NSE66660.2025.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11039279","software architecture;neural networks;large language models;architecture inference;neurosymbolic architectural reasoning","Codes;Software architecture;Large language models;Conferences;Neural networks;Computer architecture;Cognition;Software;Distance measurement;Software development management","","","","40","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Enhancing Code Generation for Low-Resource Languages: No Silver Bullet","A. Giagnorio; A. Martin-Lopez; G. Bavota","Software Institute - USI Università della Svizzera italiana, Switzerland; Software Institute - USI Università della Svizzera italiana, Switzerland; Software Institute - USI Università della Svizzera italiana, Switzerland",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","478","488","The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025886","Code Generation;Low-Resource Languages","Computer languages;Codes;Translation;Large language models;Semantics;Education;Training data;Syntactics;Boosting;Data models","","","","36","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection","S. Tarek; D. Saha; S. K. Saha; F. Farahmandi","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",2025 IEEE 43rd VLSI Test Symposium (VTS),"10 Jun 2025","2025","","","1","5","The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. These issues limit the scalability and effectiveness of security protocols, making bug detection at the Register-Transfer Level (RTL) difficult. This paper proposes a new framework named BugWhisperer that utilizes a specialized, fine-tuned Large Language Model (LLM) to address these challenges. By enhancing the LLM’s hardware security knowledge and leveraging its capabilities for text inference and knowledge transfer, this approach automates and improves the adaptability and reusability of the verification process. We introduce an open-source, fine-tuned LLM specifically designed for detecting security vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM effectively enhances the efficiency and flexibility of the security verification process. Additionally, we introduce a comprehensive hardware vulnerability database that supports this work and will further assist the research community in enhancing the security verification process.","2375-1053","979-8-3315-2144-8","10.1109/VTS65138.2025.11022958","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022958","Large Language Model;Fine-tuning;Hardware Security;Security Verification;Hardware Vulnerability Database","Protocols;Databases;Hardware security;Scalability;Large language models;Manuals;Very large scale integration;Register transfer level;System-on-chip;Optimization","","","","17","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Enhancing Engineering Education Through LLM-Driven Adaptive Quiz Generation: A RAG-Based Approach","S. Gopi; D. Sreekanth; N. Dehbozorgi","Department of Computer Science, Kennesaw State University, Kennesaw, GA; Department of Computer Science, Kennesaw State University, Kennesaw, GA; Department of Software Engineering, Kennesaw State University, Kennesaw, GA",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","8","This research-to-practice study aims to develop an Artificial Intelligence (AI) MCQ generation system for engineering students, with a focus on adaptive learning, educational technology, and innovative assessment tools, to enhance personalized learning. Engineering education faces significant academic performance challenges, with first-year retention rates in STEM fields ranging between 27% to 46%, largely due to poor academic achievements. Multiple Choice Questions (MCQs) identify misconceptions, reinforce knowledge retention, and offer efficient assessment methods for engineering education. This interactive method improves attention and memory retention, reinforces knowledge, and improves comprehension. In this context, the emergence of Large Language Models (LLMs) such as GPT-4 has marked a significant advancement. Our literature review method employed a systematic approach, analyzing peer-reviewed articles, conference papers, and authoritative reports to uncover the trends and challenges in AI-driven quiz generation. The notable gap identified in our literature review is the lack of LLM-based adaptive quiz generation methods specifically for engineering education. Our methodology involved sourcing relevant structured datasets, data pre-processing, embedding generation, vector database storage, hybrid-search retrieval, LLM query results feed, prompt engineering, and context-based response. In this research, we adopted Vectara as a vector database tool for its automatic data ingestion capabilities and seamless integration with generative AI applications. Prompt engineering involves a dual-prompt approach, where the Contextual Question Prompt formulates questions based on user topics and chat history, while the Answer Question Prompt manages MCQ responses with explanations, ensuring relevant and contextually accurate interactions. Evaluation includes topic relevancy, answer relevancy, and a contextual relevancy score. Preliminary results indicate promising results for the generation of accurate and contextually appropriate questions with minimal hallucinations. The quiz generation system was deployed using Streamlit cloud-based architecture to showcase the functionality. Looking forward, we aim to expand the dataset to include more diverse engineering disciplines and to refine the retrieval algorithms to better handle complex diagrams and mathematical expressions commonly found in engineering texts.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893146","AI quiz generation;engineering education;personalized learning;Large Language Models;GPT-4;RAG;Vec-tara;prompt engineering;LLM evaluation","Accuracy;Systematics;Databases;Learning (artificial intelligence);Market research;Vectors;Mathematical models;Prompt engineering;Systematic literature review;STEM","","2","","36","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Enhancing System Security: LLM-Driven Defense Against Prompt Injection Vulnerabilities","O. Muliarevych","Department of Electronic Computing Machines, Lviv Polytechnic National University, Lviv, Ukraine","2024 IEEE 17th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering (TCSET)","25 Nov 2024","2024","","","420","423","This article examines cybersecurity vulnerabilities in systems utilizing Language Model Interfaces, focusing on the challenges of building secure systems. It provides an overview of current interfaces and their associated risks. A key contribution is the design of a prompt analysis and injection detection subsystem, which assesses input relevance and security. The integration of an additional Filter level in the system safeguards against prompt injection attacks by pre-processing user requests and post-processing responses. This level includes a Prompt Analyzer for input wrapping, and an Attack Validator module using the LLM to classify prompts. The study evaluates various prompt injection attacks across different system setups, including core GPT -3 and GPT-4 models without additional security layers, a Fuzzy Search method, and systems using an Attack Validator based on GPT-3 and GPT -4 models.","","979-8-3315-2056-4","10.1109/TCSET64720.2024.10755823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10755823","language model;prompt injection attack;cybersecurity for LLMs;ChatGPT;security layer integration","Fault tolerance;Search methods;Focusing;Market research;Distance measurement;Telecommunications;Time factors;Computer security;Wrapping;Cyberattack","","","","10","IEEE","25 Nov 2024","","","IEEE","IEEE Conferences"
"Exploring LLMs for Verifying Technical System Specifications Against Requirements","L. M. Reinpold; M. Schieseck; L. P. Wagner; F. Gehlhoff; A. Fay","Institute of Automation Technology, Helmut Schmidt University, Hamburg, Germany; Institute of Automation Technology, Helmut Schmidt University, Hamburg, Germany; Institute of Automation Technology, Helmut Schmidt University, Hamburg, Germany; Institute of Automation Technology, Helmut Schmidt University, Hamburg, Germany; Chair of Automation, Ruhr University, Bochum, Germany",2024 IEEE 3rd Industrial Electronics Society Annual On-Line Conference (ONCON),"25 Mar 2025","2024","","","1","6","Requirements engineering is a knowledge intensive process and crucial for the success of engineering projects. The field of knowledge-based requirements engineering (KBRE) aims to support engineers by providing knowledge to assist in the elicitation, validation, and management of system requirements. The advent of large language models (LLMs) opens new opportunities in the field of KBRE. This work experimentally investigates the potential of LLMs in requirements verification. Therein, LLMs are provided with a set of requirements and a textual system specification and are prompted to assess which requirements are fulfilled by the system specification. Different experimental variables such as system specification complexity, the number of requirements, and prompting strategies were analyzed. Formal rule-based systems serve as a benchmark to compare LLM performance to. Requirements and system specifications are derived from the smart-grid domain. Results show that advanced LLMs, like GPT-4o and Claude 3.5 Sonnet, achieved f1-scores between 79 % and 94 % in identifying non-fulfilled requirements, indicating potential for LLMs to be leveraged for requirements verification.","","979-8-3315-4031-9","10.1109/ONCON62778.2024.10931625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10931625","Large Language Model;Requirements Verification;Requirements Engineering","Knowledge engineering;Industrial electronics;Electric potential;Large language models;Knowledge based systems;Benchmark testing;Smart grids;Complexity theory;Requirements engineering","","1","","15","IEEE","25 Mar 2025","","","IEEE","IEEE Conferences"
"An LLM's Medical Testing Recommendations in a Nigerian Clinic: Potential and Limits of Prompt Engineering for Clinical Decision Support","G. McPeak; A. Sautmann; O. George; A. Hallal; E. A. Simal; A. L. Schwartz; J. Abaluck; N. Ravi; R. Pless","George Washington University, Computer Science; World Bank Development Research Group; Department of Research and Innovation, EHA Clinics; Department of Research and Innovation, EHA Clinics; World Bank Development Research Group; Department of Medical Ethics and Health Policy, Division of General Internal Medicine, University of Pennsylvania, Perelman, School of Medicine; Yale University, School of Management; Department of Research and Innovation, EHA Clinics; George Washington University, Computer Science",2024 IEEE 12th International Conference on Healthcare Informatics (ICHI),"22 Aug 2024","2024","","","586","591","We explore prompt designs for a lightweight integration of a large language model (LLM) into clinical decision support in primary care in Nigeria. The LLM integration is designed to give immediate, actionable “second opinions” to frontline healthworkers on their patient interaction notes. The assessment of a physician serves as a benchmark for the quality of the LLM feedback. A particular challenge was to counter the LLM's tendency to over-recommend laboratory testing, which is more in line with medical practice in high-income countries. We evaluate the ability of a range of prompt engineering approaches to better align the LLM's medical test recommendations with locally appropriate standards of clinical care.","2575-2634","979-8-3503-8373-7","10.1109/ICHI61247.2024.00094","NSF(grant numbers:DGE-2125677,IIS-2229885); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628629","LLM;clinical decision support;prompt engineering;LMIC;Healthcare in Developing Countries;Artificial Intelligence in Medicine;Generative AI;GPT-4","Training;Costs;Large language models;Benchmark testing;Medical tests;Prompt engineering;Informatics","","1","","14","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"Validation of Generative Visual Solutions Using Prompt Engineering and Caption Based Visual Reasoning Models","M. Arora; C. Garg; D. Mangla","Samsung Electronics, Noida, India; Samsung Electronics, Noida, India; Samsung Electronics, Noida, India",2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS),"25 Apr 2025","2025","","","537","543","We introduce the novel approach of validation of artificially generated images which helps to validate the images based on the prompt given for the generated image. Existing methods involve generation of images based on the prompts, diffusion of images and modification of images, but fail to determine the correctness of the generated image with respect to the generated content and prompt given at user end. The prompt used for generating the image using generative artificial intelligence solutions can be comprehensive and can hold more than single perspective. To address this issue while validating computer visual solutions, we propose a method for the validation of generative visual solutions using prompt engineering and caption based visual reasoning models. The proposed solution determines the different perspectives and comprehensiveness of the prompts based on entities and attributes and then, multiple test cases are formed considering different perspectives, in more detailed and comprehensive format. Hence, proposed solution validates the generated image based on the text prompt engineered for comprehensive understanding based on the complexity of the prompt suitable for visual reasoning models.","","979-8-3315-0574-5","10.1109/ICMLAS64557.2025.10968513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10968513","Visual reasoning;Prompt Engineering;LLM;Generative AI;Validation","Visualization;Scalability;Computational modeling;Machine learning;Reliability engineering;Distortion;Cognition;Reflection;Prompt engineering;Testing","","","","26","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"A Survey on Security and Privacy of Multimodal LLMs - Connected Healthcare Perspective","M. A. Rahman","Cyber Security and Forensic Computing Department, University of Prince Mugrin, Madinah, KSA",2023 IEEE Globecom Workshops (GC Wkshps),"21 Mar 2024","2023","","","1807","1812","Recently, large language and vision language models have shown tremendous success and industry acceptance for healthcare applications. While the firstgeneration models mostly showed advancement in language models, the second generation now offers multi-modal inputs such as audio, image/video, sensory data, and depth, which makes these more suitable toward healthcare applications. However, security and privacy of these multimodal large AI models is largely ignored due to lack of regulatory and compliance applied on these AI models. The healthcare industry requires robust security and privacy of AI models, privacypreserving, and regulation-compliant large language models as it is applied on end users. In this paper, we survey different multimodal large language models and their security and privacy concerns that need to be addressed before these language models can be democratized and accepted by the medical industry. The survey covers different security and privacy threats and vulnerabilities that have been raised by researchers and government bodies and defensive actions such as federated learning, differential privacy, and monitoring LLM processes that have been suggested as remedial actions.","","979-8-3503-7021-8","10.1109/GCWkshps58843.2023.10465035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465035","Deep Learning;AI;Internet of Medical Things","Surveys;Industries;Privacy;Federated learning;Government;Medical services;Data models","","12","","14","IEEE","21 Mar 2024","","","IEEE","IEEE Conferences"
"Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models","R. Shekhawat; H. Li; R. Ramachandra; S. Venkatesh","Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; MOBAI AS, Gjøvik, Norway",2025 IEEE 19th International Conference on Automatic Face and Gesture Recognition (FG),"6 Aug 2025","2025","","","1","10","Leveraging the power of multimodal large language models (LLMs) offers a promising approach to enhancing the accuracy and interpretability of morphing attack detection (MAD), especially in real-world biometric applications. This work introduces the use of LLMs for differential morphing attack detection (D-MAD). To the best of our knowledge, this is the first study to employ multimodal LLMs to D-MAD using real biometric data. To effectively utilize these models, we design Chain-of-Thought (CoT)-based prompts to reduce failure-to-answer rates and enhance the reasoning behind decisions. Our contributions include: (1) the first application of multimodal LLMs for D-MAD using real data subjects, (2) CoT-based prompt engineering to improve response reliability and explainability, (3) comprehensive qualitative and quantitative benchmarking of LLM performance using data from 54 individuals captured in passport enrollment scenarios, and (4) comparative analysis of two multimodal LLMs: ChatGPT-4o and Gemini providing insights into their morphing attack detection accuracy and decision transparency. Experimental results show that ChatGPT-4o outperforms Gemini in detection accuracy, especially against GAN-based morphs, though both models struggle under challenging conditions. While Gemini offers more consistent explanations, ChatGPT-4o is more resilient but prone to a higher failure-to-answer rate.","2770-8330","979-8-3315-5341-8","10.1109/FG61629.2025.11099318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11099318","","Accuracy;Large language models;Face recognition;Biological system modeling;Gesture recognition;Benchmark testing;Reliability engineering;Cognition;Prompt engineering","","","","34","IEEE","6 Aug 2025","","","IEEE","IEEE Conferences"
"Harnessing LLMs for IoT Malware Detection: A Comparative Analysis of BERT and GPT-2","M. Omar; H. M. Zangana; J. N. Al-Karaki; D. Mohammed","College of Computing, Illinois Institute of Technology, Chicago, USA; IT Department, Duhok Technical College, Duhok Polytechnic University, Duhok, Iraq; College of Interdisciplinary Studies, Zayed University, Abu Dhabi, UAE; College of CARDS, Saint Leo University, FL, USA",2024 8th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),"28 Nov 2024","2024","","","1","6","In recent years, the proliferation of Internet of Things (IoT) devices has introduced significant vulnerabilities in cybersecurity, particularly with the rise of sophisticated malware targeting these systems. Traditional detection methods, often based on static signatures, struggle to keep pace with evolving threats, such as zero-day attacks. This paper explores the application of Large Language Models (LLMs), specifically BERT and GPT-2, in detecting IoT malware by analyzing network traffic and identifying anomalies. Using the contextual understanding and adaptability of LLM, our approach significantly enhances detection accuracy compared to conventional methods. We evaluated the models using the ToN-IoT dataset, demonstrating their capability to detect complex malware patterns with higher precision. The results indicate that BERT outperforms GPT-2 across multiple metrics, highlighting its effectiveness in generalizing to various attack types. Despite promising advancements, challenges such as computational resource demands and model interpretability persist. Future research should focus on optimizing LLMs for real-time detection in resource-constrained environments and improving transparency to enhance trust among cybersecurity professionals. Our study underscores the potential of LLMs as powerful tools in the ongoing battle against IoT malware, offering a robust framework for enhancing cybersecurity defenses.","2770-7962","979-8-3503-5442-3","10.1109/ISMSIT63511.2024.10757249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757249","LLMs;Malware Detection;Cybersecurity;Risk Mitigation;AI Security","Measurement;Adaptation models;Accuracy;Computational modeling;Large language models;Telecommunication traffic;Malware;Real-time systems;Internet of Things;Computer security","","16","","47","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat Prediction","A. Diaf; A. A. Korba; N. Elislem Karabadji; Y. Ghamri-Doudane","Laboratoire Reseaux et Systemes (LRS), Faculty of Technology, Badji Mokhtar-Annaba University, Algeria; Laboratoire Reseaux et Systemes (LRS), Faculty of Technology, Badji Mokhtar-Annaba University, Algeria; National Higher School of Technology and Engineering, Annaba, Algeria; L3I, University of La Rochelle, France",GLOBECOM 2024 - 2024 IEEE Global Communications Conference,"11 Mar 2025","2024","","","1239","1244","The integration of Internet of Things (IoT) technology in various domains has led to operational advancements, but it has also introduced new vulnerabilities to cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT devices. Intrusion detection systems are often reactive, triggered by specific patterns or anomalies observed within the network. To address this challenge, this work proposes a proactive approach to anticipate and preemptively mitigate malicious activities, aiming to prevent potential damage before it occurs. This paper proposes an innovative intrusion prediction framework empowered by Pre-trained Large Language Models (LLMs). The framework incorporates two LLMs: a fine-tuned Bidirectional and Auto-Regressive Transformers (BART) model for predicting network traffic and a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model for evaluating the predicted traffic. By harnessing the bidirectional capabilities of BART the framework then identifies malicious packets among these predictions. Evaluated using the CICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in predictive performance, attaining an impressive 98% overall accuracy, providing a powerful response to the cybersecurity challenges that confront IoT networks.","2576-6813","979-8-3503-5125-5","10.1109/GLOBECOM52923.2024.10901770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10901770","Security;Intrusion Prediction;BART;BERT;Large Language Models;Transformers;Internet of Things (IoT)","Accuracy;Prevention and mitigation;Bidirectional control;Telecommunication traffic;Predictive models;Transformers;Encoding;Robustness;Internet of Things;Surges","","2","","21","IEEE","11 Mar 2025","","","IEEE","IEEE Conferences"
"The Application of LLMs in the Analysis and Modeling of Software Requirements","Z. Wang; C. Feng; L. Liu; G. Jiao; P. Ye","School of Computer Science and Artificial Intelligence, WuGuotao Jiaohan Textile University, China; School of Computer Science and Artificial Intelligence, WuGuotao Jiaohan Textile University, China; School of Computer Science and Artificial Intelligence, WuGuotao Jiaohan Textile University, China; School of Computer Science and Artificial Intelligence, WuGuotao Jiaohan Textile University, China; School of Computer Science and Artificial Intelligence, WuGuotao Jiaohan Textile University, China","2024 IEEE 24th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","29 Oct 2024","2024","","","1143","1153","In modern software engineering, requirements analysis and modeling are key steps in requirements engineering, influencing the subsequent system design and implementation. Traditional requirements analysis and modeling methods usually involve multiple stakeholders such as requirements providers and analysts working together collaboratively and iteratively, requiring a significant amount of manpower. It is of great significance to reduce the burden on requirements providers and analysts and improve the efficiency of analysis and modeling. In existing work, some merely utilize knowledge repositories to provide more knowledge in support of analysis or modeling, while others employ natural language processing (NLP) techniques to automate the analysis or modeling process. However, neither has relieved the burden on requirements providers and analysts. The emergence of large language models(LLMs) has brought new possibilities to requirements analysis and modeling. LLMs can combine knowledge base processing and understanding of large amounts of natural language data, providing a new automated requirement analysis method. This paper proposes an automated requirements analysis and modeling method for Support Vector Machine(SVM) classifier and LLMs - SVM-LLMs. The method is deployed based on the intelligent requirement service platform - “Wisdom Requirement Communication”, which combines SVM and LLMs to effectively improve the accuracy of requirements analysis and significantly reduce the time in the software development process.","2693-9371","979-8-3503-6565-8","10.1109/QRS-C63300.2024.00151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10726944","Software Engineering;LLMs;Requirement Classification;Requirement Analysis;Requirement Modeling;Machine Learning","Support vector machines;Analytical models;Software quality;Natural language processing;Vectors;Software reliability;Stakeholders;System analysis and design;Software engineering;Software development management","","","","15","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"HDLEval Benchmarking LLMs for multiple HDLs","F. R. Kashanaki; M. Zakharov; J. Renau","Dept. of Computer Science and Engineering, University of California, Santa Cruz; Dept. of Computer Science and Engineering, University of California, Santa Cruz; Dept. of Computer Science and Engineering, University of California, Santa Cruz",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","5","Large Language Models (LLMs) are transforming code generation and documentation processes across programming languages, including hardware description languages (HDLs). However, existing benchmarks primarily focus on individual HDLs, limiting comprehensive evaluations. To address this, we introduce HDLEval, a versatile benchmarking system that evaluates LLM performance across multiple HDLs, enabling meaningful comparisons. By sharing standardized test benches, HDLEval ensures consistent performance assessments between HDLs, offering insights into language-agnostic LLM capabilities.This system employs formal verification to verify generated code and leverages prompt engineering techniques to overcome syntactic differences between HDLs. HDLEval supports scalable evaluation, making it adaptable for current and future HDLs. Our experiments demonstrate how HDLEval can identify strengths and weaknesses in LLM-generated logic, providing a framework for improving HDL programming with LLMs.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691770","LLM;Verilog","Codes;Limiting;Large language models;Benchmark testing;Syntactics;Programming;Hardware;Prompt engineering;Logic;Hardware design languages","","1","","12","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"A Domain-Adaptive Large Language Model With Refinement Framework For IoT Cybersecurity","X. Che; Y. Zheng; M. Zhu; Q. Li; X. Dong","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Nari Group Corporation, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Cyber Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2024 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics","31 Oct 2024","2024","","","224","229","To address the increasingly complex security challenges in Internet-of-Things (IoT) environments, Large Language Models (LLMs) have demonstrated effectiveness in enhancing device and data security, as well as improving the security and reliability overall IoT system. However, general LLMs struggle to effectively handle IoT security data. Therefore, developing IoT security domain-specific LLMs based on IoT-specific corpus and terminologies has become a key focus for enhancing cybersecurity defense capabilities. These areas of focus can be broadly categorized into three main directions: training from scratch, retrieval-augmented prompting, and instruction fine-tuning. Training from scratch is discouraged in cybersecurity due to its high computational and data requirements, making effective model training and convergence difficult given the scarcity of cybersecurity data. Retrieval-augmented prompting may suffer from incomplete or conflicting information, impacting the accuracy and reliability of the model. Conversely, instruction fine-tuning allows for the usage of domain-specific task instructions to adjust the model, achieving effective domain adaptation on limited datasets. However, inadequate or inaccurate instructions may degrade the generalization ability and performance. For these shortcomings, we propose a novel and robust fine-tuning framework to refine the original corpus and output a refined corpus for high-quality domain adaptation. Furthermore, we introduce two auxiliary strategies in the training phase to enhance efficiency and accuracy. Finally, our framework is evaluated across multiple task scenarios and demonstrates the best performance on most of them, which proves the effectiveness and efficiency of the model presented in this work.","2836-3701","979-8-3503-5163-7","10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10731743","Internet-of-Things (IoT);Large Language Models (LLMs);domain adaptation;instruction fine-tuning;cybersecurity","Training;Adaptation models;Accuracy;Costs;Computational modeling;Large language models;Data models;Security;Reliability;Computer security","","","","24","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Software Vulnerability Detection using Large Language Models","M. D. Purba; A. Ghosh; B. J. Radford; B. Chu","Department of Software and Information Systems, University of North Carolina at Charlotte, NC, USA; Department of Software and Information Systems, University of North Carolina at Charlotte, NC, USA; Department of Political Science and Public Administration, University of North Carolina at Charlotte, NC, USA; Department of Software and Information Systems, University of North Carolina at Charlotte, NC, USA",2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW),"2 Nov 2023","2023","","","112","119","Software development is among the first demonstrations of using Large Language Models (LLMs) to enhance human productivity. Such a co-pilot paradigm envisions LLM working side-by-side with human developers to assist in programming tasks. Ensuring the security of software products is a critical factor for the success of such a paradigm. There have been various anecdotal reports on the success of using LLMs to detect vulnerabilities in programs. This paper reports a set of experiments applying four well-known LLMs to two widely referenced public datasets to evaluate the performance of LLMs in detecting software vulnerabilities. Our results show a significant performance gap between these LLMs and those from popular static analysis tools, primarily due to their high false positive rates. However, LLMs show great promise in identifying subtle patterns commonly associated with software vulnerabilities. This observation suggests a possible path forward by combining LLMs and other program analysis techniques to achieve better software vulnerability detection.","","979-8-3503-1956-9","10.1109/ISSREW60843.2023.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301302","Cybersecurity;Large language model;AI;software vulnerability","Productivity;Codes;Static analysis;Writing;SQL injection;Programming;Software","","47","","30","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"Automated Essay Scoring and Revising Based on Open-Source Large Language Models","Y. Song; Q. Zhu; H. Wang; Q. Zheng","Research Center of Distance Education, Beijing Normal University, Beijing, China; National Engineering Research Center for Cyberlearning and Intelligent Technology, Beijing Normal University, Beijing, China; School of Systems Science, Beijing Normal University, Beijing, China; Research Center of Distance Education, Beijing Normal University, Beijing, China",IEEE Transactions on Learning Technologies,"19 Jul 2024","2024","17","","1880","1890","Manually scoring and revising student essays has long been a time-consuming task for educators. With the rise of natural language processing techniques, automated essay scoring (AES) and automated essay revising (AER) have emerged to alleviate this burden. However, current AES and AER models require large amounts of training data and lack generalizability, which makes them hard to implement in daily teaching activities. Moreover, online sites offering AES and AER services charge high fees and have security issues uploading student content. In light of these challenges and recognizing the advancements in large language models (LLMs), we aim to fill these research gaps by analyzing the performance of open-source LLMs when accomplishing AES and AER tasks. Using a human-scored essay dataset (n = 600) collected in an online assessment, we implemented zero-shot, few-shot, and p-tuning AES methods based on the LLMs and conducted a human–machine consistency check. We conducted a similarity test and a score difference test for the results of AER with LLMs support. The human–machine consistency check result shows that the performance of open-source LLMs with a 10 B parameter size in the AES task is close to that of some deep-learning baseline models, and it can be improved by integrating the comment with the score into the shot or training continuous prompts. The similarity test and score difference test results show that open-source LLMs can effectively accomplish the AER task, improving the quality of the essays while ensuring that the revision results are similar to the original essays. This study reveals a practical path to cost-effectively, time-efficiently, and content-safely assisting teachers with student essay scoring and revising using open-source LLMs.","1939-1382","","10.1109/TLT.2024.3396873","National Natural Science Foundation of China(grant numbers:62277004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10520824","Assessment;automated essay revising (AER);automated essay scoring (AES);generative artificial intelligence;open-source large language model (LLM)","Task analysis;Chatbots;Vocabulary;Engines;Standards;Long short term memory;Electronic mail","","19","","50","IEEE","6 May 2024","","","IEEE","IEEE Journals"
"Silent Guardian: Protecting Text From Malicious Exploitation by Large Language Models","J. Zhao; K. Chen; X. Yuan; Y. Qi; W. Zhang; N. Yu","CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Electromagnetic Space Information, Anhui Province Key Laboratory of Digital Security, School of Information Science and Technology, University of Science and Technology of China, Hefei, China",IEEE Transactions on Information Forensics and Security,"25 Sep 2024","2024","19","","8600","8615","The rapid development of large language models (LLMs) has yielded impressive success in various downstream tasks. However, the vast potential and remarkable capabilities of LLMs also raise new security and privacy concerns if they are exploited for nefarious purposes due to their open-endedness. For example, LLMs may be used to plagiarize or imitate writing, thereby infringing the copyright of the original content or to create indiscriminate fake information based on a certain source text. In some cases, LLMs can even analyze text from the Internet to infer personal privacy. Unfortunately, previous text protection research could not foresee the emergence of powerful LLMs, rendering it no longer effective in this new context. To bridge this gap, we introduce Silent Guardian (SG), a text protection mechanism against LLMs, which allows LLMs to refuse to generate responses when receiving protected text, preventing the malicious use of text from the source. Specifically, we first propose the concept of Truncation Protection Examples (TPE). By carefully modifying the text to be protected, TPE can induce LLMs to first sample the end token, thus directly terminating the interaction. In addition, to efficiently construct TPE in the discrete space of text data, we propose a novel optimization algorithm called Super Tailored Protection (STP), which is not only highly efficient but also maintains the semantic consistency of the text during the optimization process. The comprehensive experimental evaluation demonstrates that SG can effectively protect the target text under various configurations and achieve almost 100% protection success rate in some cases. Notably, SG also exhibits relatively good transferability and robustness, making its application in practical scenarios possible. Our code is available at https://github.com/weiyezhimeng/Silent-Guardian.","1556-6021","","10.1109/TIFS.2024.3455775","National Natural Science Foundation of China(grant numbers:62121002,U20B2047,U2336206,62102386,62472398,62372423); Open Foundation of Key Laboratory of Cyberspace Security, Ministry of Education(grant numbers:KLCS20240207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669119","Text protection;silent guardian;truncation protection example;large language model","Protection;Privacy;Computational modeling;Watermarking;Optical character recognition;Large language models;Homomorphic encryption","","4","","49","IEEE","6 Sep 2024","","","IEEE","IEEE Journals"
"Anomaly Based Intrusion Detection Using Large Language Models","Z. Maasaoui; M. Merzouki; A. Battou; A. Lbath","LIG/MRIM, CNRS, Grenoble Alpes University, Grenoble, France; Smart Connected Systems Division, National Institute of Standards and Technology, MD, USA; Smart Connected Systems Division, National Institute of Standards and Technology, MD, USA; LIG/MRIM, CNRS, Grenoble Alpes University, Grenoble, France",2024 IEEE/ACS 21st International Conference on Computer Systems and Applications (AICCSA),"11 Mar 2025","2024","","","1","8","In the context of modern networks where cyber-attacks are increasingly complex and frequent, traditional Intrusion Detection Systems (IDS) often struggle to manage the vast volume of data and fail to detect novel attacks. Leveraging Artificial Intelligence, specifically Natural Language Processing with transformer architectures, offers a promising solution. This study applies the Bidirectional Encoder Representations from Transformers (BERT) model, enhanced by a Byte-level Byte-pair tokenizer (BBPE), to effectively identify network-based attacks within IoT systems. Experiments on three datasets-UNSW-NB15, TON-IoT, and Edge-IIoT-show that our approach substantially outperforms traditional methods in multi-class classification tasks. Notably, we achieved near-perfect classification accuracy on the Edge-IIoT dataset, with significant improvements in F1 scores and reduction in validation losses across all datasets, demonstrating the efficacy of pre-trained Large Language Models (LLMs) in network security.","2161-5330","979-8-3315-1824-0","10.1109/AICCSA63423.2024.10912623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912623","Network intrusion;Network security;Natural language processing;Large Language Model;BERT;BBPE;AI;IoT","Large language models;Image edge detection;Intrusion detection;Bidirectional control;Network security;Transformers;Natural language processing;Encoding;Network intrusion;Cyberattack","","1","","25","IEEE","11 Mar 2025","","","IEEE","IEEE Conferences"
"ContextualCoder: Adaptive In-context Prompting for Programmatic Visual Question Answering","R. Shen; N. Inoue; D. Guan; R. Cai; A. C. Kot; K. Shinoda",NA; NA; NA; NA; NA; NA,IEEE Transactions on Multimedia,"","2025","PP","99","1","14","Visual Question Answering (VQA) presents a challenging task at the intersection of computer vision and natural language processing, aiming to bridge the semantic gap between visual perception and linguistic comprehension. Traditional VQA approaches do not distinguish between data processing and reasoning, limiting their interpretability and generalizability in complex and diverse scenarios. Conversely, Programmatic Visual Question Answering (PVQA) models leverage large language models (LLMs) to generate executable codes, providing answers with detailed and interpretable reasoning processes. However, existing PVQA models typically rely on simplistic input-output prompting, which struggles to elicit domain-specific knowledge from LLMs and often produces unclear or extraneous outputs. Furthermore, PVQA models typically rely on a basic in-context example (ICE) selection methodology that is heavily influenced by individual word similarity rather than the overall sentence context. This leads to suboptimal ICE selection and a reliance on dataset-specific ICE candidates. In this paper, we propose ContextualCoder, a novel prompting framework tailored for PVQA models. ContextualCoder leverages frozen LLMs for code generation and pre-trained visual models for code execution, eliminating the need for extensive training and enhancing model flexibility. By incorporating an innovative prompting methodology and a novel ICE selection strategy, ContextualCoder facilitates the use of diverse in-context information for code generation, thereby improving the performance of PVQA models. Our approach surpasses state-of-the-art models, as evidenced by comprehensive experiments across diverse VQA datasets, including multilingual scenarios.","1941-0077","","10.1109/TMM.2025.3543043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10891469","Visual question answering;prompting methods;large language models;code generation","Codes;Context modeling;Computational modeling;Visualization;Cognition;Training;Question answering (information retrieval);Adaptation models;Ice;Large language models","","","","","CCBY","17 Feb 2025","","","IEEE","IEEE Early Access Articles"
"One method of network cyber-security, based on artificial intelligence","R. Trifonov; O. Nakov; S. Manolov; G. Tsochev; G. Pavlova","Faculty of Computer Systems and Technologies Technical University of Sofia 8 St. Kliment Ohridski blvd, Sofia, Bulgaria; Faculty of Computer Systems and Technologies Technical University of Sofia 8 St. Kliment Ohridski blvd, Sofia, Bulgaria; Faculty of Computer Systems and Technologies Technical University of Sofia 8 St. Kliment Ohridski blvd, Sofia, Bulgaria; Faculty of Computer Systems and Technologies Technical University of Sofia 8 St. Kliment Ohridski blvd, Sofia, Bulgaria; Faculty of Computer Systems and Technologies Technical University of Sofia 8 St. Kliment Ohridski blvd, Sofia, Bulgaria",2019 27th National Conference with International Participation (TELECOM),"13 Feb 2020","2019","","","39","41","The Faculty of Computer Systems and Technologies at the Technical University - Sofia conducts research in the field of application of the artificial intelligence methods in cybersecurity within the framework of a project funded by the National Science Fund at the Ministry of Education and Science. The expected result is: after modeling and experimenting with different methods in different applications, to produce a ”Report on the recommended use of intelligent methods for enhancing network and information security”. The results of experiments with one such method aimed at cyber-protection of telecommunication networks are considered in this paper. The successfully tested Network Protection module of so-called Network Gateway Monitoring System could be defined as a network server to protect the network from attacks such as foot-printing, network scanning, SYN flood, ping sweep, port scanning, session hijacking and more.","","978-1-7281-4061-2","10.1109/TELECOM48729.2019.8994880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994880","cyber defense;artificial intelligence;multi-agent systems","","","3","","10","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Ansible Lightspeed: A Code Generation Service for IT Automation","P. Sahoo; S. Pujar; G. Nalawade; R. Gebhardt; L. Mandel; L. Buratti",Red Hat; IBM Research; Red Hat; Red Hat; IBM Research; IBM Research,2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2148","2158","The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been re-leased, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764992","large language models;generative models;code completion;ide;user study;ansible","Productivity;Sentiment analysis;Codes;Automation;Large language models;Natural languages;Machine learning;Software;Software engineering;Domain specific languages","","1","","49","","29 Nov 2024","","","IEEE","IEEE Conferences"
"SecureNet: A Comparative Study of DeBERTa and Large Language Models for Phishing Detection","S. Mahendru; T. Pandit","Palo Alto Networks, California, USA; Palo Alto Networks, California, USA",2024 IEEE 7th International Conference on Big Data and Artificial Intelligence (BDAI),"1 Oct 2024","2024","","","160","169","Phishing, whether through email, SMS, or malicious websites, poses a major threat to organizations by using social engineering to trick users into revealing sensitive information. This not only compromises company’s data security but also incurs significant financial losses. In this paper, we investigate whether the remarkable performance of Large Language Models (LLMs) can be leveraged for particular task like text classification, particularly detecting malicious content and compare its results with state-of-the-art Deberta V3 (DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing) model. We systematically assess the potential and limitations of both the approaches using comprehensive public datasets comprising diverse data sources such as email, HTML, URL, SMS, and synthetic data generation. Additionally, we demonstrate how LLMs can generate convincing phishing emails, making it harder to spot scams and evaluate the performance of both models in this context. Our study delves further into the challenges encountered by DeBERTa V3 during its training phases, fine-tuning methodology and transfer learning processes. Similarly, we examine the challenges associated with LLMs and assess their respective performance. Among our experimental approaches, the transformer-based DeBERTa method emerged as the most effective, achieving a test dataset (HuggingFace phishing dataset) recall (sensitivity) of 95.17% closely followed by GPT-4 providing a recall of 91.04%. We performed additional experiments with other datasets on the trained DeBERTa V3 model and LLMs like GPT 4 and Gemini 1.5. Based on our findings, we provide valuable insights into the effectiveness and robustness of these advanced language models, offering a detailed comparative analysis that can inform future research efforts in strengthening cybersecurity measures for detecting and mitigating phishing threats.","","979-8-3503-5200-9","10.1109/BDAI62182.2024.10692765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10692765","machine learning;DeBERTa V3;Large language model;GPT-4;Gemini 1.5;phishing","Training;Uniform resource locators;Analytical models;Adaptation models;Phishing;Large language models;Transformers;Data models;Electronic mail;Synthetic data","","5","","38","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"On the Security of Distributed Multi-Agent K-Means Clustering With Local Differential Privacy","C. Shi; X. Huang; P. Yu","State Grid Laboratory of Power Cyber-Security Protection and Monitoring Technology, State Grid Smart Grid Research Institute Company Ltd., Nanjing, China; State Grid Laboratory of Power Cyber-Security Protection and Monitoring Technology, State Grid Smart Grid Research Institute Company Ltd., Nanjing, China; State Grid Laboratory of Power Cyber-Security Protection and Monitoring Technology, State Grid Smart Grid Research Institute Company Ltd., Nanjing, China",IEEE Access,"17 Sep 2024","2024","12","","124751","124763","In a distributed scenario, the process of multiple agents collaborating and interacting with the server to iteratively implement k-means clustering analysis can be easily exploited by attackers, posing a huge privacy threat. Therefore, a local differential privacy k-means method (LDPKmeans) was proposed, which can effectively address the privacy protection problem in multi-agent systems. In this paper, we propose an effective attack method based on multi-agent model, which shows that the basic proposal of LDPKmeans will leak the real information of user agents if the attacker only obtains the cluster information and cluster centroid of each user. Furthermore, we enhance the attack method to crack the improved LDPKmeans method with privacy enhancement, enabling us to infer the cluster information of each user agent in the server. In other words, LDPKmeans seriously leak user agent privacy in distributed multi-agent systems if the server is untrusted. Theoretical analysis and experiments evaluate the effectiveness of our attack scheme. The results show that our method can effectively attack the distributed LDPKmeans scheme compared with the state-of-the-art attack methods. Specifically, our attack method can reduce the average relative error of inferring the true value before k-menas convergence on the 3D Road Network and Shuttle datasets by about 54% and 75% respectively when  $k=5$ .","2169-3536","","10.1109/ACCESS.2024.3454823","Science and Technology Project of State Grid Corporation of China(grant numbers:5108-202218280A-2-393-XG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666660","Multi-agent systems;distributed k-means;local differential privacy;security problem","Servers;Privacy;Vectors;Clustering algorithms;Security;Distributed databases;Proposals;Multi-agent systems;Differential privacy","","1","","37","CCBYNCND","5 Sep 2024","","","IEEE","IEEE Journals"
"Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation","Z. Jiang; M. Wen; J. Cao; X. Shi; H. Jin","Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; The Hong Kong University of Science and Technology, Hong Kong, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1408","1420","Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40%-58.57% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.CCS Concepts• Software and its engineering → Automatic programming; Software testing and debugging.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765050","LLM;Symbolic Execution;Directed Input Generation","Computer languages;Systematics;Codes;Sensitivity;Large language models;Prototypes;Transforms;Chatbots;Software;Software engineering","","","","65","","29 Nov 2024","","","IEEE","IEEE Conferences"
"On the Role of LLM to Forecast the Next Pandemic","T. Ruga; E. Vocaturo; E. Zumpano","CNR-NANOTEC, DIMES, University of Calabria, Italy; CNR-NANOTEC, DIMES, University of Calabria, Italy; CNR-NANOTEC, DIMES, University of Calabria, Italy",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","6567","6573","Large Language Models (LLMs) have emerged as powerful tools in medical applications, demonstrating signifi-cant potential for analyzing vast clinical datasets, identifying patterns, and facilitating early detection of health anomalies. These capabilities are particularly relevant for detecting both individual disease onset and potential pandemic scenarios at a population level. The integration of LLMs into medical workflows could revolutionize how data-driven insights are harnessed for pandemic prediction and management, although implementing these technologies requires careful consideration of critical issues, particularly data privacy and security. This paper presents a comprehensive examination of current research at the intersection of LLMs and pandemic response, analyzing the literature from both bibliometric and medical perspectives. Through the selection of 849 publications across major databases, we provide an overview of the current state of research in this domain, identify emerging patterns from a clinical standpoint, and evaluate potential implications for future pandemic prediction. Our findings reveal significant trends in the application of LLMs to pandemic-related challenges, highlighting both opportunities and critical areas requiring further investigation, particularly in mental health impacts and early warning systems. The analysis reveals that while LLMs show promise in early detection and pattern recognition, challenges remain in data privacy, model interpretability, and the integration of diverse data sources. This research serves as a foundation for understanding how LLMs can be effectively deployed in pandemic prediction and management, while acknowledging the complexities and ethical considerations inherent in such applications.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822835","Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822835","Large Language Models;LLMs;Healthcare;Pandemic;Literature Review","Data privacy;Pandemics;Reviews;Soft sensors;Large language models;Mental health;Medical services;Predictive models;Market research;Pattern recognition","","","","13","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"TAKA Cybersecurity Chatbot","M. Bhanushali; H. Parekh; R. Mistry; Y. Mane","Department of Information Technology, Universal College of Engineering, Mumbai; Department of Information Technology, Universal College of Engineering, Mumbai; Department of Information Technology, Universal College of Engineering, Mumbai; Department of Information Technology, Universal College of Engineering, Mumbai",2023 International Conference on Advanced Computing Technologies and Applications (ICACTA),"23 Jan 2024","2023","","","1","6","The internet has become an integral part of our daily lives, with smartphones and websites being used for communication, entertainment, shopping, banking, and more. However, this increased dependence on technology has also increased the risk of cyber threats such as hacking, identity theft, and malware attacks. Cybersecurity awareness and education are essential to safeguard against cyber threats. Collaboration between security experts can help prevent attacks and safeguard against cybercrime. Despite the numerous platforms available across various domains, we have discovered that there is currently no platform that can effectively bridge the gap between common people, developers, and cyber security researchers to protect people on the internet. This has led us to develop TAKA (meaning of TAKA in Japanese is to GIVE. Our research work aims to GIVE Security to every person in the world through our chatbot service), a conversational AI that provides continuous assistance to its users in protecting themselves from various cyber threats that occur on the Internet on a daily basis. TAKA is a conversational AI built using the RASA chatbot framework, designed to provide various cyber security services to protect its users. The chatbot offers protection against phishing links and emails, spam messages and spam calls, a range of inbuilt cyber security tools, and includes resources for security researchers and daily security and vulnerability news for developers.","","979-8-3503-4834-7","10.1109/ICACTA58201.2023.10393016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393016","Conversational AI;Cyber Security;inbuilt cyber security tools","Unsolicited e-mail;Phishing;Natural languages;Chatbots;Real-time systems;Malware;Computer security","","","","19","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"Autonomous Cyberattack with Security-Augmented Generative Artificial Intelligence","J. Gregory; Q. Liao","Department of Computer Science, Central Michigan University, Mount Pleasant, USA; Department of Computer Science, Central Michigan University, Mount Pleasant, USA",2024 IEEE International Conference on Cyber Security and Resilience (CSR),"24 Sep 2024","2024","","","270","275","Ethical hacking and penetration testing is a vital task by cybersecurity professionals to find and exploit possible vulnerabilities in a system before malicious actors do. However, system hacking has a high barrier to entry that necessitates years of experiential learning and formal education. The rapid development of generative artificial intelligence (AI) may potentially lower the barrier to entry. This research experiments with automatic penetration testing via large language models (LLMs) augmented with security information. This research uses a locally hosted Mistral 7B model with Low-Rank Adaptation (LoRA) fine-tuning and Retrieval-Augmented Generation (RAG) to improve penetration testing. When the LLMs are fine tuned with limited and unstructured security data such as privilege escalation articles from a few public web sites, the system succeeds in achieving privilege escalation on Linux hosts. The results of this research suggest that no-cost LLM-assisted penetration testing is possible even on ordinary PCs using locally hosted models. Future research is needed to achieve more diversified attacks and discover zero-day vulnerabilities, perhaps with better prompt engineering, models, and security data.","","979-8-3503-7536-7","10.1109/CSR61664.2024.10679470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10679470","Cybersecurity;Generative Artificial Intelligence;Machine Learning;Large Language Models;Low-Rank Adaptation;Retrieval-Augmented Generation;Penetration Testing;Hacking;Attacks and Defense;Vulnerability","Adaptation models;Large language models;Linux;Data models;Software;Web sites;Prompt engineering","","","","19","IEEE","24 Sep 2024","","","IEEE","IEEE Conferences"
"On the Adversarial Robustness of Multi-Modal Foundation Models","C. Schlarmann; M. Hein",Tübingen AI Center University of Tübingen; Tübingen AI Center University of Tübingen,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),"25 Dec 2023","2023","","","3679","3687","Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\left({{\varepsilon _\infty } = 1/255}\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.","2473-9944","979-8-3503-0744-3","10.1109/ICCVW60793.2023.00395","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350690","adversarial robustness;multi modal models;security;foundation models","Visualization;Computer vision;Perturbation methods;Computational modeling;Conferences;Robustness;Behavioral sciences","","28","","41","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"LLMs got the Funk: leveraging LLM, Prompt Engineering and Fine-Tuning for Topic Modeling on Brazilian Funk Lyrics","J. Yepez; B. Tavares; F. Peres; K. Becker","Institute of Informatics - Federal University of Rio Grande do Sul (UFRGS); Institute of Informatics - Federal University of Rio Grande do Sul (UFRGS); Institute of Philosophy and Human Sciences - Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Institute of Informatics - Federal University of Rio Grande do Sul (UFRGS)",2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),"5 May 2025","2024","","","133","140","Music can serve as a mirror of social conditions, and studying social phenomena requires proper computational tools to evaluate the discourses in song lyrics on a large scale. Brazilian Funk (BRFunk) is a popular genre that emerged from the favelas in Rio de Janeiro, regarded as controversial for merging stories of resilience and social criticism, with themes such as violence, drugs, and women objectification. These themes portray the social conditions of daily life in Brazilian slums ifavelas). In this paper, we propose a novel topic modeling approach that explores the power of Large Language Models (LLM) and pre-trained models to extract insights from song lyrics, using BRFunk as a case study. The generative power of LLM is leveraged through prompt engineering to summarize lyrics excerpts into themes in an iterative process to create a reliable distribution of themes. Pre-trained models are then deployed to condensate the themes into a non-redundant and cohesive set of topics using BERTopic, and through BERT fine-tuned models that classify new lyrics excerpts according to these topics. Using the top 100 BRFunk songs of 2023, we illustrate how this large-scale computational strategy is an efficient ally for understanding social phenomena and cultural movements.","","979-8-3315-0494-6","10.1109/WI-IAT62293.2024.00026","CAPES; CNPq; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973446","Topic modeling;Large Language Model;Prompting Engineer;BERT;BRFunk","Drugs;Computational modeling;Large language models;Merging;Reliability engineering;Prompt engineering;Mirrors;Iterative methods;Intelligent agents;Resilience","","","","21","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Large Language Models-Aided Program Debloating","B. Lin; S. Wang; Y. Qin; L. Chen; X. Mao","National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China",IEEE Transactions on Software Engineering,"","2025","PP","99","1","19","As software grows in complexity to accommodate diverse features and platforms, software bloating has emerged as a significant challenge, adversely affecting performance and security. However, existing approaches inadequately address the dual objectives of debloating: maintaining functionality by preserving essential features and enhancing security by reducing security issues. Specifically, current software debloating techniques often rely on input-based analysis, using user inputs as proxies for the specifications of desired features. However, these approaches frequently overfit provided inputs, leading to functionality loss and potential security vulnerabilities. To address these limitations, we propose LEADER, a program debloating framework enhanced by Large Language Models (LLMs), which leverages their semantic understanding, generative capabilities, and decision-making strengths. LEADER mainly consists of two modules: (1) a documentation-guided test augmentation module designed to preserve functionality, which leverages LLMs to comprehend program documentation and generates sufficient tests to cover the desired features comprehensively, and (2) a multi-advisor-aided program debloating module that employs a neuro-symbolic pipeline to ensure that the security of the software can be perceived during debloating. This module combines debloating and security advisors for analysis and employs an LLM as a decision-maker to eliminate undesired code securely. Extensive evaluations on widely used benchmarks demonstrate the efficacy of LEADER. It achieves a 95.5% test case pass rate and reduces program size by 42.5%. Notably, it reduces the introduction of vulnerabilities during debloating by 79.1% and decreases pre-existing vulnerabilities by 16.5% more than CovA. These results demonstrate that LEADER surpasses the state-of-the-art tool CovA in functionality and security. These results underscore the potential of LEADER to set a new standard in program debloating by effectively balancing functionality and security.","1939-3520","","10.1109/TSE.2025.3594673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11106926","Software debloating;program reduction;large language model","Security;Codes;Software;Documentation;Fuzzing;Decision making;Training;Semantics;Large language models;Benchmark testing","","","","","IEEE","1 Aug 2025","","","IEEE","IEEE Early Access Articles"
"Proposing a New Security Game with Reward and Penalty","R. Yoshioka; Y. Sakurai; S. Oyama; M. Shinoda","Nagoya Institute of Technology, Nagoya, Japan; Nagoya Institute of Technology, Nagoya, Japan; Nagoya City University, Nagoya, Japan; Nara Womens' University, Nara, Japan",2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),"19 Dec 2023","2023","","","205","212","We propose a new security game that takes into account both rewards and penalties. Security games have been widely studied in the field of multi-agent systems for many years. We abstract the security game that targets a situation where the defender must leave one vault open for a transaction as a number guessing game with reward and penalty. We investigate this security game in both simultaneous and sequential forms. We analyze the attacker's Max-Min strategy and the defender's Min-Max strategy in these games. Furthermore, we clarify the sufficient conditions for the penalty in which the defender chooses all remaining numbers. If the penalty increment is large, i.e., if the defender also incurs sufficient cost to defend the later safes, both the attacker and the defender must consider the possibility of using all the safes, but the defender can minimize the expected value of the loss by a simple strategy. On the other hand, if the penalty increment is small, the number of safes to be used is small, and thus the amount of loss may be improved, but the defender's strategy will not be as simple. The defender's strategy is formulated as a linear programming problem in this paper. We show what strategies the defender chooses as the penalty increases in the proposed security game for both simultaneous and sequential forms through computational experiments.","","979-8-3503-0918-8","10.1109/WI-IAT59888.2023.00033","JSPS KAKENHI(grant numbers:JP18H03337 and 21K12191); JST CREST(grant numbers:JP-MJCR21Dl); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350066","Multi-Agent Systems;Security Game;Number Guessing;Max-Min Strategy;Equilibrium Strategy","Sufficient conditions;Costs;Decision making;Games;Minimax techniques;Linear programming;Security","","","","11","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models","H. Aghakhani; W. Dai; A. Manoel; X. Fernandes; A. Kharkar; C. Kruegel; G. Vigna; D. Evans; B. Zorn; R. Sim","University of California, Santa Barbara; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; University of California, Santa Barbara; University of California, Santa Barbara; University of Virginia; Microsoft Corporation; Microsoft Corporation",2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","1122","1140","With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model’s training by injecting malicious data. Poisoning attacks could be designed to influence the model’s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00140","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646865","Large Language Models;Generative AI;Code Generation;Data Poisoning;Trustworthy AI","Training;Codes;Toxicology;Training data;Static analysis;Transformers;Data models","","9","","54","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro","G. Mizrahi; D. Serfaty",NA; NA,Unlocking the Secrets of Prompt Engineering: Master the art of creative language generation to accelerate your journey from novice to pro,"","2024","","","","","Enhance your writing with AI by mastering prompt engineering techniques and become an expert in developing and utilizing LLM prompts across applicationsKey FeaturesMaster prompt engineering techniques to harness AI's writing potentialDiscover diverse LLM applications for content creation and beyondLearn through practical examples, use cases, and hands-on guidancePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionUnlocking the Secrets of Prompt Engineering is your key to mastering the art of AI-driven writing. This book propels you into the world of large language models (LLMs), empowering you to create and apply prompts effectively for diverse applications, from revolutionizing content creation and chatbots to coding assistance. Starting with the fundamentals of prompt engineering, this guide provides a solid foundation in LLM prompts, their components, and applications. Through practical examples and use cases, you'll discover how LLMs can be used for generating product descriptions, personalized emails, social media posts, and even creative writing projects like fiction and poetry. The book covers advanced use cases such as creating and promoting podcasts, integrating LLMs with other tools, and using AI for chatbot development. But that’s not all. You'll also delve into the ethical considerations, best practices, and limitations of using LLM prompts as you experiment and optimize your approach for best results. By the end of this book, you'll have unlocked the full potential of AI in writing and content creation to generate ideas, overcome writer's block, boost productivity, and improve communication skills.What you will learnExplore the different types of prompts, their strengths, and weaknessesUnderstand the AI agent's knowledge and mental modelEnhance your creative writing with AI insights for fiction and poetryDevelop advanced skills in AI chatbot creation and deploymentDiscover how AI will transform industries such as education, legal, and othersIntegrate LLMs with various tools to boost productivityUnderstand AI ethics and best practices, and navigate limitations effectivelyExperiment and optimize AI techniques for best resultsWho this book is forThis book is for a wide audience, including writers, marketing and business professionals, researchers, students, tech enthusiasts, and creative individuals. Anyone looking for strategies and examples for using AI co-writing tools like ChatGPT effectively in domains such as content creation, drafting emails, and inspiring artistic works, will find this book especially useful. If you are interested in AI, NLP, and innovative software for personal or professional use, this is the book for you.","","9781835088265","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10460891.pdf&bkn=10460890&pdfType=book","","","","","","","","6 Mar 2024","","","Packt Publishing","Packt Publishing eBooks"
"Code Gradients: Towards Automated Traceability of LLM-Generated Code","M. North; A. Atapour-Abarghouei; N. Bencomo","CS, Durham University, Durham, UK; CS, Durham University, Durham, UK; CS, Durham University, Durham, UK",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","321","329","Large language models (LLMs) have recently seen huge growth in capability and usage. Within software engineering, LLMs are increasingly being used by developers to generate code. Code generated by an LLM can be seen essentially a continuous mapping from requirements to code. This represents a great opportunity within requirements engineering to use this mapping to provide traceability from requirements to LLM-generated code. The challenge is that the black-box nature of LLMs makes it difficult to trace requirements, while traditional approaches require extensive post-hoc testing or expert analysis. In this research preview, we explore the use of LLM explainability techniques to trace LLM-generated code back to requirements. By inspecting the gradients of LLM output, we develop a first attempt at tracing LLM inputs through to its generated code. We use this to estimate which low-level requirements have been met. Furthermore, through an automated iterative process, we re-query the LLM, instructing it to rewrite its code to meet the missing requirements. Our results suggest that the gradients of LLM outputs can be used to trace requirements through LLM code generation and that this traceability could potentially be used to improve generated code to better meet requirements. Future work is required to fully validate this result, but this represents a first step towards automatic traceability and verification of AI generated code.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628471","Requirements Engineering;Large Language Models;Traceability","Codes;Large language models;Natural languages;Buildings;Closed box;Requirements engineering;Iterative methods","","5","","39","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Generative AI based industrial metaverse creation methodology","A. Shrestha; K. Imamoto","Autonomous Control Research Department, Hitachi Research and Development Group, Tokyo, Japan; Autonomous Control Research Department, Hitachi Research and Development Group, Tokyo, Japan",2024 Artificial Intelligence for Business (AIxB),"3 Dec 2024","2024","","","53","57","The metaverse has been proposed as a suitable apparatus for the dissemination of information in a railway maintenance and operation context. However, the generation of such a metaverse environment requires significant investment with the creation of simple prototypes taking an extended duration. Although there are generative artificial intelligencebased methods to create small scenes, there is an absence of a method to do so for industrial applications. We devised a platform to create railway environments with the assistance of the language models for code creation and semantic inference without the need for reprogramming or editing of the project source meaning environments could be generated by the end users. With a natural language input and a coding paradigm output the code generation module is shown together with the example environments from real-life railway lines in Tokyo, Japan as preliminary results. By creating such environments leveraging the rapid generation with the help of generative artificial intelligence, we show generative artificial intelligence can be used to automate the task of the programmer to create new environments on demand from the user in natural language.","","979-8-3503-9103-9","10.1109/AIxB62249.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771265","industrial metaverse;generative ai;railway;simulation","Codes;Metaverse;Generative AI;Natural languages;Semantics;Prototypes;Rail transportation;Encoding;Maintenance;Investment","","","","9","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Smart Training Framework and Assessment Strategies","T. Gaikwad; A. Kulkarni","School of Engineering Swinburne University of Technology, Melbourne, Australia; School of Engineering Swinburne University of Technology, Melbourne, Australia",2023 IEEE Engineering Informatics,"14 May 2024","2023","","","1","7","The rapidly evolving landscape of technological advancements is significantly transforming the education sector. This integration of technology in the education sector has given rise to the edtech industry which is transforming as newer technologies are introduced. Training delivered to the learners, along with the assessment of the learners, are the fundamental components of the education sector. However, current methods of delivering training and assessing learners face numerous challenges, including skill shortage due to technology advancements, high costs, conducting complex training in high- risk environments. Similarly, assessment methods struggle with inflexible assessment strategies and limited personalized feedback to learners. Addressing these challenges in training and assessment, this study proposes a smart training and assessment framework (STAF) which leverages the benefits of augmented reality (AR) and artificial intelligence (AI) based large language models (LLMs) which stand out as a monumental leap in reshaping the training and assessment sector. As part of this study, an AR based training module was created and delivered to students. A survey was conducted of these students to gain insights about the adaptability of AR based trainings and potential to improve these trainings. It is concluded that along with AR in education, AI and LLMs with prompt engineering strategies should be integrated in the education domain for better interactivity and enhanced student performance. Currently, limited research is conducted on integration of LLMs in AR environments for the education sector and this paper provides an in-depth exploration of the immense potential of the applications of LLMs within the realm of training and assessment for improved learner performance.","","979-8-3503-3852-2","10.1109/IEEECONF58110.2023.10520594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10520594","edtech;large language models;AR training;smart assessment;prompt engineering","Training;Surveys;Industries;Visualization;Costs;Virtual assistants;Natural languages","","","","16","IEEE","14 May 2024","","","IEEE","IEEE Conferences"
"Developing Personalized Marketing Service Using Generative AI","G. H. Lee; K. J. Lee; B. Jeong; T. Kim","Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea",IEEE Access,"15 Feb 2024","2024","12","","22394","22402","In today’s world, the development of social network services (SNS) like Facebook and Instagram has enabled consumers to acquire information about products through various channels. The acquisition of diverse information has led to a diversification in consumer preferences and requirements. As consumer preferences diversify and online channels expand, there is an increasing need for companies to provide personalized marketing. Among the means of personalized marketing, personalized marketing messages are a key tool that can enhance customer engagement. However, a limitation of personalized marketing message services is the cost issue associated with manually writing individual marketing messages for personalization. To solve this problem, when developing automated technology for personalized marketing messages, there were concerns about the complexity of model development and the quality of messages generated automatically. In this study, we propose the Persuasive Message Intelligence (PMI) service, which utilizes the recently prominent Large Language Model for automated individual personalized marketing messages. PMI generates marketing messages through prompt engineering based on the theory of persuasion in marketing and prior research on AI-generated messages, and validates the elements of prompts through surveys. The trial and error of researchers presented in this study, along with the know-how and rules of prompt engineering, will serve as guidelines for those who wish to develop services through prompts in the future.","2169-3536","","10.1109/ACCESS.2024.3361946","BK21 Fostering Outstanding Universities for Research (FOUR); Ministry of Education (MOE), South Korea; National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10419357","Generative AI;personalized marketing message;persuasion theory;prompt engineering","Advertising;Psychology;Social networking (online);Public healthcare;History;Generative AI;Business;Generative adversarial networks;Artificial intelligence","","12","","24","CCBYNCND","5 Feb 2024","","","IEEE","IEEE Journals"
"WIP: Engineering Class Students' Epistemic Cognition when Interacting with Generative AI","R. Y. -Y. Chan; C. K. Y. Chan; M. S. -Y. Jong; Z. Hu; Y. Zhang","Dept. of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR; Teaching and Learning Innovation Centre, The University of Hong Kong, Pokfulam, H.K., Hong Kong SAR; Dept. of Curriculum and Instruction, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, N.T., China; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, N.T., China",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","5","This work in progress belongs to the innovative practice category. Nowadays, generative AI (also known as GenAI) can produce novel data samples that closely resemble authentic datasets. The advent of large language models (LLMs), in particular, has caused a huge interest in utilizing GenAI within and beyond the realm of higher education. However, little about engineering students' views and behaviours related to knowing and knowledge when using GenAI, such as ChatGPT, is known. In this WIP, we have engaged a class of N = 37 engineering students taking a postgraduate course titled “Social Media Analytics”. They were required to write essays related to their course learning in the form of blog posts. They were required to use LLM tools, such as ChatGPT, to assist their writing processes. Their GenAI usage was guided by the cognitive-agent approach, Search Tree, Analyze and Repair, and Selection (STARS), while STARS was proposed by Kirk et al. in AAAI 2024 to extend and complement prompt engineering. In addition, the participants were invited to fill in the Epistemic Cognition Inventory (ECI) questionnaire to associate five aspects of epistemic cognition (EC) with their writing experience. It is confirmed in our results that students' EC, i.e., their beliefs related to knowledge and knowing, significantly predict their prompting engagement and academic performance. However, students' academic performance is found to be significantly and negatively associated with their preference for GenAI usage. Here, we have uncovered engineering students' EC when interacting with generative AI, an area where little has been known so far. Our findings also suggest that proper use of GenAI prompting might promote engineering students' EC and, therefore, engineering learning.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893501","","Social networking (online);Education;Stars;Writing;Maintenance engineering;Chatbots;Cognition;Reliability;Prompt engineering;Engineering students","","1","","11","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Enhancing Autonomous System Security and Resilience With Generative AI: A Comprehensive Survey","M. Andreoni; W. T. Lunardi; G. Lawton; S. Thakkar","Technology Innovation Institute, Masdar City, Abu Dhabi, United Arab Emirates; Technology Innovation Institute, Masdar City, Abu Dhabi, United Arab Emirates; Independent Researcher, Beckenham, London, U.K.; Technology Innovation Institute, Masdar City, Abu Dhabi, United Arab Emirates",IEEE Access,"13 Aug 2024","2024","12","","109470","109493","This survey explores the transformative role of Generative Artificial Intelligence (GenAI) in enhancing the trustworthiness, reliability, and security of autonomous systems such as Unmanned Aerial Vehicles (UAVs), self-driving cars, and robotic arms. As edge robots become increasingly integrated into daily life and critical infrastructure, the complexity and connectivity of these systems introduce formidable challenges in ensuring security, resilience, and safety. GenAI advances from mere data interpretation to autonomously generating new data, proving critical in complex, context-aware environments like edge robotics. Our survey delves into the impact of GenAI technologies—including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformer-based models, and Large Language Models (LLMs)—on cybersecurity, decision-making, and the development of resilient architectures. We categorize existing research to highlight how these technologies address operational challenges and innovate predictive maintenance, anomaly detection, and adaptive threat response. Our comprehensive analysis distinguishes this work from existing reviews by mapping out the applications, challenges, and technological advancements of GenAI and their impact on creating secure frameworks for autonomous systems. We discuss significant challenges and future directions for integrating these technologies within security frameworks to address the evolving landscape of cyber-physical threats, underscoring the potential of GenAI to make autonomous systems more adaptive, secure, and efficient.","2169-3536","","10.1109/ACCESS.2024.3439363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623653","GenerativeAI;artificial intelligence;autonomous systems;security;UxV","Security;Robots;Computer security;Surveys;Artificial intelligence;Safety;Task analysis","","25","","173","CCBYNCND","6 Aug 2024","","","IEEE","IEEE Journals"
"Optimizing Mobile-Edge AI-Generated Everything (AIGX) Services by Prompt Engineering: Fundamental, Framework, and Case Study","Y. Liu; H. Du; D. Niyato; J. Kang; S. Cui; X. Shen; P. Zhang","School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Science and Engineering (SSE) and the Future Network of Intelligence Institute (FNii), The Chinese University of Hong Kong (Shenzhen), Shenzhen, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Network,"13 Sep 2024","2024","38","5","220","228","As the next-generation paradigm for content creation, AI-Generated Content (AIGC), i.e., generating content automatically by Generative AI (GAI) based on user prompts, has gained great attention and success recently. With the ever-increasing power of GAI, especially the emergence of Pretrained Foundation Models (PFMs) that contain billions of parameters and prompt engineering methods (i.e., finding the best prompts for the given task), the application range of AIGC is rapidly expanding, covering various forms of information for human, systems, and networks, such as network designs, channel coding, and optimization solutions. In this article, we present the concept of mobile-edge AI-Generated Everything (AIGX). Specifically, we first review the building blocks of AIGX, the evolution from AIGC to AIGX, as well as practical AIGX applications. Then, we present a unified mobile-edge AIGX framework, which employs edge devices to provide PFM-empowered AIGX services and optimizes such services via prompt engineering. More importantly, we demonstrate that suboptimal prompts lead to poor generation quality, which adversely affects user satisfaction, edge network performance, and resource utilization. Accordingly, we conduct a case study, showcasing how to train an effective prompt optimizer using ChatGPT and investigating how much improvement is possible with prompt engineering in terms of user experience, quality of generation, and network performance.","1558-156X","","10.1109/MNET.2023.3335255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330096","AI-generated Everything (AIGX);Prompt Engineering;Edge Networks;Optimization;Efficiency","Optimization;Visualization;Videos;Bandwidth;Generative adversarial networks;Artificial intelligence;Prompt engineering;Edge computing","","23","","15","IEEE","28 Nov 2023","","","IEEE","IEEE Magazines"
"LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis","Y. Liu; S. Tao; W. Meng; F. Yao; X. Zhao; H. Yang","Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China; Huawei, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","364","365","Automated log analysis plays a crucial role in software maintenance as it allows for efficient identification and resolution of issues. How-ever, traditional methods employed in log analysis heavily rely on extensive historical data for training purposes and lack rationales for its predictions. The performance of these traditional methods significantly deteriorates when in-domain logs for training are lim-ited and unseen log data are the majority, particularly in rapidly changing online environments. Additionally, the lack of rationales hampers the interpretability of analysis results and impacts analysts' subsequent decision-making processes. To address these challenges, we proposes LogPrompt, an novel approach that leverages large language models (LLMs) and advanced prompting techniques to achieve performance improvements in zero-shot scenarios (i.e., no in-domain training). Moreover, LogPrompt has garnered positive evaluations from experienced practitioners in its log interpretation ability. Code available at https://github.com/lunyiliu/LogPrompt.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3643108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554918","","Training;Software maintenance;Codes;Decision making;Software engineering","","13","","3","","20 Jun 2024","","","IEEE","IEEE Conferences"
"SPICEPilot: Navigating SPICE Code Generation and Simulation with AI Guidance","D. Vungarala; S. Alam; A. Ghosh; S. Angizi","Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA",2024 IEEE International Conference on Rebooting Computing (ICRC),"27 Mar 2025","2024","","","1","6","Large Language Models (LLMs) have shown great potential in automating code generation; however, their ability to generate accurate circuit-level SPICE code remains limited due to a lack of hardware-specific knowledge. In this paper, we analyze and identify the typical limitations of existing LLMs in SPICE code generation. To address these limitations, we present SPICEPilot—a novel Python-based dataset generated using PySpice, along with its accompanying framework. This marks a significant step forward in automating SPICE code generation across various circuit configurations. Our framework automates the creation of SPICE simulation scripts, introduces standardized benchmarking metrics to evaluate LLM’s ability for circuit generation, and outlines a roadmap for integrating LLMs into the hardware design process. SPICEPilot is open-sourced under the permissive MIT license at https://github.com/ACADLab/SPICEPilot.git.","","979-8-3315-4127-9","10.1109/ICRC64395.2024.10937006","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937006","SPICE;LLM-powered code generation;circuit design","Technological innovation;Codes;Automation;Navigation;Benchmark testing;SPICE;Hardware;Software;Circuit synthesis;Software reliability","","3","","29","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Trustworthy Large Language Model Prompt Engineering for Risk-Free Smart Grid Management Education","A. Imanifard; B. Majidi; A. Shamisa","Department of Computer Engineering, Faculty of Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Faculty of Engineering, Khatam University, Tehran, Iran; Department of Electrical Engineering, Faculty of Engineering, Khatam University, Tehran, Iran",2024 14th Smart Grid Conference (SGC),"16 May 2025","2024","","","1","6","The management and control of smart grids are rapidly evolving to include Artificial Intelligence (AI). Large Language Models (LLMs) are the latest solution for AI-capable Industry 4.0 management. The trustworthiness of the LLMs is an important issue in the management of such systems. In this paper, the behaviors of various locally-hosted LLMs in managing unauthorized operations within smart grids are investigated. To highlight the advantages of local-hosting, we also compare the performance of these models with online LLMs. Our findings show that locally-hosted LLMs perform better in preventing vulnerabilities than their online counterparts. This study proposes an educational platform for future generations of smart grid operators and provides guidelines for teaching the vulnerabilities of LLM prompting needed to be included in the educational curriculum of the smart grid management workforce. Furthermore, the findings of this research can provide guidelines for standardization, legislation and regulations of AI applications in smart grids.","2572-6927","979-8-3315-1133-3","10.1109/SGC64640.2024.10983889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10983889","Education Technology;Large Language Model;Prompt Engineering;Power System Education;Smart Grid","Training;Large language models;Legislation;Reliability engineering;Regulation;Smart grids;Power system reliability;Fourth Industrial Revolution;Prompt engineering;Guidelines","","2","","15","IEEE","16 May 2025","","","IEEE","IEEE Conferences"
"Code Generation Using Self-Interactive Assistant","Z. Zhao; J. Sun; C. -H. Cai; Z. Wei","School of Computer Science, University of Auckland, Auckland, New Zealand; School of Computer Science, University of Auckland, Auckland, New Zealand; Suzhou Industrial Park Monash Research, Institute of Science and Technology, Suzhou, China; School of Computer Science, Beijing Institute of Technology, Beijing, China","2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)","26 Aug 2024","2024","","","2347","2352","The rise of large language models (LLMs) has expanded the possibilities for generative tasks. While LLMs excel at tasks like information retrieval and question-answering, conventional code generation LLMs often struggle to maintain fidelity to prompts, leading to coding hallucinations that produce inaccurate results. In this study, we present Self Coder, a self-guided, single-agent framework for code generation utilising the OpenAI Assistant API built upon GPT-4. By tapping into a comprehensive Python code knowledge base, Self Coder creates an initial draft and refines it iteratively through a feedback loop using the Code Interpreter tool. This approach minimizes contextual inconsistencies and significantly reduces errors, achieving state-of-the-art results on the HumanEval and MBPP datasets, outperforming other code generation frameworks.","2836-3795","979-8-3503-7696-8","10.1109/COMPSAC61105.2024.00377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633497","Code Generation;OpenAI Assistant API;GPT;Large Language Model","Codes;Computational modeling;Large language models;Knowledge based systems;Programming;Software;Robustness","","2","","21","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"CoF: Coarse to Fine-Grained Image Understanding for Multi-modal Large Language Models","Y. Wang; D. Gao; B. Li; R. Long; L. Yi; X. Cai; L. Yang; J. Zhang; S. Yu; Q. Xuan","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China; The Key Laboratory of Measurement and Control of CSE, Ministry of Education, School of Automation, Southeast University, Nanjing, China; Zhejiang University of Technology, Hangzhou, China; Zhejiang University of Technology, Hangzhou, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","The impressive performance of Large Language Model (LLM) has prompted researchers to develop Multi-modal LLM (MLLM), which has shown great potential for various multi-modal tasks. However, current MLLM often struggles to effectively address fine-grained multi-modal challenges. We argue that this limitation is closely linked to the models’ visual grounding capabilities. The restricted spatial awareness and perceptual acuity of visual encoders frequently lead to interference from irrelevant background information in images, causing the models to overlook subtle but crucial details. As a result, achieving fine-grained regional visual comprehension becomes difficult. In this paper, we break down multi-modal understanding into two stages, from Coarse to Fine (CoF). In the first stage, we prompt the MLLM to locate the approximate area of the answer. In the second stage, we further enhance the model’s focus on relevant areas within the image through visual prompt engineering, adjusting attention weights of pertinent regions. This, in turn, improves both visual grounding and overall performance in downstream tasks. Our experiments show that this approach significantly boosts the performance of baseline models, demonstrating notable generalization and effectiveness. Our CoF approach is available online at https://github.com/Gavin001201/CoF.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889170","National Natural Science Foundation of China; Research and Development; Research and Development; Research and Development; Southeast University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889170","vision and language;prompt engineering;fine-grained understanding","Location awareness;Visualization;Image recognition;Grounding;Large language models;Interference;Signal processing;Prompt engineering;Speech processing;Visual perception","","","","39","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"A Generative Text Summarization Method Based on mT5 and Large Language Models","F. Liu; C. Xiong","School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China",2023 Eleventh International Conference on Advanced Cloud and Big Data (CBD),"8 May 2024","2023","","","174","179","A single pre-trained language model (such as mT5) still often fails to capture key information and cover sufficient content for the text summarization task. We propose a generative text summarization method (mT5-LLM) based on mT5 and a large language model. The method combines the best of both models to improve the accuracy and fluency of result summaries as follows: firstly, pre-training the mT5 model and fine-tuning it on the text summarization task so that it can generate summaries related to the input text; secondly, we use the fine-tuned mT5 model to generate a preliminary summary; thirdly, we use a large language model such as ChatGPT to modify and improve the preliminary summary according to the in-context learning. Besides, we employ some prompt engineering methods to produce a more accurate and fluent summary. After several rounds of interaction and modification, the last generated summary is taken as the final result. The proposed method is evaluated by experiments on the LCSTS public dataset. The results show that our proposed method achieved a higher ROUGE score than the traditional model algorithm. Ablation studies also show that our method effectively improves the quality of the abstract.","","979-8-3503-5337-2","10.1109/CBD63341.2023.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516496","text summarization;large language model;pretraining;mT5;fine-tuning;in-context learning;prompt engineering","Measurement;Linguistics;Big Data;Chatbots;Real-time systems;Data models;Iterative methods","","","","18","IEEE","8 May 2024","","","IEEE","IEEE Conferences"
"Repairing Infrastructure-as-Code using Large Language Models","E. Low; C. Cheh; B. Chen","Singapore University of Technology and Design, Singapore; Illinois Advanced Research Center at Singapore Ltd., Singapore; Singapore University of Technology and Design, Singapore",2024 IEEE Secure Development Conference (SecDev),"30 Oct 2024","2024","","","20","27","Infrastructure-as-Code (IaC) is the practice of provisioning and managing cloud resources using machine-readable code. IaC is seeing increased adoption because it enhances transparency and reliability of infrastructure operations. However, as any software code, IaC can also contain misconfigurations, which can lead to insecure infrastructure, which may result in data breaches. Existing IaC scanning tools are able to detect common misconfigurations in IaC but they require IaC developers to manually repair the code. Recent advances in Large Language Models (LLMs) have led to promising results in applying LLMs to Automatic Program Repair (APR) tasks for code written in different languages. In this work, we propose an LLM-based approach to fix misconfigurations in IaC code. After misconfigurations in IaC code are identified by scanning tools, we feed the LLMs with the IaC code, details about the misconfigurations, and additional context provided by a human-in-the-loop and prompt the LLM to generate the repaired IaC code. We tested our approach on several vulnerable IaC repositories and found that the GPT-4 model from OpenAI suggests fixes that reduce up to 84.7% of the misconfiguration alarms produced by the scanners and our two-pass solution significantly improves the performance over a one-pass only approach. However, of the fixes suggested, we manually determined that only 79.6% actually solve the problem, while the remaining 20.4% are hallucinated fixes. Specifically, LLM hallucinations in the generated outputs pass checks for misconfigurations but fail other syntax and schema validation checks or do not address the underlying security issue. We propose a few potential approaches to tackle this challenge.","","979-8-3503-9193-0","10.1109/SecDev61143.2024.00008","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734039","infrastructure as code;large language model;automatic program repair","Codes;Large language models;Semantics;Maintenance engineering;Syntactics;Human in the loop;Software;Software reliability;Security;Feeds","","2","","25","IEEE","30 Oct 2024","","","IEEE","IEEE Conferences"
"Exploring the Essence of Prompt Tone: A Comparative Study of Large Language Model Security","A. Nair; J. Vithayathil; S. P. Nambiar; S. S; S. Murali","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India","2024 Third International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)","23 Oct 2024","2024","","","1","8","Chatbots powered by Large Language Model(LLM) can be manipulated by malicious prompts, generating harmful content and biased responses which would raise security concerns. Growing dependence on chatbots demands robust security for ethical development and user trust, which makes the work relevant in today’s world. The motivation behind the work is to let the user have a safe experience with no negative responses being displayed while using the chatbot, which paved the way to arrive at the goal of developing a security filter that could be integrated into any LLM feature integrated application to mitigate the risk of having security vulnerabilities like prompt injection and jailbreaking, which could be achieved by converting malicious prompt into safer prompts by the method of eliminating negative sentiment phrases. The work focuses on building and implementing the security filters to popular in-production LLMs like Large Language Model Meta AI-2 (LLaMA2) and Generative Pre-trained Transformer – 3.5 turbo (GPT-3.5) to see how they handle against prompt injection and jailbreaking before and after the security filter being integrated. A large database of 200,000 prompts has been collected and preprocessed to train on a machine learning model using binary classification algorithm having 99.7% accuracy for classification of prompts into malicious or non-malicious and further checks are being done by breaking the prompt into smaller phrases and individually analyzing their compound sentiment score using Natural Language Toolkit (NLTK) Valence Aware Dictionary for Sentiment Reasoning (VADER) algorithm to detect and drop the negative sentiment phrases for the modification of the user prompt to eliminate the possibility of malicious prompt being passed to LLM. It is difficult to determine the sentiment of prompts in a detailed way and convert it into an efficient design that will perform well with models. Once this hurdle is overcome, chatbots will become even more reliable, trustworthy, and user-friendly tools, increasing the level of people’s trust in this technology, and establishing guidelines for how to use these resources in the most beneficial ways.","","979-8-3503-6908-3","10.1109/ICEEICT61591.2024.10718584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718584","Large Language Models;Manipulated;Malicious prompts;Biased responses;Robust security;Ethical development;User trust;Filters;Sentiment;Reliable;Trustworthy","Sentiment analysis;Accuracy;Machine learning algorithms;Large language models;Natural languages;Chatbots;Transformers;Classification algorithms;Security;Reliability","","","","10","IEEE","23 Oct 2024","","","IEEE","IEEE Conferences"
"Practical Guide to Prompt Engineering","I. Khan",NA,"The Quick Guide to Prompt Engineering: Generative AI Tips and Tricks for ChatGPT, Bard, Dall-E, and Midjourney","","2024","","","83","102","Summary <p>Crafting effective prompts for a generative AI system is pivotal in guiding its responses and ensuring meaningful and accurate results. This chapter offers a step‐by‐step breakdown to ensure our initiation into prompt engineering is smooth and productive. It guides on how to systematically test and evaluate our prompts for AI systems. Prompt engineering, akin to any skill, requires continual refinement. The chapter presents an in‐depth look into iterating and refining prompts within the realm of prompt engineering. It elucidates how prompt engineering can be tailored for various domains, ensuring effective and relevant AI interactions. In the realm of prompt engineering, machine learning (ML) has emerged as a powerful ally, pushing the boundaries of what's possible. By integrating ML techniques, we can optimize prompts dynamically, making interactions with AI models more precise and insightful.</p>","","9781394243341","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10950803.pdf&bkn=10950171&pdfType=chapter","","Prompt engineering;Artificial intelligence;Testing;History;Feedback loop;Analytical models;Adaptation models;Refining;Iterative methods;Urban planning","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Generative AI in Engineering and Computing Education: A Scoping Review of Empirical Studies and Educational Practices","J. Álvarez Ariza; M. Benitez Restrepo; C. Hernández Hernández","Department of Technology in Electronics, Engineering Faculty, Corporación Universitaria Minuto de Dios-UNIMINUTO, Bogotá, Colombia; Engineering Faculty, Universidad de Los Andes, Bogotá, Colombia; Engineering Faculty, Universidad de Los Andes, Bogotá, Colombia",IEEE Access,"19 Feb 2025","2025","13","","30789","30810","Since the release of diverse generative AI (GenAI) tools such as ChatGPT, Google Gemini, DALL $\cdot $ E, and GitHub Copilot, there has been much debate around the impacts and implications of these tools on education. Currently, extant literature remarks on the affordances, challenges, and opportunities of GenAI, but few studies report and analyze empirical studies and educational practices coming up by GenAI usage in learning settings. Then, in this Scoping Review (ScR) based on 146 studies retrieved from the databases SCOPUS, Web of Science (WoS), and ERIC, we analyzed the implications of integrating GenAI in engineering and computing education from K-12 to tertiary levels. We adopted an approach starting from the bibliometric features of the studies in terms of authors, cites, years, or cluster topics, and navigating to the identification of methodologies, strategies, AI literacy instruments and guidelines, learning outcomes, and students’ and teachers’ perceptions, among other features. We advocate that current educational practices in engineering and computing with GenAI can indicate to us a roadmap of its potentialities, uses, and risks from the standpoint of both teachers and students, and this could help us to create more reflexive methodologies that enhance the teaching-learning process based on the evidence. Our purpose with the outcomes and conclusions of this scoping review is to support educators, faculty members, and other stakeholders in engineering and computing education to co-create educational methodologies that articulate GenAI with curricula, AI literacy, and prompt engineering encompassing students’ learning domains such as cognitive, affective, or behavioral.","2169-3536","","10.1109/ACCESS.2025.3541424","Engineering Information Foundation (EiF)(grant numbers:EiF24.04); Vice Presidency of Research and Creation’s Publication Fund at Universidad de Los Andes; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10883995","Generative AI;GenAI;GAI;artificial intelligence;AI;computer science education;engineering education;computing education;prompt engineering;AI literacy","Artificial intelligence;Education;Ethics;Chatbots;Systematic literature review;Computational modeling;Affordances;Privacy;Plagiarism;Training","","4","","87","CCBY","13 Feb 2025","","","IEEE","IEEE Journals"
"LearnRAG: Implementing Retrieval-Augmented Generation for Adaptive Learning Systems","R. Shan","Department of Data Science, North Carolina School of Science and Mathematics, Durham, NC, USA",2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),"19 Mar 2025","2025","","","0224","0229","The rapid advancements in large language models have revolutionized natural language processing, yet their static knowledge bases limit their applicability in dynamic, domain-specific, and personalized contexts. Retrieval-Augmented Generation systems address this challenge by integrating retrieval mechanisms with generative models to deliver real-time, contextually enriched responses. This paper implements LearnRAG, an open-source RAG framework for personalized learning that is modular in architecture, hybrid in retrieval, and fine-tuned for generation to produce adaptive educational content. A holistic case study of LearnRAG showed scalability, efficiency, increasing learner engagement, and reducing educators' workload. Issues such as multimodal integration, content accuracy, and learning styles are discussed, and strategies for ethical deployment are developed. LearnRAG offers a robust, scalable, and adaptive platform to meet the evolving needs of learners and educators worldwide, representing a paradigm shift in GenAI-driven education.","2831-6983","979-8-3315-0702-2","10.1109/ICAIIC64266.2025.10920869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10920869","Retrieval-Augmented Generation (RAG);large language models (LLMs);Natural Language Processing (NLP);open source;architecture;system design;vector database;orchestration;retrieval;implementation;personalized learning","Adaptation models;Accuracy;Large language models;Scalability;Retrieval augmented generation;Natural language processing;Vectors;Real-time systems;Multisensory integration;Context modeling","","","","20","IEEE","19 Mar 2025","","","IEEE","IEEE Conferences"
"Artificial Intelligence Chatbots and Social Robots in Education: FEPER Framework for Efficiency, Pedagogical and Ethical Requirements","M. Virvou; G. A. Tsihrintzis","Department of Informatics, Software Engineering Laboratory, University of Piraeus, Piraeus, Greece; Department of Informatics, Machine Learning, Image Processing and Multimedia Applications Laboratory, University of Piraeus, Piraeus, Greece","2024 15th International Conference on Information, Intelligence, Systems & Applications (IISA)","18 Dec 2024","2024","","","1","8","Artificial Intelligence (AI) chatbots and social robots, when incorporated into educational settings, have the potential to significantly transform the learning experience, particularly by providing more personalised, one-on-one attention to students. However, deploying such technology in classrooms also presents challenges. These include unresolved issues related to human-AI interaction, pedagogy and theories, human oversight, data privacy, transparency, accuracy and trust. This paper explores the development of a comprehensive framework aimed at enhancing the efficiency, pedagogical effectiveness, and ethical use of AI-driven educational tools, focusing on social robots and chatbots. The framework addresses important factors such as interaction quality, requirements engineering strategies, and the alignment of AI functionalities with educational goals. It also examines ethical concerns, including data security, user transparency, and trustworthiness. By reviewing recent literature on social robots and chatbots and incorporating emerging ethical considerations alongside long-standing educational objectives, this research aims at establishing a holistic framework, FEPER, of best practices and requirements. The findings offer valuable insights for educators, policymakers, and developers aiming to create more dynamic, personalised, and ethical educational environments using AI technology.","","979-8-3503-6883-3","10.1109/IISA62523.2024.10786710","University of Piraeus Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786710","Human-AI Interaction;Artificial Intelligence;Human-centered AI;Large Language Models (LLMs);Generative AI;e-learning;educational software;user modelling;ChatGPT;social robots;chatbots;AI in education;requirements","Ethics;Data privacy;Data security;Social robots;Focusing;Transforms;Learning (artificial intelligence);Chatbots;Requirements engineering;Best practices","","4","","44","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"Exploiting Agents and Artifacts Metamodel to Provide Abstraction of E-Learning Resources","B. Ciloglugil; M. M. Inceoglu","Department of Computer Engineering, Ege University, Izmir, Turkey; Dept. of Computer Education & Instructional Technology, Ege University, Izmir, Turkey",2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT),"7 Aug 2017","2017","","","74","75","Even though multi agent systems (MAS) based e-learning applications are common in the literature, most of them are standalone applications containing a single type of agent responsible for accessing learning resources. Considering the highly distributed nature of learning resources, this approach is not feasible for large-scale e-learning systems. Agents and Artifacts (A&A) Metamodel focuses on environment modeling in multi agent system design and models entities in agents' environments with artifacts as first class entities like the agents. In e-learning systems, learning resources are part of the environment of the agents and agents interact with them constantly. Therefore, abstraction of learning resources from different MAS based e-learning applications can be achieved by exploiting A&A Metamodel. In this paper, an e-learning environment model based on A&A Metamodel has been proposed and implemented as a prototype with CArtAgO framework. The proposed model supports dynamically changing nature of learning materials that can be available at different sources.","2161-377X","978-1-5386-3870-5","10.1109/ICALT.2017.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001721","e-learning;learning objects;multi agent systems;agents and artifacts metamodel;environment programming;CArtAgO","Conferences","","4","","4","IEEE","7 Aug 2017","","","IEEE","IEEE Conferences"
"Explainable Generative AI: Enhancing Stable Diffusion with Machine Learning and Generative AI","A. P. Sarasan; A. M; S. S","Dept of Computer Science And IT, Amrita School of Computing, Kochi, India; Dept of Computer Science And IT, Amrita School of Computing, Kochi, India; Dept of Computer Science And IT, Amrita School of Computing, Kochi, India",2025 5th International Conference on Pervasive Computing and Social Networking (ICPCSN),"19 Jun 2025","2025","","","571","577","Explainable AI (XAI) increases the transparency of AI, it is a set of techniques that help us how machine learning algorithm make decisions however issues persist, particularly in understanding image generation tasks. Numerical interpretability is the major issue that makes it challenging for models to understand numerical values in prompts to recognize the number of subjects for image generation using diffusion models. In this study, prompt engineering using Generative AI and SHAP-based (Shapley Additive Explanations) explainability are used to enhance the prompt and understand the features that contribute to the generated image. The Gemini API prompts refined model responses and increases numerical interpretability because generative AI can reduce human errors and enhance image features through structured prompts. Prompt engineering techniques are used to guide Artificial Intelligence models to produce desired output, techniques such as dynamic prompting expands the prompt and improve the numerical interpretability and clarity of the actual input.Synthetic images were tested for distortion using Peak Signal-to-Noise Ratio (PSNR: 9.11 dB) and our model is 20% better than the existing model and the Learned Perceptual Image Patch Similarity (LPIPS: 0.69) for 69% dissimilarity between the generated images of our extensible diffusion model model and the existing stable diffusion model, the Structural Similarity Index Measure (SSIM: 24%) for structural consistency and the Con trastive Language-Image Pre-training (CLIP) Score for semantic alignment between text and image. The results indicated reduced noise, better prompt alignment, and greater transparency. By combining structured prompting, explainability, and quantitative testing, this method improves generative model control, ensuring efficiency, interpretability, and better image quality for real-world applications.","","979-8-3315-3519-3","10.1109/ICPCSN65854.2025.11035339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035339","SHAP (Shapley Additive Explanations);CLIP (Contrastive Language-Image Pre- training);PSNR (Peak Signal-to-Noise Ratio);SSIM (Structural Similarity Index Measure);XAI (Explainable Artificial Intelligence);LPIPS (Learned Perceptual Image Patch Similarity);GEN AI (Generative Artificial Intelligence)","Image quality;Training;PSNR;Additives;Explainable AI;Image synthesis;Diffusion models;Numerical models;Prompt engineering;Indexes","","","","21","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Physical Safety and Cyber Security Analysis of Multi-Agent Systems: A Survey of Recent Advances","D. Zhang; G. Feng; Y. Shi; D. Srinivasan","Research Center of Automation and Artificial Intelligence, Zhejiang University of Technology, Hangzhou; Department of Biomedical Engineering, City University of Hong Kong, Hong Kong, China; Department of Mechanical Engineering, University of Victoria, Canada; Department of Electrical and Computer Engineering, National University of Singapore, Singapore",IEEE/CAA Journal of Automatica Sinica,"8 Jan 2021","2021","8","2","319","333","Multi-agent systems (MASs) are typically composed of multiple smart entities with independent sensing, communication, computing, and decision-making capabilities. Nowadays, MASs have a wide range of applications in smart grids, smart manufacturing, sensor networks, and intelligent transportation systems. Control of the MASs are often coordinated through information interaction among agents, which is one of the most important factors affecting coordination and cooperation performance. However, unexpected physical faults and cyber attacks on a single agent may spread to other agents via information interaction very quickly, and thus could lead to severe degradation of the whole system performance and even destruction of MASs. This paper is concerned with the safety/security analysis and synthesis of MASs arising from physical faults and cyber attacks, and our goal is to present a comprehensive survey on recent results on fault estimation, detection, diagnosis and fault-tolerant control of MASs, and cyber attack detection and secure control of MASs subject to two typical cyber attacks. Finally, the paper concludes with some potential future research topics on the security issues of MASs.","2329-9274","","10.1109/JAS.2021.1003820","National Natural Science Foundation of China(grant numbers:61873237); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317716","Consensus;deception attack;deny-of-service (DoS) attack;fault detection;fault estimation;fault tolerant control;multiagent systems","System performance;Control systems;Telecommunication computing;Smart grids;Cyberattack;Multi-agent systems;Smart manufacturing","","241","","116","","8 Jan 2021","","","IEEE","IEEE Journals"
"A New AI Literacy For The Algorithmic Age: Prompt Engineering Or Eductional Promptization?","H. Haugsbaken; M. Hagelia","Dept. of Education, ICT and Learning, Østfold University College, Halden, Norway; Dept. of Education, ICT and Learning, Østfold University College, Halden, Norway",2024 4th International Conference on Applied Artificial Intelligence (ICAPAI),"31 May 2024","2024","","","1","8","The introduction and rapid adoption of particular Artificial Intelligence (AI) technologies such as Large Language Models, where among other, Chat Generative Pre-trained Transformer (ChatGPT) is a prominent one, have caused a significant increase in research interest for prompt engineering. Prompt engineering can be loosely described as a new emerging field where one attempts to develop particular methods, strategies, and frameworks with the main aim of organizing and structuring inputs in such a way that sophisticated AI systems can perform different tasks. These methods, techniques, and frameworks can be concepts like in-context learning, Chain-of-Thought (CoT), Retrieval Augmented Generation, ReAct prompting, and Directional Stimulus Prompting. These approaches are mainly developed so users can communicate more effectively and accurately with AI language models and enable obtaining better outputs or responses tailored to queries or goals they set. As an extension and new contribution to the mentioned research field, this conceptual paper aims to introduce, propose, and theorize the notion of ‘educational promptization’. This will be done by connecting prompting to a particular social scientific practice perspective, sociomateriality. The rationale for arguing, however, is related to a need to develop an alternative research perspective to those that already dominate the mentioned research field, a research stream this conceptual paper also aims to engage with. Educational promptization is suggested as there is a demand to develop a more student-centric media literacy, especially as students and teachers engage with AI language interpreters on a daily basis. This means that the proposed educational promptization can arguably be considered to be part of a future AI literacy. An essential component of it is that one has to encompass the relational and symmetrical engagement with AI models. This is perhaps required because students will most likely be challenged to master the knowledge and skills in designing prompts, while simultaneously being capable of critically and meaningfully assessing the output of the prompts they created. In other words, mastering the complex input-and-output engagements with an AI system will be essential in students’ future learning processes.","","979-8-3503-4976-4","10.1109/ICAPAI61893.2024.10541229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541229","prompt;prompt engineering;education;student;digital competence;sociomateriality","Uncertainty;Education;Media;Transformers;Chatbots;Artificial intelligence;Task analysis","","7","","40","IEEE","31 May 2024","","","IEEE","IEEE Conferences"
"Large Language Model for Smart Inverter Cyber-Attack Detection via Textual Analysis of Volt/VAR Commands","A. Selim; J. Zhao; B. Yang","Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA; Research and Development Division, Hitachi America Ltd., Santa Clara, CA, USA",IEEE Transactions on Smart Grid,"25 Oct 2024","2024","15","6","6179","6182","This letter demonstrates a proof-of-concept validation of the Large Language Model (LLM) for smart inverter cyberattack detection through textual control commands. The proposed method can detect the manipulation of Volt/VAR curves and the comparison results with state-of-the-art machine learning techniques highlight its efficacy in identifying cyber attacks. Test results obtained from a real distribution feeder in Colorado, USA, validate the high accuracy for attack classification as well as demonstrate the potential of LLMs in adding a robust security layer for cyber-attacks.","1949-3061","","10.1109/TSG.2024.3453648","U.S. Department of Energy’s Office of Cybersecurity, Energy Security, and Emergency Response; Hitachi America; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663471","Large language models;cyber-attack detection;Volt/VAR control;text security;distribution network","Cyberattack;Adaptation models;Inverters;Electric potential;Voltage control;Training;Standards","","6","","10","IEEE","2 Sep 2024","","","IEEE","IEEE Journals"
"Vulnerability Handling of AI-Generated Code - Existing Solutions and Open Challenges","S. Kaniewski; D. Holstein; F. Schmidt; T. Heer","Esslingen University of Applied Sciences, Germany; Esslingen University of Applied Sciences, Germany; Esslingen University of Applied Sciences, Germany; Esslingen University of Applied Sciences, Germany","2024 Conference on AI, Science, Engineering, and Technology (AIxSET)","3 Dec 2024","2024","","","145","148","The increasing use of generative Artificial Intelligence (AI) in modern software engineering, particularly Large Language Models (LLMs) for code generation, has transformed professional software development by boosting productivity and automating development processes. This adoption, however, has highlighted a significant issue: the introduction of security vulnerabilities into the code. These vulnerabilities result, e.g., from flaws in the training data that propagate into the generated code, creating challenges in tackling them in established ways. Traditional vulnerability handling processes often involve extensive manual review. Applying such traditional processes to AI-generated code is challenging. AI-generated code may include several similar vulnerabilities, possibly in slightly different forms as developers might not build on already implemented code, using functions or libraries, but prompt similar tasks. In this work, we explore the current state of LLM-based approaches for vulnerability handling, focusing on approaches for vulnerability detection, localization, and repair. We provide an overview of recent progress in this area and highlight open challenges that must be addressed to establish a reliable and scalable vulnerability handling process for AI-generated code.","","979-8-3503-9099-5","10.1109/AIxSET62544.2024.00026","Deutsche Forschungsgemeinschaft(grant numbers:528745080 - FIP 68); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771069","Vulnerability Handling;Software Engineering;Large Language Models;Retrieval-Augmented Generation","Productivity;Codes;Reviews;Prevention and mitigation;Training data;Manuals;Software reliability;Security;Software engineering;Software development management","","1","","42","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective","D. Zhang; B. Xia; Y. Liu; X. Xu; T. Hoang; Z. Xing; M. Staples; Q. Lu; L. Zhu","CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","92","97","The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.CCS CONCEPTS• Software and its engineering Software architectures; • Information systems World Wide Web; • Security and privacy Privacy protections; • Social and professional topics Copyrights; • Computing methodologies Machine learning.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556135","Privacy;Copyrights;Generative AI;Data Lifecycle;Software Architecture;Software Engineering for AI","Training;Data privacy;Technological innovation;Generative AI;Software architecture;Copyright protection;Software","","4","","29","","18 Jun 2024","","","IEEE","IEEE Conferences"
"CodEv: An Automated Grading Framework Leveraging Large Language Models for Consistent and Constructive Feedback","E. -Q. Tseng; P. -C. Huang; C. Hsu; P. -Y. Wu; C. -T. Ku; Y. Kang","Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Information Management, National Sun Yat-Sen University, Kaohsiung, Taiwan",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5442","5449","Grading programming assignments is crucial for guiding students to improve their programming skills and coding styles. This study presents an automated grading framework, CodEv, which leverages Large Language Models (LLMs) to provide consistent and constructive feedback. We incorporate Chain of Thought (CoT) prompting techniques to enhance the reasoning capabilities of LLMs and ensure that the grading is aligned with human evaluation. Our framework also integrates LLM ensembles to improve the accuracy and consistency of scores, along with agreement tests to deliver reliable feedback and code review comments. The results demonstrate that the framework can yield grading results comparable to human evaluators, by using smaller LLMs. Evaluation and consistency tests of the LLMs further validate our approach, confirming the reliability of the generated scores and feedback.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825949","Large Language Model;LLM Evaluation;Automated Grading","Codes;Reviews;Large language models;Big Data;Encoding;Data models;Cognition;Reliability;Programming profession;Testing","","1","","23","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"CyGPT: Knowledge Graph-Based Enhancement Techniques for Large Language Models in Cybersecurity","L. Ou; X. Ni; W. Wu; Z. Tian","Cyberspace Institute of Advanced Technology, Guangzhou University, Guangzhou, China; Cyberspace Institute of Advanced Technology, Guangzhou University, Guangzhou, China; Cyberspace Institute of Advanced Technology, Guangzhou University, Guangzhou, China; Cyberspace Institute of Advanced Technology, Guangzhou University, Guangzhou, China",2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC),"4 Feb 2025","2024","","","216","223","Large Language Models (LLMs) excel in numerous Natural Language Processing (NLP) tasks but encounter significant challenges in practical applications, including hallucinations, outdated information, and a lack of domain-specific external knowledge. This study proposes a collaborative, training-free reasoning approach, leveraging close cooperation between Knowledge Graphs (KG) and LLMs for cybersecurity applications. Our approach employs the ‘Joint Reasoning Chain,’ which dynamically integrates information from network security-specific knowledge graphs, serving as an external knowledge base to enhance the domain-specific external knowledge of LLMs. This cooperative method not only improves reliable knowledge-based reasoning but also enhances the traceability of decision-making processes. Furthermore, we introduce a novel GPT-based technique to evaluate answer quality and have performed systematic experiments on a purpose-built test set. The results confirm that our method significantly boosts GPT’s performance in network security knowledge, demonstrating the potential of knowledge graphs to augment LLMs’ reasoning abilities and their applicability in specialized fields.","","979-8-3503-9136-7","10.1109/DSC63484.2024.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858965","Large Language Model;Knowledge Graph;Retrieval-Augmented Generation;Cybersecurity","Knowledge engineering;Systematics;Large language models;Knowledge based systems;Knowledge graphs;Network security;Cognition;Natural language processing;Data models;Computer security","","","","33","IEEE","4 Feb 2025","","","IEEE","IEEE Conferences"
"PyLoomer - The Automated Weaving of Python Text","A. Sahani; R. B. Trivedi; T. Singh; S. R. Goyal","Dept. of Mechnical Engineering, Manipal University Jaipur, Jaipur, Rajasthan, India; Dept. of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, Rajasthan, India; Dept. of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, Rajasthan, India; Dept. of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, Rajasthan, India","2024 2nd International Conference on Device Intelligence, Computing and Communication Technologies (DICCT)","22 May 2024","2024","","","276","281","The introduction of attention mechanisms has evolved NLP methodologies, giving rise to Large Language Models (LLMs) that excel in various sequence-to-sequence tasks like translation, question-answering, summarization, intent, and entity classification. This research focuses on text to code translation, converting human language into executable code. Code generating bots enhances the quality and efficiency of developers and facilitates to tackle the repetitive or simple tasks. A novel and clever fusion of Python Code and the creative ability of Large Language Models to generate text is proposed in this paper named PyLoomer. PyLoomer is an innovative text-to-python code chatbot that bridges the gap between narrative expression and technical execution which uses attention mechanism. PyLoomer seeks to improve the performance of existing models for python code generation by using in-context learning and instruction fine-tuning. Rigorous evaluation metrices, including Rougel, Rouge2, RougeL, RougeLsum and model loss, demonstrated significant improvements in results. Results indicate that CodGen, among the three models, achieves the highest Rouge scores, demonstrating superior performance, the Rouge score improvements for Codegen, StarCoder and CodeParrot are 0.0224, 0.0097 and 0.0223 respectively.","","979-8-3503-7284-7","10.1109/DICCT61038.2024.10532945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532945","Large Language Models;In-context learning;Attention mechanism;Instruction Finetuning;Python;PyLoomer","Performance evaluation;Codes;Computational modeling;Chatbots;Weaving;Communications technology;Task analysis","","","","21","IEEE","22 May 2024","","","IEEE","IEEE Conferences"
"Vulnerabilities Analysis and Defense Based on MAS Method in Fast Dynamic Wireless Networks","I. Burlachenko; I. Zhuravska; Y. Davydenko; V. Savinov","Petro Mohyla Black Sea, National University 10, Mykolaiv, Ukraine; Petro Mohyla Black Sea, National University 10, Mykolaiv, Ukraine; Petro Mohyla Black Sea, National University 10, Mykolaiv, Ukraine; Petro Mohyla Black Sea, National University 10, Mykolaiv, Ukraine",2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),"8 Nov 2018","2018","","","98","102","Corporate and large-scale organizations utilize computer network technologies and dismiss the importance of networks security. The main wireless networks vulnerabilities exploitered by attackers, and methods of protection from attacks are investigated. The methodology of security analysis in fast dynamic wireless networks (FDWNs) is proposed. Model of a multi-agent system (MAS) providing the ability to create data protection strategy based on agents with an effective detailed examination of networks vulnerabilities was provided.","","978-1-5386-7587-8","10.1109/IDAACS-SWS.2018.8525692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8525692","fast dynamic wireless network;network security;ARP spoofing;MitM;SSL strip;multi-agent system;SOHO router;DD-WRT;Intercepter-NG","Wireless networks;Communication networks;Data protection;Strips;Cryptography;Authorization","","7","","19","IEEE","8 Nov 2018","","","IEEE","IEEE Conferences"
"Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation","M. Macedo; Y. Tian; F. R. Cogo; B. Adams","Queen's University, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada; Centre for Software Excellence, Huawei Canada, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada",2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","57","68","Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.","","979-8-4007-0536-6","10.1145/3650105.3652301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599575","code translation;output format;large language model;LLM;soft-ware engineering;benchmarking;evaluation;empirical study;case study","Codes;Accuracy;Terminology;Computational modeling;Large language models;Source coding;Benchmark testing","","3","","49","","30 Jul 2024","","","IEEE","IEEE Conferences"
"NLSQL: Generating and Executing SQL Queries via Natural Language Using Large Language Models","A. Attawar; S. Vora; P. Narechania; V. Sawant; H. Vora","Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Director of Data Science and Operations, CareMetx LLC, United States",2023 International Conference on Advanced Computing Technologies and Applications (ICACTA),"23 Jan 2024","2023","","","1","6","The goal of data management has long been to make data more approachable for non-technical users. Natural language interfaces (NLIs), which let users interact with data by asking questions in natural language, have become more popular in recent years. Natural language is often vague, and user queries often don’t give enough information. This makes it hard to use NLIs for database querying. In order to solve this problem, this study suggests using large language models (LLMs) to turn free-form natural language queries into SQL statements that can then be run on databases. The proposed system, NLSQL, makes use of LLMs like GPT-3 and shows how efficiently prompt engineering can be done in order to extract from LLMs the desired code for SQL queries. NLSQL shows that using pre-trained LLMs along with the suggested priming prompts is an accurate and reliable way to create and run SQL queries in natural language, even if the queries aren’t very well written or are missing important information. Unlike traditional methods, which involve making grammar rules by hand, this method improves query inference and makes the development of NLIs faster and cheaper. The study shows that the suggested method is safe, protects privacy, and can be used with many different databases. If NLSQL is used to make databases easier for people who aren’t tech-savvy to use, they may not need as much training in SQL and related technologies, which would save a company both time and money.","","979-8-3503-4834-7","10.1109/ICACTA58201.2023.10392861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392861","natural language processing;NLSQL;natural language interfaces;language models","Training;Structured Query Language;Codes;Natural languages;Relational databases;Solids;Data models","","2","","24","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"Efficient Driving Behavior Narration and Reasoning on Edge Device Using Large Language Models","Y. Huang; Y. Cheng; K. Wang","Department of Computer Science, Brunel University of London, UK; School of Computer Science, University of Birmingham, UK; Department of Computer Science, Brunel University of London, UK",IEEE Transactions on Vehicular Technology,"","2025","PP","99","1","5","Large language models (LLMs) with robust reasoning capabilities have significantly advanced the development of autonomous driving technologies, particularly in the narration and reasoning of driving behaviors, which hold substantial importance for accident analysis and traffic management. However, traditional deployment of these models relies on cloud servers, resulting in high latency and training costs, making it challenging to meet the stringent real-time requirements of autonomous driving scenarios. Recent studies suggest that edge computing, by deploying models closer to the data source, offers a promising solution to these issues. While existing general-purpose LLMs excel in video understanding and task reasoning, their generalization capabilities in rapidly changing traffic scenarios remain questionable. This paper provides a valuable reference for deploying LLMs at the edge in autonomous driving contexts. By leveraging real-world 5G networks for rapid deployment, we validate the performance and response speeds of various models in autonomous driving scenarios. Furthermore, we introduce an innovative prompt engineering strategy that enhances model performance by 25% without changing model parameters through minimal prompt tuning. Experimental results demonstrate that LLMs deployed on edge devices achieve satisfactory response times. Tests on the OpenDV-YouTube dataset further confirm that our prompt strategy significantly improves the performance of driving behavior narration and reasoning.","1939-9359","","10.1109/TVT.2025.3591733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091354","Autonomous driving;Large language model;Edge computing","Cognition;Driver behavior;Autonomous vehicles;5G mobile communication;Performance evaluation;Portable computers;Knowledge based systems;Time factors;Servers;Pedestrians","","1","","","IEEE","23 Jul 2025","","","IEEE","IEEE Early Access Articles"
"LLM-DSK: A Domain-Specific Semantic Knowledge-Guided Ocean Environment Prediction Method Based on Large Language Models","N. Song; C. Lv; J. Nie; M. Ye; E. Zhao; J. Ma; X. Liu; Z. Wei","College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China; College of Information Science and Engineering, Ocean University of China, Qingdao, China",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"11 Aug 2025","2025","18","","19453","19469","Data-driven methods learn patterns of oceanic variable changes directly from data without relying on explicit modeling of complex physical processes based on specific assumptions. This approach addresses the limitations of traditional numerical methods, which are constrained by physical assumptions, parameterized processes, and dependencies on initial and boundary conditions. However, methods based solely on probabilistic statistics and ignoring the intrinsic characteristics of ocean systems struggle to capture the complex spatiotemporal dynamics of chaotic ocean systems. With the emergence of large language models (LLMs) in time-series analysis, researchers have discovered that pretrained LLMs can leverage rich domain-specific knowledge through prompt engineering to analyze complex temporal changes. Building on this insight, we propose LLM-DSK, a domain-specific semantic knowledge-guided ocean environment prediction model based on pretrained LLMs. LLM-DSK comprises three core modules: 1) a spatiotemporal feature extraction module that utilizes geographic data (e.g., latitude, longitude, wind fields, and land–sea boundaries) to extract key domain-relevant spatiotemporal features; 2) a semantic encoding module that employs an attention mechanism to align these features with the vocabulary of LLMs, enabling cross-modal alignment between oceanic and natural language domains to enrich semantic representations; and 3) an LLM-based prediction module driven by domain-specific prompts that integrate geographic information and statistical indicators. We validated LLM-DSK using remote sensing data (sea surface temperature) and reanalysis data (significant wave height), and the results demonstrate that LLM-DSK achieves superior predictive performance compared to state-of-the-art (SOTA) models.","2151-1535","","10.1109/JSTARS.2025.3590651","Fundamental Research Funds for the Central Universities(grant numbers:202042008); National Natural Science Foundation of China(grant numbers:U23A20320); Natural Science Foundation of Shandong Province(grant numbers:ZR2024QF040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11083757","Laplace operator;large language model (LLM);ocean environment prediction;spatiotemporal variation","Oceans;Numerical models;Feature extraction;Spatiotemporal phenomena;Time series analysis;Mathematical models;Predictive models;Forecasting;Semantics;Large language models","","","","73","CCBY","18 Jul 2025","","","IEEE","IEEE Journals"
"CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation","M. DeLorenzo; V. Gohil; J. Rajendran","Electrical and Computer Engineering, Texas A&M University, College Station, USA; Electrical and Computer Engineering, Texas A&M University, College Station, USA; Electrical and Computer Engineering, Texas A&M University, College Station, USA",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","5","Large Language Models (LLMs) have proved effective and efficient in generating code, leading to their utilization within the hardware design process. Prior works evaluating LLMs’ abilities for register transfer level code generation solely focus on functional correctness. However, the creativity associated with these LLMs, or the ability to generate novel and unique solutions, is a metric not as well understood, in part due to the challenge of quantifying this quality.To address this research gap, we present CreativEval, a framework for evaluating the creativity of LLMs within the context of generating hardware designs. We quantify four creative sub-components, fluency, flexibility, originality, and elaboration, through various prompting and post-processing techniques. We then evaluate multiple popular LLMs (including GPT models, CodeLlama, and VeriGen) upon this creativity metric, with results indicating GPT-3.5 as the most creative model in generating hardware designs.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691798","Hardware Design;LLM;Creativity","Measurement;Codes;Large language models;Conferences;Hardware;Registers;Creativity","","5","","42","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"How Secure is Code Generated by ChatGPT?","R. Khoury; A. R. Avila; J. Brunelle; B. M. Camara","Université du Québec en Outaouais, Gatineau, Canada; Institut National de Recherche Scientifique, Gatineau, Canada; Université du Québec en Outaouais, Gatineau, Canada; Université du Québec en Outaouais, Gatineau, Canada","2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","29 Jan 2024","2023","","","2445","2451","In recent years, large language models have been responsible for great advances in the field of artificial intelligence (AI). ChatGPT in particular, an AI chatbot developed and recently released by OpenAI, has taken the field to the next level. The conversational model is able not only to process human-like text, but also to translate natural language into code. However, the safety of programs generated by ChatGPT should not be overlooked. In this paper, we perform an experiment to address this issue. Specifically, we ask ChatGPT to generate a number of computer programs in order to evaluate the security of the resulting source code. We further investigate whether ChatGPT can be prodded to improve code security by appropriate prompts, and discuss the ethical aspects of using AI to generate code. Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.","2577-1655","979-8-3503-3702-0","10.1109/SMC53992.2023.10394237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394237","Large language models;ChatGPT;code security;automatic code generation","Codes;Source coding;Chatbots;Safety;Artificial intelligence;Standards;Programming profession","","58","","22","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"TrustToken: A Framework for Evaluating Tokenizer Security and Robustness in NLP Pipelines","T. Pope; A. Patooghy","Department of Computer Systems Technology, North Carolina A&T State University, Greensboro, NC; Department of Computer Systems Technology, North Carolina A&T State University, Greensboro, NC",2025 IEEE Conference on Artificial Intelligence (CAI),"7 Jul 2025","2025","","","918","923","The growing reliance on tokenizers in NLP systems calls for robust security measures. TrustToken, a framework for evaluating tokenizer trustworthiness across eight key metrics, including SQL Injection Risk Score (SIRS) and XSS Vulnerability Score (XVS), is introduced in this study as a proposed measure. A custom dataset designed to expose vulnerabilities and test resilience against malformed inputs, perturbations, and boundary conditions assesses GPT-2, BERT, T5, and RoBERTa security posture. While T5 achieved the highest Trustworthiness Score (TWS) of 0.77, all other models failed SQL Injection and XSS tests. Despite firm handling of special characters and privacy safeguards, weaknesses in processing malformed inputs and boundary conditions were evident. Namely with GPT-2, BERT, and RoBERTa. These findings underscore the security risks of deploying tokenizers in real-world applications and emphasize the need for input sanitization and adversarial training. The TrustToken framework offers a systematic approach to evaluating and strengthening tokenizers, ensuring safer NLP deployments.","","979-8-3315-2400-5","10.1109/CAI64502.2025.00162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050651","Tokenization;Trustworthiness Evaluation;Natural Language Processing (NLP);Large Language Models (LLMs);Secure NLP Pipeline;Vulnerabilities;Computational Linguistics","Training;Systematics;Pipelines;SQL injection;Boundary conditions;Natural language processing;Tokenization;Robustness;Security;Resilience","","","","0","IEEE","7 Jul 2025","","","IEEE","IEEE Conferences"
"Recognizing and Predicting Business Communication Outcomes Using Local LLMs","W. Wang; C. Li; L. Hu; B. Pang; B. Balducci; D. Marinova; M. Gordon; Y. Shang","EECS, University of Missouri, Columbia, MO, USA; EECS, University of Missouri, Columbia, MO, USA; EECS, University of Missouri, Columbia, MO, USA; EECS, University of Missouri, Columbia, MO, USA; Carson College of Business Washington State University, Pullman, WA, USA; Robert Trulaske Sr. College of Business, MO, USA; Department of English, University of Missouri, Columbia, MO, USA; EECS, University of Missouri, Columbia, MO, USA",2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI),"8 Oct 2024","2024","","","158","163","In this paper, we use machine learning methods based on three popular open-source large language models (LLMs) that can run efficiently on local computers to recognize and predict business communication outcomes. The methods include zero-shot Alpaca-Lora-7B, BigBird with fine-tuning, AlpacaLora-7B with prompting and LoRA-based fine-tuning, and Llama2-70B-Chat with one-stage and two-stage prompting. Our experimental results on a real-world dataset showed promising results of LLMs for both communication outcome recognition and prediction tasks. On recognizing communication outcomes, Alpaca-Lora-7B with prompt engineering and LoRa-based fine-tuning performed the best achieving 94.66% accuracy while BigBird with fine-tuning is closely behind with 94.27% accuracy, which, in turn, outperformed prompt engineering on LLMs. On predicting communication outcomes based on partial conversations, BigBird fine-tuned using the beginning 70% of each conversations achieved more than 85% prediction accuracy based on the first 70% of each conversation.","2835-5776","979-8-3503-5118-7","10.1109/IRI62200.2024.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703751","machine learning;local LLMs;conversation outcome recognition;conversation outcome prediction;business calls","Computers;Business communication;Accuracy;Large language models;Oral communication;Machine learning;Data science;Rough surfaces;Prompt engineering;Business","","","","20","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Privacy and Security Concerns in Generative AI: A Comprehensive Survey","A. Golda; K. Mekonen; A. Pandey; A. Singh; V. Hassija; V. Chamola; B. Sikdar","School of Computer Science Engineering, Kalinga Institute of Industrial Technology, Bhubaneshwar, India; School of Computer Science Engineering, Kalinga Institute of Industrial Technology, Bhubaneshwar, India; School of Computer Science Engineering and Technology, Bennett University, Greater Noida, India; School of Computer Science Engineering, Kalinga Institute of Industrial Technology, Bhubaneshwar, India; School of Computer Science Engineering, Kalinga Institute of Industrial Technology, Bhubaneshwar, India; Department of Electrical and Electronics, Birla Institute of Technology and Science (BITS) Pilani, Pilani, Rajasthan, India; Department of Electrical and Computer Engineering, National University of Singapore, Cluny Road, Singapore",IEEE Access,"5 Apr 2024","2024","12","","48126","48144","Generative Artificial Intelligence (GAI) has sparked a transformative wave across various domains, including machine learning, healthcare, business, and entertainment, owing to its remarkable ability to generate lifelike data. This comprehensive survey offers a meticulous examination of the privacy and security challenges inherent to GAI. It provides five pivotal perspectives essential for a comprehensive understanding of these intricacies. The paper encompasses discussions on GAI architectures, diverse generative model types, practical applications, and recent advancements within the field. In addition, it highlights current security strategies and proposes sustainable solutions, emphasizing user, developer, institutional, and policymaker involvement.","2169-3536","","10.1109/ACCESS.2024.3381611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478883","Generative artificial intelligence;privacy concerns;security concerns;deep learning;adversarial attacks;synthetic data;Deepfake;ethical implications;cybersecurity;machine learning;privacy protection;ethical responsibility;misinformation;social engineering;regulatory compliance;artificial intelligence;privacy preservation;data security;threat analysis","Security;Generative AI;Privacy;Surveys;Data privacy;Data models;Computational modeling;Artificial intelligence;Generative adversarial networks;Deep learning;Ethics;Computer security;Threat assessment;Fake news;Homomorphic encryption","","83","","129","CCBYNCND","25 Mar 2024","","","IEEE","IEEE Journals"
"Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models","K. M. S. Islam; A. S. Nipu; P. Madiraju; P. Deshpande","Computer Science, Marquette University; Computer Science & Software Engineering, University of Wisconsin-Platteville; Computer Science, Marquette University; Electrical & Computer Engineering, Marquette University",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","4912","4921","The Chief Complaint (CC) is a crucial component of a patient’s medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4. We evaluate the models’ performance based on the perplexity score, modified BERTScore, and cosine similarity score. The results show that BioGPT-Large exhibits superior performance compared to the other models. It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170. Further, we evaluate and assess the proposed models’ performance and the outcome of GPT-4.0. Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386778","Chief Complaint;Electronic Health Record;Text Generation;Large Language Model;BioGPT;Prompt Engineering;LSTM","Biological system modeling;Medical services;Machine learning;Documentation;Big Data;Transformers;Data models","","3","","57","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"AI-Enabled Regulatory Change Analysis of Legal Requirements","S. Abualhaija; M. Ceci; N. Sannier; D. Bianculli; L. C. Briand; D. Zetzsche; M. Bodellini","SnT - University of Luxembourg, Luxembourg; SnT - University of Luxembourg, Luxembourg; SnT - University of Luxembourg, Luxembourg; SnT - University of Luxembourg, Luxembourg; Lero SFI centre for Software Research, University of Limerick, Ireland; FDEF, University of Luxembourg, Luxembourg; FDEF, University of Luxembourg, Luxembourg",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","5","17","Statutory law is subject to change as legislation develops over time – new regulation can be introduced, while existing regulation can be amended, or repealed. From a requirements engineering (RE) perspective, such change must be dealt with to ensure the compliance of software systems at all times. Understanding the implications of regulatory change on compliance of software requirements requires navigating hundreds of legal provisions. Analyzing instances of regulatory change entirely manually is not only time-consuming, but also risky, since missing a change may result in non-compliant software which can in turn lead to hefty fines. In this paper, we propose MURCIA, an automated approach that leverages recent language models to assist human analysts in analyzing regulatory changes. To build MURCIA, we define a taxonomy that characterizes the regulatory changes at the textual level as well as the changes in the text's meaning and legal interpretation. We evaluate MURCIA on four regulations from the financial domain. Over our evaluation set, MURCIA can identify textual changes with F1 score of 90.5%, and it can provide, according to our taxonomy, the text meaning and legal interpretation with an F1 score of 90.8% and 83.7%, respectively.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00012","Science Foundation Ireland(grant numbers:13/RC/2094-2); Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628495","Regulatory Change;Prompt Engineering;Natural Language Processing (NLP);Large Language Models (LLMs);ChatGPT;Regulatory Compliance","Automation;Law;Navigation;Taxonomy;Pipelines;Legislation;Software systems","","3","","72","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Evaluating Multi-Agent System Security using Goal/Question/Metric Approach and Fuzzy Logic","S. A. Darweesh; G. A. Ebrahim; H. M. S. Bedour","Computer and Systems Engineering Department, Ain Shams University, Cairo, Egypt; Computer and Systems Engineering Department, Ain Shams University, Cairo, Egypt; Computer and Systems Engineering Department, Ain Shams University, Cairo, Egypt","2019 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","6 Feb 2020","2019","","","1","6","Multi-Agent Systems (MASes) are commonly used in many computing services. Hence, security aspects of these services should be guaranteed, especially, if the services are related to critical tasks. The evaluation methods of MASes security should report the security concerns. This paper introduces a general approach for evaluating the security of an agent in MAS environments. The proposed framework is based on Goal/Question/Metric (GQM) approach combined with fuzzy logic. It generates the GQM structure of the agent security. Then, fuzzy logic controllers are utilized to measure the security of the agent. The performance of the proposed method is measured in percentage of satisfying a set of security criteria. Experimental studies are conducted to evaluate the strengths and the weaknesses of MAS security aspects such as confidentiality, authentication, repudiation, and access control. Finally, the proposed framework is applied to a case study and the results are assessed.","2325-0445","978-1-7281-2794-1","10.1109/PACRIM47961.2019.8985125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985125","Fuzzy Logic;Goal/Question/Metric Model;Multi-Agent System Security","","","1","","23","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems","S. Kirchner; A. C. Knoll","Technical University of Munich, Garching, Bayern, Germany; Technical University of Munich, Garching, Bayern, Germany",2025 IEEE Intelligent Vehicles Symposium (IV),"6 Aug 2025","2025","","","813","820","Developing safety-critical automotive software presents significant challenges due to increasing system complexity and strict regulatory demands. This paper proposes a novel framework integrating Generative Artificial Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The framework uses Large Language Models (LLMs) to automate code generation in languages such as C++, incorporating safety-focused practices such as static verification, test-driven development and iterative refinement. A feedback-driven pipeline ensures the integration of test, simulation and verification for compliance with safety standards. The framework is validated through the development of an Adaptive Cruise Control (ACC) system. Comparative benchmarking of LLMs ensures optimal model selection for accuracy and reliability. Results demon-strate that the framework enables automatic code generation while ensuring compliance with safety-critical requirements, systematically integrating GenAI into automotive software engineering. This work advances the use of AI in safety-critical domains, bridging the gap between state-of-the-art generative models and real-world safety requirements.","2642-7214","979-8-3315-3803-3","10.1109/IV64158.2025.11097503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11097503","","Codes;Large language models;Pipelines;Software;Safety;Software reliability;Standards;Automotive engineering;Software development management;Software engineering","","1","","36","IEEE","6 Aug 2025","","","IEEE","IEEE Conferences"
"Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models","R. Iijima; M. Tanaka; S. Shiota; H. Kiya","Tokyo Metropolitan University, Tokyo, Japan; Tokyo Metropolitan University, Tokyo, Japan; Tokyo Metropolitan University, Tokyo, Japan; Tokyo Metropolitan University, Tokyo, Japan",2023 IEEE 12th Global Conference on Consumer Electronics (GCCE),"16 Nov 2023","2023","","","948","951","Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In addition, AEs have adversarial transferability, which means AEs generated for a source model can fool another black-box model (target model) with a non-trivial probability. In previous studies, it was confirmed that the vision transformer (ViT) is more robust against the property of adversarial transferability than convolutional neural network (CNN) models such as ConvMixer, and moreover encrypted ViT is more robust than ViT without any encryption. In this article, we propose a random ensemble of encrypted ViT models to achieve much more robust models. In experiments, the proposed scheme is verified to be more robust against not only black-box attacks but also white-box ones than convention methods.","2693-0854","979-8-3503-4018-1","10.1109/GCCE59613.2023.10315690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10315690","adversarial example;transferablity;ensemble model","Closed box;Transformers;Robustness;Encryption;Cryptography;Convolutional neural networks;Consumer electronics","","2","","20","IEEE","16 Nov 2023","","","IEEE","IEEE Conferences"
"Benchmarking Open-Source Large Language Models for Log Level Suggestion","Y. W. Heng; Z. Ma; Z. Li; D. J. Kim; T. -H. Chen","Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada; Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada; York University, Toronto, Canada; DePaul University, Chicago, Illinois, USA; Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","314","325","Large Language Models (LLMs) have become a focal point of research across various domains, including software engineering, where their capabilities are increasingly leveraged. Recent studies have explored the integration of LLMs into software development tools and frameworks, revealing their potential to enhance performance in text and code-related tasks. Log level is a key part of a logging statement that allows software developers control the information recorded during system runtime. Given that log messages often mix natural language with code-like variables, LLMs' language translation abilities could be applied to determine the suitable verbosity level for logging statements. In this paper, we undertake a detailed empirical analysis to investigate the impact of characteristics and learning paradigms on the performance of 12 open-source LLMs in log level suggestion. We opted for open-source models because they enable us to utilize in-house code while effectively protecting sensitive information and maintaining data security. We examine several prompting strategies, including Zero-shot, Few-shot, and fine-tuning techniques, across different LLMs to identify the most effective combinations for accurate log level suggestions. Our research is supported by experiments conducted on 9 large-scale Java systems. The results indicate that although smaller LLMs can perform effectively with appropriate instruction and suitable techniques, there is still considerable potential for improvement in their ability to suggest log levels.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988921","","Software testing;Java;Translation;Runtime;Source coding;Large language models;Refining;Focusing;Software;Software development management","","","","71","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models","K. Zhang; S. Wang; J. Han; X. Zhu; X. Li; S. Wang; S. Wen",The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; Central University of Finance and Economics; The University of Adelaide; Swinburne University of Technology; Central University of Finance and Economics; Swinburne University of Technology,2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","3110","3122","Deep learning (DL) libraries are widely used to form the basis of various AI applications in computer vision, natural language processing, and software engineering domains. Despite their popularity, DL libraries are known to have vulnerabilities, such as buffer overflows, use-after-free, and integer overflows, that can be exploited to compromise the security or effectiveness of the underlying libraries. While traditional fuzzing techniques have been used to find bugs in software, they are not well-suited for DL libraries. In general, the complexity of DL libraries and the diversity of their APIs make it challenging to test them thoroughly. To date, mainstream DL libraries like TensorFlow and PyTorch have featured over 1,000 APIs, and the number of APIs is still growing. Fuzzing all these APIs is a daunting task, especially when considering the complexity of the input data and the diversity of the API usage patterns. Recent advances in large language models (LLMs) have illustrated the high potential of LLMs in understanding and synthesizing human-like code. Despite their high potential, we find that emerging LLM-based fuzzers are less optimal for DL library API fuzzing, given their lack of in-depth knowledge on API input edge cases and inefficiency in generating test inputs. In this paper, we propose DFuzz, a LLM-driven DL library fuzzing approach. We have two key insights: (1) With high reasoning ability, LLMs can replace human experts to reason edge cases (likely error-triggering inputs) from checks in an API's code, and transfer the extracted knowledge to test other (new or rarely-tested) APIs. (2) With high generation ability, LLMs can synthesize initial test programs with high accuracy that automates API testing. DFuzz provides LLMs with a novel “white-box view” of DL library APIs, and therefore, can leverage LLMs' reasoning and generation abilities to achieve comprehensive fuzzing. Our experimental results on popular DL libraries demonstrate that DFuzz is able to cover more APIs than SOTA (LLM-based) fuzzers on TensorFlow and PyTorch, respectively. Moreover, DFuzz successfully detected 37 bugs, with 8 already fixed and 19 replicated by the developer but still under investigation.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029835","","Codes;Large language models;Computer bugs;Fuzzing;Libraries;Cognition;Complexity theory;Software engineering;Glass box;Testing","","","","74","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Exploring Large Language Models for Requirements on String Values","A. A. Babikian; B. Chen; G. Mussbacher","Dep’t of Computer Science, University of Toronto, Toronto, Canada; Electrical and Computer Engineering, McGill University, Montreal, Canada; Electrical and Computer Engineering, McGill University, Montreal, Canada","2025 IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering (MO2RE)","11 Jun 2025","2025","","","17","23","Behavior-driven development (BDD) enables collaboration among different stakeholders by employing a natural-language representation of system requirements and of test scenarios. These scenarios often involve constraints over string values, e.g. for the validity of email addresses, which are challenging to test comprehensively. Traditional methods like SMT solvers (e.g. Z3, Ostrich) handle constraints efficiently but produce unrealistic strings and require formal specifications that are often unavailable and expensive to compute. This paper explores the potential of large language models (LLMs) in generating realistic, constraint-satisfying strings for BDD. We propose an evaluation framework to assess LLMs’ ability to (1) generate consistent string values and (2) detect constraint inconsistencies. In our experiments, three LLMs are compared to state-of-the-art solvers using constraints from a software engineering course project. Results show that while solvers dominate in precision and recall, LLMs derive realistic strings more suitable for a requirements engineering context. With these trade-offs, we believe that, when formal constraints are available, a combined LLM-solver approach could offer a more effective solution.","","979-8-3315-1464-8","10.1109/MO2RE66661.2025.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028238","large language model;specification-based testing;constraint solving","Filtering;Large language models;Natural languages;Hybrid power systems;Electronic mail;Requirements engineering;Stakeholders;Formal specifications;Testing;Software engineering","","","","29","IEEE","11 Jun 2025","","","IEEE","IEEE Conferences"
"Ethical Considerations in the Development and Deployment of Large Language Models","M. A. Khaldy; Y. Gheraibia","Department of Business Intelligence and Data Analytics, University of Petra, Amman, Jordan; School of Computer Science and Informatics, De Montfort University, Leicester, UK",2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA),"2 Jun 2025","2025","","","1","6","Large Language Models (LLMs) have emerged as a powerful tool with the potential to revolutionize various industries and aspects of human life. However, their rapid development and deployment raise significant ethical concerns. This paper delves into the key ethical considerations that must be addressed to ensure the responsible and beneficial use of LLMs. We explore issues such as bias and fairness, privacy and security, transparency and accountability, and the potential for misuse. Addressing bias requires careful data curation, algorithmic fairness techniques, and regular audits. Protecting privacy necessitates strong data privacy and security measures, as well as robust security protocols. To enhance transparency, developing techniques to interpret and explain LLM outputs is crucial. Mitigating the potential for misuse involves developing tools to detect and filter harmful content, implementing responsible AI practices, and fostering a proactive approach to addressing societal implications. By examining these challenges, we aim to foster a thoughtful and ethical approach to LLM development and deployment. A collaborative effort between researchers, developers, policymakers, and the public is essential to ensure that LLMs are used responsibly and ethically.","","979-8-3315-2365-7","10.1109/ICCIAA65327.2025.11013722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11013722","LLM;Misinformation;Societal Impacts;Privacy Concerns;Ethics Integration","Hands;Ethics;Technological innovation;Privacy;Data privacy;Reviews;Large language models;Psychology;Security;Stakeholders","","","","17","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"Mission Reliability and Optimization in Multi-Agent Systems Using Hyperledger Fabric Platform","C. Lu; J. Zhu; L. Ouyang","College of Automation Engineering, NanJing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, NanJing University of Aeronautics and Astronautics, Nanjing, China; College of Economics and Management, NanJing University of Aeronautics and Astronautics, Nanjing, China","2024 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)","15 Apr 2025","2024","","","1","6","This paper investigates the mission reliability and agent number optimization in a multi-agent system (MAS) under the presence of Byzantine agents transmitting malicious information. We propose a novel hierarchical model of MAS based on the Hyperledger Fabric blockchain. This model incorporates a P2P communication network and smart contracts, enabling the calculation of mission reliability, optimal decisions-making, and the detection and isolation of Byzantine agents. The blockchain approach presented in this study provides an interface for reliability calculation, facilitates optimal number of agent selection, and accurately identifies Byzantine agents for secure consensus control. Experimental simulations are performed using the ARGoS robot swarm simulator to demonstrate the security and effectiveness of the proposed method.","2768-1890","979-8-3503-7916-7","10.1109/SOLI63266.2024.10955996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10955996","MAS;mission reliability;Hyperledger Fabric;Byzantine attacks;smart contract;DBSCAN","Distributed ledger;Smart contracts;Fabrics;Blockchains;Reliability;Security;Optimization;Robots;Multi-agent systems;Logistics","","","","18","IEEE","15 Apr 2025","","","IEEE","IEEE Conferences"
"Corn Cultivation with Precision: Language Agents for Real-Time Decision Making","A. Chao","Dept. of CSIE, National Penghu University of Science and Technology, Makong City, Penghu County, Taiwan",2025 1st International Conference on Consumer Technology (ICCT-Pacific),"30 May 2025","2025","","","1","4","The amalgamation of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) oriented prompt engineering within the context of Large Language Models (LLMs) represents a considerable progression in agricultural decision support frameworks. This investigation introduces a prototype system aimed at augmenting the decision-making capabilities of farmers by providing accurate recommendations pertinent to maize cultivation. By utilizing government-sanctioned fertilizer data in conjunction with real-time meteorological information, the system showcases enhanced response accuracy and relevance through the deployment of structured prompts and the integration of external knowledge. Two comparative scenarios elucidate the superiority of the proposed system in contrast to generic LLM outputs, thereby demonstrating its efficacy in facilitating field operations. Prospective research endeavors will encompass the expansion of crop coverage, the incorporation of IoT -enabled real-time updates, and the enhancement of the user interface to ensure multilingual support and improved accessibility. This scholarly work exemplifies the transformative potential of LLMs in advancing sustainable agricultural practices by synchronizing artificial intelligence capabilities with the specific requirements of agricultural contexts.","","979-8-3315-0412-0","10.1109/ICCT-Pacific63901.2025.11012907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11012907","Agriculture;Retrieval-Augmented Generation;Large Language Models;Chain-of-Thought;Corn Cultivation;Smart Farming","Smart agriculture;Accuracy;Large language models;Retrieval augmented generation;Decision making;Prototypes;User interfaces;Real-time systems;Multilingual;Prompt engineering","","","","10","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
