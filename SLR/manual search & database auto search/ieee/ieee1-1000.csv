"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering","T. Zhang; Y. Yu; X. Mao; S. Wang; K. Yang; Y. Lu; Z. Zhang; Y. Zhao","College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China; College of Computer Science and Technology, National University of Defense and Technology, Changsha, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","566","577","Code snippet adaptation is a fundamental activity in the software development process. Unlike code generation, code snippet adaptation is not a “free creation”, which requires developers to tailor a given code snippet in order to fit specific requirements and the code context. Recently, large language models (LLMs) have confirmed their effectiveness in the code generation task with promising results. However, their performance on code snippet adaptation, a reuse-oriented and context-dependent code change prediction task, is still unclear. To bridge this gap, we conduct an empirical study to investigate the performance and issues of LLMs on the adaptation task. We first evaluate the adaptation performances of three popular LLMs and compare them to the code generation task. Our result indicates that their adaptation ability is weaker than generation, with a nearly 15% decrease on pass@1 and more context-related errors. By manually inspecting 200 cases, we further investigate the causes of LLMs' sub-optimal performance, which can be classified into three categories, i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication. Based on the above empirical research, we propose an interactive prompting approach to eliciting LLMs' ability on the adaptation task. Specifically, we enhance the prompt by enriching the context and decomposing the task, which alleviates context misapplication and improves requirement understanding. Besides, we enable LLMs' reflection by requiring them to interact with a human or a LLM counselor, compensating for unclear requirement. Our experimental result reveals that our approach greatly improve LLMs' adaptation performance. The best-performing Human-LLM interaction successfully solves 159 out of the 202 identified defects and improves the pass@1 and pass@5 by over 40% compared to the initial instruction-based prompt. Considering human efforts, we suggest multi-agent interaction as a trade-off, which can achieve comparable performance with excellent generalization ability. We deem that our approach could provide methodological assistance for autonomous code snippet reuse and adaptation with LLMs.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00104","National Natural Science Foundation of China(grant numbers:62302515,62172426,62332005); National Key Research and Development Program of China(grant numbers:2023YFB4503802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029912","Code Snippet Adaptation;Large Language Models;Prompt Engineering;Interactive Workflow","Bridges;Codes;Large language models;Context awareness;Reflection;Prompt engineering;Software engineering;Software development management","","","","55","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code Generation","P. Vijayaraghavan; L. Shi; S. Ambrogio; C. Mackin; A. Nitsure; D. Beymer; E. Degan","IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","6","With the unprecedented advancements in Large Language Models (LLMs), their application domains have expanded to include code generation tasks across various programming languages. While significant progress has been made in enhancing LLMs for popular programming languages, there exists a notable gap in comprehensive evaluation frameworks tailored for Hardware Description Languages (HDLs), particularly VHDL. This paper addresses this gap by introducing a comprehensive evaluation framework designed specifically for assessing LLM performance in VHDL code generation task. We construct a dataset for evaluating LLMs on VHDL code generation task. This dataset is constructed by translating a collection of Verilog evaluation problems to VHDL and aggregating publicly available VHDL problems, resulting in a total of 202 problems. To assess the functional correctness of the generated VHDL code, we utilize a curated set of self-verifying testbenches specifically designed for those aggregated VHDL problem set. We conduct an initial evaluation of different LLMs and their variants, including zero-shot code generation, in-context learning (ICL), and Parameter-efficient fine-tuning (PEFT) methods. Our findings underscore the considerable challenges faced by existing LLMs in VHDL code generation, revealing significant scope for improvement. This study emphasizes the necessity of supervised fine-tuning code generation models specifically for VHDL, offering potential benefits to VHDL designers seeking efficient code generation solutions.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691836","LLMs;large language models;VHDL Code generation;VHDL Evaluation;hardware design automation;Hardware Description Languages;HDL;PEFT;ICL","VHDL;Codes;Design automation;Large language models;Conferences;Hardware","","3","","22","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Document Information Extraction in Engineering Reports Through Prompt Engineering with Large Language Models","Y. Yao; Z. Lin; X. Liu; Y. Li","Petro China Southwest Oil and Gasfield company, Sichuan, China; Petro China Southwest Oil and Gasfield company, Sichuan, China; Petro China Southwest Oil and Gasfield company, Sichuan, China; Petro China Southwest Oil and Gasfield company, Sichuan, China","2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","23 Jun 2025","2025","","","1","4","This article delves into the application of Prompt Engineering to enhance the document information extraction capabilities of large language models (LLMs), with a focus on optimizing the performance of advanced models such as chatglm4 and qwen 2.5 through meticulous prompt design. A structured approach encompassing task decomposition, evaluation metric establishment, and prompt instruction design is proposed. Experiments conducted on engineering technical reports reveal that Prompt Engineering markedly improves the accuracy and robustness of information extraction, with the optimized models demonstrating superior performance in pinpointing and extracting crucial information. These findings underscore the potential of Prompt Engineering in propelling AI technology and its applications forward, particularly in key information extraction tasks pertaining to engineering technical reports.","","979-8-3315-2228-5","10.1109/AINIT65432.2025.11035034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035034","Prompt Engineering;document information extraction;Large Language Models (LLMs)","Training;Knowledge engineering;Accuracy;Large language models;Writing;Information retrieval;Robustness;Prompt engineering;Data mining;Context modeling","","","","12","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Automating Patch Set Generation from Code Review Comments Using Large Language Models","T. Rahman; R. Singh; M. Y. Sultan","Gannon University, Erie, PA, USA; Gannon University, Erie, PA, USA; Gannon University, Erie, PA, USA",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","273","274","The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs’ feedback, thereby evaluating their readiness to support developers in responding to code-review comments. Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.CCS CONCEPTS• Software and its engineering → Automatic programming","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556042","Large Language Models;Automated Code Review;Software Engineering;Pull Requests;Code Quality","Codes;Quality assurance;Automatic programming;Accuracy;Reviews;Software;Data models","","","","9","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Automated Code Review Using Large Language Models with Symbolic Reasoning","B. İçöz; G. Biricik","Computer Engineering, Yıldız Technical University, Istanbul, Turkiye; Computer Engineering, Yıldız Technical University, Istanbul, Turkiye",2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS),"12 Aug 2025","2025","","","1","5","Code review is one of the key processes in the software development lifecycle and is essential to maintain code quality. However, manual code review is subjective and time consuming. Given its rule-based nature, code review is well suited for automation. In recent years, significant efforts have been made to automate this process with the help of artificial intelligence. Recent developments in Large Language Models (LLMs) have also emerged as a promising tool in this area, but these models often lack the logical reasoning capabilities needed to fully understand and evaluate code. To overcome this limitation, this study proposes a hybrid approach that integrates symbolic reasoning techniques with LLMs to automate the code review process. We tested our approach using the CodexGlue dataset, comparing several models, including CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining symbolic reasoning and prompting techniques with LLMs. Our results show that this approach improves the accuracy and efficiency of automated code review.","","979-8-3315-1482-2","10.1109/ISAS66241.2025.11101776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11101776","code review;large language models;symbolic reasoning;prompt techniques;automated code review","Codes;Accuracy;Reviews;Large language models;Manuals;Programming;Cognition;Software reliability;Software engineering;Software development management","","","","29","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Advanced Techniques in Prompt Engineering for Large Language Models: A Comprehensive Study","G. Beri; V. Srivastava","Corps of Signal, MCTE, Mhow, India; Corps of Signal, MCTE, Mhow, India",2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Mar 2025","2024","","","1","4","Prompt engineering is emerging as a pivotal approach to enhance the efficiency and versatility of large language models (LLMs) like GPT-4. This paper provides a comprehensive survey of key techniques in prompt engineering, highlighting foundational methods and advanced strategies. One-shot prompting, a basic yet impactful technique, involves supplying the model with a single example to guide its responses. In contrast, Chain-of-Thought (CoT) prompting encourages the model to generate intermediate reasoning steps, significantly improving its problem-solving abilities. Self-Consistency, an advanced strategy, involves generating multiple potential responses and selecting the most coherent one, thereby increasing the reliability of outputs. The transformative potential of these techniques is showcased through various applications in education, content creation, and programming. In education, prompt engineering can personalize learning experiences and provide tailored feedback to students. Content creation benefits from enhanced creativity and coherence in generated text, aiding writers and marketers. In programming, LLMs assist in code generation and debugging, streamlining the development process. This paper not only outlines the methodologies and their practical applications but also proposes future research directions to further refine and expand the capabilities of prompt engineering. By understanding and leveraging these techniques, practitioners can significantly optimize the performance and adaptability of LLMs across diverse domains. This framework aims to foster innovation and efficiency, laying the groundwork for future advancements in AI-driven technologies.","","979-8-3315-1898-1","10.1109/ICTBIG64922.2024.10911672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911672","Prompt engineering;Chain-of-Thought;Large Language Models;Self-consistency;Application;AI-generated content","Surveys;Technological innovation;Large language models;Education;Government;Debugging;Prompt engineering;Reliability;Problem-solving;Programming profession","","","","12","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair","K. Huang; X. Meng; J. Zhang; Y. Liu; W. Wang; S. Li; Y. Zhang","University of Chinese Academy of Sciences, Bejiing, China; Beihang University, Beijing, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; University of Chinese Academy of Sciences, Bejiing, China; Zhongguancun Laboratory, Beijing, China; University of Chinese Academy of Sciences, Bejiing, China",2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","1162","1174","The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\mathrm{C}/\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298532","Automated Program Repair;Large Language Models of Code;Neural Machine Translation;Fine-Tuning","Measurement;Java;Computer languages;Codes;Computer bugs;Maintenance engineering;Benchmark testing","","43","","70","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Large Language Models in Healthcare: Prompt Engineering Competition","P. Kocbek; L. Gosak; G. Stiglic","Faculty of Health Sciences, University of Maribor, Maribor, Slovenia; Faculty of Health Sciences, University of Maribor, Maribor, Slovenia; Faculty of Health Sciences Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia",2025 IEEE 13th International Conference on Healthcare Informatics (ICHI),"22 Jul 2025","2025","","","696","696","This tutorial/competition explores prompt engineering for large language models (LLMs) with a focus on OpenAI's ChatGPT, such as GPT-4o or GPT-4o-mini, showcasing the potential usefulness in healthcare. It is structured in two parts: 1) a practical tutorial focusing on advanced prompt engineering methods and highlighting privacy requirements, such as HIPAA, GDPR, in healthcare and 2) a hands-on part in the form of a prompt engineering competition. Participants will be asked to find the best solution for a specific healthcare related task on healthcare data using free OpenAI ChatGPT. The intended audience are healthcare researchers and professionals, AI scientists, data scientists, and PhD students in the relevant fields as well as others. Attendees will gain insights into prompt engineering strategies and get insights how desirable outcomes for specific tasks can be achieved. This tutorial aims to educate participants with the knowledge on how to improve utilization of LLMs in healthcare and encourage creative thinking in solving real-world problems in healthcare.","2575-2634","979-8-3315-2094-6","10.1109/ICHI64645.2025.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081568","large language models;prompt engineering;healthcare","Privacy;Large language models;Focusing;Medical services;Tutorials;Chatbots;Prompt engineering;Informatics","","","","2","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"LLM-Powered Multi-Agent Systems: A Technical Framework for Collaborative Intelligence Through Optimized Knowledge Retrieval and Communication","V. Gogineni","Columbia Business School, Columbia University, NewYork, United States","2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)","15 Jul 2025","2025","","","452","456","This paper presents a comprehensive technical framework for constructing effective multi-agent systems powered by large language models (LLMs). We examine how multiple LLM-based agents can collaborate to solve complex problems beyond the capabilities of single agents through specialized knowledge integration and optimized communication protocols. Our novel architecture enables efficient collaboration between heterogeneous LLM agents, each with domain-specific capabilities and knowledge bases. Experimental results demonstrate that our multi-agent LLM system achieves 42% higher accuracy on complex knowledge tasks, 37% reduction in hallucinations, and 29% faster convergence on collaborative problem-solving compared to baseline approaches. By implementing optimized knowledge retrieval mechanisms and structured communication patterns, our system reduces token usage by 45% while maintaining semantic fidelity across agent interactions. These findings establish key design principles for building more effective and reliable collaborative LLM-based multi-agent systems for enterprise applications","","979-8-3315-4348-8","10.1109/AIRC64931.2025.11077480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11077480","Large language models;multi-agent systems;retrieval-augmented generation;vector databases;prompt engineering;knowledge retrieval","Knowledge engineering;Accuracy;Databases;Large language models;Semantics;Collaboration;Vectors;Prompt engineering;Robots;Convergence","","","","25","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Large Language Models for Software Engineering: Survey and Open Problems","A. Fan; B. Gokkaya; M. Harman; M. Lyubarskiy; S. Sengupta; S. Yoo; J. M. Zhang","Generative AI Team Meta Platforms Inc., New York, NY, USA; PyTorch Team Meta Platforms Inc., Menlo Park, CA, USA; Instagram Product Foundation Meta Platforms Inc., London, UK; Developer Infrastructure Meta Platforms Inc., London, UK; FAIR Meta Platforms Inc., Menlo Park, CA, USA; School of Computing KAIST, Daejeon, Korea; Department of Informatics, King's College London, London, UK",2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE),"4 Mar 2024","2023","","","31","53","This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.","","979-8-3503-2496-9","10.1109/ICSE-FoSE59343.2023.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449667","Automated Program Repair;Documentation generation;Generative AI;Genetic Improvement;Human-Computer Interaction;Large Language Models;Refactoring;Requirements engineering;Search Based Software Engineering (SBSE);Software Analytics;Software Engineering Education;Software Processes;Software Maintenance and Evolution;Software Testing","Surveys;Maintenance engineering;Reliability engineering;Software;Software reliability;Software engineering;Testing","","192","","236","IEEE","4 Mar 2024","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models to Boost Dafny’s Developers Productivity","Á. Silva; A. Mendes; J. F. Ferreira","Independent Researcher, Porto, Portugal; HASLab / INESC TEC & Faculty of Engineering, University of Porto, Porto, Portugal; INESC-ID & IST, University of Lisbon, Lisbon, Portugal",2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE),"18 Jun 2024","2024","","","138","142","This research idea paper proposes leveraging Large Language Models (LLMs) to enhance the productivity of Dafny developers. Although the use of verification-aware languages, such as Dafny, has increased considerably in the last decade, these are still not widely adopted. Often the cost of using such languages is too high, due to the level of expertise required from the developers and challenges that they often face when trying to prove a program correct. Even though Dafny automates a lot of the verification process, sometimes there are steps that are too complex for Dafny to perform on its own. One such case is that of missing lemmas, i.e. Dafny is unable to prove a result without being given further help in the form of a theorem that can assist it in the proof of the step.In this paper, we describe preliminary work on using LLMs to assist developers by generating suggestions for relevant lemmas that Dafny is unable to discover and use. Moreover, for the lemmas that cannot be proved automatically, we attempt to provide accompanying calculational proofs. We also discuss ideas for future work by describing a research agenda on using LLMs to increase the adoption of verification-aware languages in general, by increasing developers productivity and by reducing the level of expertise required for crafting formal specifications and proving program properties.CCS CONCEPTS• Software and its engineering → Software notations and tools; Software verification and validation.","2575-5099","979-8-4007-0589-2","","Fundação para a Ciência e a Tecnologia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555641","Verification-Aware Languages;Dafny;Large Language Models;Generative AI;Software Productivity;Software Verification;Lemma Inference;Proof Inference;Automated Program Repair","Productivity;Costs;Manuals;Maintenance engineering;Syntactics;Software;Formal specifications","","","","45","CCBY","18 Jun 2024","","","IEEE","IEEE Conferences"
"Comprehensive Fine-Tuning Large Language Models of Code for Automated Program Repair","K. Huang; J. Zhang; X. Bao; X. Wang; Y. Liu","Technical University of Munich, Heilbronn, Germany; Nanyang Technological University, Singapore; Beihang University, Beijing, China; Beihang University, Beijing, China; Nanyang Technological University, Singapore",IEEE Transactions on Software Engineering,"17 Apr 2025","2025","51","4","904","928","Automated program repair (APR) research has entered the era of large language models (LLM), and researchers have conducted several empirical studies to explore the repair capabilities of LLMs for APR. Many studies adopt the zero/few-shot learning paradigm for APR, which directly use LLMs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in the conference version of this work, we conduct an initial study on the program repair capability of million-level LLMs in the fine-tuning paradigm. We select 5 popular million-level LLMs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Our experimental results show that fine-tuning these LLMs can significantly outperform previous state-of-the-art APR tools. However, the repair capabilities of billion-level LLMs for APR remain largely unexplored. Moreover, their substantial model sizes significantly increase the computational cost of fine-tuning. While parameter-efficient fine-tuning (PEFT) techniques offer a promising solution, their effectiveness in repair tasks and the selection of appropriate PEFT strategies remain unclear. Similarly, many novel APR strategies have been developed for non-pre-trained models, yet their applicability and effectiveness on LLMs are still unexamined. To address these gaps, we extend our prior study through three key dimensions: 1) LLM4APR, which evaluates the repair capabilities of five billion-level LLM families (InCoder, CodeGeeX, CodeGen, StarCoder, and CodeLlama) under the fine-tuning paradigm; 2) PEFT4LLM, which compares full-parameter fine-tuning (FPFT) with three PEFT techniques (LoRA, AdaLoRA, and IA3) to determine optimal strategies that balance repair cost and performance of LLMs; and 3) APR4LLM, which investigates the potential of a basic neural machine translation (NMT) approach alongside three advanced repair strategies (TENURE, ITER, and KATANA) to enhance the repair capabilities of LLMs. Overall, our extensive results suggest that larger scale models typically have better repair capabilities. The LoRA technique is still the best choice for LLM4APR studies. Different repair strategies result in different repair capabilities for the foundation models, but some of the strategies that performed well on the non-pre-trained model did not show an advantage on LLMs. Besides, we released all LLMs fine-tuned with repair tasks to facilitate LLM4APR research, and we encourage researchers to develop more powerful APR tools on the basis of these repair LLMs.","1939-3520","","10.1109/TSE.2025.3532759","National Research Foundation, Singapore, and DSO National Laboratories through the AI Singapore Programme (AISG)(grant numbers:AISG2-GC-2023-008); National Research Foundation Singapore and the Cyber Security Agency through the National Cybersecurity R&D Programme(grant numbers:NCRP25-P04-TAICeN); National Research Foundation, Prime Minister’s Office, Singapore through the Campus for Research Excellence and Technological Enterprise (CREATE) Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850625","Large language model;automated program repair;fine-tuning","Maintenance engineering;Codes;Computer bugs;Costs;Tuning;Training;Large language models;Translation;Source coding;Programming","","","","136","IEEE","23 Jan 2025","","","IEEE","IEEE Journals"
"DemoCraft: Using In-Context Learning to Improve Code Generation in Large Language Models","K. N. Joshua; M. Sreejith","Department of Electrical Engineering, Indian Institute of Technology, Kanpur; Department of Computer Science and Engineering, Indian Institute of Technology, Guwahati","2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)","14 Mar 2025","2025","","","1","7","Producing executable code from natural-language directives via Large Language Models (LLMs) involves obstacles like semantic uncertainty and the requirement for task-focused context interpretation. To resolve these difficulties, we introduce DemoCraft, a framework that refines code generation through in-context learning and demonstration selection, in tandem with latent concept learning. Latent concept learning appends extra concept tokens, which act as trainable embeddings encapsulating task-oriented insights. We evaluate our approach on two promi- nent datasets: MBPP and Humaneval. Our experimental findings reveal that this method attains roughly a 2x boost in the pass@k metric compared to baseline models. Moreover, we propose two new evaluation measurements: correctness@k and similarity@k. Empirical analyses indicate that our system achieves nearly a 3x advancement in these measures as well.","","979-8-3315-1591-1","10.1109/IITCEE64140.2025.10915349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915349","in-context learning;code generation;latent concept learning;demonstration selection;large language models","Measurement;Computer languages;Adaptation models;Codes;Uncertainty;Large language models;Computational modeling;Semantics","","","","6","IEEE","14 Mar 2025","","","IEEE","IEEE Conferences"
"Large Language Models Based Communication Simulation Platform","Y. Wang; S. Cui; R. Wan; J. Wang; F. Wang","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China",2024 IEEE 24th International Conference on Communication Technology (ICCT),"8 Apr 2025","2024","","","1891","1895","In recent years, Large Language Models (LLMs) have been widely used in various fields, including personalized education, data analysis, disease diagnosis, and engineering design. These advancements have opened new possibilities for wireless communication engineering. In this paper, we propose an LLM-based human-machine collaborative framework to generate a simulation platform for the communication system. The proposed framework effectively combines human experience with the powerful generative capabilities of LLMs through well-designed prompt engineering techniques, enhancing the design of wireless communication systems. Specifically, the proposed prompt engineering framework directs the LLM in tasks such as requirement elicitation, system modeling, and code generation for different modules of wireless communication systems. Parallel tests on the commercially mature LLMs like GPT-3.5 and Claude 3 further demonstrate that our approach can improve the efficiency, quality, and reliability of the design process.","2576-7828","979-8-3503-6376-0","10.1109/ICCT62411.2024.10946635","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10946635","Code generation;human-machine collaboration;large language models;prompt engineering;wireless communication","Wireless communication;Codes;Large language models;Human-machine systems;Collaboration;Systems modeling;Reliability engineering;Prompt engineering;Medical diagnosis;Signal to noise ratio","","","","9","IEEE","8 Apr 2025","","","IEEE","IEEE Conferences"
"Generative AI in Cybersecurity: Generating Offensive Code from Natural Language","P. Liguori; R. Natella; D. Cotroneo","University of Naples Federico II, Italy; University of Naples Federico II, Italy; University of Naples Federico II, Italy",2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S),"9 Jul 2025","2025","","","174","175","In recent years, Generative AI has emerged as a transformative force across a variety of domains. In particular, the ability of Large Language Models (LLMs) to produce coherent and functional source code has generated considerable interest within the cybersecurity community. Offensive security, traditionally characterized by manual and labor-intensive processes, is now being reshaped by these powerful AI-driven tools. Generative models can translate high-level natural language descriptions into working offensive code artifacts, thereby accelerating exploit development and lowering the barrier to entry for adversarial activities [1] , [2] .","2833-292X","979-8-3315-1203-3","10.1109/DSN-S65789.2025.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11068383","generative ai;offensive security;ai code generation;large language models (llms)","Codes;Translation;Generative AI;Source coding;Large language models;Force;Manuals;Computer security","","","","11","IEEE","9 Jul 2025","","","IEEE","IEEE Conferences"
"Evaluating the Text Summarization Efficiency of Large Language Models","M. M. Provakar","Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh",2024 2nd International Conference on Information and Communication Technology (ICICT),"21 Jan 2025","2024","","","6","10","Text summarization is crucial in condensing lengthy texts into concise summaries, enabling efficient information retrieval and content evaluation. Large language models (LLMs) have emerged as pivotal tools in this domain, demonstrating a remarkable ability to generate coherent summaries across diverse languages and domains. This study explores the effectiveness of LLMs, particularly Llama-2 and Gemma, in text summarization tasks. The study evaluates how well these LLMs autonomously generate summaries using zero-shot prompt engineering and supervised fine-tuning. Evaluation metrics such as BLEU, ROUGE, and BERTScore are utilized to evaluate the quality and coherence of the generated summaries. The study shows notable differences between Llama-2 and Gemma, with Llama-2 outperforming particularly well after fine-tuning. It achieves higher scores across all evaluation metrics, highlighting its ability to generate contextually relevant summaries more effectively than Gemma. Efforts to optimize Large Language Models (LLMs) for summarization should prioritize refining model architectures, exploring innovative fine-tuning methods, and expanding training datasets. These efforts are essential to fully harness the potential of LLMs in producing high-quality summaries across diverse domains and linguistic contexts.","","979-8-3315-0822-7","10.1109/ICICT64387.2024.10839646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839646","Text summarization;Large Language Models (LLMs);Llama-2;Gemma;Zero-Shot Prompt Engineering;Supervised Fine-Tuning;Low-Rank-Adaptation","Measurement;Training;Large language models;Refining;Text summarization;Coherence;Linguistics;Information retrieval;Information and communication technology;Prompt engineering","","","","20","IEEE","21 Jan 2025","","","IEEE","IEEE Conferences"
"Large Language Models in Automated Repair of Haskell Type Errors","S. Santos; J. Saraiva; F. Ribeiro","HASLab/INESC TEC, University of Minho, Braga, Portugal; HASLab/INESC TEC, University of Minho, Braga, Portugal; HASLab/INESC TEC, University of Minho, Braga, Portugal",2024 IEEE/ACM International Workshop on Automated Program Repair (APR),"2 Sep 2024","2024","","","42","45","This paper introduces a new method of Automated Program Repair that relies on a combination of the GPT-4 Large Language Model and automatic type checking of Haskell programs. This method identifies the source of a type error and asks GPT-4 to fix that specific portion of the program. Then, QuickCheck is used to automatically generate a large set of test cases to validate whether the generated repair behaves as the correct solution. Our publicly available experiments revealed a success rate of 88.5% in normal conditions. However, more detailed testing should be performed to more accurately evaluate this form of APR.","","979-8-4007-0577-9","10.1145/3643788.3648012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10653230","automated program repair;large language model;fault localization;code generation;type checking;automatic testing","Large language models;Conferences;Maintenance engineering;Testing","","","","11","","2 Sep 2024","","","IEEE","IEEE Conferences"
"WIP: Active Learning Through Prompt Engineering and Agentic AI Simulation-A Pilot Project in Computer Networks Education","X. Ma; J. Wang","Electrical and Computer Engineering Department, University of Wisconsin-Platteville, Platteville, WI; Global Operations Dell. Inc",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","5","This work-in-progress paper introduces the AIca-demic system, an innovative framework employing Agentic AI and Agile methodologies to enhance learning in complex domains such as computer networks. Positioned within the topic of AI and Machine Learning Tools to Enhance Instruction, it aims to revolutionize the learning experience and outcomes for the intricate subject matter. This system emphasizes active, adaptive learning experiences through AI generated or AI improved educational materials with multiple iterations of feedback and improvement cycles. Utilizing fine tuned large language models (LLM), the AIcademic system assembles an interactive AI team, e.g. AIcademic Professor, Student, and Instructional Designer. Each AI agent is uniquely configured with our POISE prompt engineering model to analyze and simulate real-time classroom interactions from multiple viewpoints. Active learning pedagogy is embedded into the system through prompt engineering during the creation of each agent. Agile methodology is employed to organize collaborations of the AI agents for complex task planning and implementation, feedback integration, output con-tinuous improvement, and agent self-enhancement. A suite of AI tools is explored to dynamically create tailored educational materials aligned with the educator's teaching preferences and students' needs. Preliminary results from a pilot implementation of teaching the transport layer in computer networks demon-strated improvements in student engagement and comprehension over previous materials. This AIcademic framework presents a promising and scalable paradigm for AI applications in educational environments. While still under development, this research aims to refine and expand these findings, exploring the full potential of integrating Prompt Engineering and Agentic AI for creating active learning environments across complex technical subjects. The implications extend beyond computer network education, offering a blueprint to redefine teaching and learning in a technology-enhanced era. We invite collaboration from the broader academic community to refine the Agent prompt design, automate AI to AI interactions, assess long term impacts, and explore further applications.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10892925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892925","Active Learning;Agentic AI;Prompt Engineering;Human-in-the-Loop;Agile Methodology","Large language models;Active learning;Collaboration;Machine learning;Computer networks;Real-time systems;Planning;Complexity theory;Prompt engineering;Artificial intelligence","","1","","13","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model","Y. -C. Lin; A. Kumar; N. Chang; W. Zhang; M. Zakir; R. Apte; H. He; C. Wang; J. -S. R. Jang","National Taiwan University, Taipei, Taiwan; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; Ansys, Inc., San Jose, Califonia, USA; National Taiwan University, Taipei, Taiwan",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","5","We introduce four principal contributions to augment the capabilities of Large Language Models (LLMs) in generating domain-specific code: (i) leveraging LLM-based data splitting and data renovation techniques to refine the semantic representation within the embedding space; (ii) proposing an effective method for refactoring existing scripts, enabling the generation of new and high-quality scripts with the aid of LLMs; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) showcasing the efficacy of our data pre-processing approach through a case study using engineering simulation software RedHawk-SC. Our contributions collectively advance the Retrieval-Augmented Generation (RAG) framework, enabling more relevant and precise information retrieval. An arena-style evaluation by 28 domain experts and 182 votes confirms the significant effectiveness of our methods. Notably, our approach achieves up to 1.43 times the improvement in code generation for MapReduce applications compared to the Chain-of-Thought (CoT) technique.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691715","Large language models (LLMs);domain-specific;code generation;data preprocessing;data splitter;data renovation;domain-specific;Retrieval-Augmented Generation (RAG);prompt engineering;MapReduce;RedHawk-SC (RH-SC)","Knowledge engineering;Codes;Large language models;Conferences;Semantics;Data preprocessing;Information retrieval;Software;Prompt engineering","","1","","16","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Balancing Security and Correctness in Code Generation: An Empirical Study on Commercial Large Language Models","G. S. Black; B. P. Rimal; V. M. Vaidyan","Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Department of Computer Science, University of Idaho, Moscow, ID, USA; Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA",IEEE Transactions on Emerging Topics in Computational Intelligence,"23 Jan 2025","2025","9","1","419","430","Large language models (LLMs) continue to be adopted for a multitude of previously manual tasks, with code generation as a prominent use. Multiple commercial models have seen wide adoption due to the accessible nature of the interface. Simple prompts can lead to working solutions that save developers time. However, the generated code has a significant challenge with maintaining security. There are no guarantees on code safety, and LLM responses can readily include known weaknesses. To address this concern, our research examines different prompt types for shaping responses from code generation tasks to produce safer outputs. The top set of common weaknesses is generated through unconditioned prompts to create vulnerable code across multiple commercial LLMs. These inputs are then paired with different contexts, roles, and identification prompts intended to improve security. Our findings show that the inclusion of appropriate guidance reduces vulnerabilities in generated code, with the choice of model having the most significant effect. Additionally, timings are presented to demonstrate the efficiency of singular requests that limit the number of model interactions.","2471-285X","","10.1109/TETCI.2024.3446695","University of Idaho, Moscow, ID, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658990","Code generation;code security;CWE;large language models;prompt engineering;vulnerability","Codes;Security;Testing;Task analysis;Software;Logic;Computational intelligence","","3","","42","IEEE","29 Aug 2024","","","IEEE","IEEE Journals"
"Large Language Models for Decision-Making Enhancement in Tourism Package Customization","L. Naimi; A. Souha; L. Benaddi; C. Ouaddi; E. M. Bouziane; A. Jakimi; M. Manaouch","Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Informatics, GL-ISI Team, FST of Errachidia, Moulay Smail University, Errachidia, Morocco; Department of Geography, Faculty of Humanities and Social Sciences, Ibn Tofail University, Kenitra, Morocco",2024 International Conference on Decision Aid Sciences and Applications (DASA),"17 Jan 2025","2024","","","1","5","The tourism industry faces significant challenges in providing personalized travel recommendations that cater to diverse traveler preferences and needs. This paper presents an approach utilizing Large Language Models (LLMs), specifically the open-source LLaMA model, to enhance decision-making in tourism package recommendations. By integrating a comprehensive database of attractions, amenities, and activities, the proposed system generates tailored suggestions based on user profiles, including demographics, interests, and travel duration. The application of few-shot learning techniques in prompt engineering allows the model to effectively interpret user inputs and produce relevant recommendations with minimal examples. This research aims to demonstrate how LLMs can serve as powerful decision aids in logistics within the tourism sector, ultimately improving user satisfaction and streamlining the travel planning process.","","979-8-3503-6910-6","10.1109/DASA63652.2024.10836343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10836343","Large Language Models (LLMs);Tourism Decision-Making;Few-Shot Learning;Prompt Engineering;LLaMA","Databases;Large language models;Decision making;Tourism industry;Planning;Prompt engineering;Few shot learning;Faces;Logistics","","","","21","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Automatic Unit Test Code Generation Using Large Language Models","A. K. Öçal; M. Keskinöz","Bilgisayar Mühendisliği Bölümü, İstanbul Teknik Üniversitesi, İstanbul, Türkiye; Bilgisayar Mühendisliği Bölümü, İstanbul Teknik Üniversitesi, İstanbul, Türkiye",2024 32nd Signal Processing and Communications Applications Conference (SIU),"23 Jul 2024","2024","","","1","4","This study aimed to automate the production of unit tests, a critical component of the software development process. By using pre-trained Large Language Models, manual effort and training costs were reduced, and test production capacity was increased. Instead of directly feeding the test functions obtained from the Java projects to be tested into the model, the project was analyzed to extract additional information. The data obtained from this analysis were used to create an effective prompt template. Furthermore, the sources of the problematic tests produced were identified, and this information was fed back into the model, enabling it to autonomously correct the errors. The results of the study showed that the model was able to generate tests covering %55.58 of the functions collected from Java projects across different domains and that re-feeding the model with the generated erroneous tests resulted in a %29.3 improvement in the number of executable tests.","2165-0608","979-8-3503-8896-1","10.1109/SIU61531.2024.10600772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10600772","software testing;unit test generation;large language models;automatic test generation","Training;Java;Large language models;Production;Manuals;Signal processing;Software","","","","0","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Large Language Models for Fraud Detection","R. Jin; Y. Wen; T. Liu; C. Shen","School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; School of Management, Xi'an Jiaotong University, Xi'an, China; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China",2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD),"21 Jul 2025","2025","","","441","446","Large Language Models (LLMs) are reshaping the landscape of Machine Learning (ML) application development. This study addresses the problem of whether a general language model pre-trained on a large amount of data samples could be effective in social security data fraud detection tasks. We selected the GPT-4 model and designed a template for prompt engineering to explore its potential capability in analyzing. Additionally, we compared it with other methods such as Ensemble Bayesian, BP neural network, Bert, and Transformer. The experimental results indicate that the large language model, after prompt engineering, achieved an F1 score of 0.89 on the same test set, surpassing other state-of-the-art works.","2769-3554","979-8-3315-1936-0","10.1109/ICAIBD64986.2025.11082030","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082030","large language models;prompt engineering;fraud detection;social security","Measurement;Knowledge engineering;Large language models;Neural networks;Machine learning;Transformers;Fraud;Bayes methods;Security;Prompt engineering","","","","31","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"On-Device Input Analysis to Proactively Enhance Data Security in Large Language Models","A. S; A. R; R. Sekar; G. R","School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India",2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP),"12 Feb 2025","2024","","","1","4","Large Language Models (LLMs) have become essential in many industries, helping with productivity gains and automation. However its utilisation creates concerns regarding the unintentional disclosure of sensitive information, which puts security and privacy in jeopardy. In order to reduce these risks, this qualitative study suggests a technique called “Input Refining” that involves methodically identifying and removing personal data from LLM inputs. The process entails a careful examination of the input data, which is then paraphrased to exclude sensitive material while maintaining the input's semantic integrity. Through the incorporation of Input Refining into AI workflows, entities can enhance data security and privacy procedures, cultivating conscientious implementation of LLMs. By providing a workable solution to the problem of unintentional information security breaches in the age of large language models, our study adds to the ongoing discourse on AI ethics and data privacy.","2640-5768","979-8-3503-5065-4","10.1109/AISP61711.2024.10870695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870695","Large Language models;Input Refining;Information Security;Artificial Intelligence;Corporate Security practices;Reinforcement Learning with AI Feedback","Privacy;Data privacy;Large language models;Data security;Refining;Medical services;Chatbots;Security;Artificial intelligence;Biomedical imaging","","","","22","IEEE","12 Feb 2025","","","IEEE","IEEE Conferences"
"Mitigating Hallucinations in Large Language Models for Educational Application","H. -T. Ho; D. -T. Ly; L. V. Nguyen","Department of Artificial Intelligence, FPT University, Danang, Vietnam; Department of Artificial Intelligence, FPT University, Danang, Vietnam; Department of Artificial Intelligence, FPT University, Danang, Vietnam",2024 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),"10 Dec 2024","2024","","","1","4","Large Language Models (LLMs) have transformed various fields, including education, by providing intelligent and automated text generation. However, the tendency of these models to generate hallucinated or factually incorrect information poses significant challenges, especially in educational contexts where ac-curacy is paramount. This paper explores various methodologies to mitigate hallucinations in LLMs, focusing on their application in educational settings. We review current approaches, propose a multi-stage framework for reducing hallucinations, and evaluate its effectiveness through empirical studies.","","979-8-3315-3083-9","10.1109/ICCE-Asia63397.2024.10773965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773965","Large Language Models (LLMs);Hallucination;Prompt Engineering;Educational;Hybrid Retrieval;Empirical Evaluation;Knowledge Graphs","Accuracy;Reviews;Large language models;Education;Focusing;Manuals;Reliability engineering;Context modeling","","2","","12","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Leveraging Pre-Trained Large Language Models (LLMs) for On-Premises Comprehensive Automated Test Case Generation: An Empirical Study","H. Yin; H. Mohammed; S. Boyapati","Samsung Research America Mountain View, CA, USA; Samsung Research America Mountain View, CA, USA; Samsung Research America Mountain View, CA, USA",2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),"17 Dec 2024","2024","9","","597","607","The rapidly evolving field of Artificial Intelligence (AI)-assisted software testing has predominantly focused on automated test code generation, with limited research exploring the realm of automated test case generation from user stories requirements. This paper presents a comprehensive empirical study on harnessing pre-trained Large Language Models (LLMs) for generating concrete test cases from natural language requirements given in user stories. We investigate the efficacy of various prompting and alignment techniques, including prompt chaining, few-shot instructions, and agency-based approaches, to facilitate secure on-premises deployment. By integrating our learnings with an on-premises model setup, wherein we deploy a RoPE scaled 4-bit quantized LLaMA 3 70B Instruct model, optionally augmented with LoRA adapters trained on QA datasets, we demonstrate that this approach yields more accurate and consistent test cases despite video random-access memory (VRAM) constraints, thereby maintaining the security benefits of an on-premises deployment.","2189-8723","979-8-3503-6304-3","10.1109/ICIIBMS62405.2024.10792720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792720","Test Case Generation;Large Language Models (LLMs);AI-Assisted Testing;On-Premises Deployment;Software Testing;Artificial Intelligence (AI);Machine Learning (ML);Deep Learning;Neural Networks;Intelligent Systems;Software Development and Testing;Natural Language Processing (NLP);Pre-trained LLMs;Prompting and Alignment Techniques;Onpremises Deployment;Data Security and Privacy;Quality Assurance (QA)","Software testing;Adaptation models;Accuracy;Large language models;Scalability;Natural languages;Memory management;Software;Security;Informatics","","","","19","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Digital Transformation of Security Standards: Requirements Extraction using Large Language Models","M. Siavvas; G. Xanthopoulou; I. Kalouptsoglou; D. Kehagias; D. Tzovaras","Centre for Research and Technology Hellas, Thessaloniki, Greece; Centre for Research and Technology Hellas, Thessaloniki, Greece; Centre for Research and Technology Hellas, Thessaloniki, Greece; Centre for Research and Technology Hellas, Thessaloniki, Greece; Centre for Research and Technology Hellas, Thessaloniki, Greece",2024 11th International Conference on Dependable Systems and Their Applications (DSA),"1 Jan 2025","2024","","","430","431","Compliance with international security standards is essential for ensuring the security of information systems, and thereby their dependability and trustworthiness. Compliance evaluation is performed by experts who check whether critical security requirements (i.e., criteria), which have been extracted from standard documents, are met by the system. The extraction of security requirements from the standards documents is a tedious and labor-intensive task that is typically performed manually by experts. In order to facilitate the process, in the present paper we propose an approach utilizing Transformer-based models, specifically Large Language Models (LLMs), to automate the identification and extraction of these requirements, effectively transforming security standards into a comprehensive list of requirements. In particular, the proposed approach is based on fine-tuning pre-trained LLMs, including BERT, T5, and BART, on a domain-specific dataset constructed from real security standards in the downstream task of requirements identification and extraction. The capabilities of the approach are illustrated through representative examples.","2767-6684","979-8-3315-3239-0","10.1109/DSA63982.2024.00066","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818262","Cybersecurity;Security Standards;Transformer Architecture;Large Language Models","Accuracy;Large language models;Digital transformation;Buildings;Transformers;Security;Computer crime;Standards;Information systems","","","","2","IEEE","1 Jan 2025","","","IEEE","IEEE Conferences"
"Guided Code Generation with LLMs: A Multi-Agent Framework for Complex Code Tasks","A. Almorsi; M. Ahmed; W. Gomaa","ECCE School, Faculty of Engineering, Egypt-Japan University of Science and Technology; ECCE School, Faculty of Engineering, Egypt-Japan University of Science and Technology; ECCE School, Faculty of Engineering, Egypt-Japan University of Science and Technology","2024 12th International Japan-Africa Conference on Electronics, Communications, and Computations (JAC-ECC)","9 Jul 2025","2024","","","215","218","Large Language Models (LLMs) have shown remarkable capabilities in code generation tasks, yet they face significant limitations in handling complex, long-context programming challenges and demonstrating complex compositional reasoning abilities. This paper introduces a novel agentic framework for “guided code generation” that tries to address these limitations through a deliberately structured, fine-grained approach to code generation tasks. Our framework leverages LLMs’ strengths as fuzzy searchers and approximate information retrievers while mitigating their weaknesses in long sequential reasoning and long-context understanding. Empirical evaluation using OpenAI’s HumanEval benchmark with Meta’s Llama 3.1 8B model (int4 precision) demonstrates a $23.79 \%$ improvement in solution accuracy compared to direct one-shot generation. Our results indicate that structured, guided approaches to code generation can significantly enhance the practical utility of LLMs in software development while overcoming their inherent limitations in compositional reasoning and context handling.","2690-3385","979-8-3315-3144-7","10.1109/JAC-ECC64419.2024.11061204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11061204","Large Language Models;Code Generation;Prompting Techniques;Agents","Waste materials;Codes;Large language models;Programming;Benchmark testing;Software systems;Information retrieval;Cognition;Software engineering;Software development management","","","","12","IEEE","9 Jul 2025","","","IEEE","IEEE Conferences"
"Taxonomy of Multi-Agent Systems Attacks and their Defense Tactics in Certifying Security of Cyber Physical Systems","G. Jain; N. R. Solomon Jebaraj","Department of Computer Science & Business System, Noida Institute Of Engineering and Technology, Greater Noida, Uttar Pradesh, India; Department of Computer Science and Information Technology, Jain (Deemed to be University), Bangalore, India","2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)","20 Mar 2024","2023","","","648","652","The utilization of multi-agent systems has been increasingly prevalent across various sectors, owing to their notable efficacy in execution. However, a multitude of hazards exist that possess the capability to undermine the integrity of the agent and pose a threat to the overall security of the system. Therefore, it is crucial to carefully evaluate and address security vulnerabilities during the process of creating Multi-Agent Systems (MAS). This survey assesses different models that aim to guarantee the security of Multi-Agent Systems (MAS). These models were developed based on principles related to the agents' functionality and communication. This study offers a comprehensive examination and categorization of the prevailing threats targeting Multi-Agent Systems (MASs). Following this, we conduct a comprehensive analysis and assessment of diverse security strategies described in scholarly sources, classifying them into prevention, detection, and resiliency approaches based on their established credibility and reliability. In conclusion, we offer a suggestion for the optimal security approach to address particular types of threats, considering the most recent breakthroughs in the field of study.","","979-8-3503-4438-7","10.1109/ICAICCIT60255.2023.10466176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466176","Multi Agent Systems;Resiliency;Security;Cyber Physical Systems;Attacks","Surveys;Taxonomy;Hazards;Information and communication technology;Security;Reliability;Multi-agent systems","","","","25","IEEE","20 Mar 2024","","","IEEE","IEEE Conferences"
"Topic Modeling Analysis in the Field of Large Language Models with BERTopic (2020–2024)","S. Uğuz; Ç. N. Tülü","Software Engineering Department, Adana Alparslan Türkes Science and Technology University, Adana, Türkiye; Software Engineering Department, Adana Alparslan Türkes Science and Technology University, Adana, Türkiye",2024 Innovations in Intelligent Systems and Applications Conference (ASYU),"28 Nov 2024","2024","","","1","6","One of the primary factors behind the success of contemporary artificial intelligence backed applications is the Large Language Models (LLMs). Remarkably, this concept that was virtually unknown just a few years ago has now become increasingly popular. Whereas initial Language Models (LMs) were updated and enhanced every few years, but we now witness new versions with new features and advanced capabilities are being released within mere months. This widespread adoption LLM's in daily life has also sparked our curiosity about the direction of academic research on this topic. In this study, academic papers related to LLM between 2020 and 2024 (until first quarter of 2024) were analyzed with topic modelling technique. The data used in this study were collected from the Scopus database and topic modeling analysis was performed using BERTopic. According to the analysis results obtained, it can be stated that the research topics named Large Language Models (LLMs), ChatGPT in Medicine, Sentiment Bias, Code Generation, Visual Language Models, and System Requirements for LLMs are quite remarkable. It has been revealed that the analysis results obtained and the developments in academic and professional business life are interrelated. Especially today, with the increasing use of ChatGPT, it is known that specialization has been made in this field in order to ensure better execution of queries and a new profession called Prompt Engineering has emerged.","2770-7946","979-8-3503-7943-3","10.1109/ASYU62119.2024.10757169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757169","natural language processing;large language models;topic modeling;bertopic;prompt engineering","Analytical models;Visualization;Technological innovation;Databases;Large language models;Natural languages;Chatbots;Data models;Prompt engineering;Intelligent systems","","","","26","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Automatic Bug Fixing via Deliberate Problem Solving with Large Language Models","G. Weng; A. Andrzejak","Heidelberg University & SAP SE, Heidelberg, Germany; Heidelberg University, Heidelberg, Germany",2023 IEEE 34th International Symposium on Software Reliability Engineering Workshops (ISSREW),"2 Nov 2023","2023","","","34","36","Developers dedicate a significant share of their activities to finding and fixing defects in their code. Automated program repair (APR) attempts to reduce this effort by a set of techniques for automatically fixing errors or vulnerabilities in software systems. Recent Large Language Models (LLMs) such as GPT-4 offer an effective alternative to existing APR methods, featuring out-of-the-box bug fixing performance comparable to even sophisticated deep learning approaches such as CoCoNut. In this work we propose a further extension to LLM-based program repair techniques by leveraging a recently introduced interactive prompting technique called Tree of Thoughts (ToT). Specifically, we ask a LLM to propose multiple hypotheses about the location of a bug, and based on the aggregated response we prompt for bug fixing suggestions. A preliminary evaluation shows that our approach is able to fix multiple complex bugs previously unsolved by GPT-4 even with prompt engineering. This result motivates further exploration of hybrid approaches which combine LLMs with suitable meta-strategies.","","979-8-3503-1956-9","10.1109/ISSREW60843.2023.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301342","","Deep learning;Codes;Conferences;Computer bugs;Maintenance engineering;Software systems;Software reliability","","5","","11","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"Improving Machine Translation Capabilities by Fine-Tuning Large Language Models and Prompt Engineering with Domain-Specific Data","L. J. Laki; Z. G. Yang","Globalese GmbH., Trier, Germany; HUN-REN Hungarian Research Centre for Linguistics, Budapest, Hungary",2024 IEEE 3rd Conference on Information Technology and Data Science (CITDS),"17 Dec 2024","2024","","","1","5","This study examines the applicability and performance of large language models (LLMs) in the field of machine translation for in-domain texts, with a particular focus on fine-tuning LLMs, few-shot prompting, and word vector-based example sentence search methods. The aim of the study is to determine the extent to which the few-shot technique can improve translation quality for domain-specific texts. Our results indicate that the few-shot learning approach consistently improved translation quality across all examined LLM systems, with performance enhancements ranging from 10% to 25% in BLEU scores. Surprisingly, the word vector-based method, which uses the vectorial representation of words to select translation examples, did not perform as well as the character similarity-based fuzzy matching technique. The study discusses the performance of various systems, highlighting significant advancements achieved through fine-tuning and few-shot prompting.","","979-8-3503-8788-9","10.1109/CITDS62610.2024.10791375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10791375","machine translation;large language models;prompt;fine-tuning;retrieval augmented generation;llamaindex;domain-specific data","Large language models;Search methods;Semantics;Data science;Data models;Distance measurement;Machine translation;Prompt engineering;Information technology;Few shot learning","","","","20","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"A Study on Performance Improvement of Prompt Engineering for Generative AI with a Large Language Model","D. Park; G. -t. An; C. Kamyod; C. G. Kim","Department of Computer Science, Namseoul University, Cheonan, Republic of Korea; Korea Food Research Institute, Wanju-gun, Republic of Korea; Computer and Communication Engineering for Capacity Building Research Center, School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; Department of Computer Science, Namseoul University, Cheonan, Republic of Korea",Journal of Web Engineering,"27 Feb 2024","2023","22","8","1187","1206","In the realm of Generative AI, where various models are introduced, prompt engineering emerges as a significant technique within natural language processing-based Generative AI. Its primary function lies in effectively enhancing the results of sentence generation by large language models (LLMs). Notably, prompt engineering has gained attention as a method capable of improving LLM performance by modifying the structure of input prompts alone. In this study, we apply prompt engineering to Korean-based LLMs, presenting an efficient approach for generating specific conversational responses with less data. We achieve this through the utilization of the query transformation module (QTM). Our proposed QTM transforms input prompt sentences into three distinct query methods, breaking them down into objectives and key points, making them more comprehensible for LLMs. For performance validation, we employ Korean versions of LLMs, specifically SKT GPT-2 and Kakaobrain KoGPT-3. We compare four different query methods, including the original unmodified query, using Google SSA to assess the naturalness and specificity of generated sentences. The results demonstrate an average improvement of 11.46% when compared to the unmodified query, underscoring the efficacy of the proposed QTM in achieving enhanced performance.","1544-5976","","10.13052/jwe1540-9589.2285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452390","AI;large language model;generative AI;few-shot learning;prompt engineering;AI Chatbot","Generative AI;Transforms;Chatbots;Internet","","17","","29","","27 Feb 2024","","","River Publishers","River Publishers Journals"
"Issues with Generic Large Language Models (GLLMs)","D. Dasgupta; A. Roy","Dept. of Computer Science, The University of Memphis, Memphis, TN, USA; Dept. of Computer Science, The University of Memphis, Memphis, TN, USA",2024 Artificial Intelligence for Business (AIxB),"3 Dec 2024","2024","","","47","50","Generic Large Language Models (GLLMs) are continuously being released with enhanced size and capabilities, promoting the abilities of these tools for different use. GLLMs excel in text, image, and video generation (assembling, summarizing, translating) with proper queries and prompts. However, the reliability of GLLMs’ responses is questionable in critical applications due to factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on the data collection-privacy, legal and ethical issues. This short report emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications.","","979-8-3503-9103-9","10.1109/AIxB62249.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771198","Generative AI;Large Language Models (LLMs);Generative Pre-Trained Models (GPTs);Small Parameterized Data Models (SPDM)","Ethics;Law;Large language models;Data models;Reliability;Security;Business","","","","34","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Large Language Models for Hardware Security (Invited, Short Paper)","H. Pearce; B. Tan",University of New South Wales; University of Calgary,"2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","420","423","Secure digital hardware is the foundation of secure systems. However, achieving hardware security requires a lot of disparate expertise, ranging from knowledge of tools, awareness of myriad threats, and a fundamental understanding of how digital hardware is used in a given application. Such expertise is rare, so mistakes are made. We believe that recent advancements in AI, specifically Large Language Models (LLMs), provide a potential pathway for alleviating the burden on digital hardware designers. In this paper, we outline some of the challenges faced by cybersecurity experts in the hardware space, our vision for how LLMs can revolutionize hardware security, briefly present current progress and recent works in this research direction, and finally reflect on open challenges that remain for the community.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00055","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835653","Hardware Security;Large Language Models;Artificial Intelligence;Cybersecurity","Privacy;Hardware security;Large language models;Benchmark testing;Distance measurement;Intelligent systems;Computer security;System analysis and design","","1","","41","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Few-Shot and Zero-Shot Classification with Large Language Models: A Comparative Study","A. T. Bayrak; F. N. Korkmaz; İ. U. Sayan","Ata Teknoloji Platformları Veri Bilimi ve Inovasyon, İstanbul, Türkiye; Ata Teknoloji Platformları Veri Bilimi ve Inovasyon, İstanbul, Türkiye; Ata Teknoloji Platformları Veri Bilimi ve Inovasyon, İstanbul, Türkiye",2025 9th International Symposium on Innovative Approaches in Smart Technologies (ISAS),"12 Aug 2025","2025","","","1","4","This study systematically compares the performance of large language models in zero-shot and few-shot text classification tasks. Experiments were conducted on a balanced Turkish news dataset, evaluating five different prompting strategies using the ChatGpt model: label list only (zero-shot), one example per label (one-shot), five examples (five-shot), ten examples (ten-shot), and label summaries (summary). The results indicate that increasing the number of examples per label positively affects overall performance, while summary-based prompts emerge as an effective alternative, offering both high accuracy and lower cost. According to macro and micro F1 scores, the highest performance was achieved with the five-shot approach, whereas the summary method demonstrated remarkable effectiveness in zero-shot settings. These findings highlight the critical role of prompt length, content diversity, and contextual clarity in the classification performance of LLM-based system.","","979-8-3315-1482-2","10.1109/ISAS66241.2025.11101921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11101921","large language models;zero-shot classification;fewshot learning;prompt engineering;generative AI;text classification","Costs;Accuracy;Large language models;Text categorization;Chatbots;Prompt engineering","","","","0","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"A new approach for automatic test case generation from use case diagram using LLMs and prompt engineering","L. Naimi; E. M. Bouziane; M. Manaouch; A. Jakimi","GL-ISI Team, Faculty of Sciences and Techniques of Errachidia, UMI, Meknes, Morocco; GL-ISI Team, Faculty of Sciences and Techniques of Errachidia, UMI, Meknes, Morocco; Department of Geography, Faculty of Social Sciences and Humanities of Kenitra, Ibn Tofail University, Morocco; GL-ISI Team, Faculty of Sciences and Techniques of Errachidia, UMI, Meknes, Morocco","2024 International Conference on Circuit, Systems and Communication (ICCSC)","6 Aug 2024","2024","","","1","5","The automation of test case generation from UML diagrams is a growing field that aims to make the software development process smoother. This paper suggests a new framework that uses generative artificial intelligence (AI) to turn use case diagrams into test cases that can be executed. By getting information from the XML representation of use case diagrams, we can create detailed instructions that guide a generative AI model to make test cases for each use case scenario. This method not only makes test case creation easier but also ensures we cover everything well and accurately, which could make software products get to market faster. this approach shows how traditional software engineering methods and new AI techniques can work well together, giving us an idea of what automated software testing might look like in the future.","","979-8-3503-6530-6","10.1109/ICCSC62074.2024.10616548","Ministry of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616548","software testing;test cases;LLMs prompt engineering","Software testing;Automation;Unified modeling language;XML;Software;Prompt engineering;Software engineering;Software development management","","5","","22","IEEE","6 Aug 2024","","","IEEE","IEEE Conferences"
"Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models","J. Hoffmann; D. Frister","AIFB - BIS Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; AIFB - BIS Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany",2024 IEEE/ACM International Conference on Automation of Software Test (AST),"18 Jun 2024","2024","","","76","77","Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13–15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.","2833-9061","979-8-4007-0588-5","","Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556388","Software Testing;Mobile Testing;Machine Learning;Large Language Models","Software testing;Codes;Computational modeling;Training data;Transformers;Software;Data models","","1","","17","CCBY","18 Jun 2024","","","IEEE","IEEE Conferences"
"Applying Large Language Models to Power Systems: Potential Security Threats","J. Ruan; G. Liang; H. Zhao; G. Liu; X. Sun; J. Qiu; Z. Xu; F. Wen; Z. Y. Dong","Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University, Hong Kong, SAR, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), Shenzhen, China; Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University, Hong Kong, SAR, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; Department of Electrical and Electronic Engineering and the Research Institute of Smart Energy, The Hong Kong Polytechnic University, Hong Kong, SAR, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Smart Grid,"22 Apr 2024","2024","15","3","3333","3336","Applying large language models (LLMs) to modern power systems presents a promising avenue for enhancing decision-making and operational efficiency. However, this action may also incur potential security threats, which have not been fully recognized so far. To this end, this article analyzes potential threats incurred by applying LLMs to power systems, emphasizing the need for urgent research and development of countermeasures.","1949-3061","","10.1109/TSG.2024.3373256","Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:AoE/P-601/23-N); General Research Fund of the Hong Kong Special Administrative Region(grant numbers:PolyU15209322); Hong Kong Polytechnic University(grant numbers:1-YWCV,1-W29V); Shenzhen Natural Science Fund in the Stable Support Plan Program(grant numbers:GXWD20231128112434001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459250","Power systems;large language models;security threats","Power system stability;Security;Decision making;Training;Semantics;Reliability;Real-time systems","","25","","12","IEEE","7 Mar 2024","","","IEEE","IEEE Journals"
"A Study on C Code Defect Detection with Fine-Tuned Large Language Models","Y. Wang; X. Wang; H. Yu; F. Gao; X. Liu; X. Wang","SKLCCSE, Beihang University, Beijing, China; SKLCCSE, Beihang University, Beijing, China; SKLCCSE, Beihang University, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China; Beijing Aerospace Automatic Control Institute, Beijing, China",2024 31st Asia-Pacific Software Engineering Conference (APSEC),"25 Apr 2025","2024","","","437","441","Large Language Models(LLMs) have demonstrated excellent capabilities in many areas of software engineering(SE), including code completion, code generation, code understanding, code repair, etc., and the most prominent performer in this regard is ChatGPT. However, its cost of use makes the integration of ChatGPT into code defect detection techniques costly. In this paper, we focus on low-cost-of-use, fine-tunable, open-source large language models with less than 10B parameters, and study their capabilities of C code defect detection when fine-tuned with real-world data and improved with prompt engineering. We studied LLaMa3-8B, DeepSeek-Coder-7b and Qwen2-7B, as they are the typical models with prompt capabilities, whose performance in SE is close to ChatGPT, and they are open-source models. Experimental results show that our method can significantly improve the performance of LLMs within 10B parameters on code defect detection, and the output of the models can be applied to several downstream tasks, such as improving the report quality of static analysis tools.","2640-0715","979-8-3315-3401-1","10.1109/APSEC65559.2024.00055","National Key Research and Development Program of China(grant numbers:2022YFB4502003); National Natural Science Foundation of China(grant numbers:62072017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967324","Defect Detection;Large Language Models;Fine-tuning;Prompt Engineering","Codes;Costs;Large language models;Static analysis;Maintenance engineering;Chatbots;Software;Prompt engineering;Defect detection;Software engineering","","","","19","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"SafeGen-X: A Comprehensive Framework for Enhancing Security, Compliance, and Robustness in Large Language Models","Y. Weng; X. Yang; X. Zhang; Y. Jin; F. Lei; T. Zhang","University of Maryland College Park, Maryland, USA; TikTok, California, USA; Independent Researcher, California, USA; University of Illinois at Urbana-Champaign, Illinois, USA; TikTok, California, USA; Independent Researcher, California, USA",2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE),"9 Jun 2025","2025","","","2294","2298","Ensuring the security and compliance of outputs from large language models (LLMs) remains a significant challenge, particularly in safety-critical applications. This article proposes SafeGen-X, an innovative framework that integrates hi-erarchical attention mechanisms, dynamic reinforcement learning alignment, and adversarial fine-tuning to enhance the robustness and compliance of generative models. By leveraging multilevel feature extraction, a safety discriminator, and semantic consistency reconstruction, SafeGen-X provides a comprehensive solution to address unsafe outputs and adversarial vulnerabilities. Experimental analysis confirms its superiority over existing methods in safety and robustness, setting a new benchmark for secure LLM deployment in real-world scenarios.","","979-8-3315-3508-7","10.1109/ICAACE65325.2025.11019350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11019350","Kerwords-Large Language Models;Security Compliance;Hi-erarchical Attention;Reinforcement Learning;Adversarial Fine-Tuning","Control engineering;Large language models;Semantics;Reinforcement learning;Reliability engineering;Feature extraction;Robustness;Safety;Security;Standards","","","","7","IEEE","9 Jun 2025","","","IEEE","IEEE Conferences"
"Strategic Application of Prompt Engineering in Multi-Modal Large Language Models","M. Son; W. Jun; S. Lee","Department of Metabiohealth, Autonomous Driving Lab, MODULABS, Sungkyunkwan University, Republic of Korea; Department of Electric Engineering, Autonomous Driving Lab, MODULABS, Dong Seoul University, Republic of Korea; Department of Smart Automobile, Autonomous Driving Lab, MODULABS, Soonchunhyang University, Republic of Korea",2025 IEEE International Conference on Consumer Electronics (ICCE),"26 Mar 2025","2025","","","1","5","Multimodal Large Language Models (MLLM), which integrate large language models (LLMs) with vision models, aim to overcome the text-centric limitations of traditional LLMs. While models like GPT-4 and PaLM-E excel at processing text data, they face limitations in complex fields such as medical image analysis and cross-modality reasoning. MLLMs enhance understanding and reasoning by combining textual and non-linguistic data, such as images, expanding their applicability across various domains. This study proposes optimization strategies for effectively handling multimodal inputs through prompt engineering techniques. We evaluate three MLLMs–Llama-3.2, Phi-3.5, and Qwen2–VL-using datasets such as Flickr30k, NoCaps, and MSCOCO, and analyze their performance on tasks including image captioning, object recognition, and scene understanding. Furthermore, a comparison of the impact of Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques on model performance shows that CoT is more effective for tasks requiring logical reasoning, while ICL enhances adaptability across diverse scenarios.","2158-4001","979-8-3315-2116-5","10.1109/ICCE63647.2025.10930109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930109","Multimodal Large Language Model;Prompt Engineering;In-context learning;Chain of Thought","Measurement;Adaptation models;Analytical models;Image analysis;Large language models;Face recognition;Cognition;Prompt engineering;Object recognition;Optimization","","","","13","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"A Method of SQL Injection Attack Detection Based on Large Language Models","Y. Zhang; Z. Jiang; X. Cheng; H. Wu; Z. Tian; S. Lou","Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China; Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China; Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China; Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China; Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China; Guangdong Branch of National Computer Network Emergency Response Technical Team, Coordination Center of China, Guangzhou, China",2024 2nd International Conference on Computer Network Technology and Electronic and Information Engineering (CNTEIE),"12 May 2025","2024","","","154","158","SQL injection attacks are a serious threat to the security of cyberspace. In view of the problems with traditional SQL injection attack detection methods, such as high false positive rates and insufficient generalization, this paper proposes a new detection method based on large language models (LLMs). Through prompt engineering and instruction tuning technologies, a dedicated large language model for SQL injection attack detection is constructed. The experimental results show that the proposed method achieves an accuracy of 99.85% and a false positive rate of 0.26% on general benchmark datasets, which verifies the effectiveness of large language models in the field of SQL injection attack detection.","","979-8-3315-2443-2","10.1109/CNTEIE66268.2024.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10987904","SQL injection;vulnerability detection;large language models;prompt engineering;instruction tuning","Performance evaluation;Electric potential;Accuracy;Large language models;Data preprocessing;SQL injection;Benchmark testing;Network security;Prompt engineering;Tuning","","","","16","IEEE","12 May 2025","","","IEEE","IEEE Conferences"
"SpecRover: Code Intent Extraction via LLMs","H. Ruan; Y. Zhang; A. Roychoudhury","National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","963","974","Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better “signal” to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029735","large language model;automated program repair;llm agent;autonomous software engineering;autonomous software improvement","Codes;Costs;Large language models;Computer bugs;Production;Maintenance engineering;Software;Iterative methods;Software development management;Software engineering","","1","","33","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies","V. Bayrı; E. Demirel","Technology Architecture KoçSistem, Istanbul, Türkiye; Technology Architecture KoçSistem, Istanbul, Türkiye",2023 4th International Informatics and Software Engineering Conference (IISEC),"19 Jan 2024","2023","","","1","4","Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.","","979-8-3503-1803-6","10.1109/IISEC59749.2023.10391027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391027","artificial intelligence;large language model;software testing life cycle;software testing","Software testing;Location awareness;Industries;Chatbots;Software systems;Software reliability;Artificial intelligence","","4","","7","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"VeriBench: Benchmarking Large Language Models for Verilog Code Generation and Design Synthesis","M. Agarwal; Z. Momin; K. Prasad; J. Mekie",Indian Institute Of Technology Gandhinagar; Indian Institute Of Technology Gandhinagar; ARM Embedded Technologies Pvt Ltd; Indian Institute Of Technology Gandhinagar,2025 IEEE International Symposium on Circuits and Systems (ISCAS),"27 Jun 2025","2025","","","1","5","In the rapidly advancing field of hardware design, Electronic Design Automation (EDA) tools can be significantly improved using Machine Learning. This study evaluates the efficacy of various Large Language Models (LLMs) for automating Electronic Design Automation for Verilog design, testbench generation, and Formal Verification (FV) assertion synthesis by comparing 3 closed-source LLMs and 14 Open-Source LLM variants. In our setup of 33 Verilog designs, ChatGPT-4 generates 22 synthesizable Verilog designs in one-shot without feedback, while the Llama 3 (8B) model generates 20. Both models generate all testbenches correctly, 9 of which are given in our setup. For generating Formal Verification properties, ChatGPT-4 generates all properties correctly, whereas Llama 3 synthesizes 7 out of 9 properties correctly. Of the sample synthesized in Vivado, ChatGPT-4 codes result into power-efficient designs as compared to Llama-3, whereas in Genus there is no clear winner. These results underscore the efficacy of open-source models, which perform competitively despite having significantly fewer parameters (8 billion) compared to closed-source models such as ChatGPT-4. This study demonstrates the potential of parameter-efficient, open-source models for hardware design and verification tasks.","2158-1525","979-8-3503-5683-0","10.1109/ISCAS56072.2025.11044004","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11044004","Large Language Models;AI-enabled Hardware Design Automation;Verilog Generation;Testbech;Formal Verification;Fine-Tuning;Few-shot Prompting","Design automation;Codes;Large language models;Machine learning;Benchmark testing;Market research;Hardware;Hardware design languages;Integrated circuit modeling;Formal verification","","","","17","IEEE","27 Jun 2025","","","IEEE","IEEE Conferences"
"Work in Progress: Unlocking Code Generation Through Synergistic Prompt Engineering","K. -H. Ho; M. Georgiades; T. -K. J. Fan; Y. Hou; K. C. K. Fong; T. -T. Chan","Department of Business Administration, Hong Kong Shue Yan University, Hong Kong SAR, China; Department of Computer Science, Intelligence Systems Laboratory (ISLab) Neapolis University Pafos, Cyprus; Department of Mathematics and Information Technology, The Education University of Hong Kong, Hong Kong SAR, China; Department of Computer Science, The Hang Seng University of Hong Kong, Hong Kong SAR, China; Division of Artificial Intelligence, School of Data Science Lingnan University, Hong Kong SAR, China; Department of Mathematics and Information Technology, The Education University of Hong Kong, Hong Kong SAR, China",2025 IEEE Engineering Education World Conference (EDUNINE),"5 May 2025","2025","","","1","4","Prompt engineering is crucial for optimizing large language models in code generation. This paper explores a synergistic prompt engineering approach that integrates complementary prompting techniques for solving programming problems. Preliminary experiments show that by leveraging the strengths of various prompting techniques, our synergistic approach significantly outperforms traditional single- prompting techniques, improving the accuracy of code generation for Python and C++ exercises. These findings suggest that our synergistic approach is a valuable tool for students, enhancing their interactions with large language models and improving AI-driven programming education.","","979-8-3315-4278-8","10.1109/EDUNINE62377.2025.10980842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10980842","Code generation;prompt engineering","Codes;Accuracy;Large language models;C++ languages;Prompt engineering;Engineering education;Programming profession;Python;Software development management","","","","13","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Research of Evaluating the Effectiveness of Large Language Models in Identifying and Correcting Code Anomalies","T. Chen","University of Liverpool, Liverpool, UK","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","11 Jul 2024","2024","","","360","363","Large language models (LLMs) are powerful tools that can generate code in various programming languages, and assist programmers in writing, debugging, or improving code. However, LLMs can also introduce or overlook code anomalies, such as errors, bugs, vulnerabilities, or deviations from the expected behavior or style of a program, due to their lack of semantic understanding, generalizability, or reliability. In this paper, we investigate the use of anomaly detection techniques to evaluate the effectiveness of LLMs, such as OpenAI Codex, in identifying and correcting code anomalies. We propose a novel methodology for evaluating the effectiveness of LLMs in identifying and correcting code anomalies, using anomaly detection techniques, and we apply it to several LLMs, such as OpenAI Codex. We present and analyze the results of the evaluation, and compare the performance of different LLMs, anomaly detection methods, and evaluation metrics. We also discuss the implications, challenges, and limitations of using LLMs for code quality and security, and suggest some directions for future research. We hope that this paper will inspire and inform researchers, developers, and users of LLMs and code quality and security, and that it will stimulate further research and innovation in this field.","","979-8-3503-8555-7","10.1109/AINIT61980.2024.10581582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581582","large language models;code generation;code assistance;code anomalies;anomaly detection","Measurement;Technological innovation;Codes;Accuracy;Large language models;Computer bugs;Data models","","","","12","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"Prompt Engineering in Business Analytics for Next-Gen Digital Banking Services","F. Qiyam; Y. Jaradat; S. AlZu’Bi","Alzaytoonah University of Jordan, Amman, Jordan; Alzaytoonah University of Jordan, Amman, Jordan; Alzaytoonah University of Jordan, Amman, Jordan",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","432","436","The digital banking industry is undergoing significant transformation due to rapid technological advancements. This study investigates the integration of Generative Artificial Intelligence (AI) and prompt engineering into business analytics to elevate next-generation digital banking services. Utilizing large language models, we present a detailed framework that combines generative AI techniques with refined prompt engineering to enhance decision-making, customer personalization, and risk management. Our results demonstrate that the generative AI model achieves higher accuracy, precision, and recall compared to baseline models, underscoring its potential to revolutionize business analytics in the financial sector.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852452","Generative AI;Prompt Engineering;Business Analytics;Digital Banking Services;Large Language Models","Analytical models;Accuracy;Computational modeling;Large language models;Decision making;Banking;Predictive models;Data models;Prompt engineering;Business","","2","","22","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Prompt Engineering in Business Analytics for Next-Gen Digital Banking Services","F. Qiyam; Y. Jaradat; S. AlZu’bi","Alzaytoonah University of Jordan, Amman, Jordan; Alzaytoonah University of Jordan, Amman, Jordan; Alzaytoonah University of Jordan, Amman, Jordan",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","168","172","The digital banking industry is undergoing significant transformation due to rapid technological advancements. This study investigates the integration of Generative Artificial Intelligence (AI) and prompt engineering into business analytics to elevate next-generation digital banking services. Utilizing large language models, we present a detailed framework that combines generative AI techniques with refined prompt engineering to enhance decision-making, customer personalization, and risk management. Our results demonstrate that the generative AI model achieves higher accuracy, precision, and recall compared to baseline models, underscoring its potential to revolutionize business analytics in the financial sector.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852470","Generative AI;Prompt Engineering;Business Analytics;Digital Banking Services;Large Language Models","Analytical models;Accuracy;Computational modeling;Large language models;Decision making;Banking;Predictive models;Data models;Prompt engineering;Business","","","","22","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs","S. Jiang; J. Zhang; W. Chen; B. Wang; J. Zhou; J. M. Zhang","Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Huawei Cloud Computing Technologies Co., Ltd., Beijing, China; King’s College London, London, United Kingdom",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","75","78","Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.CCS CONCEPTS• Software and its engineering → Search-based software engineering; Software testing and debugging.","","979-8-4007-0579-3","","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734644","Large Language Model;Fault Localization;Program Repair;Software Debugging","Location awareness;Software testing;Large language models;Computer bugs;Debugging;Maintenance engineering;Chatbots;Software;Sparks;Software engineering","","1","","23","","30 Oct 2024","","","IEEE","IEEE Conferences"
"Application of large language models in professional fields","M. Zhou; W. Chen; S. Zhu; T. Cai; J. Yu; G. Dai","Financial Technology Products Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China; Financial Technology Products Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China; Financial Technology Products Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China; Technology Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China; Technology Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China; Technology Department, China Mobile (Shanghai) Information and Communication Technology Co., Ltd., Shanghai, China",2023 11th International Conference on Information Systems and Computing Technology (ISCTech),"21 Feb 2024","2023","","","142","146","The large language model (LLM) is an emerging artificial intelligence technology. Increasingly, researchers are focusing on its application in professional fields. This paper provides a summary of the current state and progress of large language model applications in professional settings. It also analyzes the impact of prompt engineering on the generation results of large language models and explores evaluation methods for such models. Currently, LLM has found applications in various professional domains, including finance, healthcare, and education, among others. This technology not only enhances data processing efficiency but also offers higher levels of intelligence. The first chapter provides a brief introduction to the large language model, while the second chapter presents its applications in diverse fields. The third chapter discusses the influence of prompt engineering on LLM results and proposes a specialized domain-based question-answering (QA) approach. The fourth chapter covers evaluation methods for large language models. Finally, the paper concludes with a summary of future research directions and challenges related to LLM.","","979-8-3503-4240-6","10.1109/ISCTech60480.2023.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10438316","Large Language models;Prompt engineering;ChatGPT","Training;Industries;Computational modeling;Finance;Medical services;Data processing;Information systems","","5","","29","IEEE","21 Feb 2024","","","IEEE","IEEE Conferences"
"Prompt Incorporates Math Knowledge to Improve Efficiency and Quality of Large Language Models to Translate Math Word Problems","H. Wu; X. Yu","Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China; Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China",2023 International Conference on Intelligent Education and Intelligent Research (IEIR),"16 Jan 2024","2023","","","1","5","Large Language Models (LLMs) models perform excellently in normal translation tasks [1], but are not effective in translating Math Word Problems (MWPs) with LLMs. The main problems can be summarized into two categories: one is that LLMs are unable to accurately translate specific mathematical knowledge in MWPs, such as formulas and mathematical patterns. The other is that LLMs often generate redundant content unrelated to the translation of MWPs. This research aims to investigate how to define the format of prompt and incorporate mathematical knowledge to improve the efficiency and quality of translating MWPs by LLMs. Firstly, the input and output formats of prompt are defined to reduce the generation of content unrelated to the translation of MWPs. Secondly, it is found that simply defining the roles and capabilities of LLMs in a zero-shot setting cannot improve the effectiveness of LLMs in translating MWPs, and even leads to a decrease in translation quality. This paper finds that the reason for the low-quality phenomenon of LLMs in translating MWPs is due to the lack of corresponding mathematical knowledge, therefore, this paper summarizes some of the mathematical knowledge that needs to be involved in translating MWPs and integrates it into prompt, so that the effect of LLMs in translating MWPs has been significantly improved.","","979-8-3503-4289-5","10.1109/IEIR59294.2023.10391245","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391245","prompt engineering;large language models;natural language translation","Knowledge engineering;Education;Mathematical models;Task analysis","","","","16","IEEE","16 Jan 2024","","","IEEE","IEEE Conferences"
"Human Review for Post-Training Improvement of Low-Resource Language Performance in Large Language Models","D. -M. Lewis; B. DeRenzi; A. Misomali; T. Nyirenda; E. Phiri; L. Chifisi; C. Makwenda; N. Lesh","Dimagi, Inc., USA; Dimagi, Inc., USA; Dimagi, Inc., Malawi; Dimagi, Inc., USA; Pachi, Malawi; Pachi, Malawi; Pachi, Malawi; Dimagi, Inc., USA",2024 IEEE 12th International Conference on Healthcare Informatics (ICHI),"22 Aug 2024","2024","","","592","597","Large language models (LLMs) have significantly improved natural language processing, holding the potential to support health workers and their clients directly. Unfortunately, there is a substantial and variable drop in performance for low-resource languages. This paper presents an exploratory case study in Malawi, aiming to enhance the performance of LLMs in Chichewa through innovative prompt engineering techniques. By focusing on practical evaluations over traditional metrics, we assess the subjective utility of LLM outputs, prioritizing end-user satisfaction. Our findings suggest that tailored prompt engineering may improve LLM utility in underserved linguistic contexts, offering a promising avenue to bridge the language inclusivity gap in digital health interventions.","2575-2634","979-8-3503-8373-7","10.1109/ICHI61247.2024.00095","Bill & Melinda Gates Foundation(grant numbers:INV-062580); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628927","Large language models;prompt engineering;low-resource languages","Measurement;Reviews;Large language models;Buildings;Focusing;Linguistics;Chatbots","","","","24","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning","J. Lu; L. Yu; X. Li; L. Yang; C. Zuo","Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; School of Software, Tsinghua University, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Sinosoft Company Limited, Beijing, China",2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE),"2 Nov 2023","2023","","","647","658","The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.","2332-6549","979-8-3503-1594-3","10.1109/ISSRE59848.2023.00026","Science and Technology Service Network Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299938","Code Review Automation;Large Language Models (LLMs);Parameter-Efficient Fine-Tuning (PEFT);Deep Learning;LLaMA;Software Quality Assurance","Codes;Automation;Quality assurance;Software reliability;Task analysis;Tuning;Software engineering","","48","","68","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"The Practice of Differentiated Instruction Based on the Large Language Models in the Introduction to Computer Security Course","Q. Yan; Z. Lin","School of Computing and Software, Shenzhen University, Shenzhen, China; School of Computing and Software, Shenzhen University, Shenzhen, China",2024 5th International Conference on Information Science and Education (ICISE-IE),"12 Jun 2025","2024","","","463","467","Differentiated instruction has always been a goal pursued in university education, aiming to meet the personalized learning needs of diverse students. However, due to the large number of students and limited teaching resources, traditional instruction methods often struggle to achieve true differentiation. With the development of large language models (LLMs) technology, its application in the field of education offers new possibilities for implementing differentiated instruction. This paper proposes a differentiated instruction model based on LLMs, which encompasses the utilization of prompt engineering with LLMs to craft survey forms and to aid in comprehending and enhancing educational content, as well as reinforcing the accuracy of concepts through objective assessments. A detailed introduction to the practical implementation of this model through the example of the ""Introduction to Computer Security"" course is provided. Through an analysis of course teaching outcomes, it demonstrates that the introduction of LLMs can enhance students' learning experiences and improve teaching effectiveness, while also having a positive impact on cultivating students' independent learning abilities and innovative thinking.","","979-8-3315-0676-6","10.1109/ICISE-IE64355.2024.11025499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025499","Differentiated instruction;LLMs;Introduction to Computer Security","Surveys;Information science;Accuracy;Computational modeling;Large language models;Education;Prompt engineering;Computer security","","","","7","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"NetLLMBench: A Benchmark Framework for Large Language Models in Network Configuration Tasks","K. Aykurt; A. Blenk; W. Kellerer","Chair of Communication Networks, Technical University of Munich, Germany; Siemens AG, Munich, Germany; Chair of Communication Networks, Technical University of Munich, Germany",2024 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN),"27 Dec 2024","2024","","","1","6","Traditional network management techniques often struggle with the scale and dynamism of modern networks, requiring significant human oversight and being prone to high error rates. Large Language Models (LLMs) present a promising alternative to conventional approaches by automating network configuration and management. However, a systematic way to evaluate their performance is lacking in the literature.This paper introduces NetLLMBench, a novel framework designed to rigorously assess the performance of LLMs in managing computer networks. By integrating prompt engineering and network emulation in a closed loop, NetLLMBench benchmarks and validates LLMs’ responses in various configuration scenarios. The findings establish foundational benchmarks to guide future applications of LLMs in enhancing network management efficiency.","2832-2231","979-8-3503-8053-8","10.1109/NFV-SDN61811.2024.10807499","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807499","Large Language Models (LLMs);Autonomous Network Management;Benchmark","Systematics;Error analysis;Large language models;Emulation;Configuration management;Computer architecture;Benchmark testing;Network function virtualization;Prompt engineering;Software defined networking","","","","19","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Prompt Engineering Approach Study for Supervised Fine-Tuned (SFT) Large Language Models (LLMs) in Spacecraft Fault Diagnosis","Q. Xia; H. Zhao; M. Liu","School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Astronautics, Harbin Institute of Technology, Harbin, China",2024 3rd Conference on Fully Actuated System Theory and Applications (FASTA),"23 Jul 2024","2024","","","819","824","Traditional correlation and causality analyses based on telemetry data often lack depth in analyzing the multi-level coupling of faults and their emergent effects in spacecraft. Large Language Models (LLMs) offer a viable solution to this issue, necessitating meticulous research and design in prompt engineering. Against this backdrop, we have conducted a series of studies on using LLMs for diagnosing coupled faults in spacecraft. Initially, we systematically integrated a comprehensive fine-tuning dataset that includes prior expert knowledge, historical fault records, maintenance reports, and operation logs, to fine-tune the ERNIE-Bot-turbo model on the Baidu AI Cloud Qianfan platform. Subsequently, in the context of practical spacecraft fault diagnosis, we quantitatively explored the impact of different prompt engineering approaches on the fine-tuned LLMs using a specially designed multi-label spacecraft fault diagnosis test dataset. Finally, we proposed an Advanced Min Max Ant System (AMMAS) algorithm for autonomously iterating to explore the optimal prompts. AMMAS leverages the strong language generation capability of LLMs to optimize prompt text generation, using F1-score as the fitness function, aiming to autonomously iterate and optimize the prompt generation process, uncovering the most effective prompts for spacecraft fault diagnosis.","","979-8-3503-7369-1","10.1109/FASTA61401.2024.10595331","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10595331","LLMs;Prompt Engineering;Advanced Min Max Ant System Algorithm;Spacecraft Dault Diagnosis","Space vehicles;Fault diagnosis;Couplings;Correlation;Large language models;Web and internet services;Cause effect analysis","","3","","12","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Transforming Software Development: A Study on the Integration of Multi-Agent Systems and Large Language Models for Automatic Code Generation","R. Ramírez-Rueda; E. Benítez-Guerrero; C. Mezura-Godoy; E. Bárcenas","Facultad de Estadística e Informática, Universidad veracruzana, Xalapa, Mexico; Facultad de Estadística e Informática, Universidad veracruzana, Xalapa, Mexico; Facultad de Estadística e Informática, Universidad veracruzana, Xalapa, Mexico; Facultad de Ingeniería, Universidad Nacional Autónoma de México, Ciudad de México, México",2024 12th International Conference in Software Engineering Research and Innovation (CONISOFT),"18 Dec 2024","2024","","","11","20","This paper explores the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs) for auto-matic code generation, addressing the limitations of traditional manual coding. By conducting a comprehensive review of existing literature and analyzing a practical case study, we demonstrate how MAS and LLMs can collaboratively enhance software development processes. The research focuses on the technical and theoretical challenges of this integration, highlighting the potential for improved productivity, adaptability, and quality in code generation. The findings contribute to AI-based software engineering by revealing new research directions in collective intelligence and automated programming.","","979-8-3315-3211-6","10.1109/CONISOFT63288.2024.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795597","Multi-Agent;Generative AI;LLM;Software Development","Productivity;Technological innovation;Codes;Large language models;Collective intelligence;Software;Standards;Software development management;Software engineering;Multi-agent systems","","","","29","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"SECoT: Security-Oriented Chain-of-Thought Make Large Language Models Safer","M. Yu; L. Sun; X. Chang; C. Hu","School of cryptographic engineering PLA Information Engineering University, Zhengzhou, China; School of cryptographic engineering PLA Information Engineering University, Zhengzhou, China; School of cryptographic engineering PLA Information Engineering University, Zhengzhou, China; School of cryptographic engineering PLA Information Engineering University, Zhengzhou, China",2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE),"15 May 2025","2025","","","810","814","The problem of harmful content generation in LLMs arises from the varied and intricate nature of their training data. Although filtering mechanisms are employed during pre-training, completely eliminating the impact of harmful content remains challenging. Additionally, the open-ended generation nature of the model during the inference phase further complicates content control. Therefore, how to effectively constrain the model's output while preserving its generative capabilities has become a critical topic in current AI safety research. Emerging research has revealed that the Chain of Thought (CoT) prompting strategy plays a pivotal role in augmenting the problem-solving abilities of large language models (LLMs). By guiding these models to generate intermediate reasoning steps, CoT facilitates a more structured approach to addressing intricate tasks, particularly in fields like mathematics and programming. This advanced reasoning ability provides new pathways for enhancing the safety of LLMs. This research introduces a Security-Oriented Chain of Thought (SECoT) mechanism. The SECoT framework consists of three parts: analyzing the safety of the problem, reflecting on the safety strategies involved, and formulating strategies to answer the problem, ultimately providing a final safe answer based on the above analysis, safety strategies, and response strategies. The SECoT process ensures that the response is not only well-considered but also safe. Building on this, a high-quality SECoT dataset (each entry includes five components: question, safety analysis, safety strategy, response principle, and safe answer) is further constructed and used as fine-tuning data to fine-tune LLMs in an end-to-end manner. Experimental results show that the SECoT framework enhances the safety of content generated by LLMs during the inference phase. The intermediate steps of the SECoT framework help LLMs learn how to reason about safe responses, reducing the harmful output rate of the fine-tuned model to 0% and lowering the average harmfulness score from 4.75 to 2.01 (on a 5-point scale, where higher scores indicate greater harmfulness).","","979-8-3315-3311-3","10.1109/ICCECE65250.2025.10985388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10985388","Large language models;Content safety;Chain of Thought","Ethics;Large language models;Training data;Cognition;Reflection;Safety;Problem-solving;Sustainable development;Standards;Programming profession","","","","33","IEEE","15 May 2025","","","IEEE","IEEE Conferences"
"The Impact of Generative AI on Cloud Data Security: A Systematic Study of Opportunities and Challenges","H. Ruparel; H. Daftary; V. Singhai; P. Kumar","School of Computer Science, University of California, Irvine, San Jose, CA, USA; School of Computer Science, Stony Brook University, New York, Seattle, WA, USA; College of Engineering, Carnegie Mellon University, Seattle, WA, USA; Computer Science and Engineering, The Pennsylvania State University, San Jose, CA, USA",2024 IEEE/ACM 17th International Conference on Utility and Cloud Computing (UCC),"23 Apr 2025","2024","","","185","188","Since the launch of ChatGPT, Generative AI (GenAI) has transformed virtually every domain, addressing complex problems in areas previously considered insurmountable. At the same time, the adoption of cloud computing for storing confidential data has been rapidly increasing. This swift shift to the cloud has highlighted the critical need for robust data protection measures. In this paper, we merge these two emerging fields to examine how GenAI influences cloud data security. While research on traditional AI-based solutions for cyber threats has been ongoing, the recent surge in GenAI adoption calls for a focused, systematic study to address specific research questions related to GenAI. We formulate 7 research questions and analyze data from 4 major digital repositories (IEEE, Springer, ACM, Wiley) to provide an extensive overview of how GenAI contributes to mitigating key cloud security threats such as Malware, Denial-Of-Service (DoS), and Phishing, among others. We also discuss the potential risks posed by GenAI in cloud security and highlight the operational and ethical challenges that impede its widespread adoption. This systematic study aims to provide valuable insights for fellow researchers and IT professionals, guiding them in effectively leveraging GenAI to enhance cloud data security while being mindful of its limitations and challenges.","","979-8-3503-6720-1","10.1109/UCC63386.2024.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971861","Cloud Data Security;Cyber Threats;Generative AI;Artificial Intelligence;Deep Learning;Information Security;Large Language Models","Ethics;Systematics;Generative AI;Shape;Data security;Cloud computing security;Surge protection;Solids;Surges;Research and development","","","","20","IEEE","23 Apr 2025","","","IEEE","IEEE Conferences"
"Automatic Robotic Development through Collaborative Framework by Large Language Models","Z. Luan; Y. Lai; R. Huang; X. Lan; L. Chen; B. Chen; Y. Yan; S. Yu","School of Electrical Engineering Xi'an University of Technology, Xi'an, China; School of Electrical Engineering Xi'an University of Technology, Xi'an, China; School of Electrical Engineering Xi'an University of Technology, Xi'an, China; College of Artificial Intelligence Xi'an Jiaotong University, Xi'an, China; College of Artificial Intelligence Xi'an Jiaotong University, Xi'an, China; College of Artificial Intelligence Xi'an Jiaotong University, Xi'an, China; China Academy of Aerospace Science and Innovation, Bei'jing, China; China Academy of Aerospace Science and Innovation, Bei'jing, China",2023 China Automation Congress (CAC),"19 Mar 2024","2023","","","7736","7741","Despite the remarkable code generation abilities of large language models (LLMs), they still face challenges in complex task handling. Robot development, a highly intricate field, inherently demands human involvement in task allocation and collaborative teamwork[1]. To enhance robot development, we propose an innovative automated collaboration framework inspired by real-world robot developers. This framework employs multiple LLMs in distinct roles-analysts, programmers, and testers. Analysts delve deep into user requirements, enabling programmers to produce precise code, while testers fine-tune the parameters based on user feedback for practical robot application. Each LLM tackles diverse, critical tasks within the development process. Clear collaboration rules emulate real-world teamwork among LLMs. Analysts, programmers, and testers form a cohesive team overseeing strategy, code, and parameter adjustments [2]. Through this framework, we achieve complex robot development without requiring specialized knowledge, relying solely on non-experts' participation.","2688-0938","979-8-3503-0375-9","10.1109/CAC59555.2023.10450432","National Natural Science Foundation of China(grant numbers:U21A20485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450432","Large Language Models;Automatic Robot Development;Prompt Engineering;Collaborative Teamwork","Codes;Automation;Federated learning;Teamwork;Quadrupedal robots;Resource management;Task analysis","","2","","24","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Enhancing Code Generation for Dataflow Programming: Fine-Tuning Large Language Models with the DFCPP Dataset","L. Qiuming; M. Xi; W. Xuan","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Tianjin Yuyi Kaihong Intelligent Technology, Tianjin, China",2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA),"20 Feb 2025","2024","","","2246","2247","In recent years, large language models (LLMs) based on the Transformer architecture have demonstrated excellent performance in code generation, but there have been fewer studies on data flow languages. This study proposes a scheme for fine-tuning large language models based on the DFCPP dataset. We demonstrate the model's ability to generate dataflow graph (DAG) topologies and achieve significant performance improvements. Experimental results show that the BLEU score of the fine-tuned model in the DFCPP code generation task reaches 0.193, which is an increase of 112.1% compared to the non-fine-tuned model (0.091). This demonstrates the effectiveness of fine-tuning techniques in domain-specific code generation.","2158-9208","979-8-3315-0971-2","10.1109/ISPA63168.2024.00314","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10885225","Large Models;Dataflow Language;Fine-tuning;DFCPP;Code Generation","Distributed processing;Codes;Large language models;Programming;Transformers;Topology;Error correction codes;Complexity theory","","","","9","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Chinese Text Simplification Based on Large Language Models","H. Huo; Z. Fu; H. Yu; S. Yang; G. Tang","School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China; School of Information Science, Beijing Language and Culture University, Beijing, China",2024 International Conference on Computational Linguistics and Natural Language Processing (CLNLP),"12 Nov 2024","2024","","","52","56","Chinese is a complex language, creating a demand for text simplification in various contexts. Traditional text simplification models have primarily relied on pre-trained models. With the advancement of large language models (LLMs), researchers have begun leveraging their robust text comprehension and generation capabilities for text simplification tasks. However, these implementations are often simplistic, merely instructing the LLM to simplify text without addressing inherent challenges. One notable issue is the hallucination phenomenon where LLMs introduce inaccuracies, especially in fine-grained knowledge. This paper proposes an approach to interact with LLMs by embedding the text’s underlying knowledge through carefully designed prompts. These prompts provide clearer guidance to the LLMs, thereby improving the quality of text simplification. In addition, LLMs may be further improved on specific tasks after instruction fine-tuning. Thus, we constructed a text simplification training dataset with semantic knowledge from Chinese Parataxis graph, and fine-tuned existing large models specifically for the text simplification task. Experimental results demonstrate that fine-tuning LLMs can significantly enhance the quality of text simplification. Finally, we provide the proposed methods as a service via a web interface and a browser extension.","","979-8-3503-9209-8","10.1109/CLNLP64123.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10743168","text simplification;Large Language Models;prompt engineering;Chinese parataxis graph","Training;Large language models;Computational modeling;Semantics;Natural language processing;Computational linguistics;Browsers;Prompt engineering","","","","14","IEEE","12 Nov 2024","","","IEEE","IEEE Conferences"
"Generative-AI in E-Commerce: Use-Cases and Implementations","S. Ghaffari; B. Yousefimehr; M. Ghatee","Dept. of Math. & Computer Sci. Amirkabir, University of Technology, Tehran, Iran; Dept. of Math. & Computer Sci. Amirkabir, University of Technology, Tehran, Iran; Dept. of Math. & Computer Sci. Amirkabir, University of Technology, Tehran, Iran",2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP),"25 Mar 2024","2024","","","1","5","In recent years, the advancement of generative AI has profoundly influenced its application across various industries. One such industry is e-commerce where this technology can enhance both customer experience as well as merchants’ productivity and profitability. In this paper, our objective is to review some of the potential use cases of generative AI technology in different areas of an online store. Specifically, we will focus on the use cases of product description generation, sentiment analysis of product reviews, and product tagging and categorization. We utilize various prompt engineering techniques to suggest exemplary implementations of these applications using large language models (LLMs). Lastly, the paper will discuss the possible risks and challenges that come with using generative AI in these contexts.","2640-5768","979-8-3503-8394-2","10.1109/AISP61396.2024.10475266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475266","product description;taxonomy;generative AI;large language models (LLMs);e-commerce;product reviews","Productivity;Industries;Sentiment analysis;Ethics;Reviews;Profitability;Tagging","","5","","15","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"Financial Sentiment Analysis on News and Reports Using Large Language Models and FinBERT","Y. Shen; P. K. Zhang","Zhejiang University, Hangzhou, Zhejiang, China; Lehigh University, Bethlehem, Pennsylvania, USA","2024 IEEE 6th International Conference on Power, Intelligent Computing and Systems (ICPICS)","24 Dec 2024","2024","","","717","721","Financial sentiment analysis (FSA) is crucial for evaluating market sentiment and making well-informed financial decisions. The advent of large language models (LLMs) such as BERT and its financial variant, FinBERT, has notably enhanced sentiment analysis capabilities. This paper investigates the application of LLMs and FinBERT for FSA, comparing their performance on news articles, financial reports and company announcements. The study emphasizes the advantages of prompt engineering with zero-shot and few-shot strategy to improve sentiment classification accuracy. Experimental results indicate that GPT -40, with few-shot examples of financial texts, can be as competent as a well fine-tuned FinBERT in this specialized field.","2834-8567","979-8-3503-7431-5","10.1109/ICPICS62053.2024.10796670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796670","Sentiment Analysis;BERT;NLP;Large Language Models;Prompt Engineering;FinBERT","Sentiment analysis;Analytical models;Accuracy;Fluctuations;Large language models;Computational modeling;Zero shot learning;Real-time systems;Prompt engineering;Context modeling","","11","","16","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"The Discovery and Solution of the Pseudo-Reasoning Issue for Constructing Cost-Effective Multi-Agent Frameworks in Large Language Models","J. Y. Ng; S. -Y. Liew; C. H. Chng; T. W. Teo; Z. Z. Khoo","Department of Computer Science, Faculty of Information Communication & Techonology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Science, Faculty of Information Communication & Techonology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Science, Faculty of Information Communication & Techonology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Science, Faculty of Information Communication & Techonology, Universiti Tunku Abdul Rahman, Kampar, Malaysia; Department of Computer Science, Faculty of Information Communication & Techonology, Universiti Tunku Abdul Rahman, Kampar, Malaysia",2024 5th International Conference on Artificial Intelligence and Data Sciences (AiDAS),"30 Oct 2024","2024","","","1","5","Large Language Models (LLMs) show exceptional logical reasoning capabilities. Despite these advancements, a significant performance and cost gap remains between smaller and larger models. Various prompting techniques have been developed to enhance LLM performance, but the performance gap remains unresolved. Additionally, multi-agent frameworks have also been introduced, however, there is often insufficient emphasis on token consumption, which impacts cost and time efficiency. This paper presents a comparative analysis of different LLMs and prompting strategies using the Counter-Intuitive AR dataset. Our findings reveal that sophisticated prompt engineering methods often result in diminishing returns and are less cost-effective than simply using larger models. This phenomenon, which we term the “pseudo-reasoning issue”, occurs when models appear to demonstrate genuine reasoning but rely on past learned patterns. We show that GPT-4o, even without additional prompting strategies, achieves 76% accuracy, surpassing all the tested methods in enhancing GPT-3.5- Turbo performance. Conversely, a heterogeneous approach, which combines large and small LLMs in solving complex problems is feasible in terms of cost-effectiveness. Our linear graph shows that increased involvement of GPT-4o in text generation correlates with higher model accuracy. These findings challenge the current trend towards increasingly complex strategies and suggest the crucial need for evaluating cost-effectiveness when designing novel prompting strategies.","","979-8-3315-2855-3","10.1109/AiDAS63860.2024.10730162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10730162","Large Language Models;Multi-Agent;Generative AI;Prompting;Cost-Effectiveness","Costs;Accuracy;Limiting;Large language models;Focusing;Market research;Cognition;Robustness;Data models;Prompt engineering","","","","7","IEEE","30 Oct 2024","","","IEEE","IEEE Conferences"
"When Software Security Meets Large Language Models: A Survey","X. Zhu; W. Zhou; Q. -L. Han; W. Ma; S. Wen; Y. Xiang","School of Computer and Mathematical Sciences, University of Adelaide, Adelaide, South Australia, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia",IEEE/CAA Journal of Automatica Sinica,"20 Jan 2025","2025","12","2","317","334","Software security poses substantial risks to our society because software has become part of our life. Numerous techniques have been proposed to resolve or mitigate the impact of software security issues. Among them, software testing and analysis are two of the critical methods, which significantly benefit from the advancements in deep learning technologies. Due to the successful use of deep learning in software security, recently, researchers have explored the potential of using large language models (LLMs) in this area. In this paper, we systematically review the results focusing on LLMs in software security. We analyze the topics of fuzzing, unit test, program repair, bug reproduction, data-driven bug detection, and bug triage. We deconstruct these techniques into several stages and analyze how LLMs can be used in the stages. We also discuss the future directions of using LLMs in software security, including the future directions for the existing use of LLMs and extensions from conventional deep learning research.","2329-9274","","10.1109/JAS.2024.124971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846956","Large language models (LLMs);software analysis;software security;software testing","Deep learning;Surveys;Reviews;Large language models;Computer bugs;Focusing;Maintenance engineering;Fuzzing;Software;Security","","16","","131","","20 Jan 2025","","","IEEE","IEEE Journals"
"The Feasibility of Applying Prompt Engineering Based on Large Language Models in the Public Domain","Y. Gao; L. Xiao; Y. Shen; R. Chen","College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China","2025 11th International Symposium on System Security, Safety, and Reliability (ISSSR)","9 Jun 2025","2025","","","467","468","With the rapid development of Large Language Models (LLMs), they have demonstrated powerful capabilities in fields such as natural language processing (NLP) and data mining. Currently, we are in an era of data explosion, with various types of data emerging in vast quantities. Data processing methods need to be constantly updated to adapt to the development of the times. This article aims to explore and focus on the existing LLMs, continuously attempting to change the prompt words from different angles, such as tone and providing context information, etc., to achieve different output effects.","2835-2823","979-8-3315-0124-2","10.1109/ISSSR65654.2025.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022621","Large Language Model;prompt engineering","Data analysis;Large language models;Reliability engineering;Natural language processing;Explosions;Safety;Prompt engineering;Security;Data mining","","","","5","IEEE","9 Jun 2025","","","IEEE","IEEE Conferences"
"Synthetic Data Generation Using Large Language Models: Advances in Text and Code","M. Nadǎş; L. Dioşan; A. Tomescu","Department of Computer Science, Faculty of Mathematics and Computer Science, Babeş-Bolyai University, Cluj-Napoca, Romania; Department of Computer Science, Faculty of Mathematics and Computer Science, Babeş-Bolyai University, Cluj-Napoca, Romania; KlusAI Laboratories, Cluj-Napoca, Romania",IEEE Access,"1 Aug 2025","2025","13","","134615","134633","This survey reviews how large language models (LLMs) are transforming synthetic training data generation in both natural language and code domains. By producing artificial but task-relevant examples, these models can significantly augment or even substitute for real-world datasets, particularly in scenarios where labeled data is scarce, expensive, or sensitive. This paper surveys recent advances in leveraging LLMs to create synthetic text and code, highlighting key techniques such as prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We examine how these methods can enrich low-resource tasks (e.g. classification, question answering) and facilitate code-centric applications (e.g. instruction tuning, code translation, bug repair) through automated verification of functional correctness. Alongside potential benefits—cost-effectiveness, broad coverage, and controllable diversity—we discuss the accompanying challenges, including factual inaccuracies in generated text, insufficient stylistic or distributional realism, and risks of bias amplification. Proposed mitigation strategies range from filtering and weighting synthetic outputs to reinforcement learning with execution feedback in code domains. We conclude by outlining open research directions, such as automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, underscoring the growing importance of LLM-generated synthetic data in accelerating AI development while emphasizing ethical and quality safeguards.","2169-3536","","10.1109/ACCESS.2025.3589503","project “Romanian Hub for Artificial Intelligence - HRIA,” Smart Growth, Digitization and Financial Instruments Program 2021–2027, MySMIS(grant numbers:334906); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11080380","Synthetic data generation;large language models (LLMs);text data augmentation;code data synthesis;prompt engineering;instruction tuning;machine learning training data;natural language processing (NLP);code generation;reinforcement learning for code;automated data annotation;bias and fairness in synthetic data;retrieval-augmented generation (RAG);evaluation of synthetic data;model collapse in LLMs","Codes;Synthetic data;Surveys;Data models;Translation;Reviews;Tuning;Natural language processing;Large language models;Training","","1","","64","CCBY","15 Jul 2025","","","IEEE","IEEE Journals"
"Stoic Wisdom Meets Modern AI: Leveraging Large Language Models for Philosophical Guidance","L. Samantaray; S. S. Rautaray; M. Pandey; S. Dalai; P. A. Patro; A. Singh","dept.Computer science and enigineering, KIIT Polytechnic, Bhubaneswar, India; dept.Computer science and enigineering, KIIT Deemed to be University, Bhubaneswar, INDIA; dept.Computer science and enigineering, KIIT Deemed to be University, Bhubaneswar, INDIA; dept.Computer science and enigineering, KIIT Polytechnic, Bhubaneswar, India; dept.Computer science and enigineering, KIIT Deemed to be University, Bhubaneswar, INDIA; dept.Computer science and enigineering, KIIT Deemed to be University, Bhubaneswar, INDIA","2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","24 Dec 2024","2024","","","1387","1390","This research study introduces a novel platform that merges the ancient wisdom of Stoicism with contemporary artificial intelligence to offer personalized philosophical guidance. By harnessing the power of Large Language Models (LLMs) and employing Prompt Engineering techniques, the platform generates customized Stoic advice in response to user inquiries. Utilizing the advanced Generative Pre-trained Transformer (GPT) architecture, the system delivers contextually relevant insights and actionable steps, making Stoic principles more accessible and practical for modern users. This fusion of cutting-edge technology and timeless wisdom provides a unique resource for individuals seeking to apply Stoic teachings in their daily lives, thereby addressing the challenges of personalizing and implementing philosophical concepts in a contemporary context.","","979-8-3503-6790-4","10.1109/ICECA63461.2024.10801036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801036","Stoicism;Large Language Models;Prompt Engineering;Generative Pre-trained Transformers;Philosophy","Philosophical considerations;Generative Pre-trainer transformer;Computational modeling;Memory management;Knowledge based systems;Education;Oral communication;Aerospace electronics;Transformers;Prompt engineering","","","","20","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"Utilizing Large Language Models for Psychological Assessment: Enhancing Suicide Risk Detection Through Social Media Analysis","Z. Xu; J. Xu; Y. Luo; K. Zhang; J. Zhang; Y. Zou; L. Liu","School of Computer Science and Engineering, Faculty of Innovation Engineering, Macau University of Science and Technology, Avenida Wai Long, Taipa, Macao Special Administrative Region of China, China; School of International Studies, Guangdong University of Education, Guangzhou, Guangdong, China; Second Clinical Medical College, Guangdong Medical University, Dongguan, Guangdong, China; Second Clinical Medical College, Guangdong Medical University, Dongguan, Guangdong, China; Basic Medical School, Hebei Medical University, Shijiazhuang, Hebei, China; School of Computer Science and Engineering, Faculty of Innovation Engineering, Macau University of Science and Technology, Avenida Wai Long, Taipa, Macao Special Administrative Region of China, China; School of Television, Sichuan Film and Television University, Chengdu, Sichuan, China",2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC),"13 Mar 2025","2024","","","1418","1421","Mental health issues are increasingly prevalent due to the fast-paced lifestyle and rising work stress. Traditional psychological assessments face limitations due to resource scarcity and inefficiency. This paper presents a novel system using large language models (LLMs) to detect suicidal tendencies through social media analysis. Our system integrates prompt engineering and Retrieval-Augmented Generation (RAG) techniques to enhance detection accuracy. The architecture includes a knowledge retrieval-enhanced component and a judgment component, utilizing a fine-tuned LLM. Experiments with a Kaggle dataset demonstrate high performance, with consistent accuracy, precision, recall, and F1-Score across multiple runs. This system offers a promising solution to improve early psychological assessment and intervention, addressing the mental health resource gap. Future work aims to optimize the model and explore broader applications in psychological research.","","979-8-3315-4175-0","10.1109/ICFTIC64248.2024.10913098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10913098","Large language Models;Prompt Engineering;Retrieval-Augmented Generation;Sentiment Analysis;Suicide Detection;Music Therapy","Training;Sentiment analysis;Accuracy;Social networking (online);Large language models;Retrieval augmented generation;Mental health;Real-time systems;Prompt engineering;Monitoring","","","","18","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Arithmetic-Aware Question-Answering on Tabular Data Using a Large Language Model-Based Code Generation Agent","Á. Pándy; R. Lakatos; A. Hajdu","Dept. of Data Science & Visualization, Faculty of Informatics, University of Debrecen, Debrecen, Hungary; Dept. of Data Science & Visualization, Faculty of Informatics, University of Debrecen, Debrecen, Hungary; Dept. of Data Science & Visualization, Faculty of Informatics, University of Debrecen, Debrecen, Hungary",2025 IEEE 29th International Conference on Intelligent Engineering Systems (INES),"17 Jul 2025","2025","","","000083","000088","Large language models excel in language understanding but struggle with precise numerical reasoning, making accurate tabular data question-answering challenging, especially with arithmetic operations. This study proposes an arithmetic-aware question-answering agent that enhances the accuracy of tabular question-answering by integrating code generation and knowledge injection techniques. We employ structured prompt engineering, table preprocessing, and dynamic function synthesis to improve numerical computations without applying additional model fine-tuning. Our results show that a native large language model-based approach achieves only 29% exact match accuracy, while our code generation agent reaches 58%. If we further exclude scale mismatches, the accuracy improves to 80%, which also shows an increase in arithmetic precision.","1543-9259","979-8-3315-9771-9","10.1109/INES67149.2025.11078188","Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11078188","generative large language models;code generation agent;tabular data;prompt engineering;arithmetic calculation","Accuracy;Codes;Computational modeling;Large language models;Cognition;Robustness;Numerical models;Prompt engineering;Noise measurement;Arithmetic","","","","15","IEEE","17 Jul 2025","","","IEEE","IEEE Conferences"
"Optimizing LLMs for Code Generation: Which Hyperparameter Settings Yield the Best Results?","C. Arora; A. I. Sayeed; S. Licorish; F. Wang; C. Treude","Monash University, Melbourne, Australia; Monash University, Melbourne, Australia; University of Otago, Dunedin, New Zealand; Monash University, Melbourne, Australia; Singapore Management University, Singapore",2024 31st Asia-Pacific Software Engineering Conference (APSEC),"25 Apr 2025","2024","","","281","290","Large Language Models (LLMs), such as GPT models, are increasingly used in software engineering for various tasks, such as code generation, requirements management, and debugging. While automating these tasks has garnered significant attention, a systematic study on the impact of varying hyperparameters on code generation outcomes remains unexplored. This study aims to assess LLMs' code generation performance by exhaustively exploring the impact of various hyperparameters. Hyperparameters for LLMs are adjustable settings that affect the model's behaviour and performance. Specifically, we investigated how changes to the hyperparameters-temperature, top probability (top_p), frequency penalty, and presence penalty-affect code generation outcomes. We systematically adjusted all hyperparameters together, exploring every possible combination by making small increments to each hyperparameter at a time. This exhaustive approach was applied to 13 Python code generation tasks, yielding one of four outcomes for each hyperparameter combination: no output from the LLM, non-executable code, code that fails unit tests, or correct and functional code. We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome. Using correlation coefficient and regression tree analyses, we ascertained which hyperparameters influence which aspect of the LLM. Our results indicate that optimal performance is achieved with a temperature below 0.5, top probability below 0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1. We make our dataset and results available to facilitate replication.","2640-0715","979-8-3315-3401-1","10.1109/APSEC65559.2024.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967317","Large Language Models;LLMs;Generative AI;Software Engineering;Code Generation;Hyperparameter Analysis;LLM Temperature;Empirical Study","Temperature distribution;Codes;Systematics;Requirements management;Large language models;Tuning;Regression tree analysis;Python;Software engineering;Software development management","","","","42","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Autorepairability of ChatGPT and Gemini: A Comparative Study","C. Sriwilailak; Y. Higo; P. Lapvikai; C. Ragkhitwetsagul; M. Choetkiertikul","Faculty of Information and Communication Technology (ICT), Mahidol University, Nakhon Pathom, Thailand; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan; Faculty of Information and Communication Technology (ICT), Mahidol University, Nakhon Pathom, Thailand; Faculty of Information and Communication Technology (ICT), Mahidol University, Nakhon Pathom, Thailand; Faculty of Information and Communication Technology (ICT), Mahidol University, Nakhon Pathom, Thailand",2024 31st Asia-Pacific Software Engineering Conference (APSEC),"25 Apr 2025","2024","","","442","446","In recent years, Automated Program Repair (APR), which focuses on automatically fixing source code without human intervention, has become a hot topic in the field of software engineering, leading to the proposal of various automatic repair techniques. Additionally, Lapvikai et al. introduced a new software quality metric called “Autorepairability.” Autorepairability is a metric that indicates how easily bugs in the target source code can be fixed using APR techniques. By utilizing Autorepairability, it becomes possible to pre-check whether the program repair techniques will work effectively on the target software and to perform refactoring to improve Autorepairability. However, in the past two to three years, program repair using large language models (LLMs) has become more prevalent, and several studies have revealed that these models exhibit superior repair capabilities compared to traditional APR techniques. In this study, we applied Autorepairability to compare the performance of multiple APR techniques. Specifically, we measured and compared Autorepairability using ChatGPT and Gemini, which are representative large language models, as well as kGenProg, a traditional APR technique. The results demonstrated that Gemini exhibited higher repair capabilities compared to both ChatGPT and the traditional APR technique kGenProg. The five code functionalities that Gemini offers higher Autorepairability scores than ChatGPT include (1) geographic and mathematic operations, (2) validation, comparison, and searching operations, (3) data conversion operations, (4) data extraction and comparison operations, and (5) encoding operations.","2640-0715","979-8-3315-3401-1","10.1109/APSEC65559.2024.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967298","Automated program repair;Large language models;ChatGPT;Gemini","Measurement;Large language models;Source coding;Software quality;Maintenance engineering;Chatbots;Mathematics;Proposals;Data mining;Software engineering","","","","12","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests","J. E. Naulty; E. Chen; J. Wang; G. Digkas; K. Chalkias","Mysten Labs, Palo Alto, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Mysten Labs, Palo Alto, CA, USA; Mysten Labs, Palo Alto, CA, USA; Mysten Labs, Palo Alto, CA, USA",2025 IEEE Conference on Artificial Intelligence (CAI),"7 Jul 2025","2025","","","613","616","As software systems grow increasingly complex, ensuring security during development poses significant challenges. Traditional manual code audits are often expensive, time-intensive, and ill-suited for fast-paced workflows, while automated tools frequently suffer from high false-positive rates, limiting their reliability. To address these issues, we introduce Bugdar, an AI-augmented code review system that integrates seamlessly into GitHub pull requests, providing near real-time, context-aware vulnerability analysis. Bugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval Augmented Generation (RAGs) to deliver project-specific, actionable feedback that aligns with each codebase's unique requirements and developer practices. Supporting multiple programming languages, including Solidity, Move, Rust, and Python, Bugdar demonstrates exceptional efficiency, processing an average of 56.4 seconds per pull request or 30 lines of code per second. This is significantly faster than manual reviews, which could take hours per pull request. By facilitating a proactive approach to secure coding, Bugdar reduces the reliance on manual reviews, accelerates development cycles, and enhances the security posture of software systems without compromising productivity.","","979-8-3315-2400-5","10.1109/CAI64502.2025.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050647","Artificial Intelligence;Secure Code Review;GitHub Integration;CI/CD;Vulnerability Analysis;Cybersecurity;Blockchain;Web3;Security;Large Language Models","Codes;Reviews;Large language models;Retrieval augmented generation;Manuals;Software systems;Real-time systems;Software reliability;Software development management;Python","","","","14","IEEE","7 Jul 2025","","","IEEE","IEEE Conferences"
"A Simple Guide to Retrieval Augmented Generation","A. Kimothi",Manning Publications,A Simple Guide to Retrieval Augmented Generation,"","2025","","","","","Everything you need to know about Retrieval Augmented Generation in one human-friendly guide. Retrieval Augmented Generation—or RAG—enhances an LLM’s available data by adding context from an external knowledge base, so it can answer accurately about proprietary content, recent information, and even live conversations. RAG is powerful, and with A Simple Guide to Retrieval Augmented Generation, it’s also easy to understand and implement! In A Simple Guide to Retrieval Augmented Generation you’ll learn:  The components of a RAG system How to create a RAG knowledge base The indexing and generation pipeline Evaluating a RAG system Advanced RAG strategies RAG tools, technologies, and frameworks  A Simple Guide to Retrieval Augmented Generation gives an easy, yet comprehensive, introduction to RAG for AI beginners. You’ll go from basic RAG that uses indexing and generation pipelines, to modular RAG and multimodal data from images, spreadsheets, and more.","","9781633435858","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11079739.pdf&bkn=11079738&pdfType=book","retrieval augmented generation;RAG guide;generative AI;LLMs;large language models;knowledge base;vector databases;AI pipelines;prompt engineering;RAG evaluation;AI hallucination reduction;RAG tools;LangChain;OpenAI;Transformers;Python AI","","","","","","","14 Jul 2025","","","Manning","Manning eBooks"
"Exploring Large Language Models for Verilog Hardware Design Generation","E. H. D'Hollander; E. Danneels; K. -B. Decorte; S. Loobuyck; A. Vanheule; I. Van Kets; D. Stroobandt","Department of Electronics and Information Systems, Ghent University, Ghent, Belgium; School of Computer Science Engineering, Ghent University, Ghent, Belgium; School of Computer Science Engineering, Ghent University, Ghent, Belgium; School of Computer Science Engineering, Ghent University, Ghent, Belgium; School of Computer Science Engineering, Ghent University, Ghent, Belgium; School of Computer Science Engineering, Ghent University, Ghent, Belgium; Department of Electronics and Information Systems, Ghent University, Ghent, Belgium",2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"26 Jul 2024","2024","","","111","115","Deep Learning Large Language Models (LLMs) have the potential to automate and simplify code writing tasks. One of the emerging applications of LLMs is hardware design, where natural language interaction can be used to generate, annotate, and correct code in a Hardware Description Language (HDL), such as Verilog. This work provides an overview of the current state of using LLMs to generate Verilog code, highlighting their capabilities, accuracy, and techniques to improve the design quality. It also reviews the existing benchmarks to evaluate the correctness and quality of generated HDL code, enabling a fair comparison of different models and strategies.","","979-8-3503-6460-6","10.1109/IPDPSW63119.2024.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596478","large language models;Verilog;AI;fine-tuning;prompt engineering;pass@k metric","Training;Codes;Accuracy;Reviews;Large language models;Natural languages;Hardware","","1","","13","IEEE","26 Jul 2024","","","IEEE","IEEE Conferences"
"Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing","R. Santos; I. Santos; C. Magalhaes; R. de Souza Santos","UNINASSAU, Triunfo, PE, Brazil; Northern Arizona University, Flagstaff, AZ, US; UFRPE, Recife, PE, Brazil; University of Calgary, Calgary, AB, Canada","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","27 Aug 2024","2024","","","353","360","A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.","2159-4848","979-8-3503-0818-1","10.1109/ICST60714.2024.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638556","software testing;survey;test engineers;large language models;LLMs","Software testing;Codes;Automation;Large language models;Manuals;Debugging;Software","","13","","34","IEEE","27 Aug 2024","","","IEEE","IEEE Conferences"
"Coding Agents: A Comprehensive Survey of Automated Bug Fixing Systems and Benchmarks","M. Puvvadi; S. K. Arava; A. Santoria; S. S. P. Chennupati; H. V. Puvvadi","Adobe Systems, Head of Inference Platform, San Jose, USA; Adobe Systems, Senior Machine Learning Manager, San Jose, USA; Dept. of Electrical Engineering, IIT Mandi, Mandi, India; Rippling Senior Software Engineer, San Jose, USA; Shell India Markets Pvt. Ltd, Associate QA Specialist, Bangalore, India",2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT),"23 Apr 2025","2025","","","680","686","One of the trickiest problems in software engineering is automating software issue fixes, which calls for a thorough comprehension of contextual relationships, code semantics, and dynamic debugging techniques. The development of automatic program repair (APR) is examined in this survey, which traces a path from early template and constraint-based approaches to more recent developments powered by large language models (LLMs). Three main paradigms are compared here: retrieval-augmented approaches that integrate external knowledge sources, agent-based systems that use multi-agent frameworks, and agentless systems that use simplified repair pipelines. Real-world benchmarks that mimic actual engineering workflows and repository-level difficulties, such as SWE-bench, CODEAGENT-BENCH, and CodeRAG-Bench, are used to assess these cutting-edge technologies. This study demonstrates how agentic, agentless, and retrieval-augmented systems use LLMs to achieve previously unheard-of precision and scalability by following the shift from localized, single-file solutions to solving complicated, multi-file, and repository-wide difficulties. According to our findings, while complex agent architectures have potential, straightforward test-time scaling frequently produces better outcomes, especially when paired with containerized environments that allow for parallel exploration. Additionally, the survey looks at industrial applications, emphasizing effective connections with quality assurance and DevOps procedures. In order to further the development of more resilient and flexible APR frameworks that blend in perfectly with contemporary software engineering practices, we conclude by highlighting important issues in context handling and validation and suggesting future research directions in improved contextual models, human-AI collaboration, and multi-modal debugging systems.","2473-5655","979-8-3315-3193-5","10.1109/CSNT64827.2025.10968728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10968728","Agent-Based Models;Automated Program Repair;Large Language Models;Multi-Agent Systems;Context-Aware Debugging;Software Engineering Automation;Debugging Benchmarks;AI in Software Development","Surveys;Large language models;Scalability;Semantics;Debugging;Maintenance engineering;Benchmark testing;Software;Software engineering;Context modeling","","","","19","IEEE","23 Apr 2025","","","IEEE","IEEE Conferences"
"Enhancing Autonomous Task Execution in Social Robots with Large Language Models","L. R. Becerra; J. R. Colmenares; R. Manrique","Department of Systems and Computing Engineering, Universidad de los Andes, Colombia, Bogotá, Colombia; Department of Systems and Computing Engineering, Universidad de los Andes, Colombia, Bogotá, Colombia; Department of Systems and Computing Engineering, Universidad de los Andes, Colombia, Bogotá, Colombia","2024 10th International Conference on Automation, Robotics and Applications (ICARA)","18 Jun 2024","2024","","","40","44","The field of Social Robotics focuses on developing autonomous robots that can interact socially and assist with various tasks. However, the design and execution of such robots are complex they need to understand their environment and accurately interpret human-level instructions, and a common issue is the mismatch between the instructions inputted and the robot's actual performance in specific contexts. This study therefore proposes to enhance a Pepper robot's autonomy and natural interaction by enhanced instructions via Large Language Models (LLMs). Our study involves evaluating commercial and open-source LLMs' performance and comparing different prompting strategies. Both automatic and human evaluations were carried out and showed the potential and limitations of an approach guided by LLMs. Results showed that 55 percent of tasks generated passed the automatic runtime execution assessment, and GPT -4 achieved the highest success rate.","2767-7745","979-8-3503-9424-5","10.1109/ICARA60736.2024.10552978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552978","Social Robotics;Natural Language Processing;Autonomous Robots;Human-Robot Interaction;Large Language Models;Prompting Strategies;Pepper Robot;Code Generation","Codes;Runtime;Automation;Natural languages;Social robots;Data models;Task analysis","","1","","15","IEEE","18 Jun 2024","","","IEEE","IEEE Conferences"
"Leveraging Open Source Large Language Models to generate datasets from existing field-specific texts","C. Trăistaru; F. Pop; C. Bădică; D. Ciochiu; M. Bădoi; G. -C. Nedianu","University Politehnica of Bucharest University of Craiova, Craiova, Romania; National Institute of Research and Development in Informatics (ICI) Academy of Romanian Scientists, University Politehnica of Bucharest, Bucharest, Romania; Computer and Information Technology Department, University of Craiova, Craiova, Romania; Software Development Department, NetDania SRL, Craiova, Romania; Software Development Department, NetDania SRL, Craiova, Romania; Software Development Department, NetDania SRL, Craiova, Romania",2024 International Conference on INnovations in Intelligent SysTems and Applications (INISTA),"24 Sep 2024","2024","","","1","5","Network engineers are essential to the management of computer systems, as they configure services and devices on the network to guarantee effective data routing and improve computer networks which make easier to perform everyday tasks like sending and receiving emails as well as more sophisticated ones like cloud computing and online gaming. In addition, modern networks have to serve a rising number of IoT devices without compromising performance and handle more sophisticated cybersecurity threats. Network engineers must always be learning and adapting to new protocols and technologies if they are to successfully tackle these difficulties. Moreover, the introduction of Large Language Models (LLMs) that are available as open-source software has revolutionized technical innovation by enabling the automation of network setups and augmenting the capabilities of network administration. These developments represent an important step forward in the field of network engineering, with the goal of maximizing efficiency and guaranteeing strong network security and functioning.","2768-7295","979-8-3503-6813-0","10.1109/INISTA62901.2024.10683817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10683817","Computer networks;Network engineers;Continuous learning;Open-source;Large Language Models (LLMs);Network configurations;Network management","Performance evaluation;Technological innovation;Protocols;Large language models;Network security;Routing;Internet of Things","","","","14","IEEE","24 Sep 2024","","","IEEE","IEEE Conferences"
"Prompt engineering and its implications on the energy consumption of Large Language Models","R. Rubei; A. Moussaid; C. Di Sipio; D. Di Ruscio","University of L’Aquila, L’Aquila, Italy; University of L’Aquila, L’Aquila, Italy; University of L’Aquila, L’Aquila, Italy; University of L’Aquila, L’Aquila, Italy",2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software (GREENS),"19 Jun 2025","2025","","","60","67","Reducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs’ energy consumption during the inference phase without compromising performance, paving the way for further investigations.","2473-1161","979-8-3315-3815-6","10.1109/GREENS66463.2025.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11039291","LLMs;Generative AI;Prompt Engineering;Energy Consumption","Energy consumption;Data centers;Codes;Large language models;Conferences;Green products;Carbon dioxide;Software systems;Prompt engineering;Software engineering","","","","44","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Exploring the Potential of LLMs and Attributed Prompt Engineering for Efficient Text Generation and Labeling","W. Alsakran; R. Alabduljabbar","Information Technology Department, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Information Technology Department, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","244","252","In the realm of Generative AI and natural language processing (NLP), the demand for labeled textual data has increased due to the need for vast amounts of data by machine learning (ML) models. Additionally, obtaining and collecting such data has become increasingly challenging due to data scarcity, such as limited training data, or privacy concerns, such as those involving medical or personal data. Consequently, there is a compelling need for advanced systems capable of generating and labeling high-quality textual data to bridge this gap in various NLP applications, including machine translation, and content generation.In our work, we aim to develop a web-based platform to automate textual data generation and labeling by leveraging large language models (LLMs), specifically Llama 3. The proposed system is designed to automate, accelerate, and streamline data generation and labeling processes accurately and efficiently for users, minimizing dependence on expert annotators. To achieve this, the system will integrate prompt engineering techniques, including zero-shot learning, few-shot learning, role-playing, and chain-of-thought.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852475","Large Language Models (LLMs);LLAMA 3;data generation;data labeling;classification;Generative AI;Machine Learning (ML);Natural Language Processing (NLP);prompt engineering;Few-shot learning;Zero-shot learning","Sentiment analysis;Large language models;System performance;Zero shot learning;Training data;Data collection;User interfaces;Labeling;Prompt engineering;Testing","","1","","45","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"A Self-Iteration Code Generation Method Based on Large Language Models","T. Chang; S. Chen; G. Fan; Z. Feng","College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China",2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS),"26 Mar 2024","2023","","","275","281","Although large language models (LLMs) have demonstrated impressive performance in code generation, they still face challenges when dealing with complex code generation tasks. In the software development process, humans often refine complex tasks iteratively and continuously modify and improve them. Inspired by this, we propose a self-iteration code generation framework based on large language models like ChatGPT. To realize this idea, we incorporated software development methodologies into the self-iteration framework. We introduced four roles into each cycle, including analyst, designer, developer, and tester. Each role performs different tasks during the self-iteration cycle, with analyst and designer continuously improving requirements analysis and task design based on the testing feedback provided by tester, while developer are responsible for refactoring or modifying code until it passes testing, concluding the entire self-iteration process. We conducted extensive experiments on multiple benchmarks. The experimental results indicate: (1) The code generated by the self-iteration framework achieves up to a 21.3% relative improvement in Pass@1 compared to direct code generation. (2) The self-iteration framework also exhibits strong generalization performance, enhancing code generation quality for different large language models.""Failure is simply the opportunity to begin again, this time more intelligently.""- Henry Ford","2690-5965","979-8-3503-3071-7","10.1109/ICPADS60453.2023.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476069","Iteration;Code generation;Large language models","Codes;Benchmark testing;Chatbots;Software;Task analysis;Faces","","1","","27","IEEE","26 Mar 2024","","","IEEE","IEEE Conferences"
"Template-Guided Program Repair in the Era of Large Language Models","K. Huang; J. Zhang; X. Meng; Y. Liu","Technical University of Munich, Germany; Nanyang Technological University, Singapore; Beihang University, China; Nanyang Technological University, Singapore",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1895","1907","Recent advancements in automated program repair (APR) have been significantly driven by the application of Large Language Models (LLMs). In particular, the integration of LLMs with traditional template-based repair methods has demonstrated effective outcomes. Despite this, the synergy between the strengths of traditional methods and LLMs remains underexploited. This oversight originates from the indiscriminate use of templates and their insufficient coverage. Also, using small-scale LLMs within the zero-shot learning context proves to be suboptimal. To alleviate the limitations, we propose NTR (Neural Template Repair), a two-stage repair framework including template selection and patch generation, both of which are under the fine-tuning paradigm. In the template selection phase, we formulate it as a multiclass classification problem and fine-tune million-level LLMs for better selecting possible templates. During the patch generation phase, we leverage the chosen templates as probable directions (e.g., ‘Mutate Conditional Expression’) to guide the fine-tuning process of LLMs at the billion-level scale for precise patch creation. Moreover, we incorporate a unique template to signify the absence of a suitable template and employ a probability-based prioritization of templates, thereby optimizing patch generation. This framework not only effectively addresses template mismatch issues, but also enables the billion-level LLMs to explore the patch space more efficiently, despite the GPU memory constraints. We evaluate NTR with different foundational models on Defects4J V1.2 and HumanEval-Java, the framework consistently demonstrates significant effectiveness. When utilizing StarCoder as the foundational model for patch generation, NTR fixes 128 and 129 bugs in Defects4J and HumanEval, outperforming the best baseline APR tool by 14 and 59 bugs. With the larger CodeLlama model, the fixed bugs rise to 139 and 136, respectively, exceeding the baseline by 25 and 66 bugs. Notably, the performance stems not only from the foundational models but also benefits greatly from our NTR framework. Specifically, NTR's implementation with StarCoder and CodeLlama leads to 22 and 23 additional fixes, which is beyond what the models achieve on their own. This emphasizes the success of our new perspective on utilizing templates to unlock the bug-fixing potential of LLMs.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00030","National Research Foundation, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029846","Automated Program Repair;Large Language Models;Fine-Tuning;Repair Template","Accuracy;Large language models;Computer bugs;Zero shot learning;Memory management;Graphics processing units;Maintenance engineering;Space exploration;Software engineering","","","","65","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Interactive Rubric Generator for Instructor's Assessment Using Prompt Engineering and Large Language Models","H. Du; P. Rashid; Q. Jia; E. Gehringer","North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","9","This research paper describes an interactive system using large language models and prompt engineering to generate rubrics. Rubrics have long been employed to ensure a grading system that is both equitable and consistent. In practice, generating rubrics could be challenging for instructors for many reasons (e.g., a tight course schedule, limited resources, and varying materials for different projects in the same course), which urges the need to generate the rubric automatically. To the best of our knowledge, little research has been performed on generating rubrics. In this work, we present a novel system based on Large Language Models (LLMs) and Prompt Engineering to help instructors generate rubric items interactively based on course materials, as well as assess the student's work using these rubrics to give timely feedback automatically. In this system, we applied several LLMs (e.g., GPT4, Llama, Falcon, and Hermes) to generate both rubric and feedback using this process: 1) a set of text chunks are initially generated from the textual materials (these textual materials may from various sources), then LDA (Latent Dirichlet Allocation) is applied to extract a set of keywords from the preprocessed text chunks for rubric generation; 2) a web page was designed to let the instructor choose if the keywords from the set are adequate as rubric words; 3) the rubric items are generated by LLMs from the rubric words. In our experiments, a total number of 1017 documents (including the syllabus, the course website, the requirement of projects, the students' works, and the instructors' feedback) were used to build the corpus to generate the rubric-related keywords. Three users (including one instructor and two teaching assistants) participated in generating the rubric interactively using the webpage. The results of experiments show that the interactively generated rubrics from the LLM-instructor system can achieve a level similar to manually created rubrics. We utilized different prompts to let the LLMs generate feedback for the student's work, based on the generated rubrics. Our study shows that generating automatic rubrics and feedback for student project reports is feasible, yet it also identifies significant challenges that future research needs to address.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893448","Large Language Models (LLMs);Automatic Rubric Generation;Latent Dirichlet Allocation (LDA);Feedback Automation","Schedules;Automation;Large language models;Interactive systems;Education;Web pages;Generators;Prompt engineering;Resource management;Standards","","","","29","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"A Framework for Security Testing of Large Language Models","K. Traykov","Sofia, Bulgaria",2024 IEEE 12th International Conference on Intelligent Systems (IS),"9 Oct 2024","2024","","","1","7","The purpose of this paper is to present a framework for testing of large language models (LLMs) for security vulnerabilities before their implementation to production environment. The paper discusses the latest developments in the Artificial Intelligence (AI) and Generative Artificial Intelligence (Generative AI) adoption in the industry, the expectations for further accelerated adoption and evolving regulatory landscape. An overview of the most significant risks and vulnerabilities of the LLMs such as prompt injection and denial of service have been presented with their mitigation strategies. A testing approach and testing framework have been developed and implemented with simple chatbot app. The test scenarios have been executed and results have been obtained for three open-source LLMs from which two pass the test and one failed and demonstrated the application of the proposed testing framework. Source code of the application and test script are published open source for reproducibility and reuse. In conclusion the with the confirmation of the results the limitation of the reliance on semantic similarity for the responses of the models was discussed together with three areas for further development: expanding the test scenarios to significant risks, integration with popular cloud continuous development platforms and integrating blockchain for transparent publication of the final test results.","2767-9802","979-8-3503-5098-2","10.1109/IS61756.2024.10705238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10705238","Large language models (LLMs);Cybersecurity;LLM Security;LLM Risks;LLM Vulnerabilities;Secure Software Development;Software Testing","Industries;Generative AI;Large language models;Prevention and mitigation;Source coding;Semantics;Reproducibility of results;Blockchains;Computer crime;Testing","","","","39","IEEE","9 Oct 2024","","","IEEE","IEEE Conferences"
"Some Security Model Based on Multi Agent Systems","G. Tsochev; R. Trifonov; R. Yoshinov; S. Manolov; G. Popov; G. Pavlova","Faculty Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Faculty Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Bulgarian Academy of Sciences Sofia, Laboratory of Telematics, Bulgaria; Faculty Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Faculty Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Faculty Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria","2018 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)","25 Apr 2019","2018","","","32","36","Computer security is defined as the protection of computer systems against threats to confidentiality, integrity and availability. Penetration is defined as a set of actions to compromise the integrity, confidentiality, and availability of resources. To monitor the events that occur in computer systems or networks is called intrusion detection system (IDS). This paper introduces a model for IDS based on multi-agent systems and artificial intelligence.","","978-1-5386-9576-0","10.1109/ICCAIRO.2018.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698410","multi-agent systems;artificial intelligence;network and information security;intrusion detection system;intrusion prevention system","Monitoring;Servers;Intrusion detection;Malware;Computer hacking","","3","","13","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"Observer-Based Security Consensus of Multi-agent Systems with DoS Attack and Unknown Input Disturbance","Z. Yang; F. Sun; X. Yu; X. Yang","School of Science, Chongqing University of Posts and Telecommunications, Chongqing, P. R. China; School of Science, Chongqing University of Posts and Telecommunications, Chongqing, P. R. China; School of Science, Chongqing University of Posts and Telecommunications, Chongqing, P. R. China; Key Lab of Intelligent Air-Ground Cooperative Control for Universities in Chongqing, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, P. R. China",2024 43rd Chinese Control Conference (CCC),"17 Sep 2024","2024","","","6022","6026","A consensus protocol has been developed for a category of multi-agent systems(MASs) that are susceptible to DoS attacks and unknown input disturbance, considering two aspects: the online state cannot be obtained under unknown input disturbance by agent and common communication cannot be accomplished among agents under DoS attacks. Firstly, an improved proportion-integral observer is designed based on the Luenberger observer, which estimate to the agent’s state through the output information of the agent; Secondly, in order to resist the impact of DoS attacks and unknown input disturbance on MASs, a communication protocol with disturbance estimation is designed; By using theories such as stability theory, linear matrix inequality(LMI) and algebraic graph theory, sufficient conditions for the system to achieve security consensus is obtained; Finally, numerical experiments were conducted to verify the effectiveness of the relevant theories.","1934-1768","978-9-8875-8158-1","10.23919/CCC63176.2024.10661383","National Natural Science Foundation of China; Natural Science Foundation of Chongqing; Chongqing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10661383","Multi-agent systems;Observer;DoS attack;Security consensus;Unknown input disturbance","Sufficient conditions;Resists;Observers;Numerical simulation;Stability analysis;Matrices;Linear matrix inequalities","","1","","26","","17 Sep 2024","","","IEEE","IEEE Conferences"
"Observer-Based Consensus Security Control for Multi-Agent Systems Under FDI Attacks","J. Li; Y. Sun; Q. Su","School of Automation Engineering, Northeast Electric Power University, Jilin, China; School of Automation Engineering, Northeast Electric Power University, Jilin, China; School of Automation Engineering, Northeast Electric Power University, Jilin, China",2023 China Automation Congress (CAC),"19 Mar 2024","2023","","","4925","4930","To solve the consensus security control issue of fake data injection (FDI) attacks on the communication link of discrete-time multi-agent systems (MASs), a consensus control protocol based on discrete state observers is proposed. First, a distributed state observer is built based on the relative outputs of the neighboring agents in order to estimate the leader's state and tracking error. A discrete consensus control protocol is additionally created to allow followers to monitor the leader's state trajectories despite FDI attacks and to confirm the system's stability. The effectiveness of the proposed control strategy is then verified using numerical simulation and simulation using a model of a smart grid system.","2688-0938","979-8-3503-0375-9","10.1109/CAC59555.2023.10451481","Natural Science Foundation of Jilin Province(grant numbers:20230101239JC); Education Department of Jilin Province(grant numbers:JJKH20220113KJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10451481","Multi-agent systems;Communication link;False data injection attacks;Security control;Consensus","Protocols;Observers;Consensus control;Numerical simulation;Stability analysis;Trajectory;Smart grids","","","","13","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Test Code Generation for Telecom Software Systems Using Two-Stage Generative Model","M. Nabeel; D. D. Nimara; T. Zanouda","Global AI Accelerator, Ericsson, Sweden; Global AI Accelerator, Ericsson, Sweden; Global AI Accelerator, Ericsson, Sweden",2024 IEEE International Conference on Communications Workshops (ICC Workshops),"12 Aug 2024","2024","","","1231","1236","In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.","2694-2941","979-8-3503-0405-3","10.1109/ICCWorkshops59551.2024.10615269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10615269","TelcoAI;Large Language Models for Software Testing;Generative AI for Test automation;and Code Generation","Adaptation models;Codes;Conferences;Natural languages;Software systems;Data models;Telecommunications","","2","","29","IEEE","12 Aug 2024","","","IEEE","IEEE Conferences"
"Automated Program Repair in the Era of Large Pre-trained Language Models","C. S. Xia; Y. Wei; L. Zhang","University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE),"14 Jul 2023","2023","","","1482","1494","Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.","1558-1225","978-1-6654-5701-9","10.1109/ICSE48619.2023.00129","NSF(grant numbers:CCF-2131943,CCF-2141474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172803","Automated Program Repair;Machine Learning","Codes;Computer bugs;Maintenance engineering;Software;Distance measurement;Task analysis;Faces","","215","","89","IEEE","14 Jul 2023","","","IEEE","IEEE Conferences"
"FormalEval: A Method for Automatic Evaluation of Code Generation via Large Language Models","S. Yang; Y. Yang","X-Epic Ltd., Chengdu, China; X-Epic Ltd., Chengdu, China",2024 2nd International Symposium of Electronics Design Automation (ISEDA),"8 Aug 2024","2024","","","660","665","One of the promising applications of Large Language Models (LLMs) is code generation. However, evaluating the quality of the generated code poses a significant challenge. Existing evaluation methods such as Rouge or HumanEval have limitations in terms of accuracy or efficiency. In this paper, we propose a formal evaluation method called FormalEval, which automates the process of checking generated code without the need for manual test case curation. We evaluated our method on common tasks related to Register-Transistor-Level (RTL) Verilog and SystemVerilog Assertions (SVA) generation in the field of Electronic Design Automation (EDA). Our method not only identifies 23 % of evaluation error in existing RTL benchmarking dataset, but also fixes the error via test case augmentation. We show FormalEval can help to identify better LLM prompting techniques on SVA generation task. Our method demonstrates state-of-the-art accuracy on the testing dataset.","","979-8-3503-5203-0","10.1109/ISEDA62518.2024.10617643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10617643","LLM;Code Generation;Formal;Evaluation Metric;RTL;SVA","Training;Codes;Accuracy;Design automation;Large language models;Scalability;Manuals","","2","","14","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Data Service Discovery","D. Bianchini; M. Garda; M. Melchiori; A. Rula","Dept. of Information Engineering, University of Brescia, Brescia, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy; Dept. of Information Engineering, University of Brescia, Brescia, Italy",2024 IEEE International Conference on Web Services (ICWS),"15 Oct 2024","2024","","","1097","1099","In the context of the Internet of Services paradigm for Industry 4.0, data services can be discovered and composed to accomplish different data analytics scenarios amongst the actors of a production network. Recently, Large Language Models (LLMs) have been increasingly considered for service discovery and composition as a promising alternative to previous approaches that often require substantial effort to produce formal service descriptions and/or annotations. In this paper, we introduce some exploratory experiments on the use of an LLM-based system for the discovery of data services to fulfil data analysis scenarios. First, a data service model, that represents in a declarative way data service operations, is provided. Then, we propose prompt templates for the interaction with the LLM-based system, that leverages the service model, aimed at reducing trial-and-error interactions for identifying potential service candidates. The effectiveness of the approach is being assessed in a real-world case study of a research project.","2836-3868","979-8-3503-6855-0","10.1109/ICWS62655.2024.00128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707425","data service model;Large Language Models;prompt engineering;Internet of Services","Data analysis;Web services;Annotations;Large language models;Production;Data models;Fourth Industrial Revolution","","","","5","IEEE","15 Oct 2024","","","IEEE","IEEE Conferences"
"Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks","Y. Guo; Z. Wan","Department of Biomedical Engineering, ShanghaiTech University, Shanghai, China; Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, USA",2024 IEEE 12th International Conference on Healthcare Informatics (ICHI),"22 Aug 2024","2024","","","541","543","Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.","2575-2634","979-8-3503-8373-7","10.1109/ICHI61247.2024.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628740","large language model (LLMs);prompt engineering;medical image processing;ChatGPT;GPT-4;LLaVA;VGG;tumor classification;Brain Tumor MRI;Breast Ultrasound Images","Privacy;Ultrasonic imaging;Large language models;Chatbots;Safety;Prompt engineering;Task analysis","","6","","12","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"Imperfect Code Generation: Uncovering Weaknesses in Automatic Code Generation by Large Language Models","X. Lian; S. Wang; J. Ma; X. Tan; F. Liu; L. Shi; L. Zhang; C. Gao","SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; SKLSDE, Beihang University, China; Harbin Institute of Technology, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","422","423","The task of code generation has received significant attention in recent years, especially when the pre-trained large language models (LLMs) for code have consistently achieved state-of-the-art performance. However, there is currently a lack of a comprehensive weakness taxonomy in the field, uncovering weaknesses in automatic code generation by LLMs. This may lead the community to invest excessive efforts into well-known hotspots while neglecting many crucial yet unrecognized issues that deserve more attention. To bridge this gap, we conduct a systematic study on analyzing the weaknesses based on three state-of-the-art LLMs across three widely-used code generation datasets. Our study identifies eight types of weaknesses and assesses their prevalence across each LLM and dataset, aiming to inform and shape the trajectory of future research in the domain.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3643081","National Science Foundation of China(grant numbers:62102014); State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2023ZX-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554837","","Bridges;Codes;Systematics;Shape;Taxonomy;Trajectory;Task analysis","","1","","3","","20 Jun 2024","","","IEEE","IEEE Conferences"
"Poster: Fuzzing for Command Injections in Medical Software with Large Language Models","Y. Wang; Q. Guan; J. Chen","Dept. of Computer Science, Oakland University, Rochester, MI, USA; Dept. of Computer Science, Oakland University, Rochester, MI, USA; Dept. of Computer Science, Oakland University, Rochester, MI, USA","2024 IEEE International Conference on Mobility, Operations, Services and Technologies (MOST)","30 Jul 2024","2024","","","272","274","In the evolving landscape of medical software security, the threat of command injection looms large, with potential ramifications including compromised patient data and disrupted healthcare services. Addressing these vulnerabilities is paramount, and fuzz testing, or fuzzing, has proven as an indispensable tool for identifying command injection. By identifying and mitigating threats, fuzzing not only guarantees the integrity and reliability of medical software but also upholds patient trust and regulatory adherence, ensuring the continuity and safety of healthcare services.Recent advancements have seen the integration of Large Language Models (LLMs) into security fuzzing, offering a revolutionary approach to uncovering bugs and vulnerabilities. The application of LLMs in fuzzing transcends traditional language-specific limitations, enabling more comprehensive and effective testing across a spectrum of programming languages and systems. In this paper, we exploits the potential of leveraging LLMs for fuzzing medical software against command injection, aiming to harness these advancements to fortify the cybersecurity defenses of critical healthcare technologies.","","979-8-3503-0773-3","10.1109/MOST60774.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606996","Fuzzing;Security;Large Language Models","Computer languages;Large language models;Computer bugs;Medical services;Fuzzing;Software;Software reliability","","","","15","IEEE","30 Jul 2024","","","IEEE","IEEE Conferences"
"Invited: Automated Code generation for Information Technology Tasks in YAML through Large Language Models","S. Pujar; L. Buratti; X. Guo; N. Dupuis; B. Lewis; S. Suneja; A. Sood; G. Nalawade; M. Jones; A. Morari; R. Puri",IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; Red Hat; Red Hat; IBM Research; IBM Research,2023 60th ACM/IEEE Design Automation Conference (DAC),"15 Sep 2023","2023","","","1","4","The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible YAML code generation tool, aimed at improving IT automation productivity. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247987","Generative Model;Ansible;Code Generation","Training;Productivity;Codes;Automation;Markup languages;Natural languages;Transformers","","6","","11","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"SALLMA: A Software Architecture for LLM-Based Multi-Agent Systems","M. Becattini; R. Verdecchia; E. Vicario","Software Technologies Laboratory, University of Florence; Software Technologies Laboratory, University of Florence; Software Technologies Laboratory, University of Florence",2025 IEEE/ACM International Workshop New Trends in Software Architecture (SATrends),"12 Jun 2025","2025","","","5","8","As a new and disruptive technology, the introduction of large language models (LLMs) may be the first step into a paradigm shift of how we develop and deploy software-intensive systems. While the capabilities of LLM agents for software engineering and architecture tasks are currently explored, how to architect LLM-based systems appears to be to date an uncharted territory. Software architectures based on a single LLM agent face inherent challenges, such as lack of task customization, lack of memory, and limited access to ground truth. These challenges become especially pressing in real-world contexts that demand persistent context, validated information, and task-specific flexibility. As a potential solution to overcome these challenges, multiple LLM-agents can be adopted for specialized tasks within a single software-intensive system. In this contribution, we open the discourse on architecting LLM-intensive software products by presenting SALLMA, a Software Architecture for LLMbased Multi-Agent systems. SALLMA leverages two core layers, namely (i) the Operational Layer, responsible for request intent management, handling real-time task execution and dynamic orchestration of agents, and (ii) the Knowledge Layer, used to to store and manage metamodels and configurations for workflows and agents. To primarily assess the viability of SALLMA, we develop a proof of concept leveraging as key technologies Docker, Kubernetes, Python, LangChain, Hugging Face, Mistral, LLaMA, and SQL and NoSQL databases. Currently, SALLMA is deployed to provide information on behalf of public administration offices, and is currently utilized in a business simulation scenario.","","979-8-3315-2587-3","10.1109/SATrends66715.2025.00006","European Union; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029425","software architecture;se4ai;llm","Structured Query Language;Software architecture;NoSQL databases;Pressing;Market research;Software;Real-time systems;Faces;Multi-agent systems;Python","","","","5","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Flakiness Repair in the Era of Large Language Models","Y. Chen","University of Illinois Urbana-Champaign, USA",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","441","443","Flaky tests can non-deterministically pass or fail regardless of any change to the code, which negatively impacts the effectiveness of the regression testing. Prior repair techniques for flaky tests mainly leverage program analysis techniques to mitigate test flakiness, which only focus on Order-Dependent (OD) and Implementation-Dependent (ID) flakiness with known flakiness patterns and root causes. In this paper, we propose an approach to repair flaky tests with the power of Large Language Models (LLMs). Our approach successfully repaired 79% of OD tests and 58% of ID tests in an extensive evaluation using 666 flaky tests from 222 projects. We submitted pull requests to fix 61 flaky tests; at the time of submission, 19 tests have already been accepted. However, we observed that currently LLMs are ineffective in adequately repairing Non-Order-Dependent (NOD) flaky tests by analyzing 118 of such tests from 11 projects.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3641227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554925","Software Testing;Test Flakiness;Large Language Models","Codes;Maintenance engineering;Software;Testing;Software engineering","","1","","28","","20 Jun 2024","","","IEEE","IEEE Conferences"
"Application of Large Language Models for Intent Mining in Goal-Oriented Dialogue Systems","A. E. Shukhman; V. R. Badikov; L. V. Legashev","Institute of Mathematics and Information Technologies, Orenburg State University, Orenburg, Russia; Institute of Mathematics and Information Technologies, Orenburg State University, Orenburg, Russia; Research Institute of Digital Intelligent Technologies, Orenburg State University, Orenburg, Russia",2024 V International Conference on Neural Networks and Neurotechnologies (NeuroNT),"12 Jul 2024","2024","","","28","31","Modern machine learning techniques in the natural language processing domain can be used to automatically generate scripts for goal-oriented dialogue systems. In this article we present a general framework for studying the automatic generation of scripts for goal-oriented dialogue systems. One of the promising methods is to extract knowledge from large language models based on prepared text prompts. We conduct a study of five popular large language models with English and Russian prompts and draws conclusions about the advisability of using certain models to solve specific problems of goal-oriented dialogue systems.","","979-8-3503-6373-9","10.1109/NeuroNT62606.2024.10585408","Russian Science Foundation(grant numbers:23-21-00503); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10585408","large language models;prompt engineering;natural language processing;dialogue systems","Training;Large language models;Machine learning;Transformers;Natural language processing;Neurotechnology;Biological neural networks","","","","16","IEEE","12 Jul 2024","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models in Software Testing: A Review of Applications and Challenges","A. Sezgin; G. Özkan; E. Coşgun","Research and Development Siemens A.S., Istanbul, Turkey; Research and Development Siemens A.S., Istanbul, Turkey; Research and Development Siemens A.S., Istanbul, Turkey",2025 13th International Symposium on Digital Forensics and Security (ISDFS),"2 Jun 2025","2025","","","1","7","The integration of Large Language Models (LLMs), such as GPT and BERT, into software testing has introduced a transformative approach to ensuring software reliability, functionality, and security. This review explores the diverse applications of LLMs in automating and enhancing software testing processes, addressing challenges in traditional methodologies. LLMs excel in automated test case generation by leveraging vast datasets, user stories, and bug reports to produce comprehensive and context-aware test scenarios. These capabilities reduce manual effort, improve coverage, and identify critical edge cases. Moreover, their application in bug detection and code analysis enhances early issue identification, improving software quality and reducing costs. LLMs also streamline the creation of high-quality documentation, enabling better collaboration and scalability in software projects. Despite their potential, LLM adoption in software testing faces challenges, including model interpretability, scalability, and the need for high-quality, diverse training datasets. Security and ethical considerations, such as data privacy and the risk of misuse, also demand attention. The paper evaluates state-of-the-art LLM applications, showcasing advancements in test automation and vulnerability detection while identifying areas for improvement. Future directions include refining hybrid approaches, enhancing domain-specific adaptability, and addressing ethical and governance concerns. This study provides a comprehensive analysis of LLM-driven testing methodologies, emphasizing their transformative potential in modern software development workflows. By fostering interdisciplinary collaboration, the field can harness LLMs to revolutionize software testing, paving the way for efficient, scalable, and secure software solutions.","2768-1831","979-8-3315-0993-4","10.1109/ISDFS65363.2025.11011986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011986","large language models;software testing;automation;bug detection","Software testing;Training;Ethics;Reviews;Scalability;Large language models;Computer bugs;Collaboration;Software reliability;Security","","","","24","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation","X. Jiang; Y. Dong; Y. Tao; H. Liu; Z. Jin; G. Li","Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; University of Electronic Science and Technology of China, Chengdu, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","334","346","Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose Rocode,which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1 %. The test pass rate is relatively improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00133","National Natural Science Foundation of China(grant numbers:62192733,61832009,62192731,62192730,62072007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029868","Code Generation;Large Language Models;Backtracking Mechanism;Program Analysis","Training;Backtracking;Codes;Costs;Large language models;Benchmark testing;Decoding;Software engineering;Software development management","","","","56","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Fixing Large Language Models' Specification Misunderstanding for Better Code Generation","Z. Tian; J. Chen; X. Zhang","College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; Department of Computer Science, Purdue University, USA",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1514","1526","Code generation is to automatically generate source code conforming to a given programming specification, which has received extensive attention especially with the development of large language models (LLMs). Due to the inherent difficulty of code generation, the code generated by LLMs may not be aligned with the specification. Although thought-eliciting prompting techniques have been proposed to enhance the code generation performance of LLMs, producing correct understanding for complicated programming problems remains challenging, resulting in unsatisfactory performance. Also, some feedbackbased prompting techniques have been proposed to fix incorrect code using error messages produced by test execution. However, when the generated code deviates significantly from the ground truth, they encounter difficulties in improving performance based on such coarse-grained information. In this work, we propose a novel prompting technique, called $\mu\mathbf{FiX}$, to improve the code generation performance of LLMs by devising both sophisticated thought-eliciting prompting and feedback-based prompting and making the first exploration on their synergy. It first exploits test case analysis to obtain specification understanding and enables a self-improvement process to identify and refine the misunderstanding in the thoughteliciting prompting phase. $\mu\mathbf{FiX}$ further fixes the specification understanding towards the direction reducing the gap between the provided understanding (from the first phase) and the actual understanding implicity utilized by LLMs for code generation in the feedback-based prompting phase. By improving the understanding with $\mu \text{FiX}$, the code generation performance of LLMs can be largely improved. Our evaluation on two advanced LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing with 15 baselines, demonstrates the effectiveness of $\mu\mathbf{FiX}$. For example, $\mu\mathbf{FiX}$ outperforms the most effective baseline with an average improvement of 35.62 % in terms of Pass@1 across all subjects.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00108","National Key Research and Development Program of China(grant numbers:2024YFB4506300); National Natural Science Foundation of China(grant numbers:62322208,12411530122); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029745","Code Generation;Large Language Models;Prompting Engineering","Codes;Accuracy;Large language models;Source coding;Programming;Benchmark testing;Chatbots;Software engineering","","","","60","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Fuzzy-Assisted Contrastive Decoding Improving Code Generation of Large Language Models","S. Wang; L. Ding; Y. Zhan; Y. Luo; S. Liu; W. Ding","School of Computer Science, National Engineering Research Center for Multimedia Software and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, Australia; Yunnan United Vision Technology Company Ltd, Kunming, China; School of Computer Science, National Engineering Research Center for Multimedia Software and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; School of Educational Science, Institute of Interdisciplinary Studies, Hunan Normal University, Changsha, China; School of Artificial Intelligence and Computer Science, Nantong University, Nantong, China",IEEE Transactions on Fuzzy Systems,"6 Aug 2025","2025","33","8","2689","2703","Large language models (LLMs) play a crucial role in intelligent code generation tasks. Most existing work focuses on pretraining or fine-tuning specialized code LLMs, e.g., CodeLlama. However, pretraining or fine-tuning a code LLM requires a vast corpus of data, significant computational resources, and considerable human effort. Compared to pretraining or fine-tuning LLMs, a simple and flexible method of contrastive decoding has garnered widespread attention to improve the text generation quality of LLMs. While contrastive decoding can indeed improve the text generation quality of LLMs, our research has found that directly using contrastive decoding: 1) introduces erroneous information into the logit distribution generated from normal prompts (i.e., user’s input), particularly in the code generation of LLMs; 2) significantly impedes the inference and decoding time of LLMs. In this work, the limitations of using contrastive decoding directly are systematically highlighted, and a novel real-time fuzzy-assisted contrastive decoding (FCD) mechanism is proposed to improve the code generation quality of LLMs. The proposed FCD mechanism initially categorizes prompts into high-quality and low-quality groups based on the results of the evaluator (i.e., unit test) before integrating the LLM. Next, feature values (e.g., standard deviation, peak value, etc.) related to the logit distribution of predicted tokens during the LLM’s inference process for both high-quality and low-quality prompts are extracted. Finally, the extracted feature values are used to train the fuzzy neural network (i.e, fuzzy min–max neural network) offline, allowing for the prejudgement of the reliability of the logit distribution for normal prompt outputs. This prevents the direct use of erroneous information from contrastive decoding and improves the code generation quality of LLMs. Through extensive experiments, it has been demonstrated that the proposed FCD mechanism can significantly improve the code generation quality of LLMs through FCD. Moreover, the FCD mechanism can also reduce the time required for inference and contrastive decoding.","1941-0034","","10.1109/TFUZZ.2025.3575060","National Key R & D Plan of China(grant numbers:2024YFE0202700); National Natural Science Foundation of China(grant numbers:U23A20318,U2433216,62276195); S & T Major Project of Hubei(grant numbers:2024BAB046); Foundation for Innovative Research Groups of Hubei(grant numbers:2024AFA017); Natural Science Foundation of Jiangsu(grant numbers:BK20231337); Natural Science Foundation of Jiangsu Higher Education Institutions(grant numbers:24KJB520032); National Key Laboratory of Security Communication Foundation(grant numbers:WD202404); Natural Science Foundation of Changsha(grant numbers:KQ2402164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11018203","Contrastive decoding;generative artificial intelligence;large language models (LLMs);normal prompts","Decoding;Codes;Computational modeling;Solid modeling;Training;Feature extraction;Software development management;Predictive models;Data models;Standards","","","","60","IEEE","29 May 2025","","","IEEE","IEEE Journals"
"Generating Pandas Code for Big Table Question Answering Using Large Language Models","A. A. Vyatkin; V. D. Oliseenko","St. Petersburg Federal Research Center of the Russian Academy of Sciences, St. Petersburg, Russian Federation; St. Petersburg Federal Research Center of the Russian Academy of Sciences, St. Petersburg, Russian Federation",2025 XXVIII International Conference on Soft Computing and Measurements (SCM),"8 Jul 2025","2025","","","164","166","Currently, large language models are extensively utilized across various domains to address a wide range of challenges. Such models exhibit the capability to process data in diverse formats effectively, including tabular data. This paper describes a method for generating pandas code to answer queries related to big tables. The approach relies solely on specific elements of the table - namely, column names and sample rows - as input data for the model. The performance evaluation employed the DataBench dataset, which provided questions encompassing both numeric and categorical answers. The research contrasted the GigaChat Max and GPT-4o-mini models; GigaChat Max achieved an accuracy of $\mathbf{7 8. 6 \%}$, whereas GPT-4o-mini demonstrated an accuracy of 87%. The GPT-40-mini model attained state-of-the-art performance in numeric answers (88.5%) and categorical answers (86.6%) as per the DataBench benchmark. The experiments indicated that an increase in the number of input rows could adversely impact model performance. The findings of this study may contribute to enhancing the efficiency of analytical systems that incorporate large language models.","","979-8-3315-2667-2","10.1109/SCM66446.2025.11060118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11060118","large language models;analyzing large tables;pandas code generation","Performance evaluation;Codes;Accuracy;Data analysis;Large language models;Data handling;Current measurement;Data models;Question answering (information retrieval);Numerical models","","","","18","IEEE","8 Jul 2025","","","IEEE","IEEE Conferences"
"A Software Bug Fixing Approach Based on Knowledge-Enhanced Large Language Models","L. Bo; Y. He; X. Sun; W. Ji; X. Wu",Yangzhou University; Yangzhou University; Yangzhou University; Yangzhou University; Yangzhou University,"2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","26 Sep 2024","2024","","","169","179","Software Bug Fixing is a time-consuming task in software development and maintenance. Despite the success of Large Language Models (LLMs) using in Automatic Program Repair (APR), they still have the limitations of generating patches with low accuracy and explainability. In this paper, we propose a software bug-fixing approach based on knowledge-enhanced large language models. First, we collect bugs as well as their fix information from bug tracking systems, such as Github and Stack Overflow. Then, we extract bug entities and inter-entity relationships using Named Entity Recognition (NER) to construct a Bug Knowledge Graph (BKG). Finally, we utilize LLMs (e.g., GPT-4) which is enhanced by the knowledge of the similar historical bugs as well as fix information from BKG to generate patches for new bugs. The experimental results show that the our approach can fix 28.52% (85\298) bugs correctly, which is significantly better than the state-of-the-art approaches. Furthermore, the generated patches are explainable and more credible.","2693-9177","979-8-3503-6563-4","10.1109/QRS62785.2024.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684661","Bug fixing;Knowledge Graph;Generative AI;Explainable","Codes;Accuracy;Large language models;Computer bugs;Knowledge graphs;Maintenance engineering;Software","","","","33","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Random Perturbation Attack on LLMs for Code Generation","Q. Peng; C. Zhang; R. Mangal; C. Pasareanu; L. Jia",Carnegie Mellon University; Carnegie Mellon University; Colorado State University; Carnegie Mellon University; Carnegie Mellon University,2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN),"16 Jun 2025","2025","","","285","287","Large language models (LLMs) have shown impressive capabilities in coding tasks, including code understanding and generation. However, these models are also susceptible to input perturbations, such as case changes, whitespace or typo modifications, which can affect their performance. This study investigates the impact of different types of perturbations on code and natural language on the performance of LLM code generation tasks. In addition to evaluating individual perturbations, the research examines combined perturbation attacks, where multiple perturbations from different categories are applied together. While combined attacks showed only marginal overall improvement over individual ones, they demonstrated a synergistic effect in specific scenarios, exploiting complementary vulnerabilities in the models.","","979-8-3315-0219-5","10.1109/CAIN66642.2025.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030003","LLMs;Code generation;Random Perturbations;Attack","Codes;Perturbation methods;Large language models;Natural languages;Encoding;Robustness;Software engineering","","","","10","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Towards Understanding the Characteristics of Code Generation Errors Made by Large Language Models","Z. Wang; Z. Zhou; D. Song; Y. Huang; S. Chen; L. Ma; T. Zhang","University of Alberta, Edmonton, AB, Canada; University of Illinois Urbana-Champaign, Champaign, IL, USA; University of Alberta, Edmonton, AB, Canada; The University of Tokyo, Tokyo, Japan; Purdue University, West Lafayette, IN, USA; The University of Tokyo, Tokyo, Japan; Purdue University, West Lafayette, IN, USA",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2587","2599","Large Language Models (LLMs) have demonstrated unprecedented capabilities in code generation. However, there remains a limited understanding of code generation errors that LLMs can produce. To bridge the gap, we conducted an in-depth analysis of code generation errors across six representative LLMs on the HumanEval dataset. Specifically, we first employed open coding and thematic analysis to distill a comprehensive taxonomy of code generation errors. We analyzed two dimensions of error characteristics-semantic characteristics and syntactic characteristics. Our analysis revealed that LLMs often made non-trivial, multi-line code generation errors in various locations and with various root causes. We further analyzed the correlation between these errors and task complexity as well as test pass rate. Our findings highlighted several challenges in locating and fixing code generation errors made by LLMs. In the end, we discussed several future directions to address these challenges.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029951","Empirical Study;Code Generation;Large Language Models","Codes;Correlation;Large language models;Taxonomy;Semantics;Syntactics;Encoding;Complexity theory;Software reliability;Software engineering","","1","","92","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Optimizing Response Consistency of Large Language Models in Medical Education through Prompt Engineering","T. Weerathunge; S. Jayalal; K. Wijayasiriwardhane","Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka; Center for Information Technology, Waikato Institute of Technology, New Zealand; Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka",2025 5th International Conference on Advanced Research in Computing (ICARC),"16 Apr 2025","2025","","","1","6","This research focuses on optimizing the response consistency of Large Language Models (LLMs) in medical education through advanced prompt engineering techniques. LLMs often give different answers to the same question, making self-consistency a critical parameter for assessing their performance. Addressing this inconsistency is essential in high-stakes fields like healthcare, where reliable and accurate information is important. The study employed custom prompt engineering strategies, including zero-shot, few-shot, and Chain-of-Thought (CoT) prompting, to improve LLM output consistency and accuracy. We implemented a retrieval-augmented generation (RAG) framework to use external knowledge from trusted medical resources, keeping the responses accurate and contextually appropriate. Responses were scored on several dimensions: content relevance, completeness, and clinical correctness, and assessed for consistency by asking repeated queries. The results showed significant enhancement in the consistency and accuracy of the responses, proving the effectiveness of the presented method. This work outlines suggestions for the use of LLMs in a way that can be incorporated into medical education while considering the limitations. It underscores the need for further exploration of prompt engineering to improve LLM performance and establishes these tools as reliable resources for training healthcare professionals.","","979-8-3315-3098-3","10.1109/ICARC64760.2025.10963313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10963313","Large Language Models;Medical Education;Response Consistency;Response Accuracy;Prompt Engineering","Training;Accuracy;Large language models;Retrieval augmented generation;Medical services;Reliability engineering;Prompt engineering","","","","37","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"Adaptive Multi-Agent AI Framework for Real-Time Energy Optimization and Context-Aware Code Review in Software Development","T. Sharanarthi","IBM Silicon Valley Labs California, USA",2025 5th International Symposium on Computer Technology and Information Science (ISCTIS),"11 Jul 2025","2025","","","353","358","Energy efficiency has become a critical concern in modern software development, particularly for applications deployed in resource-constrained environments such as IoT, mobile devices, and cloud infrastructure. Traditional static analysis tools and single-model code review solutions fail to provide timely, adaptive, and context aware recommendations, leading to suboptimal energy performance and increased technical debt. This paper introduces a novel adaptive multi-agent AI framework that delivers real-time, personalized energy optimization feedback directly within the software development workflow. The proposed system leverages transformer-based code embeddings for semantic analysis, a FAISS-powered contextual memory for historical learning, and a collaborative multi-agent system consisting of specialized AI agents for energy profiling, compliance monitoring, resource allocation, and maintainability assessment. Adaptive reinforcement learning dynamically refines energy recommendations based on developer interactions, ensuring continuous improvement over time. Empirical evaluations demonstrate that the framework effectively reduces redundant feedback, improves system-wide energy efficiency, and enhances developer trust through explainable AI techniques. The results highlight the potential of integrating intelligent, energy-efficient coding practices into modern software engineering workflows, fostering sustainability without compromising performance.","","979-8-3315-4450-8","10.1109/ISCTIS65944.2025.11066037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11066037","Adaptive AI;Multi-Agent Systems;Energy Optimization;Real-Time Code Review;Transformer Models;Reinforcement Learning;Explainable AI;Software Engineering","Codes;Reviews;Explainable AI;Reinforcement learning;Transformers;Real-time systems;Energy efficiency;Optimization;Software development management;Multi-agent systems","","","","10","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"Automatically Generated Multi-Agent Framework for Jailbreaking Large Language Models","A. Yang; B. Wang; A. Liu; H. Li","School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China",2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA),"1 Jul 2025","2025","","","1500","1503","LLMs have developed rapidly at an unprecedented speed and are being widely used, including question answering, code generation and reasoning. LLMs are not completely safe and reliable, and may output harmful or illegal content under attack. Jailbreak attacks can bypass the security measures of LLMs and use LLMs to generate harmful content. However, jailbreak attack methods often rely on manually crafted prompt templates or not having sufficient diversity or jailbreak effect. In this paper, we introduce a new automatically generated multi-agent framework for automatic jailbreak testing of black-box large language models. We use the collaborative capabilities of multiple agents to automatically generate diverse jailbreak attack prompts to cover a wide range of vulnerability scenarios. We carefully implement three key modules: an automatically generated multi-agent framework, jailbreaking strategies for black-box large language models, and an iterative reflection mechanism. We evaluate on many different attack scenarios and multiple different black-box LLMs. Our results show that our work can achieve jailbreak with a high success rate. The attack success rate for GPT-4 exceeds 95%, which shows that our method can help current LLMs improve security and reliability.","","979-8-3315-0976-7","10.1109/AIITA65135.2025.11047880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11047880","Multi-Agent;Large Language Models;Jailbreak;Red-Teaming","Large language models;Closed box;Collaboration;Reflection;Question answering (information retrieval);Cognition;Security;Reliability;Iterative methods;Testing","","","","17","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Advancing Large Language Models in Code Generation: Usaco Benchmark and Bug Mitigation Insights","J. Trentini; V. Liu; Y. Peng; Z. Zong",Monte Vista High School; Seven Lakes High School; Vandegrift High School; Texas State University,2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","01","12","Recently, Large Language Models (LLMs) have made substantial progress in code generation, but they still frequently generate code containing logic errors or syntax bugs. While research has focused on improving performance through fine-tuning and data collection, less attention has been given to analyzing error patterns and employing prompt-engineering to address these issues. Existing benchmarks primarily assess LLMs on easy to intermediate-level coding tasks, often neglecting more complex challenges involving advanced algorithms and data structures. Additionally, data contamination in these benchmarks limits their ability to accurately measure the capability of LLMs in code generation. In this paper, we present the new USACO Benchmark, derived from the USA Computing Olympiad (USACO) competition, to evaluate 11 closed and open-source LLMs. Through a detailed analysis, we identify common code generation errors across the models and propose Hint-Driven Prompts to address logic errors, alongside the Syntax Mitigation Prompt to reduce syntax bugs. Our results demonstrate that the Hint-Driven Prompt boosts pass rates for DBRX 132B, Deepseek-Coder 33B, Codegemma 7B, Codellama 7B, Llama 3, and GPT-4o by 6.6×, 4.7×, 3×, 2.5×, 2.1×, and 25%, respectively. Additionally, the Syntax Mitigation Prompt significantly reduces syntax errors, with reductions of 71.32% for Codegemma 7B, 25.56% for Deepseek-Coder 33B, 23.39% for Llama 3, and 11.19% for Codellama 70B.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025909","Large Language Models;Code Generation;USACO Benchmark;Software Bugs;Error Mitigation","Codes;Prevention and mitigation;Large language models;Computer bugs;Syntactics;Benchmark testing;Data structures;Encoding;Pollution measurement;Logic","","","","65","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis","S. Alhazbi; A. Hussain; G. Oligeri; P. Papadimitratos","College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; Networked Systems Security Group, KTH Royal Institute of Technology, Stockholm, Sweden; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; Networked Systems Security Group, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Open Journal of the Communications Society,"24 Jun 2025","2025","6","","5050","5071","As Large Language Models (LLMs) become increasingly integrated into many technological ecosystems across various domains and industries, identifying which model is deployed or being interacted with is critical for the security and trustworthiness of the systems. Current verification methods typically rely on analyzing the generated output to determine the source model. However, these techniques are susceptible to adversarial attacks, operate in a post-hoc manner, and may require access to the model weights to inject a verifiable fingerprint. In this paper, we propose a novel passive fingerprinting framework that operates in real-time and remains effective even under encrypted network traffic conditions. Our method leverages the intrinsic autoregressive generation nature of language models, which generate text one token at a time based on all previously generated tokens, creating a unique temporal pattern-like a rhythm or heartbeat-that persists even when the output is streamed over a network. We find that measuring the Inter-Token Times (ITTs)–time intervals between consecutive tokens-can identify different language models with high accuracy. We develop a Deep Learning (DL) pipeline to capture these timing patterns using network traffic analysis and evaluate it on 16 Small Language Models (SLMs) and 10 proprietary LLMs across different deployment scenarios, including local host machine (GPU/CPU), Local Area Network (LAN), Remote Network, and when using Virtual Private Network (VPN). Our experimental results demonstrate high classification performance with weighted F1-scores of 85% when tested on a different day, 74% across different networks, and 71% when traffic is tunneled through a VPN connection. This work opens a new avenue for model identification in real-world scenarios and contributes to more secure and trustworthy language model deployment.","2644-125X","","10.1109/OJCOMS.2025.3577016","Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:GSRA7-1-0510-20045); Swedish Research Council (VR); Knut and Alice Wallenberg (KAW) Foundation; Qatar National Library; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11026013","Large language models;small language models;fingerprinting;network traffic analysis;deep learning;network security","Fingerprint recognition;Computational modeling;Analytical models;Large language models;Watermarking;Telecommunication traffic;Virtual private networks;Timing;Feature extraction;Local area networks","","","","39","CCBY","5 Jun 2025","","","IEEE","IEEE Journals"
"Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models","M. Bruni; F. Gabrielli; M. Ghafari; M. Kropp","University of Applied Sciences and Arts Northwestern Switzerland, Switzerland; University of Applied Sciences and Arts Northwestern Switzerland, Switzerland; Technische Universität Clausthal, Germany; University of Applied Sciences and Arts Northwestern Switzerland, Switzerland",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","93","103","Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a ""prompt agent"" that demonstrates how the most effective techniques can be applied in real-world development workflows.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052790","Secure Code Generation;Prompt Engineering;Large Language Models","Codes;Sensitivity;Foundation models;Large language models;Benchmark testing;Maintenance engineering;Prompt engineering;Security;Iterative methods;Software engineering","","","","20","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Towards Run-Time Search for Real-World Multi-Agent Systems","A. C. Diller; E. M. Fredericks","Grand Valley State University, Allendale, Michigan, USA; Grand Valley State University, Allendale, Michigan, USA",2022 IEEE/ACM 15th International Workshop on Search-Based Software Testing (SBST),"4 Jul 2022","2022","","","14","15","Multi-agent systems (MAS) may encounter uncertainties in the form of unexpected environmental conditions, sub-optimal system configurations, and unplanned interactions between autonomous agents. The number of combinations of such uncertainties may be innumerable, however run-time testing may reduce the issues impacting such a system. We posit that search heuristics can augment a run-time testing process, in-situ, for a MAS. To support our position we discuss our in-progress experimental testbed to realize this goal and highlight challenges we anticipate for this domain.","","978-1-4503-9318-8","10.1145/3526072.3527537","Michigan Space Grant Consortium(grant numbers:80NSSC20M0124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810710","search-based software testing;multi-agent systems;cyber-physical systems","Software testing;Uncertainty;Conferences;Software;Autonomous agents;Multi-agent systems","","","","16","","4 Jul 2022","","","IEEE","IEEE Conferences"
"Coft: Making Large Language Models Better Zero-Shot Learners for Code Generation","W. Li; Y. Qian; K. Gao; H. Chen; X. Wang; Y. Tong; L. Li; Y. Wu; C. Zhao","Institute of Software Chinese Academy of Sciences, Beijing, China; China Electric Power University, Baoding, China; Institute of Software Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Institute of Software Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Institute of Software Chinese Academy of Sciences, Beijing, China; Institute of Software Chinese Academy of Sciences, Beijing, China",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","489","499","The Chain-of-Thought (CoT) prompting mechanism has effectively enhanced the performance of large language models (LLMs) across a variety of natural language processing (NLP) tasks, including complex zero-shot learning scenarios. Recent studies suggest that this effectiveness arises from CoT's capacity to direct LLMs' attention toward task-relevant keywords. However, traditional CoT methods yield only marginal improvements in the realm of code generation, particularly for models with fewer than 10 billion parameters. We posit that this limitation stems from the substantial disparity between the logical structure and representational form of code compared to natural language. Considering the training and deployment costs, enhancing the performance of small LLMs through advanced prompting and instruction-tuning is essential. In this paper, we introduce COFT (Chain of Functional Triggers), a novel prompting strategy specifically designed for code generation tasks. The design of COFT is based on the following important observation: An optimal CoT tailored for code generation should clearly indicate the core functionality of each critical step, while employing standard identifiers prevalent within the coding domain. Extensive experiments conducted on representative small LLMs ($<10 \mathrm{B}$) benchmarks demonstrate that our COFT substantially outperforms vanilla CoT methods. In challenging zeroshot scenarios and the Pass@1 metric, COFT can improve the performance of fundation LLMs by up to 35.3 %. These empirical findings support our hypothesis that an appropriate design for CoT alongside instruction tuning can fully activate even smallersized LLMs, making them better zero-shot learners for code generation. The source code of COFT and the constructed instruction-tuning dataset will be released.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00059","NSF(grant numbers:92364202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025884","large language models;chain-of-thought;code generation","Training;Measurement;Codes;Costs;Large language models;Source coding;Zero shot learning;Natural language processing;Tuning;Standards","","","","45","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"The Impact of Large Language Models on IT Security in the Corporate Environment","S. Scholz; A. Lawall; K. Schaaff","IU International University of Applied Science, Erfurt, Germany; IU International University of Applied Science, Erfurt, Germany; IU International University of Applied Science, Erfurt, Germany",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","109","115","This paper investigates the impact of Large Language Models (LLMs) on both offensive and defensive aspects of IT security within corporate environments. Addressing the fragmented understanding of LLMs in connection with IT security, the study aims to answer the research question: What attack vectors are enabled by LLMs, and what defense measures can be implemented? Employing a systematic literature analysis and developing use cases, the research combines theoretical insights with practical scenarios. Results provide a comprehensive overview of LLM applications in the realm of Cyber Defense, identifying potential threats and effective defense strategies.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852476","Generative AI;IT Security;Cyber Security;Corporate Environments;Large Language Models","Systematics;Large language models;Vectors;Security","","","","49","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair","G. Li; C. Zhi; J. Chen; J. Han; S. Deng","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Hangzhou City University, Hangzhou, China; Zhejiang University, Hangzhou, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","719","731","Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that ""pre-training and fine-tuning"" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.","2643-1572","979-8-4007-1248-7","","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764872","Automated Program Repair;Parameter-Effective Fine-Tuning;Large Language Model;Execution-based Evaluation","Training;Codes;Large language models;Computer bugs;Memory management;Training data;Maintenance engineering;Prompt engineering;Creativity;Software engineering","","","","73","","29 Nov 2024","","","IEEE","IEEE Conferences"
"A Large Language Models Security Performance Evaluation Method Based on Analytic Hierarchy Process and Shapley Value Fusion","J. Mao; H. Wen; W. Hou; W. Luo; Y. Chen; Y. Pang","School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China; School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China; School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China; School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China; School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China; School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, People’s Republic of China",2025 5th International Symposium on Computer Technology and Information Science (ISCTIS),"11 Jul 2025","2025","","","691","694","This paper investigates a method for assessing the safety performance of Large Language Models (LLMs) based on the fusion of Analytic Hierarchy Process (AHP) and Shapley values. By weighted fusion of AHP subjective weights and Shapley objective weights, the final comprehensive weights are obtained and the model security score is calculated, and the model security score is used to assess the model security performance. The experimental results show that the security performance of LLMs is importantly linked to the attack success rate and toxicity detection rate, and the security performance of GPT-4 is optimal among the five LLMs set up in the experiment. This method can evaluate the security performance of LLMs more scientifically.","","979-8-3315-4450-8","10.1109/ISCTIS65944.2025.11065980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11065980","LLMs;Analytic Hierarchy Process;Shapley Value;Safety Performance Assessment","Performance evaluation;Information science;Toxicology;Accuracy;Large language models;Analytic hierarchy process;Data models;Safety;Security;Computer security","","","","14","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"On Protecting the Data Privacy of Large Language Models (LLMs): A Survey","B. Yan; K. Li; M. Xu; Y. Dong; Y. Zhang; Z. Ren; X. Cheng","Shandong University, Qingdao, China; Shandong University, Qingdao, China; Shandong University, Qingdao, China; Shandong University, Qingdao, China; Drexel University, Philadelphia, USA; Leiden University, Leiden, Netherlands; Shandong University, Qingdao, China",2024 International Conference on Meta Computing (ICMC),"9 Jul 2025","2024","","","1","12","Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints.","","979-8-3503-5599-4","10.1109/ICMC60390.2024.00008","National Natural Science Foundation of China(grant numbers:62232010,62302266,U23A20302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11062758","Large Language Models (LLMs);Security;Data Privacy;Privacy Protection;Survey","Surveys;Data privacy;Privacy;Translation;Large language models;Focusing;Oral communication;Writing;Security;Protection","","4","","98","IEEE","9 Jul 2025","","","IEEE","IEEE Conferences"
"Hacking LLMs: A Technical Analysis of Security Vulnerabilities and Defense Mechanisms","G. Raj; Hamzah; N. Raj; N. Ranjan","Sharda University; Sharda University; Sharda University; Sharda University, Greater Noida","2025 2nd International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)","26 Mar 2025","2025","","","555","560","Large Language Models (LLMs) such as GPT-4 and Google’s Gemini have revolutionized the landscape of artificial intelligence, enabling sophisticated natural language processing capabilities across diverse domains. However, the rapid adoption of these models has unveiled a spectrum of security vulnerabilities exploitable by adversaries [1], [2]. This paper presents a comprehensive analysis of various attack vectors targeting LLMs, including prompt injection, data poisoning, model inversion, and side-channel attacks. We examine real-world incidents and vulnerabilities discovered in widely-used LLMs like ChatGPT and Gemini [3], highlighting the intricate mechanisms through which these attacks compromise model integrity and user trust. Furthermore, we explore existing defense mechanisms and best practices, emphasizing runtime protection systems, adversarial training approaches, and robust access control systems [4], [5]. Building upon these foundations, we introduce a novel Multi-Layer Defense Framework with Dynamic Trust Scoring, designed to enhance the security and robustness of LLM deployments through dynamic evaluation and adaptive response systems. By dissecting recent advancements in LLM security research and proposing innovative defense strategies, this paper aims to guide the development of resilient defenses against the evolving landscape of AI-driven threats.","","979-8-3315-3038-9","10.1109/CICTN64563.2025.10932638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932638","Large Language Models;LLM Security;Prompt Injection;Data Poisoning;Model Inversion;Side-Channel Attacks;Adversarial Training;Runtime Protection;Multi-Layer Defense Framework;Dynamic Trust Scoring","Training;Access control;Analytical models;Runtime;Large language models;Scalability;Side-channel attacks;Data models;Vectors;Protection","","","","21","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models","H. Luo; C. Ji","College of Computing and Information Science, Cornell University, New York, USA; Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign, Champaign, Illinois, USA",2025 5th International Symposium on Computer Technology and Information Science (ISCTIS),"11 Jul 2025","2025","","","163","166","With the widespread application of edge computing and cloud systems in AI-driven applications, how to maintain efficient performance while ensuring data privacy has become an urgent security issue. This paper proposes a federated learningbased data collaboration method to improve the security of edge cloud AI systems, and use large-scale language models (LLMs) to enhance data privacy protection and system robustness. Based on the existing federated learning framework, this method introduces a secure multi-party computation protocol, which optimizes the data aggregation and encryption process between distributed nodes by using LLM to ensure data privacy and improve system efficiency. By combining advanced adversarial training techniques, the model enhances the resistance of edge cloud AI systems to security threats such as data leakage and model poisoning. Experimental results show that the proposed method is 15% better than the traditional federated learning method in terms of data protection and model robustness.","","979-8-3315-4450-8","10.1109/ISCTIS65944.2025.11065696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11065696","Federated Learning;Large Language Models (LLM);Secure Multi-party Computation (SMC);Adversarial Training","Training;Protocols;Federated learning;Computational modeling;Collaboration;Data models;Robustness;Multi-party computation;Security;Protection","","","","13","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation","P. Wu; N. Guo; X. Xiao; W. Li; X. Ye; D. Fan","Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China",2025 IEEE International Symposium on Circuits and Systems (ISCAS),"27 Jun 2025","2025","","","1","5","Recently, large language models (LLMs) have demonstrated excellent performance, inspiring researchers to explore their use in automating register transfer level (RTL) code generation and improving hardware design efficiency. However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire. To mitigate these issues, we innovatively introduce an iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in current loop. Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA) open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human benchmark. Under similar conditions of data quantity and quality, our approach significantly outperforms the baseline. Extensive experiments validate the effectiveness of the proposed method.","2158-1525","979-8-3503-5683-0","10.1109/ISCAS56072.2025.11044046","Beijing Nova Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11044046","RTL code;large language models;iterative fine-tuning","Training;Codes;Filtering;Large language models;Benchmark testing;Register transfer level;Hardware;Data models;Iterative methods;Hardware design languages","","","","17","IEEE","27 Jun 2025","","","IEEE","IEEE Conferences"
"Anomaly Detection in 6G Networks Using Large Language Models (LLMs)","A. K. Abasi; M. Aloqaily; M. Guizani","Information Security Engineering Technology, Abu Dhabi Polytechnic, MBZ, Abu Dhabi, UAE; Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE; Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE",2025 International Wireless Communications and Mobile Computing (IWCMC),"2 Jul 2025","2025","","","1466","1471","The anticipated deployment of 6G networks by 2030 is expected to introduce advanced capabilities, such as ultra-low latency and massive device connectivity, thereby increasing the complexity of potential security threats. Traditional anomaly detection methods, primarily based on Ensemble Learning (EL) classifiers, often lack the adaptability required to address evolving cyber threats and zero-day attacks. This paper proposes a novel approach that leverages Large Language Models (LLMs) to enhance anomaly detection in 6G networks. By transforming structured network traffic data into natural language descriptions, LLMs can be fine-tuned to effectively identify anomalous patterns. Experimental evaluations demonstrate that this LLM-based method outperforms traditional EL classifiers in terms of recall, precision, and adaptability to unforeseen threats, marking a significant advancement in AI-driven network security.","2376-6506","979-8-3315-0887-6","10.1109/IWCMC65282.2025.11059535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11059535","6G Networks;Anomaly Detection;Large Language Models (LLMs);Network Security;Ensemble Learning","6G mobile communication;Wireless communication;Large language models;Computational modeling;Natural languages;Telecommunication traffic;Network security;Real-time systems;Ensemble learning;Anomaly detection","","","","15","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Lightweight Design of Robotic Arm Based on Multimodal Large Language Models","Z. Feng; J. Dou; Z. Ni; H. Guan","School of Mechanical Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Southeast University, Nanjing, China; School of Future Technology, Southeast University, Nanjing, China","2025 5th International Conference on Computer, Control and Robotics (ICCCR)","14 Jul 2025","2025","","","60","64","With the rapid development of society, the application of robotic arms is becoming increasingly widespread. Lightweighting of robotic arms can not only increase flexibility and operability, but also reduce costs and energy. In recent years, Large Language Models (LLMs) have witnessed rapid development. Applying LLMs to the lightweight design of robotic arms can enhance design efficiency and reduce design difficulties. In this article, a method that directly utilizes prompt engineering to guide a multimodal LLM (MLLM) in automatically generating lightweight design schemes for the upper arm of a SCARA robot is proposed. Firstly, static simulation of the upper arm is conducted using ANYSY via Python. Secondly, utilizing the MLLM and prompt engineering, automatic generation of lightweight design schemes for drilling is achieved, with corresponding Python code generated, based on input stress diagram. Finally, the geometry model is reconstructed and subjected to static simulation. The results showed that the lightweight design scheme generated by MLLM achieved the optimization goal, yielding results comparable to those of existing works. The new method using MLLM for the lightweight design of robot arms is able to effectively improve design efficiency and reducing design difficulty, with broad application prospects.","","979-8-3315-4354-9","10.1109/ICCCR65461.2025.11072572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11072572","Multimodal LLMs;Prompt Engineering;Lightwe-ight design;Finite element analysis","Drilling;Geometry;Digital control;Costs;Large language models;Manipulators;Finite element analysis;Prompt engineering;Stress;Optimization","","","","15","IEEE","14 Jul 2025","","","IEEE","IEEE Conferences"
"LLM-Driven Code Refactoring: Opportunities and Limitations","J. Cordeiro; S. Noei; Y. Zou","Queen's University, Kingston, Canada; Queen's University, Kingston, Canada; Queen's University, Kingston, Canada",2025 IEEE/ACM Second IDE Workshop (IDE),"1 Jul 2025","2025","","","32","36","Refactoring is a systematic process of improving code quality while preserving the functional behavior of the software. In recent years, integrated development environments (IDEs) have added or improved automatic refactoring in their features, to enhance developers' productivity and reduce the likelihood of human errors. With the advancement and increasing popularity of large language models (LLMs), coding automation using them has gained enormous attention and has shown to be effective in performing refactorings on the source code automatically. However, this automation can carry the risk of introducing errors or hallucinations that may break or alter the software functionality. The error-proneness and the possibility of hallucinations in LLMs limit their ability to be fully integrated into an automated refactoring pipeline (e.g., IDEs) and often require humans in the loop to verify the performed modifications. In this position paper, we examine the limitations of existing LLM-based refactoring techniques. We propose research directions to address these limitations and improve the quality of LLM-based code refactoring for reliable software maintenance.","","979-8-3315-0188-4","10.1109/IDE66625.2025.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052685","code refactoring;code quality improvement;large language models;integrated development environments","Productivity;Software maintenance;Codes;Automation;Systematics;Large language models;Source coding;Pipelines;Human in the loop;Software reliability","","","","28","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Knowledge-Aware Code Generation with Large Language Models","T. Huang; Z. Sun; Z. Jin; G. Li; C. Lyu","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Key Lab of HCST (PKU), MOE; SCS, Peking University, Beijing, China; Key Lab of HCST (PKU), MOE; SCS, Peking University, Beijing, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China",2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC),"18 Jun 2024","2024","","","52","63","Large Language Models (LLMs) perform well on basic programming problems. However, they encounter challenges when dealing with complex tasks involving the use of diverse algorithmic and data structure skills, particularly programming competition-level problems. Notably, ChatGPT exhibits proficient performance on problems it has encountered during its pre-training phase, but this performance deteriorates when faced with novel problems. Consequently, enhancing the ability of LLMs to address unfamiliar problems has emerged as a pivotal research focus. The problem-solving process of LLMs mirrors human programmers’ approach to a certain extent. When confronted with new programming tasks, human programmers engage in task planning and code writing with the previously acquired knowledge about algorithms and data structures. Despite having learned such knowledge, LLMs struggle to effectively apply it when faced with specific new problems. To address this issue, we constructed a novel dataset, CodeF, which contains a portion of programming problems that ChatGPT has not previously encountered. Furthermore, we developed a Knowledge Library tailored for Python programming contest problems and introduced the concept of Knowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models’ understanding and problem-solving capabilities by integrating prompt and knowledge from the library into the LLMs’ code generation reasoning process, especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets, KareCoder demonstrated outstanding performance in handling novel problems previously unencountered by LLMs. In contrast with the code directly generated by ChatGPT, KareCoder achieved a relative improvement of 23.3 % on the Pass@1 metric on the CodeF post2021-9 dataset. Additionally, it performs well compared to other methods when dealing with problems that LLMs have previously encountered. Our dataset and experiment data are open-sourced and can be accessed at https://github.com/CodeGeneration3/KareCoder.CCS CONCEPTS• Software and its engineering $\rightarrow$ Automatic programming.","2643-7171","979-8-4007-0586-1","","Natural Science Foundation of Shandong Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556459","Code Generation;Large Language Models;Knowledge Library","Measurement;Codes;Training data;Programming;Writing;Data structures;Chatbots","","3","","33","","18 Jun 2024","","","IEEE","IEEE Conferences"
"A Review on Code Generation with LLMs: Application and Evaluation","J. Wang; Y. Chen","National Engineering Research Center of Trustworthy Embedded Software, East China Normal University, Shanghai, China; National Engineering Research Center of Trustworthy Embedded Software, East China Normal University, Shanghai, China",2023 IEEE International Conference on Medical Artificial Intelligence (MedAI),"31 Jan 2024","2023","","","284","289","Code generation is a longstanding subject in the field of computer science and software engineering, which aims at realizing an agent capable of writing code automatically aligning with human desire. With the booming development of large language models (LLMs) in recent years, code generation techniques powered by LLMs with strong coding ability have caught many researchers' interest. In this study, we conduct a review of recent studies about code generation with LLMs, from the application of LLM-based code generation to the evaluation of LLM-generated code. We find, with the powerful code understanding and writing ability LLMs provide, these novel techniques can be applied to manage various software engineering tasks, and indeed boost the productivity of developers to a great extent. But we also find, as an equally important subject, the evaluation receives less attention from researchers than the application. We conclude some limitations in existing studies about the evaluation of code generated by LLMs, like inadequate quality characteristics considered. And we think more effort is needed to narrow the gap between research on the evaluation and the application.","","979-8-3503-5878-0","10.1109/MedAI59581.2023.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403378","large language models (LLMs);code generation;code completion;automatic program repair;code quality evaluation","Productivity;Computer science;Codes;Writing;Encoding;Task analysis;Software engineering","","23","","38","IEEE","31 Jan 2024","","","IEEE","IEEE Conferences"
"Empirical Evaluation of Large Language Models for Novice Program Fault Localization","Y. Liu; H. Liu; Z. Yang; Z. Li; Y. Liu","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","26 Sep 2024","2024","","","180","191","Integrating Large Language Models (LLMs) into software fault localization represents a significant advancement in improving debugging efficiency for programmers. However, novice program fault localization, which is essential for computer science education, has not been thoroughly investigated in previous studies. In contrast to industrial programs target practical functionality, novice programs primarily deal with individual algorithmic issues. The distinct logic structures between novice and industrial programs can impact how effectively LLM understand and process them. Moreover, this difference reveals the inapplicability of the Competent Programmer Hypothesis, a fundamental assumption in industrial fault localization, to novice program fault localization. Therefore, industrial methodologies are unsuitable for novice programming, emphasizing the need for our empirical studies. To fill this gap, we evaluate LLMs’ effectiveness in localizing faults for novice programs in statement level. Using the widely used novice programs dataset Codeflaws and Condefects, we compare the performance of two commercial LLMs (i.e., ChatGPT-3.5 and ChatGPT-4) and three open-source LLMs (i.e., ChatGLM3, Llama2, and Code Llama) against traditional fault localization methods, examining their accuracy and overlap. Additionally, we investigate how prompt engineering improves localization precision. Our findings show ChatGPT-4’s overall superior performance, with ChatGPT-3.5 exhibiting minor advantages in certain cases. ChatGPT-4 outperforms the traditional methods with best performance by 592% and 137% on Codeflaws and Condefects. Specifically, each method exhibits unique strengths in localizing novice programming faults. Moreover, carefully crafted prompts can improve LLMs’ precision. These insights underscore the promising potential of utilizing LLMs for fault localization in novice programming.","2693-9177","979-8-3503-6563-4","10.1109/QRS62785.2024.00027","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684637","Large Language Model;Fault Localization;Empirical Study;Novice Programming;Prompt Engineering","Location awareness;Codes;Accuracy;Large language models;Software algorithms;Software quality;Reliability engineering","","1","","65","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Leveraging Prompt Engineering for Curriculum Design in Short-term Adult Training","H. Wang; P. Chen; Ailiya; Z. Shen","College of Computing and Data Science, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore",2024 4th International Conference on Educational Technology (ICET),"6 Feb 2025","2024","","","81","85","This paper explores the integration of large language models (LLMs) into the curriculum design process for short-term adult training programs, highlighting the role of prompt engineering as a transformative tool. As adult education needs to adapt rapidly to changing professional landscapes, the ability to quickly develop and customize educational content is paramount. This study introduces a framework that employs prompt engineering to leverage LLMs, such as ChatGPT, for generating customized curriculum design. Through a systematic process, curriculum designers input tailored prompts into LLMs to produce relevant and accurate educational content. The methodology involves iterating these prompts based on feedback to refine and enhance the curriculum. A survey conducted among educational professionals, who have varied familiarity with AI, supports the effectiveness of this approach. The results indicate that well-crafted prompts not only expedites the curriculum development process but may also enhance the educational outcomes by providing more customized and adaptive learning experiences. This approach is posited as a promising solution to the challenges faced in adult education, particularly in settings demanding rapid course development and delivery.","","979-8-3503-7694-4","10.1109/ICET62460.2024.10868563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10868563","Curriculum design;Large Language Models;prompt engineering","Training;Surveys;Accuracy;Systematics;Large language models;Curriculum development;Focusing;Prompt engineering;Fake news;Research and development","","","","21","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"Serving LLMs as Detectors in Workflows with Guardrails","G. Kumbhat; E. Ju","watsonx Platform Engineering, IBM Research, Austin, United States; watsonx Platform Engineering, IBM Research, Denver, United States",2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C),"30 May 2025","2025","","","503","507","With the growing popularity of Large Language Model (LLM) usage in generative AI applications comes a growing need to be able to verify, moderate, or “guardrail” LLM inputs and outputs. “Guardrailing” can be done with anything from simple regex detections, to more complicated techniques like using LLMs themselves to detect undesired content. Additionally, considerable effort has been going into creating and optimizing various LLM serving solutions. This paper describes our experience of using an adapter pattern with an LLM serving architecture to provide LLMs as guardrail models. The details on design trade-offs, such as performance or model accessibility, can aid in creating other LLM-based software architectures.","2768-4288","979-8-3315-3336-6","10.1109/ICSA-C65153.2025.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11015002","large language models;generative AI;machine learning;software architecture;guardrails;APIs","Adaptation models;Analytical models;Production systems;Software architecture;Generative AI;Biological system modeling;Large language models;Detectors;Computer architecture;Servers","","","","12","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"Beyond Accuracy: Evaluating Source Code Capabilities in Large Language Models for Software Engineering","A. Velasco","William & Mary Williamsburg, Virginia, USA",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","162","164","This dissertation aims to introduce interpretability techniques to comprehensively evaluate the performance of Large Language Models (LLMs) in software engineering tasks, beyond canonical metrics. In software engineering, Deep Learning techniques are widely employed across various domains, automating tasks such as code comprehension, bug fixing, code summarization, machine translation, and code generation. However, the prevalent use of accuracy-based metrics for evaluating Language Models trained on code often leads to an overestimation of their performance. Our work seeks to propose novel and comprehensive interpretability techniques to evaluate source code capabilities and provide a more nuanced understanding of LLMs performance across downstream tasks.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3639815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554826","Large Language Models;Interpretability;DL4SE;Category Theory;Causal Inference","Measurement;Deep learning;Codes;Accuracy;Source coding;Computer bugs;Machine translation","","1","","28","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Comparative Analysis of RAG, Fine-Tuning, and Prompt Engineering in Chatbot Development","H. K. Chaubey; G. Tripathi; R. Ranjan; S. k. Gopalaiyengar","Computer Science and Engineering, IIIT Naya Raipur, Raipur, India; Electronics and Communication Engineering, IIIT Naya Raipur, Raipur, India; Data Science and Applications, IIT, Madras, Chennai, India; Data Science and Artificial Intelligence, IIIT, Naya Raipur, Raipur, India",2024 International Conference on Future Technologies for Smart Society (ICFTSS),"30 Sep 2024","2024","","","169","172","This paper examines the integration and comparative effectiveness of Retriever-Augmented Generation (RAG), fine-tuning, and prompt engineering in the development of advanced chatbots. By employing domain-specific fine-tuning, the study addresses contextual misunderstandings and inaccuracies prevalent in base Large Language Models (LLMs). RAG enhances chatbot functionality by incorporating real-time data retrieval, ensuring relevance in dynamically changing environments. Prompt engineering is utilized to refine input prompts, thereby optimizing the accuracy of responses. Employing the “openassistant-guanaco” dataset from Hugging Face, this research assesses the performance improvements offered by these methodologies, both quantitatively and qualitatively. The fine-tuned model outperforms other methods with an accuracy of $87.8 \backslash \%$ and a BLEU score of 0.81, proving its effectiveness in generating the most relevant responses. In contrast, while the RAG with LLM approach shows promising results with a reasonable accuracy of $84.5 \backslash \%$, the Prompt Engineering method, though slightly less effective with an accuracy of $83.2 \backslash \%$, still maintains competitive performance. This study highlights the unique and combined strengths of these technologies, contributing valuable insights into their synergistic potential for enhancing chatbot interactions","","979-8-3503-7384-4","10.1109/ICFTSS61109.2024.10691338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691338","Chatbots;Large Language Models (LLM);Retriever-Augmented Generation (RAG);Fine-tuning;Prompt Engineering","Analytical models;Accuracy;Large language models;Chatbots;Real-time systems;Prompt engineering;Faces","","4","","8","IEEE","30 Sep 2024","","","IEEE","IEEE Conferences"
"Prompt Engineering as Code (PEaC): an approach for building modular, reusable, and portable prompts","G. Perrone; S. P. Romano","DIETI - Dipartimento di Ingegneria Elettrica e Delle Tecnologie Dell’Informazione, Università Degli Studi di Napoli Federico II, Napoli, Italy; DIETI - Dipartimento di Ingegneria Elettrica e Delle Tecnologie Dell’Informazione, Università Degli Studi di Napoli Federico II, Napoli, Italy",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","289","294","Prompt engineering is an emerging skill associated with improving the way we interact with Large Language Models (LLMs). However, natural language lacks key features such as modularity, reusability, and portability, which are essential for creating efficient, scalable prompt systems. In programming, these features are managed through Infrastructure as Code (IaC), where developers use modular code to manage infrastructure. This work aims to show how it is possible to achieve the same objective in the prompt engineering sector, too. Prompt Engineering as Code (PEaC) is a novel approach that organizes prompts through a human-readable data serialization language, in order to realize modular, reusable, and portable prompts. We design a syntax language in which prompts can be assembled as modular components, akin to importing functions or defining reusable variables in conventional programming languages. We assess the methodology by showcasing its implementation to multiple LLM-driven applications and evaluating enhancements in prompt management and adaptability. Preliminary findings suggest that PEaC increases prompt reusability, reduces redundancy, and promotes the adaptability of prompt systems across many applications. This method represents progress in the establishment of standardized and scalable engineered prompts.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852434","Prompt Engineering;Large Language Models;Infrastructure as Code;Data Serialization","Computer languages;Codes;Large language models;Redundancy;Natural languages;Buildings;Syntactics;Programming;Prompt engineering","","","","34","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"SnipGen: A Mining Repository Framework for Evaluating LLMs for Code","D. Rodriguez-Cardenas; A. Velasco; D. Poshyvanyk","Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","508","512","Large Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227K data points from 338K recent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs’ code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs’ performance in software engineering contexts.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025700","Deep learning;code generation;datasets;large language models;evaluation","Training;Codes;Large language models;Neural networks;Transformers;Software;Data mining;Prompt engineering;Software engineering;Software development management","","","","48","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"LLMs for Code: The Potential, Prospects, and Problems","T. Sharma","Faculty of Computer Science, Dalhousie University, Halifax, Canada",2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C),"21 Aug 2024","2024","","","373","374","With the introduction of Large Language Models (LLMS) and their integration with software development tasks, the software development landscape has changed drastically in the last couple of years. In this session, we delve into the intricate world of large language models for code (LLMs4code) and explore their benefits, challenges, and threats. On one hand, these models have revolutionized code completion, bug detection, and even generated entire sections of code with remarkable accuracy. However, on the other side, several concerns have emerged surrounding inaccurate, buggy, and vulnerable code generation, biases, implications for climate, and the potential for unintended consequences. Together, we'll dissect real-world examples, dis-cussing the transformative power of large language models while exploring the gray side of LLMs4code that developers tread. The talk will discuss strategies for effectively leveraging these tools, mitigating risks, and contributing to the ongoing dialogue about responsible AI in the coding ecosystem. The talk promises an exploratory take that not only seeks to harness the potential of LLMs4code but also ensures a conscientious and mindful approach toward their integration into our coding practices.","2768-4288","979-8-3503-6625-9","10.1109/ICSA-C63560.2024.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628197","Large language models for code;large language models;software development and artificial intelligence","Codes;Software architecture;Large language models;Biological system modeling;Ecosystems;Computer bugs;Encoding","","1","","4","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Directions Towards Efficient and Automated Data Wrangling with Large Language Models","Z. Zhang; P. Groth; I. Calixto; S. Schelter",University of Amsterdam; University of Amsterdam; Amsterdam UMC; University of Amsterdam,2024 IEEE 40th International Conference on Data Engineering Workshops (ICDEW),"17 Jun 2024","2024","","","301","304","Data integration and cleaning have long been a key focus of the data management community. Recent research indicates the potential of large language models (LLMs) for such tasks. However, scaling and automating data wrangling with LLMs for real-world use cases poses additional challenges. Manual prompt engineering for example, is expensive and hard to operationalise, while full fine-tuning of LLMs incurs high compute and storage costs. Following up on previous work, we evaluate parameter-efficient fine-tuning (PEFT) methods for efficiently automating data wrangling with LLMs. We conduct a study of four popular PEFT methods on differently sized LLMs for ten benchmark tasks, where we find that PEFT methods achieve performance on-par with full fine-tuning, and that we can leverage small LLMs with negligible performance loss. However, even though such PEFT methods are parameter-efficient, they still incur high compute costs at training time and require labeled training data. We explore a zero-shot setting to further reduce deployment costs, and propose our vision for ZEROMATCH, a novel approach to zero-shot entity matching. It is based on maintaining a large number of pretrained LLM variants from different domains and intelligently selecting an appropriate variant at inference time.","2473-3490","979-8-3503-8403-1","10.1109/ICDEW61823.2024.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555056","Data Wrangling;Large language models;Parameter-efficient fine-tuning;Entity matching","Training;Costs;Conferences;Training data;Data integration;Benchmark testing;Data engineering","","","","17","IEEE","17 Jun 2024","","","IEEE","IEEE Conferences"
"Manual Prompt Engineering is Not Dead: A Case Study on Large Language Models for Code Vulnerability Detection with DSPy","F. Trad; A. Chehab","Electrical and Computer Engineering, American University of Beirut, Beirut, Lebanon; Electrical and Computer Engineering, American University of Beirut, Beirut, Lebanon",2025 8th International Conference on Data Science and Machine Learning Applications (CDMA),"7 Mar 2025","2025","","","168","173","Automated prompt engineering tools have recently emerged as a promising solution to simplify the traditional man-ual task of crafting prompts for large language models (LLMs). This study investigates whether such tools can fully replace manual prompt engineering for code vulnerability detection. We leverage the DSPy (Declarative Self-improving Python) frame-work, which uses modular “signatures” rather than prompts to specify tasks. In DSPy, a signature defines the expected input-output behavior of a task and can optionally include a description to guide the model's objective. This signature is then auto-matically translated into an optimized prompt through DSPy's modules and optimizers. This study compares the performance of GPT-4o-mini on basic and detailed signatures to determine how DSPy optimizations affect model performance in each case. The basic signature prompts the model to classify code as vulnerable or not, while the detailed signature specifies particular vulnerabilities to identify. For each signature, we used DSPy's modules and optimizers to create three prompt configurations: zero-shot (baseline, where the model performs the task without examples), chain-of-thought (where the model shows step-by-step reasoning), and bootstrap few-shot (where the model is guided through a small set of examples). Results show that DSPy's automated optimizations improve performance for both signature types over the zero-shot baseline; however, detection performance increases significantly with detailed signatures. Specifically, the detailed signatures achieved higher F1 scores, with improvements of approximately 23 % for zero-shot, 13 % for chain-of-thought, and 11 % for bootstrap few-shot techniques. These findings indicate that while automated tools enhance prompt efficiency, optimal results are achieved by combining automated techniques with human-crafted signature details, underscoring the ongoing importance of manual refinement in specialized tasks.","","979-8-3315-3969-6","10.1109/CDMA61895.2025.00034","Maroun Semaan Faculty of Engineering and Architecture; American University of Beirut; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908746","Prompt engineering;DSPy;Large Language Models;Code Vulnerability Detection","Codes;Translation;Large language models;Scalability;Manuals;Machine learning;Software;Prompt engineering;Multiaccess communication;Optimization","","","","23","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"A Dual-Model Solution for Business Reasoning with Large Language Models","S. Trivedi; S. Roy; R. Das; D. Mukherjee; S. Mahapatra; P. Nandy","Data and Analytics, PwC India, Kolkata, India; Data and Analytics, PwC India, Kolkata, India; Data and Analytics, PwC India, Kolkata, India; Data and Analytics, PwC India, Kolkata, India; Data and Analytics, PwC India, Kolkata, India; Data and Analytics, PwC India, Kolkata, India",2024 IEEE Pune Section International Conference (PuneCon),"27 Feb 2025","2024","","","1","6","Recent advancements in large language models (LLMs) have shown significant potential in automating tasks using natural language processing. However, as industry demands grow and generative AI advances, more complex tasks—such as intelligent reasoning, strategic decision-making, and business analysis—remain challenging for foundational models trained on general datasets. These tasks require domain-specific knowledge and a deeper contextual understanding, which current models struggle to provide. This research presents an approach that fine-tunes LLMs for specific business use cases by integrating domain knowledge and data analysis capabilities. Leveraging recent developments in structured query (SQL) based fine-tuned models and semantic search techniques, our methodology combines structured data retrieval from organizational sources with relevant domain expertise. This enables the model to answer complex analytical questions by connecting factual data from generated SQL queries to relevant business knowledge. We propose a dual-model system: a fine-tuned code-generating model for data extraction and a foundational model with visual reasoning capabilities. This combination ensures accurate alignment of data with the appropriate knowledge base, reducing the risk of hallucinated answers. The performance of our approach is compared against stand-alone fine-tuned models and retrieval-augmented generation (RAG) models, demonstrating superior results in answering reasoning-based questions and supporting decision-making tasks. Overall, this research offers a systematic solution for integrating data-driven insights with business-specific knowledge, empowering leaders to make informed decisions based on accurate, visualized data and market-specific information.","2831-5022","979-8-3315-2782-2","10.1109/PuneCon63413.2024.10895709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895709","Visual Reasoning;Code Generation;Large Language Models (LLMs);retrieval-augmented generation (RAG);Generative AI","Visualization;Structured Query Language;Accuracy;Semantic search;Large language models;Decision making;Data retrieval;Data models;Cognition;Business","","","","31","IEEE","27 Feb 2025","","","IEEE","IEEE Conferences"
"Large Language Models: Principles and Practice","I. Trummer","Bowers CIS Cornell University, Ithaca, NY",2024 IEEE 40th International Conference on Data Engineering (ICDE),"23 Jul 2024","2024","","","5354","5357","The last few years have been marked by several breakthroughs in the domain of generative AI. Large language models such as GPT-4 are able to solve a plethora of tasks, ranging from text and code generation to multimodal data analysis, without task-specific training data. This tutorial, targeted at database researchers without prior background in language models, introduces language models as well as relevant use cases in the context of data management. The tutorial covers the fundamental principles enabling language models, including the Transformer architecture, pre-training, and alignment. Furthermore, the tutorial will show how to use language models in practice, leveraging OpenAI's GPT model to build a natural language query interface as a demonstration. Finally, the tutorial will discuss recent research exploiting language models in the context of data management.","2375-026X","979-8-3503-1715-2","10.1109/ICDE60146.2024.00404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597873","large language models;gpt-4;data management;database","Generative AI;Large language models;Natural languages;Training data;Tutorials;Transformers;Data models","","5","","39","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization","H. Manjunath; L. Heublein; T. Feigl; F. Ott","Fraunhofer Institute for Integrated Circuits IIS, Nürnberg, Germany; Fraunhofer Institute for Integrated Circuits IIS, Nürnberg, Germany; Fraunhofer Institute for Integrated Circuits IIS, Nürnberg, Germany; Fraunhofer Institute for Integrated Circuits IIS, Nürnberg, Germany",2025 IEEE Wireless Communications and Networking Conference (WCNC),"9 May 2025","2025","","","1","7","Large language models (LLMs) are advanced AI systems applied across various domains, including NLP, information retrieval, and recommendation systems. Despite their adaptability and efficiency, LLMs have not been extensively explored for signal processing tasks, particularly in the domain of global navigation satellite system (GNSS) interference monitoring. GNSS interference monitoring is essential to ensure the reliability of vehicle localization on roads, a critical requirement for numerous applications. However, GNSS-based positioning is vulnerable to interference from jamming devices, which can compromise its accuracy. The primary objective is to identify, classify, and mitigate these interferences. Interpreting GNSS snapshots and the associated interferences presents significant challenges due to the inherent complexity, including multipath effects, diverse interference types, varying sensor characteristics, and satellite constellations. In this paper, we extract features from a large GNSS dataset and employ LLaVA to retrieve relevant information from an extensive knowledge base. We employ prompt engineering to interpret the interferences and environmental factors, and utilize t-SNE to analyze the feature embeddings. Our findings demonstrate that the proposed method is capable of visual and logical reasoning within the GNSS context. Furthermore, our pipeline outperforms state-of-the-art machine learning models in interference classification tasks. Github: https://gitlab.cc-asp.fraunhofer.de/darcy_gnss","1558-2612","979-8-3503-6836-9","10.1109/WCNC61545.2025.10978760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10978760","Large Language Models;LLaVA;Multimodal-to-Text;Prompt Engineering;In-context Learning;Global Navigation Satellite System;Interference Characterization","Global navigation satellite system;Accuracy;Large language models;Knowledge based systems;Pipelines;Interference;Feature extraction;Prompt engineering;Monitoring;Context modeling","","4","","49","IEEE","9 May 2025","","","IEEE","IEEE Conferences"
"Exploring the Potential of Prompt Engineering: A Comprehensive Analysis of Interacting with Large Language Models","V. Pawar; M. Gawande; A. Kollu; A. S. Bile","Department of Computer Engineering, Pimpri Chinchwad College Of Engineering And Research, India; Department of Computer Engineering, Pimpri Chinchwad College Of Engineering And Research, India; Department of Computer Engineering, Pimpri Chinchwad College Of Engineering And Research, India; Department of Computer Engineering, Pimpri Chinchwad College Of Engineering And Research, India","2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)","10 Dec 2024","2024","","","1","9","This article focused on exploring the core principles of prompt engineering, aiming to provide a comprehensive understanding of how to effectively interact with and instruct large language models (LLMs) using prompts. The article extensively covered various aspects of prompt engineering, including an analysis of its fundamental components, different types of prompts, ethical considerations in the prompting process, and the wide-ranging applications of prompt engineering. Additionally, the study conducted a comparative analysis between two conversational AI bots, ChatGPT 3.5 and BardAI, by subjecting them to similar prompts. The results of this comparison indicated a high level of similarity in their performances, suggesting that both bots are comparable in their capabilities. The study also highlighted the immense potential for further advancements in the field of prompt engineering. By investigating the intricacies of prompt design, ethical implications, and showcasing the promising applications, this article contributes significantly to the knowledge base surrounding prompt engineering. The findings not only shed light on the current state of the technology but also point towards numerous opportunities for future articles and development in this domain.","2771-1358","979-8-3503-9177-0","10.1109/ICCUBEA61740.2024.10775016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10775016","prompts;prompt engineering;bots;ChatGPT 3.5;BardAI;LLMs","Ethics;Technological innovation;Shape;Large language models;Knowledge based systems;Collaboration;Chatbots;Prompt engineering;Guidelines;Context modeling","","","","16","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models","A. Z. H. Yang; S. Kolak; V. Hellendoorn; R. Martins; C. L. Goues","Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2561","2573","The problem of software quality has motivated the development of a variety of techniques for Automatic Program Repair (APR). Meanwhile, recent advances in AI and Large Language Models (LLMs) have produced orders of magnitude performance improvements over previous code generation techniques, affording promising opportunities for program repair and its constituent subproblems (e.g., fault localization, patch generation). Because models are trained on large volumes of code in which defects are relatively rare, they tend to both simultaneously perceive faulty code as unlikely (or “unnatural”) and to produce generally correct code (which is more “natural”). This paper comprehensively revisits the idea of (un)naturalness for program repair. We argue that, fundamentally, LLMs can only go so far on their own in reasoning about and fixing buggy code. This motivates the incorporation of traditional tools, which compress useful contextual and analysis information, as a complement to LLMs for repair. We interrogate the role of entropy at every stage of traditional repair, and show that it is indeed usefully complementary to classic techniques. We show that combining measures of naturalness with class Spectrum-Based Fault Localization (SBFL) approaches improves Top-5 scoring by 50 % over SBFL alone. We show that entropy delta, or change in entropy induced by a candidate patch, can improve patch generation efficiency by 24 test suite executions per repair, on average, on our dataset. Finally, we show compelling results that entropy delta for patch classification is highly effective at distinguishing correct from overfitting patches. Overall, our results suggest that LLMs can effectively complement classic techniques for analysis and transformation, producing more efficient and effective automated repair techniques overall.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029753","Program Repair;Deep Learning;Large Lan","Location awareness;Codes;Large language models;Computer bugs;Training data;Software quality;Maintenance engineering;Entropy;Overfitting;Software engineering","","","","48","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Large Language Models as a Cyber Threat: Towards Countering LLM-based Spam Attacks","M. Josten; T. Weis","Department of Distributed Systems, University of Duisburg-Essen, Duisburg, Germany; Department of Distributed Systems, University of Duisburg-Essen, Duisburg, Germany",2025 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),"19 Jun 2025","2025","","","606","607","The rapid advancement and accessibility of large language models (LLMs) have significantly enhanced their utility across various disciplines. However, this pervasive presence of LLMs raises substantial security and trust concerns, particularly at the user-level, within the context of cybersecurity. This paper investigates the potential misuse of LLMs in crafting spam emails that are convincingly legitimate, thus bypassing traditional spam filters. We already conducted an experiment utilizing ChatGPT 3.5 Turbo to alter spam emails, resulting in a 70% success rate of these emails being misclassified as legitimate. We present a pipeline demonstrating the ease with which LLMs can be exploited to undermine email security, highlighting the urgency for improved defensive mechanisms. To counteract these threats, we propose a novel methodology designed to enhance the robustness of spam filters against such sophisticated attacks. This methodology comprises three phases: assessing the vulnerability of current systems to LLM-modified spam, detailed examination of the changes imposed by LLMs, and applying insights gained to fortify existing security infrastructures.","2766-8576","979-8-3315-3553-7","10.1109/PerComWorkshops65533.2025.00138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11038539","natural language processing;cybersecurity;large language models;spam","Pervasive computing;Filters;Unsolicited e-mail;Large language models;Conferences;Pipelines;Chatbots;Robustness;Computer security;Testing","","","","9","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Anchor Attention, Small Cache: Code Generation With Large Language Models","X. Zhang; Y. Zhou; G. Yang; H. C. Gall; T. Chen","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; University of Zurich, Zurich, Switzerland; School of Computing and Mathematical Sciences, Birkbeck, University of London, London, U.K.",IEEE Transactions on Software Engineering,"23 Jun 2025","2025","51","6","1866","1881","The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model’s performance.","1939-3520","","10.1109/TSE.2025.3570680","National Natural Science Foundation of China(grant numbers:62372232); State Key Laboratory of Novel Software Technology, Nanjing University(grant numbers:KFKT2023A04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11005718","Code generation;attention mechanism;transformers;large language models","Codes;Attention mechanisms;Semantics;Decoding;Context modeling;Transformers;Needles;Large language models;Data mining;Computational modeling","","","","72","IEEE","15 May 2025","","","IEEE","IEEE Journals"
"Software architecture of C2 mission system based on MAS and SOA for manned/unmanned aerial vehicle formation","X. Li; N. Bo; W. Pang; J. Tang","Department of Ordnance Science and Technology, Naval Aeronautical and Astronautical University, Yantai, Shandong Province, China; Department of Ordnance Science and Technology, Naval Aeronautical and Astronautical University, Yantai, Shandong Province, China; Department of Ordnance Science and Technology, Naval Aeronautical and Astronautical University, Yantai, Shandong Province, China; Department of Ordnance Science and Technology, Naval Aeronautical and Astronautical University, Yantai, Shandong Province, China",2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS),"23 Apr 2018","2017","","","842","845","Efficient and flexible mission system structure plays a key role in the cooperative operation of Manned/unmanned Aerial Vehicle Formation(MAVF). The software architecture of mission system for MAVF is studied in this paper. Firstly, the command and control (C2) system architecture is proposed based on the Multi-Agent System(MAS) after the quantitative analysis evaluation and simulated with MATLAB. Secondly, the software architecture of mission system based on the Service-Oriented Architecture(SOA) model is constructed, and the seven-level software system based on service is designed to improve the flexibility and reusability of the system.","2327-0594","978-1-5386-0497-7","10.1109/ICSESS.2017.8343042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8343042","Manned/unmanned aerial vehicle formation;mission system;software architecture;MAS;SOA","Computer architecture;Service-oriented architecture;Load modeling;Decision making;Semiconductor optical amplifiers;Unmanned aerial vehicles","","1","","10","IEEE","23 Apr 2018","","","IEEE","IEEE Conferences"
"Responsible AI Framework For Large Language Models (LLMs)","N. Kandhari; B. Tripathi; S. Kumar; K. Singh; N. P. Singh","Department of Computer Science & Engineering, Chandigarh University, Punjab, India; Department of Computer Application, Allenhouse Business School, UP, India; Department of Computer Science & Engineering, Chandigarh University, Punjab, India; Department of Computer Science & Engineering, Chandigarh University, Punjab, India; Department of Computer Science & Engineering, Invertis University Bareilly, UP, India",2024 11th International Conference on Advances in Computing and Communications (ICACC),"22 Jan 2025","2024","","","1","6","Large language models (LLMs) have become quite popular in recent years due to their remarkable capacity to produce language that is human-like. The creation and use of LLMs, however, also bring up serious ethical questions on topics like prejudice, discrimination, privacy, security, transparency, and unexpected consequences. Responsible AI frameworks for LLMs are becoming more and more necessary in order to solve these issues. A responsible AI framework can offer direction on how to create and use LLMs in a way that is morally right, just, and advantageous for everyone. This paper covers important factors for creating responsible AI frameworks for LLMs and emphasizes the need for such frameworks. Transparency, justice, privacy, responsibility, safety, and social duty are some of these factors. We can ensure that these models are created and used in a way that is consistent with societal values and goals and that they help to create a better and more just world by adopting a responsible AI framework for LLMs.","2766-2829","979-8-3503-7913-6","10.1109/ICACC63692.2024.10845690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845690","Large language models (LLMs);Responsible AI frameworks;Ethical considerations;Transparency;Fairness;Privacy;Accountability;Safety;Social responsibility","Privacy;Ethics;Large language models;Safety;Security","","","","22","IEEE","22 Jan 2025","","","IEEE","IEEE Conferences"
"Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles","S. Singh; F. Abri; A. S. Namin","Department of Computing Science, Texas Tech University, USA; Department of Computer Science, San Jose State University, USA; Department of Computer Science, Texas Tech University, USA",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","2508","2517","With the recent advent of Large Language Models (LLMs), such as ChatGPT from OpenAI, BARD from Google, Llama2 from Meta, and Claude from Anthropic AI, gain widespread use, ensuring their security and robustness is critical. The widespread use of these language models heavily relies on their reliability and proper usage of this fascinating technology. It is crucial to thoroughly test these models to not only ensure its quality but also possible misuses of such models by potential adversaries for illegal activities such as hacking. This paper presents a novel study focusing on exploitation of such large language models against deceptive interactions. More specifically, the paper leverages widespread and borrows well-known techniques in deception theory to investigate whether these models are susceptible to deceitful interactions. This research aims not only to highlight these risks but also to pave the way for robust countermeasures that enhance the security and integrity of language models in the face of sophisticated social engineering tactics. Through systematic experiments and analysis, we assess their performance in these critical security domains. Our results demonstrate a significant finding in that these large language models are susceptible to deception and social engineering attacks.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386814","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386814","Large Language Models (LLM);Deception Theory;Deception Techniques;Social Engineering;Security;Prompt;ChatGPT;BARD;Claude;Llama2","Systematics;Focusing;Big Data;Chatbots;Data models;Robustness;Internet","","9","","20","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models for Code Generation: Assessing Accuracy, Quality, and Performance","M. A. Shehab; M. Wardat; S. Omari; Y. Jararweh","Dept. Concordia Continuing Education, Concordia University, Montreal, Canada; Dept. of Computer Science and Engineering, Oakland University, Rochester Hills, USA; Dept. of ECaMS, Lewis University, Romeoville, USA; Dept. of Computer Science, Jordan University of Science and Technology, Irbid, Jordan",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","407","416","Large Language Models (LLMs) are increasingly utilized for software engineering tasks, including code generation. While prior research has primarily focused on code completion, there remains a gap in understanding LLMs’ ability to generate code from Natural Language Processing (NLP) descriptions. In this study, we investigated three LLMs’ performance in generating code from scratch based on NLP task descriptions. We employed three evaluation levels, Accuracy, Quality, and Performance, to assess the LLMs’ results. Accuracy metrics focused on error types and counts, quality assessed code readability and maintainability, and performance measured the models’ ability to generate optimized solutions. Codex, Copilot, and PaLM2 exhibited error rates of 9.68%, 32.25%, and 48.38%, respectively. Copilot showed the best performance with a higher maintainability index. However, all LLMs struggled with tasks of high complexity, with time complexities of O(m•n•log(n)), as evidenced by their performance metrics.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852439","Large Language Models;Natural Language Processing;Code Generation;Software Quality","Java;Codes;Accuracy;Error analysis;Large language models;C++ languages;Syntactics;Natural language processing;Complexity theory;Indexes","","","","23","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Securing Applications of Large Language Models: A Shift-Left Approach","Q. Lan; A. Kaul; N. K. Das Pattanaik; P. Pattanayak; V. Pandurangan",Global Information Security at eBay; Global Information Security at eBay; Global Information Security at eBay; Global Information Security at eBay; Global Information Security at eBay,2024 IEEE International Conference on Electro Information Technology (eIT),"31 Jul 2024","2024","","","1","2","The emergence of large language models (LLMs) has brought forth remarkable capabilities in various domains, yet it also poses inherent risks to trustfulness, encompassing concerns such as toxicity, stereotype bias, adversarial robustness, ethics, privacy, and fairness. Particularly in sensitive applications like customer support chatbots, AI assistants, and digital information automation, which handle privacy-sensitive data, the adoption of generative pre-trained transformer (GPT) models is pervasive. However, ensuring robust security measures to mitigate potential security vulnerabilities is imperative. This paper advocates for a proactive approach termed ""security shift-left,"" which emphasizes integrating security measures early in the development lifecycle to bolster the security posture of LLM-based applications. Our proposed method leverages basic machine learning (ML) techniques and retrieval-augmented generation (RAG) to effectively address security concerns. We present empirical evidence validating the efficacy of our approach with one LLM-based security application designed for the detection of malicious intent, utilizing both open-source datasets and synthesized datasets. By adopting this security shift-left methodology, developers can confidently develop LLM-based applications with robust security protection, safeguarding against potential threats and vulnerabilities.","2154-0373","979-8-3503-3064-9","10.1109/eIT60633.2024.10609922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609922","Security Integration;LLMs;Security Shift-Left","Privacy;Ethics;Toxicology;Large language models;Transformers;Particle measurements;Robustness","","","","11","IEEE","31 Jul 2024","","","IEEE","IEEE Conferences"
"AI Tutor Enhanced with Prompt Engineering and Deep Knowledge Tracing","R. Makharia; Y. C. Kim; S. Bin Jo; M. A. Kim; A. Jain; P. Agarwal; A. Srivastava; A. V. Agarwal; P. Agarwal","AI R&D, TagHive Inc., India; AI R&D, TagHive Inc., Republic of Korea; AI R&D, TagHive Inc., Republic of Korea; AI R&D, TagHive Inc., Republic of Korea; Development Team, TagHive Inc., India; Development Team, TagHive Inc., India; Planning Team TagHive Inc., Republic of Korea; Planning Team TagHive Inc., Republic of Korea; TagHive Inc., Republic of Korea",2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),"24 Apr 2024","2024","2","","1","6","The evolving educational landscape necessitates creative solutions to address the demand for immediate and personalized academic support. This study explores the integration of prompt engineering of the OpenAI’s Generative Pre-trained Transformer (GPT) and Deep Knowledge Tracing (DKT) to develop an AI tutor capable of shaping responses to students’ knowledge levels, promoting a dynamic and adaptive learning experience. By leveraging Large Language Models (LLMs) like GPT-3.5 and integrating DKT, our AI tutor addresses the need for real-time, tailored academic assistance. LLMs serve as virtual instructors, explaining concepts and providing detailed solutions, while DKT ensures responses align with the student’s knowledge level, optimizing challenge and engagement. Our research introduces an AI tutor that revolutionizes personalized learning experiences. Students can interact with the AI tutor by shaking their device during quizzes, initiating customized assistance and encouraging a deeper understanding of concepts, ultimately enhancing academic performance through individualized learning experiences.","","979-8-3503-6052-3","10.1109/IATMSI60426.2024.10503187","Ministry of SMEs and Startups; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503187","prompt engineering;deep knowledge tracing;large language models;chatbots;virtual instructors;educational technology","Knowledge engineering;Performance evaluation;Technological innovation;Databases;Oral communication;Chatbots;Transformers","","4","","10","IEEE","24 Apr 2024","","","IEEE","IEEE Conferences"
"Leveraging RAG for Effective Prompt Engineering in Job Portals","F. Haneef; V. M; P. M. P U","School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu; CapitalOne, USA","2025 2nd International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)","26 Mar 2025","2025","","","717","721","Traditional recruitment methods are time-consuming and heavily reliant on manual processes, leading to inefficiencies. This paper explores the integration of AI technologies into a job portal to streamline the hiring process, focusing on AI’s ability to automatically parse and extract information from resumes and job descriptions to match candidates with relevant job opportunities. The platform leverages Large Language Models (LLMs) to extract key information, such as skills, education, and work experience, from both resumes and job descriptions. Fine-tuning through prompt engineering ensures accurate extraction, even when data fields are incomplete or missing. To further enhance matching accuracy, Retrieval-Augmented Generation (RAG) techniques are employed. These mechanisms retrieve relevant information from a structured skills database to provide context, which, combined with the generative capabilities of LLMs, enables more contextually accurate matches. Candidates receive personalized job recommendations based on the information extracted from their resumes, while employers can post job descriptions and use AI-driven tools to match candidates. This approach of using contextual prompts not only improves matching accuracy but also reduces computational time, eliminating the need for custom models tailored specifically to resumes or job descriptions.","","979-8-3315-3038-9","10.1109/CICTN64563.2025.10932524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932524","Retrieval-Augmented Generation (RAG);Large Language Models (LLM);Artificial Intelligence (AI);Job Portal;Prompt Engineering;TF-IDF","Accuracy;Large language models;Resumes;Retrieval augmented generation;Manuals;Real-time systems;Data mining;Prompt engineering;Portals;Recruitment","","","","11","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Assessing the Reliability of Large Language Models in Generating Synthetic and Representative Datasets for International Study Program Recommendations","R. Zonjić; L. Alfirević; K. Bašić; I. Džanija; M. Vranić","Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia",2025 5th International Conference on Smart Grid Metrology (SMAGRIMET),"17 Jun 2025","2025","","","1","8","The rise of generative AI, particularly Large Language Models (LLMs), has revolutionized the tech industry, research, and society, sparking excitement about their potential to automate tedious tasks while also raising concerns over their reliability, ethical implications, and misuse. This study investigates the reliability of popular open and free LLMs, such as ChatGPT, Gemini, and DeepSeek, in generating accurate, well-supported datasets in the field of education. Our research is rooted in a project where a web application was developed to help students find suitable international study programs. The project required extensive data collection, including tuition fees, university rankings, healthcare costs, general cost of living, etc. The sheer difficulty of finding and accumulating that data, which was oftentimes poorly documented, prompted us to examine LLMs' performance in replicating these datasets. We evaluated their outputs by calculating the coefficient of determination between synthetic and fact-checked real data. Results demonstrate that LLMs can generate representative data, though occasional deviations raise questions about their limitations and the complexity of prompt engineering. This research sheds light on the practicality of LLMs in automating data-intensive tasks while highlighting their potential and limitations, offering valuable insights into the broader capabilities of generative AI.","","979-8-3503-5702-8","10.1109/SMAGRIMET66590.2025.11030048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030048","generative AI;Large Language Models;data reliability;prompt engineering;synthetic datasets;data bias;international study program recommendations","Costs;Accuracy;Large language models;Chatbots;Reliability engineering;Smart grids;Reliability;Prompt engineering;Standards;Synthetic data","","","","30","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring","M. Borg","CodeScene and Lund University, Malmö, Sweden",2025 IEEE/ACM Second IDE Workshop (IDE),"1 Jul 2025","2025","","","37","41","In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions.","","979-8-3315-0188-4","10.1109/IDE66625.2025.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052687","software maintainability;AI refactoring;large language models;trust","Industries;Codes;Large language models;Conferences;Collaboration;Human factors;Software;Calibration;Security;Testing","","","","40","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"EDA-Aware RTL Generation with Large Language Models","M. ul Islam; H. Sami; P. -E. Gaillardon; V. Tenace","PrimisAI, Los Gatos, CA, USA; PrimisAI, Los Gatos, CA, USA; University of Utah, Salt Lake City, UT, USA; PrimisAI, Los Gatos, CA, USA","2025 Design, Automation & Test in Europe Conference (DATE)","21 May 2025","2025","","","1","6","Large Language Models (LLMs) have become increasingly popular for generating RTL code. However, producing error-free RTL code in a zero-shot setting remains highly challenging even for state-of-the-art LLMs, often leading to issues that require manual, iterative refinement. This additional debugging process can dramatically increase the verification workload, underscoring the need for robust, automated correction mechanisms to ensure code correctness from the start. In this work, we introduce AIVRIL2, a self-verifying, LLM-agnostic agentic framework aimed at enhancing RTL code generation through iterative corrections of both syntax and functional errors. Our approach leverages a collaborative multi-agent system that incorporates feedback from error logs generated by EDA tools to automatically identify and resolve design flaws. Experimental results, conducted on the VerilogEval-Human benchmark suite, demonstrate that our framework significantly improves code quality, achieving nearly a 3.4× enhancement over prior methods. In the best-case scenario, functional pass rates of 77% for Verilog and 66% for VHDL were obtained, thus substantially improving the reliability of LLM-driven RTL code generation.","1558-1101","978-3-9826741-0-0","10.23919/DATE64628.2025.10992789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992789","Large language models;Multi-agent systems;Generative AI;Electronic design automation","Codes;VHDL;Generative AI;Large language models;Manuals;Syntactics;Benchmark testing;Robustness;Iterative methods;Multi-agent systems","","","","18","","21 May 2025","","","IEEE","IEEE Conferences"
"Aligning Crowd-Sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models","M. F. Wong; C. W. Tan","Department of Computer Science, City University of Hong Kong, Hong Kong, China; College of Computing and Data Science, Nanyang Technological University, Singapore",IEEE Transactions on Big Data,"","2024","PP","99","1","12","This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance text-to-code generation. Additionally, we demonstrate that our Bayesian optimization framework supports AI alignment in code generation by distributing the feedback collection burden, highlighting the value of collecting human feedback of good quality. Our empirical evaluations demonstrate the efficacy of this approach, showcasing how LLM agents can be effectively trained for improved text-to-code generation. Our Bayesian optimization framework can be designed for general domain-specific languages, promoting the alignment of large language model capabilities with human feedback in AI-assisted programming for code generation.","2332-7790","","10.1109/TBDATA.2024.3524104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818581","AI-assisted programming;bayesian analysis;code generation;convex optimization;large language models;reinforcement learning with human feedback","Codes;Programming;Computational modeling;Big Data;Bayes methods;Training;Reinforcement learning;Optimization;Large language models;Artificial intelligence","","12","","","IEEE","30 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Robust Event-Triggered Security Control for Multi-Agent Systems Under Deception Attacks","J. Chen; D. Liu; Y. Xing; Y. Liu","College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; 2011 College, Nanjing Tech University, Nanjing, China; School of Automation, Nanjing University of Information Science & Technology, Nanjing, China",2025 37th Chinese Control and Decision Conference (CCDC),"5 Aug 2025","2025","","","5560","5565","This paper presents an event-triggered control method based on dynamic thresholds and dynamic factors, aimed at enhancing the disturbance rejection ability and robustness of multi-agent systems. Unlike traditional fixed-threshold control strategies, this method dynamically adjusts the threshold and control factors, allowing real-time optimization of the control strategy based on changes in the system's state. This reduces the frequency of information exchange and minimizes communication overhead, while ensuring system stability in the presence of external disturbances and deception attacks. Through theoretical analysis and simulation validation, the proposed control method demonstrates improved system stability and strong robustness, effectively addressing disturbances and attack scenarios. Finally, simulation results show that the method exhibits good performance and promising practical applications in multi-agent systems.","1948-9447","979-8-3315-1056-5","10.1109/CCDC65474.2025.11090197","National Natural Science Foundation of China(grant numbers:62103185,62303232); Natural Science Foundation of Jiangsu Province(grant numbers:BK20220441); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11090197","Multi-agent systems;disturbance rejection;dynamic event-triggered control;deception attacks","Event detection;Simulation;Stability analysis;Robustness;Real-time systems;Security;Resource management;Optimization;Information exchange;Multi-agent systems","","","","17","IEEE","5 Aug 2025","","","IEEE","IEEE Conferences"
"Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques","Q. -K. Lin; C. Hsu; T. -S. Chang","Department of Electronics and Electrical Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electronics and Electrical Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electronics and Electrical Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",2024 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS),"27 Dec 2024","2024","","","475","478","Large Language Models (LLMs) have attracted considerable attention in recent years due to their remarkable compatibility with Hardware Description Language (HDL) design. In this paper, we examine the performance of three major LLMs, Claude 3 Opus, ChatGPT-4, and ChatGPT-4o, in designing finite state machines (FSMs). By utilizing the instructional content provided by HDLBits, we evaluate the stability, limitations, and potential approaches for improving the success rates of these models. Furthermore, we explore the impact of using the prompt-refining method, To-do-Oriented Prompting (TOP) Patch, on the success rate of these LLM models in various FSM design scenarios. The results show that the systematic format prompt method and the novel prompt refinement method have the potential to be applied to other domains beyond HDL design automation, considering its possible integration with other prompt engineering techniques in the future.","2768-3516","979-8-3503-7877-1","10.1109/APCCAS62602.2024.10808959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10808959","","Design automation;Systematics;Circuits and systems;Large language models;Asia;Automata;Stability analysis;Circuit stability;Prompt engineering;Hardware design languages","","","","5","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Enhancing Natural Language Instruction Document Comprehension with Large Language Models","S. Li; Y. Chen; X. Zhang","Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China","2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)","26 Mar 2025","2024","","","622","626","Recent advancements in Large Language Models (LLMs) have created new avenues for automated understanding and analysis of natural language instruction documents. This paper presents an innovative approach that leverages pre-trained LLMs with specialized fine-tuning techniques to facilitate understanding and analysis of instruction documents across a range of domains. Our method capitalizes on the semantic understanding capabilities of LLMs, adapting to instruction comprehension tasks through few-shot learning and prompt engineering. The model demonstrates proficiency in identifying key steps, entities, and their logical relationships within instructions, exhibiting robust cross-domain generalization capabilities. A multi-domain instruction document dataset is constructed to evaluate the model’s performance. Experimental results indicate excellent performance across a range of instruction comprehension tasks, including step sequencing, entity recognition, and contextual interpretation. This research contributes to advancing document comprehension using LLMs, with the potential for significant impact on technical documentation management, intelligent customer service, and automated workflow processes.","","979-8-3315-3399-1","10.1109/ICCBD-AI65562.2024.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933784","instruction document comprehension;natural language processing;large language models","Adaptation models;Sequential analysis;Text analysis;Large language models;Semantics;Documentation;Natural language processing;Data models;Prompt engineering;Few shot learning","","","","27","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems","H. T. Otal; M. A. Canbaz","Department of Information Science and Technology, College of Emergency Preparedness, Homeland Security, and Cybersecurity, University at Albany, SUNY, Albany, New York, United States; Department of Information Science and Technology, College of Emergency Preparedness, Homeland Security, and Cybersecurity, University at Albany, SUNY, Albany, New York, United States",2024 IEEE Conference on Communications and Network Security (CNS),"31 Oct 2024","2024","","","1","6","The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model’s performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.","2994-5895","979-8-3503-7596-1","10.1109/CNS62487.2024.10735607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735607","Honeypot;Large Language Models;Cybersecurity;Fine-Tuning","Measurement;Adaptation models;Large language models;Reinforcement learning;Network security;Data collection;Data models;Prompt engineering;Computer security;Context modeling","","3","","26","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Large Language Models, Fine Tuning, Code Generation","L. Z. Juhász","Teacher Training Center, University of Dunaújváros, Dunaújváros, Hungary",2024 IEEE 7th International Conference and Workshop Óbuda on Electrical and Power Engineering (CANDO-EPE),"10 Dec 2024","2024","","","000191","000200","Automated software synthesis, text-to-code (code-generation), can be an important aid to software developers. Developments in this area have existed practically since the advent of computing. The new artificial intelligence methods, deep learning (DL), and one type of them, large language models (LLMs) emerged in the last decade, represent a new opportunity for creating this type of application. We survey the transformer technology that underlies the operation of language models. Present paper briefly reviews the topic of software synthesis and code generation, briefly mentioning the main trends. The document concludes with an empirical study demonstrating the described results, where the T5 language model is fine-tuned to generate SQL queries. A fine-tuning set of a few thousand examples train T5 to produce quality queries. The economic principle of diminishing returns (Gossen I) applies, and it is not worth using too large a fine-tuning set. In fact, it is the attractiveness of fine-tuning.","2831-4506","979-8-3315-3111-9","10.1109/CANDO-EPE65072.2024.10772760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772760","deep learning;transformers;large language models;fine-tuning;code generation","Surveys;Structured Query Language;Power engineering;Codes;Reviews;Large language models;Biological system modeling;Transformers;Software;Tuning","","","","64","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"How Large Language Models Work","D. Farris; E. Raff; S. Biderman",Manning Publications; Manning Publications; Manning Publications,How Large Language Models Work,"","2025","","","","","Learn how large language models like GPT and Gemini work under the hood in plain English. How Large Language Models Work translates years of expert research on Large Language Models into a readable, focused introduction to working with these amazing systems. It explains clearly how LLMs function, introduces the optimization techniques to fine-tune them, and shows how to create pipelines and processes to ensure your AI applications are efficient and error-free. In How Large Language Models Work you will learn how to:  Test and evaluate LLMs Use human feedback, supervised fine-tuning, and Retrieval Augmented Generation (RAG) Reducing the risk of bad outputs, high-stakes errors, and automation bias Human-computer interaction systems Combine LLMs with traditional ML  How Large Language Models Work is authored by top machine learning researchers at Booz Allen Hamilton, including researcher Stella Biderman, Director of AI/ML Research Drew Farris, and Director of Emerging AI Edward Raff. They lay out how LLM and GPT technology works in plain language that’s accessible and engaging for all.","","9781633437081","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11079741.pdf&bkn=11079740&pdfType=book","large language models;GPT;Gemini;ChatGPT;AI book;artificial intelligence;natural language processing;transformers;tokenization;prompt engineering;RLHF;retrieval augmented generation;supervised fine-tuning;machine learning;LLM application","","","","","","","14 Jul 2025","","","Manning","Manning eBooks"
"Empowering Circuit Prototyping with Generative AI: A Framework for Vision-Based Real-Time Code Generation and Adaptation","N. A. Walee; A. Shalan; H. Wimmer; C. Kadlec","College of Engineering and Computing, Georgia Southern University, Statesboro, GA, USA; College of Engineering and Computing, Georgia Southern University, Statesboro, GA, USA; College of Engineering and Computing, Georgia Southern University, Statesboro, GA, USA; College of Engineering and Computing, Georgia Southern University, Statesboro, GA, USA","2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)","15 Jul 2025","2025","","","245","249","Circuit prototyping is crucial in electronic design but remains a challenging and time-intensive process, often demanding manual coding, iterative testing, and domain expertise. Despite the popularity of platforms like Arduino and Raspberry Pi, translating designs into functional code continues to be a bottleneck, especially for non-experts. This research introduces a generative AI-driven framework to automate and adapt code generation in real-time, streamlining prototyping workflows and enhancing accessibility and efficiency. Leveraging state-of-the-art language models, the proposed system monitors physical changes in circuit configurations on breadboards, focusing on the Arduino platform, and dynamically translates visual data into functional code. The study explores AI's potential in generating Arduino code, presenting approaches to achieve real-time adaptation during circuit design. Empirical results demonstrate the framework's effectiveness through time-series comparisons, API usage analysis, and multi-angle viewport assessments. This foundational work highlights AI's transformative potential in modernizing and simplifying circuit prototyping and design workflows.","","979-8-3315-4348-8","10.1109/AIRC64931.2025.11077546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11077546","Generative AI;Electrical Circuit;OpenAI;Code Generation;Image Analysis;Breadboard","Adaptation models;Electric potential;Codes;Translation;Generative AI;Computational modeling;Breadboard;Real-time systems;Circuit synthesis;Integrated circuit modeling","","","","21","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation","W. C. Ouédraogo; K. Kaboré; Y. Song; J. Klein; H. Tian; A. Koyuncu; D. Lo; T. F. Bissyandé",University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Melbourne; Bilkent University; Singapore Management University; University of Luxembourg,2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2464","2465","Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764990","Automatic Test Generation;Unit Tests;Large Language Models;Prompt Engineering;Empirical Evaluation","Java;Codes;Large language models;Computer bugs;Test pattern generators;Time factors;Prompt engineering;Faces;Testing;Software engineering","","2","","19","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Investigating the Efficacy of Large Language Models for Code Clone Detection","M. Khajezade; J. J. Wu; F. H. Fard; G. Rodríguez-Pérez; M. S. Shehata","University of British Columbia, Kelowna, B.C, Canada; University of British Columbia, Kelowna, B.C, Canada; University of British Columbia, Kelowna, B.C, Canada; University of British Columbia, Kelowna, B.C, Canada; University of British Columbia, Kelowna, B.C, Canada",2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC),"18 Jun 2024","2024","","","161","165","Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are ‘generative’ tasks. However, there is limited research on the usage of LLMs for ‘non-generative’ tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis 1.1Our code and data is open-sourced at https://github.com/mkhfring/llm-for-ccd","2643-7171","979-8-4007-0586-1","","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556419","Large Language Models;Code Clone Detection;Zero-shot Learning;Few-shot Learning","Charge coupled devices;Codes;Buildings;Cloning;Chatbots;Data models;Test pattern generators","","5","","26","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Research on Security Control of Multi Agent Systems Based on State Estimation","Z. Zhong; J. Wang","Dongguan electric power supply bureau, Guangdong, China; Dongguan electric power supply bureau, Guangdong, China","2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","15 Jul 2025","2025","","","1144","1149","Aiming at the issue that the communication network of multi-agent system (MAS) is vulnerable to injection attack, which may lead to the loss of consensus, a strategy based on neighbor agent state estimation is proposed to establish the system security consensus controller. Each agent can use the local embedded sensor to obtain state measurements and apply the Kalman filter algorithm to complete the optimal neighbor state estimation. An attack detection mechanism is established to judge whether the channel has been malicious injected or not. By selecting the appropriate state information to update the next state, the MAS can achieve mean square consensus. Experimental findings demonstrate that the security control method based on state estimation is effective, and can maintain mean square consistency of the system under injection attacks.","","979-8-3315-0796-1","10.1109/NNICE64954.2025.11063919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11063919","Multi agent systems;state estimation;injection attack;Kalman filter","Noise;Process control;Reliability engineering;Maintenance;Security;Kalman filters;Communication networks;Noise measurement;State estimation;Multi-agent systems","","","","12","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects","A. Mushtaq; R. Naeem; I. Ghaznavi; I. Taj; I. Hashmi; J. Qadir","Department of Computer Science, Information Technology University, Lahore, Pakistan; Department of Computer Science, Information Technology University, Lahore, Pakistan; Department of Computer Science, Information Technology University, Lahore, Pakistan; College of Interdisciplinary Studies, Zayed University, Abu Dhabi, UAE; Department of Computer Science, University of Oxford, Oxford, UK; Department of Computer Science and Engineering, Qatar University, Doha, Qatar",2025 IEEE Global Engineering Education Conference (EDUCON),"3 Jun 2025","2025","","","1","10","Multi-Agent Large Language Models (LLMs) are gaining attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the wisdom of crowds concept, where diverse agents collectively generate effective solutions, making them well-suited for educational settings. Senior design projects, pivotal in engineering education, integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. These projects often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. In this paper, we explore a framework where distinct LLM agents embody expert perspectives, including problem formulation, system complexity, societal and ethical considerations, and project management. These agents engage in rich, collaborative dialogues, leveraging multi-agent system principles like coordination, cooperation, and negotiation. Prompt engineering is employed to create diverse personas, simulating human engineering teams and incorporating swarm AI principles to balance contributions efficiently. To evaluate the framework, we analyzed six senior capstone project proposals from engineering and computer science, comparing Multi-Agent and single-agent LLMs using metrics developed with engineering faculty and widely used NLP-based measures. These metrics assess technical quality, ethical considerations, social impact, and feasibility, aligning with the educational objectives of engineering design. Our findings suggest that Multi-Agent LLMs can provide a richer, more inclusive problem-solving environment compared to single-agent systems with 89% alignment with engineering-faculty scores, offering a promising tool for enhancing the educational experience of engineering and computer science students by simulating the complexity and collaboration of real-world engineering and computer science practice. By supporting senior design projects, this tool not only aids in achieving academic excellence but also prepares students for the multifaceted challenges they will face in their professional engineering careers. We have open-sourced our framework for further development and adaptation on GitHub11Copilot is available at GitHub Repository: https://github.com/AbdullahMushtaq78/Multi-Agent-SDP-Copliot.","2165-9567","979-8-3315-3949-8","10.1109/EDUCON62633.2025.11016653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016653","Large Language Models;Gen AI;LLM Agents;LLM-Based Multi-Agent Systems;Multi-Agent Collaboration;Agentic AI","Measurement;Computer science;Ethics;Large language models;Complexity theory;Teamwork;Problem-solving;Stakeholders;Engineering education;Multi-agent systems","","","","35","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Large Language Models and Computer Security","A. Iyengar; A. Kundu",Cisco Research; Cisco Research,"2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)","16 Feb 2024","2023","","","307","313","Large language models (LLMs) are having a significant impact in many different fields. Security and privacy issues related to LLMs are of paramount importance. LLMs can be used to defend against cyberattacks and thwart attackers. LLMs can enhance computer security in several ways including identifying phishing attacks, identifying malware, analyzing incident reports from security breaches, determining vulnerability to attacks, analzying log information, and providing valuable information, education, and training materials. However, large language models can also be used by malicious parties to enhance cyberattacks. This paper looks at how LLMs can be used by both defenders and attackers. We also examine attacks on large language models which have been deployed to cause LLMs to output harmful information.","","979-8-3503-2385-6","10.1109/TPS-ISA58951.2023.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431614","Large language models;security;privacy;cyber-attacks;malware;phishing attacks","Training;Privacy;Computational modeling;Phishing;Malware;Computer crime;Intelligent systems","","2","","33","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"On the Security and Privacy Implications of Large Language Models: In-Depth Threat Analysis","L. Ruhländer; E. Popp; M. Stylidou; S. Khan; D. Svetinovic","Department of Information Systems and Operations Management, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Vienna University of Economics and Business, Vienna, Austria; Department of Computer Science, Center for Secure Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE","2024 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics","31 Oct 2024","2024","","","543","550","Large Language Models (LLMs) have gained popularity since the release of ChatGPT in 2022. These systems utilize Artificial Intelligence (AI) algorithms to analyze natural language, enabling users to have sophisticated real-time conversations with them. The existing literature on LLMs is mostly focused on system design and lacks dedicated research on investigating privacy and security issues. To safeguard the interests of various stakeholders, it is crucial to understand the associated security and privacy risks of these models. Our study utilized STRIDE and LINDDUN methodologies to investigate security and privacy threats of LLMs. We presented a detailed system model of LLMs and analyzed the potential threats, vulnerabilities, security considerations, and mitigation tactics intrinsic to the design and deployment of various system components. Our comprehensive threat assessment showcases potential threats imminent to the current generation of LLMs, such as unintentional data leakage or system misuse by malicious actors. Furthermore, our study discusses the importance of proactive security measures in LLM development, deployment, and maintenance.","2836-3701","979-8-3503-5163-7","10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10731739","Large Language Models (LLMs);STRIDE;LINDDUN;Threat Modeling;Cybersecurity;Privacy","Privacy;Social computing;Large language models;Prevention and mitigation;Oral communication;Threat assessment;Real-time systems;Security;Stakeholders;System analysis and design","","1","","38","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Fault Localization via Fine-Tuning Large Language Models with Mutation Generated Stack Traces","N. Jambigi; B. Bogacz; M. Mueller; T. Bach; M. Felderer","University of Cologne, Cologne, Germany; SAP, Walldorf, Germany; SAP, Walldorf, Germany; SAP, Walldorf, Germany; German Aerospace Center, University of Cologne, Cologne, Germany","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","417","428","Abrupt and unexpected terminations of software are termed as software crashes. They can be challenging to analyze. Finding the root cause requires extensive manual effort and expertise to connect information sources like stack traces, source code, and logs. Typical approaches to fault localization require either test failures or source code. Crashes occurring in production environments, such as that of SAP HANA, provide solely crash logs and stack traces. We present a novel approach to localize faults based only on the stack trace information and no additional runtime information, by fine-tuning large language models (LLMs). We address complex cases where the root cause of a crash differs from the technical cause, and is not located in the innermost frame of the stack trace. As the number of historic crashes is insufficient to fine-tune LLMs, we augment our dataset by leveraging code mutators to inject synthetic crashes into the code base. By fine-tuning on 64,369 crashes resulting from 4.1 million mutations of the HANA code base, we can correctly predict the root cause location of a crash with an accuracy of 66.9% while baselines only achieve 12.6% and 10.6%. We substantiate the generalizability of our approach by evaluating on two additional open-source databases, SQLite and DuckDB, achieving accuracies of 63% and 74%, respectively. Across all our experiments, fine-tuning consistently outperformed prompting non-finetuned LLMs for localizing faults in our datasets.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988982","fault localization;machine learning;large language models;mutation-based testing;stack traces","Location awareness;Software testing;Codes;Accuracy;Runtime;Large language models;Source coding;Production;Computer crashes;Software","","","","54","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Unsupervised Human Activity Recognition Via Large Language Models and Iterative Evolution","J. Gao; Y. Zhang; Y. Chen; T. Zhang; B. Tang; X. Wang","Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Tsinghua University, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","91","95","Human activity recognition (HAR) is crucial for health monitoring and disease diagnosis in Internet-of-Things environments. However, existing HAR approaches either suffer from poor accuracy or achieve high accuracy at the expense of costly manual annotations. To overcome the challenge above, we propose a novel method named LLMIE-UHAR that that leverages LLMs and Iterative Evolution to realize Unsupervised HAR. Specifically, with our designed prompt engineering mechanism, we employ large language models to fuse both contextual and semantic information, and annotate key samples selected by a clustering algorithm. Moreover, LLMIE-UHAR enhances the recognition accuracy with iterative evolution of clustering algorithm, large language models and the neural network based recognition model. Experiments conducted on the public ARAS datasets show the efficiency of our method, achieving an accuracy of 96.00%. This highlights the practical value of our approach.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446819","Large Language Models;Human Activity Recognition;Ubiquitous Computing;Unsupervised Learning;Computing methodologies","Semantics;Neural networks;Signal processing algorithms;Clustering algorithms;Speech recognition;Signal processing;Robustness","","5","","21","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Extract Information from Hybrid Long Documents Leveraging LLMs: A Framework and Dataset","C. Yue; X. Xu; X. Ma; L. Du; Z. Ding; S. Han; D. Zhang; Q. Zhang","School of Software & Microelectronics, Peking University, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Microsoft, Beijing, China; Ant Group, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Microsoft, Beijing, China; Microsoft, Beijing, China; Microsoft, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains unexplored. The hybrid text often appears in the form of hybrid long documents (HLDs), which far exceed the token limit of LLMs. Consequently, we apply an Automated Information Extraction framework (AIE) to enable LLMs to process the HLDs and carry out experiments to analyse four important aspects of information extraction from HLDs. Given the findings: 1) The effective way to select and summarize the useful part of a HLD. 2) An easy table serialization way is enough for LLMs to understand tables. 3) The naive AIE has adaptability in many complex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To address the issue of dataset scarcity in HLDs and support future work, we also propose the Financial Reports Numerical Extraction (FINE) dataset. The dataset and code are publicly available in the attachments.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889045","Information Extraction (IE);Large Language Models (LLMs);Hybrid Long Documents;Financial Reports","Large language models;Encyclopedias;Signal processing;Information retrieval;Cognition;Internet;Prompt engineering;Online services;Data mining;Speech processing","","","","31","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Flexible and Secure Code Deployment in Federated Learning using Large Language Models: Prompt Engineering to Enhance Malicious Code Detection","J. Seo; N. Zhang; C. Rong","University of Stavanger, Stavanger, Norway; University of Stavanger, Stavanger, Norway; University of Stavanger, Stavanger, Norway",2023 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),"25 Mar 2024","2023","","","341","349","Federated Learning is a machine learning methodology that emphasizes data privacy, involving minimal interaction with each other’s systems, primarily exchanging model parameters. However, this approach can introduce challenges in system development and operation because it inherently faces statistical and system heterogeneity issues. The diverse data storage formats and system environments across clients limit the feasibility of training with a uniform code. To distribute a new code to each environment, active participation of Federated Learning collaborators is necessary, incurring time and cost. Moreover, it impedes adopting modern automated development and deployment paradigms such as DevOps or MLOps. This study investigates how Large Language Models (LLMs) can automatically tailor a single code to individual client environments in heterogeneous scenarios without human intervention. Moreover, to enable the automatic adaptation of the deployed code for conducting new experiments within the system, it is imperative to assess the presence of potentially malicious code that could jeopardize data security. To address this challenge, we introduce a novel prompt engineering technique to enhance LLMs’ detection capabilities, thereby bolstering our ability to detect malicious code effectively.","2380-8004","979-8-3503-3982-6","10.1109/CloudCom59040.2023.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475813","Software Engineering;Federated Learning;Large Language Models;Security","Training;Privacy;Cloud computing;Technological innovation;Codes;Federated learning;Memory","","1","","34","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"Pitfalls of Generic Large Language Models (GLLMs) from reliability and security perspectives","D. Dasgupta; A. Roy","Dept. of Computer Science, The University of Memphis, Memphis, TN, USA; Dept. of Computer Science, The University of Memphis, Memphis, TN, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","412","419","Generic Large Language Models (GLLMs) have grown popularity in many professions with limited or no technical knowledge. Larger and larger GLLMs are continuously being released with enhanced capabilities, promoting the abilities of these Generative AI at the grassroots level in businesses. These tools excel in text, image, and video generation (assembling, summarizing, translating) when proper queries and prompts are given; moreover, various augmentation of up-to-date knowledge bases, making these more efficient in providing current events. Practitioners and marketers showcase the benefits of GLLMs by demonstrating various use cases. However, the reliability of GLLMs' responses is yet questionable in certain scenarios, particularly due to issues like hallucinations, factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on data collection, privacy and ethical issues that need to be addressed. This study emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications. We also provide some insides of social impacts and future directions of AI/ML applications.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835578","Generative AI;Large Language Models (LLMs);Generative Pre-Trained Models (GPTs);Small Parameterized Data Models (SPDM)","Data privacy;Ethics;Translation;Generative AI;Large language models;Knowledge based systems;Data models;Security;Reliability;Intelligent systems","","","","56","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models in Exercises of UML Use Case Diagrams Modeling","G. Garaccione; P. F. Vega Carrazan; R. Coppola; L. Ardito","Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy",2025 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE),"13 Jun 2025","2025","","","41","44","In recent years, Large Language Models (LLMs) have been extensively used in several Software Engineering tasks, from requirements analysis to coding and software testing. Research has proved that LLMs can effectively generate software models to assist in software documentation. The goal of this study is to assess the capability of LLM agents to generate UML Use Case Diagrams (UCD), starting from software requirements in natural language. We perform the assessment in an educational setting, i.e., we evaluate the capability to solve software modeling exercises tailored for master's students in SE curricula. Our results, based on the comparison of the results obtained by a human and an LLM solver on 17 UCD modeling exercises, show that LLMs have comparable results in terms of completeness and redundancy of the generated diagrams, with no significant difference if compared to human-proposed solutions.","","979-8-3315-3864-4","10.1109/NLBSE66842.2025.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029431","Large Language Models;Use Case Diagrams;Software Modeling","Software testing;Large language models;Conferences;Unified modeling language;Redundancy;Natural languages;Documentation;Software;Encoding;Software engineering","","","","15","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Security Patch Detection and CWE Classification","B. Gokkaya","School of Electronics and Computer Science, University of Southampton, Southampton, UK","2025 7th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA)","2 Jun 2025","2025","","","1","10","Open-source software is critical for modern digital infrastructure, yet security vulnerabilities remain a significant concern as attackers exploit unpatched systems. Large Language Models (LLMs) have shown promise in vulnerability detection, but their ability to detect security patches solely based on code changes remains underexplored. This capability is crucial for identifying security patches when commit messages lack explicit security labels. This study evaluates six LLMs, e.g., GPT-4o, Claude 3.5 Haiku, and DeepSeek V3, using various prompting approaches to assess their capacity to distinguish between security and non-security patches and analyze their effectiveness in classifying security patches into their corresponding CWE categories. The results show that LLMs can detect security patches, but performance varies across models and prompting strategies. DeepSeek V3 (Chain-of-Thought) and GPT-4o (Zero-Shot) demonstrate the most consistent performance across all evaluation metrics, each achieving over 70% in accuracy, precision, recall, and F1-score. New prompting techniques have also led to notable improvements in certain areas, particularly in precision. However, CWE classification remains a major challenge, with most models misclassifying over 70% of security patches. Even the best-performing model, Claude Haiku 3.5 (Few-Shot), achieves only 31.1% accuracy, with memory-related vulnerabilities like out-of-bounds write and use-after-free being the most frequently misclassified. These findings highlight the potential of LLMs in security patch detection but emphasize the need for improved CWE classification. Source code available at: https://github.com/betulgkkaya/LLMs_Patch_Detection.git.","2996-4393","979-8-3315-1088-6","10.1109/ICHORA65333.2025.11017081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11017081","Security Patch Detection;LLMs;Silent Fix Identification;Prompt Engineering;CWE Classification","Measurement;Human computer interaction;Accuracy;Large language models;Source coding;Security;Prompt engineering;Robots;Optimization;Open source software","","","","16","Crown","2 Jun 2025","","","IEEE","IEEE Conferences"
"The Impact of Prompt Engineering on Large Language Models: A Case Study of Sustainable Development Goals","P. Chuayrod; S. Soysangwarn; P. Siripathavanich; P. Raknim; N. Ketui","Saraburi Wittayakhom School, Saraburi, Thailand; Satriwitthaya 2 School, Bangkok, Thailand; Saengthong Vitthaya School, Songkhla, Thailand; College of Integrated Science and Technology, Rajamangala University of Technology Lanna, Chiang Mai, Thailand; Faculty of Science and Agricultural Technology, Rajamangala University of Technology Lanna, Thailand",2024 19th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),"18 Dec 2024","2024","","","1","6","Large Language Models (LLMs) have demonstrated an impressive performance over a wide range of Natural Language Processing (NLP) tasks, demonstrating potential in the realms of understanding human language with great precision. In this paper, we compare a number of state-of-the-art LLMs and Prompts on how well they achieve in an application-specific task to identify Sustainable Development Goals (SDGs) from textual content. Our evaluation of these models is built around essential BERTScore metrics. Our results provide a more nuanced perspective of the advantages and weaknesses in each model’s presentation, uncovering their appropriateness for SDG identification tasks as well as implications for wider deployment across other NLP based applications. Our results show that GPT-4.0 stands out as the best performing model for identifying Sustainable Development Goals (SDGs) from textual content, achieving the highest precision, recall, and F1-Score across various datasets. Moreover, the Few-Shot prompting technique emerged as the most effective method for guiding the model’s responses. By providing GPT-4.0 with a few carefully selected examples, Few-Shot prompting significantly enhances its ability to accurately map text to relevant SDGs, making it the optimal choice for this task.","2831-4565","979-8-3315-0991-0","10.1109/iSAI-NLP64410.2024.10799499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10799499","Large Language Models (LLMs);Sustainable Development Goals (SDGs);Propmt Engineering;NLP","Measurement;Large language models;Decision making;Environmental degradation;Organizations;Natural language processing;Pattern recognition;Prompt engineering;Sustainable development;Context modeling","","","","21","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"The (ab)use of Open Source Code to Train Large Language Models","A. Al-Kaswan; M. Izadi","Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands",2023 IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering (NLBSE),"26 Jul 2023","2023","","","9","10","In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue.","","979-8-3503-0178-6","10.1109/NLBSE59153.2023.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189147","NLP;Large-Language-Models;GPT;Deep-Learning;Software-Engineering;Memorization;Open-Source;Copyleft;Copyright","Privacy;Ethics;Codes;Law;Source coding;Conferences;Licenses","","18","","5","IEEE","26 Jul 2023","","","IEEE","IEEE Conferences"
"A Multi-dimensional Generic Evaluation Framework for the Security of Large Language Models","Z. Yu","School of Future Technology, South China University of Technology, Guangzhou, China","2023 4th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","17 Oct 2023","2023","","","410","414","In light of the widespread adoption of large language models, their susceptibility to security vulnerabilities cannot be overlooked. As a result, it has become imperative to evaluate their proficiency in addressing issues such as toxicity, bias, and disinformation. However, current research focused on appraising and mitigating security risks has predominantly concentrated on specific facets, leading to disparities in evaluation criteria. In contrast, there has been relatively limited attention given to multidimensional and universal frameworks for security evaluation. In this context, this paper delves into the realm of generic evaluation frameworks for security that offer support for cross-language and multi-category analysis. We underscore the existing challenges associated with prominent large language models concerning security issues and develop a comprehensive test data set to furnish researchers with a tool for quantifying security aspects. Through comprehensive evaluations across three major benchmark tests, we identify distinct strengths and weaknesses exhibited by each open source large language model to varying degrees. By employing a multi-dimensional security evaluation framework, we can attain a more holistic comprehension of the performance exhibited by each model across diverse security dimensions. This approach holds significant value in advancing the domain of security research and facilitating the practical application of language models.","","979-8-3503-4361-8","10.1109/ICBAIE59714.2023.10281279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10281279","security evaluation;benchmark test;framework;generic;large language model","Toxicology;Media;Big Data;Benchmark testing;Data models;Security;Internet of Things","","2","","8","IEEE","17 Oct 2023","","","IEEE","IEEE Conferences"
"Integrating Generative AI for Enhanced Automation in System Design Processes","J. Guntupalli; K. Watanabe","Services Computing Research Department, Hitachi Ltd., Tokyo, Japan; Services Computing Research Department, Hitachi Ltd., Tokyo, Japan",2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA),"16 Oct 2024","2024","","","1","4","This work delves into the potential of applying Generative AI techniques to the system design phase, aiming to streamline processes and augment human expertise. Gen AI is used to automate the creation of complex design elements and is emerging as a powerful tool for system engineers. Also, with LLM -generated content, evaluating and verifying the correctness of the responses is a challenge. The SE Assistant designed for system engineers intends to create detailed system design documents quickly and accurately along with its evaluation. At the core of the SE Assistant is a sophisticated system that combines the power of GPT-4 with a Multimodal Retrieval Augmented Generation (RAG) pipeline capable of understanding text, images, and tables to provide valuable context. The evaluation uses the strength of strong LLMs in analyzing content based on design-specific criteria. The SE Assistant prototype demonstrates its ability to streamline the system design process, from initial data gathering to the final design output, making it an invaluable tool for system engineers.","1946-0759","979-8-3503-6123-0","10.1109/ETFA61755.2024.10710979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710979","Generative AI;System Engineering;System Design;Large Language Model;Retrieval Augmented Generation","Generative AI;Pipelines;Prototypes;Streaming media;System analysis and design;Manufacturing automation","","","","12","IEEE","16 Oct 2024","","","IEEE","IEEE Conferences"
"Uncertainty-Aware Autonomous Driving System Testing with Large Language Models","J. Wu","Simula Research Laboratory, University of Oslo, Oslo, Norway","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","776","778","Autonomous Driving Systems (ADSs) operate in highly dynamic and uncertain environments, requiring testing methods that address both internal (e.g., algorithm randomness, sensor limitations) and external (e.g., unpredictable events, human interactions) uncertainties. However, current approaches often fall short in realistically quantifying these uncertainties, as they struggle to address the complex interplay between known and unknown factors in real-world scenarios. To address these challenges, this work explores leveraging Large Language Models (LLMs) to enhance ADS testing by incorporating human-like reasoning and domain-specific knowledge through prompt engineering, retrieval-augmented generation, and fine-tuning. By integrating LLMs with techniques like Search-Based Testing, we aim to improve testing realism, enhance efficiency, and handle uncertainties more effectively. The proposed strategies seek to develop optimized ADS testing frameworks, enabling safer and more reliable ADS deployments.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988929","Research Council of Norway(grant numbers:314544); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988929","autonomous driving systems;uncertainty;large language models;realism","Software testing;Knowledge engineering;System testing;Uncertainty;Large language models;Heuristic algorithms;Retrieval augmented generation;Software reliability;Prompt engineering;Autonomous vehicles","","","","26","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method with Large Language Models","Z. Yin; S. Liu; G. Xu","Shenyang Institute of Computing Technology, Chinese Academy of Sciences; Shenyang Institute of Computing Technology, Chinese Academy of Sciences; Shenyang Institute of Computing Technology, Chinese Academy of Sciences","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","The increasing number of Distributed Denial of Service (DDoS) attacks poses a major threat to the Internet, highlighting the importance of DDoS mitigation. Most existing approaches require complex training methods to learn data features, which increases the complexity and generality of the application. In this paper, we propose DrLLM, which aims to mine anomalous traffic information in zero-shot scenarios through Large Language Models (LLMs). To bridge the gap between DrLLM and existing approaches, we embed the global and local information of the traffic data into the reasoning paradigm and design three modules, namely Knowledge Embedding, Token Embedding, and Progressive Role Reasoning, for data representation and reasoning. In addition we explore the generalization of prompt engineering in the cybersecurity domain to improve the classification capability of DrLLM. Our ablation experiments demonstrate the applicability of DrLLM in zero-shot scenarios and further demonstrate the potential of LLMs in the network domains. The implementation of DrLLM is available at https://github.com/liuup/DrLLM.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889735","Distributed Denial-of-Service;zero-shot;Large Language Models","Training;Resistance;Large language models;Prevention and mitigation;Signal processing;Cognition;Internet;Prompt engineering;Computer crime;Speech processing","","","","30","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Drug Pills Identification System using Google Gemini LLM: A Generative AI approach","M. R; A. R; M. P. N; A. Unnikrishnan; S. S","Department of Information Technology, Sri Eshwar College of Engineering, Coimbatore, India; Department of Information Technology, Sri Eshwar College of Engineering, Coimbatore, India; Department of Information Technology, Sri Eshwar College of Engineering, Coimbatore, India; Department of Information Technology, Sri Eshwar College of Engineering, Coimbatore, India; Department of Information Technology, Sri Eshwar College of Engineering, Coimbatore, India",2024 International Conference on Emerging Research in Computational Science (ICERCS),"27 Feb 2025","2024","","","1","5","Generative AI is emerging as a disruptive force in the healthcare industry, bringing novel solutions ranging from drug development and clinical decision support to personalized patient care. This study is focused on drug discovery using the Generative AI model. In this paper, a system is proposed for providing drug descriptions from drug pill images. The system is implemented by utilizing Large Language Models (LLMs) in combination with computer vision to detect and provide detailed information about drugs from pill images. In the proposed system, the identification process begins by taking the medicinal drug pills and their cover images. Then, the image is converted into binary values using a standard built-in function. In addition, the target language for providing audio descriptions about the drugs is also used. Then, the Google Gemini LLM model is customized by using binary values of the image, target language, and ontology-based prompt engineering. As a result, the LLM model provides drug descriptions in text. Then, the textual description of the drug is converted into the target language audio format by using the Google Text to Speech Converter. The system is experimented by using 807 medicinal drug images which are collected from web resources. The performance of the system is measured by using accuracy. The system achieved an accuracy of 95.04% which is a little higher when compared with the current state-of-the-art model.","","979-8-3315-3496-7","10.1109/ICERCS63125.2024.10895371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895371","Drug Pill Identification;Generative AI;Large Language Models;Pharmaceutical Image Analysis;Multimodal Learning;Medical Text Analysis","Drugs;Accuracy;Text analysis;Computational modeling;System performance;Medical services;Streaming media;Internet;Text to speech;Biomedical imaging","","","","17","IEEE","27 Feb 2025","","","IEEE","IEEE Conferences"
"Attack Detection Based on Domain-Specific Large Language Models","X. Chen; M. Zhu; X. Shang","School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; Shanghai Moule Network Technology Co., Ltd., Shanghai, China","2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","15 Jul 2025","2025","","","1190","1193","SQL Injection Attacks (SQLIA) and Cross-Site Scripting (XSS) attacks represent significant threats to web security. To effectively detect and identify these attacks and address the challenges of poor adaptability and high false positive rates in traditional methods, this paper proposes a novel attack detection approach based on domain-specific large language models. By constructing a specialized dataset, applying prompt engineering techniques, and employing precise prompt-based finetuning, we develop a domain-specific large language model tailored for attack detection. The number of fine-tuning samples and various training parameters are analyzed and optimized to improve the detection performance of the model. Experimental results demonstrate that the domain-specific large language model-based detection method outperforms general models in identifying SQLIA and XSS attacks, achieving an accuracy rate exceeding 96.7% and a false positive rate below 0.6%. These results indicate the method's promising potential for practical application.","","979-8-3315-0796-1","10.1109/NNICE64954.2025.11064357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11064357","Domain-Specific Large Language Models;SQL injection attacks;Cross-Site Scripting;attack detection","Training;Analytical models;Accuracy;Large language models;Cross-site scripting;Artificial neural networks;SQL injection;Prompt engineering;Security","","","","12","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Generating Requirements Elicitation Interview Scripts with Large Language Models","B. Görer; F. B. Aydemir","Department of Computer Engineering, Boğaziçi University, Istanbul, Türkiye; Department of Computer Engineering, Boğaziçi University, Istanbul, Türkiye",2023 IEEE 31st International Requirements Engineering Conference Workshops (REW),"28 Sep 2023","2023","","","44","51","Requirements elicitation interviews are the most popular requirements elicitation technique and an integral part of requirements engineering education. Good and bad interview scripts provide students with examples of applying the theory. Constructing an interview script requires technical knowledge, practical experience, and creativity. As a result, only a few educational interview scripts are available to the community. This paper explores automatically generating interview scripts with large language models through prompt engineering. Our contribution is two-fold: First, we present a graph representation of interactive interview scripts. Second, we apply prompt engineering techniques to generate business domain descriptions, linear scripts, and conversation pieces focused on certain types of mistakes. Our findings indicate that large language models face challenges in handling interview conversation graphs. However, we can enhance the quality of the generated interview scripts by decomposing the task into smaller components and refining the prompts to provide more precise instructions.","2770-6834","979-8-3503-2691-8","10.1109/REW57809.2023.00015","Scientific and Technological Research Council of Türkiye through BIDEB 2232(grant numbers:118C255); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260795","large language models;prompt engineering;elicitation interview script generation;requirements engineering education","Training;Terminology;Refining;Oral communication;Requirements engineering;Stakeholders;Interviews","","7","","21","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"FaultLines - Evaluating the Efficacy of Open-Source Large Language Models for Fault Detection in Cyber-Physical Systems*","H. Mühlburger; F. Wotawa","Christian-Doppler Laboratory QAMCAS, Institute of Software Technology Graz University of Technology, Graz, Austria; Christian-Doppler Laboratory QAMCAS, Institute of Software Technology Graz University of Technology, Graz, Austria",2024 IEEE International Conference on Artificial Intelligence Testing (AITest),"25 Sep 2024","2024","","","47","54","Cyber-physical systems are integral to the infrastructure of global communication and transportation networks, which makes it crucial to detect faults, prevent cyber attacks, and ensure operational safety. Although machine learning techniques, including large language models (LLMs), have been explored for fault detection, the efficacy of open-source LLMs remains underexplored. In this work, we assess the capabilities of eight open-source LLMs in identifying faults in cyber-physical systems using a simulation dataset from monitoring an electrified vehicle's battery management system. By applying pretrained LLMs without fine-tuning and incorporating retrieval augmented generation (RAG) techniques alongside textual encoding methods, our study aims to explore the potential of open LLMs in fault detection. Our results show that open LLMs can effectively identify faults, with Mistral out-performing alternative models such as Mixtral, codellama, and Gemma in precision, recall, and Fl-score metrics. Furthermore, our results highlight the importance of textual encoding strategies in enhancing the fault detection capabilities of LLMs, which possess a degree of explanatory power with respect to the detected anomalies. This work demonstrates the feasibility of using open LLMs for fault detection in cyber-physical systems and opens avenues for future research to enhance fault detection and fault localization.","2835-3560","979-8-3503-6505-4","10.1109/AITest62860.2024.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685216","Cyber-physical systems fault detection;Open-source large language models (LLMs);Retrieval Augmented Generation (RAG) techniques;Textual encoding strategies for LLMs;Anomaly detection","Fault diagnosis;Analytical models;Fault detection;Large language models;Transportation;Cyber-physical systems;Encoding","","","","24","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"Optimizing the Performance of Generative Artificial Intelligence ,Recent Approaches to Engineering Large Language Models","E. Ellahi; K. P. R; M. Talha; M. Hariprabhu; V. M. Aragani; M. Tiwari","Information Systems, Cleveland State University, Cleveland, Ohio, USA; Research and Consultancy Department, University of Technology and Applied Sciences, Khasab, Governorate of Musandam, Sultanate of Oman; IT Business Analyst II, Information Systems Cleveland State University, Cleveland, Ohio, USA; Department of Electrical and Electronics Engineering, M.Kumarasamy College of Engineering, Karur; Eliassen Group, Software Engineer Architect, NC, USA; Department of Computer Science and Engineering, Bharati Vidyapeeth's College of Engineering","2024 International Conference on Advances in Computing, Communication and Materials (ICACCM)","2 Jul 2025","2024","","","1","6","In the ever-growing field of Generative Artificial Intelligence, quick engineering has become a revolutionary method in the field of NLP for big language versions. This technique entails the manipulation of input queries to improve the quality and relation of the textual outputs of these models. New findings point to the fact that through the enhancement of the aspect of prompt engineering it could indeed be a way of significantly boosting LLM performance through a change of the input query format. This paper highlights the practical use of the prompt engineering concept to Tamil-based LLMs with an explicit purpose of developing a well-structured process capable of generating accurate and contextually relevant conversational responses. Thus, with subtle changes to how prompts are offered, this research aims to enhance the efficacy of Tamil LLMs that demand trivial data entry. The research uses the Query Transformation Module (QTM), which is an elaborate method developed to systematically change the input prompts into three forms of inquiries. Therefore, each format is built with objectives and key points in mind to enhance the understanding of the models as well as the results. To measure the effectiveness of this strategy, the QTM was employed with leading Tamil LLMs SKT GPT-2 and Tamil ChatGPT. We employed four different query methods in our experiments: all the initially employed unaltered request and three modified requests according to the QTM. The evaluation was made using Google SSA to fit the evaluation criteria of naturalness and specificity for the sentences generated by the models. Our experimental findings show that overall, there is a reasonable improvement in the performance by an average of 11 percent. Identified that there is an increase of 46% in the quality of generated sentences when using the transformed queries as compared to the unmodified prompts. This improvement also depicts how the QTM has an effectiveness of enhancing the performance of Tamil LLMs, resulting into its usability as a tool in the corresponding field of prompt engineering.","2642-7354","979-8-3503-6797-3","10.1109/ICACCM61117.2024.11059049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11059049","AI;LLMS;generative AI;prompt engineering;Tamil AI models;QTM;performance assessment of LLMs;AI chatbots","Training;Measurement;Adaptation models;Large language models;Training data;Chatbots;Real-time systems;Internet;Prompt engineering;Usability","","","","16","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation","K. Jin; C. -Y. Wang; H. V. Pham; H. Hemmati","York University, Toronto, Canada; York University, Toronto, Canada; York University, Toronto, Canada; York University, Toronto, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","167","171","Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers’ conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.","2574-3864","979-8-4007-0587-8","","Alberta Innovates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555687","","Codes;Refining;Collaboration;Oral communication;Documentation;Chatbots;Software","","1","","26","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Interoperability rules for heterogenous multi-agent systems: Levels of conceptual interoperability model applied for multi-agent systems","E. Wassermann; A. Fay","Institute of Automation Technology Helmut Schmidt University, Hamburg, Germany; Helmut-Schmidt-Universitat Universitat der Bundeswehr Hamburg, Hamburg, Hamburg, DE",2017 IEEE 15th International Conference on Industrial Informatics (INDIN),"13 Nov 2017","2017","","","89","95","Multi-agent systems (MAS) are a promising technology for solving the challenges posed by the trend towards increasing digitalization — Industrie 4.0, Industrial Internet of Things, Cyber-Physical Systems or Smart Grids. In spite of this promise and many showcases and prototypes, multi-agent systems are not widely applied in industry nowadays. Reasons for this are for example lacking trust in the technology and special demand for safety and security in these domains. To address these topics, the Levels of Conceptual Interoperability Model is applied on heterogeneous multi-agent systems. Additionally interoperability rules are introduced for enhancing the interoperability of MAS. It is further outlined how such rules can be used for unit testing single agents and for runtime verification.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104752","Multi-agent systems;interoperability;cyber-physical sytems;verification","Interoperability;Semantics;Multi-agent systems;Pragmatics;Protocols;Syntactics;Industries","","6","","48","IEEE","13 Nov 2017","","","IEEE","IEEE Conferences"
"RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects","A. Allam; M. Shalan","Department of Computer Science and Engineering, The American University in Cairo, Cairo, Egypt; Department of Computer Science and Engineering, The American University in Cairo, Cairo, Egypt",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","5","Large Language Models (LLMs) have demonstrated potential in assisting with Register Transfer Level (RTL) design tasks. Nevertheless, there remains to be a significant gap in benchmarks that accurately reflect the complexity of real-world RTL projects. To address this, this paper presents RTL-Repo, a benchmark specifically designed to evaluate LLMs on large-scale RTL design projects. RTL-Repo includes a comprehensive dataset of more than 4000 Verilog code samples extracted from public GitHub repositories, with each sample providing the full context of the corresponding repository. We evaluate several state-of-the-art models on the RTL-Repo benchmark, including GPT-4, GPT-3.5, Starcoder2, alongside Verilog-specific models like VeriGen and RTLCoder, and compare their performance in generating Verilog code for complex projects. The RTL-Repo benchmark provides a valuable resource for the hardware design community to assess and compare LLMs’ performance in real-world RTL design scenarios and train LLMs specifically for Verilog code generation in complex, multi-file RTL projects. RTL-Repo is open-source and publicly available on Github1.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691810","LLM-Aided Hardware Design;Verilog Code Generation;RTL Design Automation;Benchmarking Large Language Models","Codes;Design automation;Large language models;Conferences;Benchmark testing;Hardware;Registers;Complexity theory;Hardware design languages;Software development management","","4","","15","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Evaluating Correct-Consistency and Robustness in Code-Generating LLMs","S. Honarvar","Department of Computing, Imperial College London, London, UK","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","797","800","Ensuring the reliability of large language models (LLMs) for code generation is crucial for their safe integration into software engineering practices, especially in safety-critical domains. Despite advances in LLM tuning, they frequently generate incorrect code, raising concerns about robustness and trustworthiness. This PhD research introduces Turbulence, a novel benchmark and evaluation framework designed to assess both the correct-consistency and robustness of LLMs through a structured coding question neighbourhood approach. By evaluating model performance across sets of semantically related but non-equivalent coding tasks, Turbulence identifies discontinuities in LLM generalisation, revealing patterns of success and failure that standard correctness evaluations often overlook. Applied to 22 instruction-tuned LLMs across Python coding question neighbourhoods, the benchmark highlights significant variability in correctness, including error patterns persisting even under deterministic settings. Future work will extend the question neighbourhood concept to Capture The Flag (CTF) challenges, enabling a deeper analysis of model reasoning capabilities in progressively complex tasks. This extension has attracted interest from the UK AI Safety Institute, which recognises the frame-work's potential for advancing rigorous evaluation methodologies in the context of safe and trusted AI for software engineering.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988971","Large Language Models;Correct-consistency;Robustness;Evaluation;Code Generation","Software testing;Codes;Large language models;Benchmark testing;Robustness;Encoding;Software reliability;Tuning;Standards;Software engineering","","","","39","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Large Language Models in Healthcare: A Review","S. Zou; J. He","College of Information and Communication, National University of Defense Technology, Wuhan, China; College of Information and Communication, National University of Defense Technology, Wuhan, China",2023 7th International Symposium on Computer Science and Intelligent Control (ISCSIC),"26 Jan 2024","2023","","","141","145","This paper examines the potential of large language models (LLMs) in the healthcare sector, delving into their prospective applications, challenges, and future trajectories. LLMs have demonstrated encouraging results in various healthcare-related domains, including the development of clinical decision support systems, natural language processing in electronic health records, healthcare question/answer systems, and healthcare education. However, integrating these models into healthcare practice also raises several concerns, such as data privacy and security issues, the requirement for vast amounts of training data, model biases, and the limited interpretability of model predictions. Overcoming these hurdles necessitates a collaborative effort from experts across multiple disciplines. Despite these obstacles, the deployment of LLMs in healthcare holds the potential to transform the industry and significantly enhance patient outcomes.","","979-8-3503-4298-7","10.1109/ISCSIC60498.2023.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409672","Large Language Models;ChatGPT;Applications in Healthcare","Computational modeling;Training data;Medical services;Transforms;Predictive models;Data models;Trajectory","","3","","25","IEEE","26 Jan 2024","","","IEEE","IEEE Conferences"
"Intelligent Orientation Robot Based on Large Language Models and Retrieval-Augmented Generation","X. Deng; D. Yang; Y. Zhang","School of Information, North China University of Technology, Beijing, China; School of Information, North China University of Technology, Beijing, China; School of Information, North China University of Technology, Beijing, China",2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE),"6 Feb 2025","2024","","","779","782","To address the issues of hallucination and knowledge security in large language models (LLMs) during Question and Answer processes, this project developed a Q&A platform integrating Retrieval-Augmented Generation (RAG) with LLMs and enabled communication with intelligent robots. This project, applied to the university orientation scenario, built an orientation knowledge base comprising 11 categories of documents, about 300,000 words in all. By leveraging RAG for precise retrieval and using LLMs to generate logical and traceable answers, the system enhances both the accuracy and reliability of responses. The project also established remote communication with robots through the WebSocket protocol, offering flexible voice and touchscreen interaction, greatly improving the user experience. This platform not only improves the logical consistency of text-based Q&A but also strengthens the interaction between robots and users, providing a certain technical foundation for future multi-scenario intelligent interactions.","","979-8-3315-2891-1","10.1109/ICAICE63571.2024.10863858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10863858","Large Language Models;Retrieval-Augmented Generation;WebSocket;LangChain;Embodied Intelligence","Protocols;Large language models;Retrieval augmented generation;Knowledge based systems;Information security;Touch sensitive screens;Data collection;User experience;Reliability;Intelligent robots","","","","15","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"Research on Applying Large Language Models for Software Updates in Power Communication Networks","J. Niu; L. Yan; Z. Zhao; Y. He; N. Zhang; G. Song; X. Yuan","State Grid Information&Telecommuniccation Branch, Beijing, China; State Grid Information&Telecommuniccation Branch, Beijing, China; State Grid Information&Telecommuniccation Branch, Beijing, China; State Grid Information&Telecommuniccation Branch, Beijing, China; State Grid Information&Telecommuniccation Branch, Beijing, China; State Grid Information&Telecommuniccation Branch, Beijing, China; Beijing Kedian Tuoyu Intelligent Technology Co., Ltd, Beijing, China","2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","11 Feb 2025","2024","4","","727","731","The widespread use of various software in power communication networks introduces more and more challenges, particularly in ensuring the security and stability of software updates. Large language models (LLMs) with their excellent capabilities in natural language processing and understanding complex systems offer a new solution. This paper examines basic workflow of software updates in power communication networks and discusses the application of LLMs to facilitate the software updates process. This applying of LLMs can provide a more efficient and insightful approach to manage the complexities and risks of software updates in power communication networks.","","979-8-3503-6360-9","10.1109/ICIBA62489.2024.10867881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10867881","Large Language models;power communication networks;software updates","Large language models;System performance;Software;Stability analysis;Natural language processing;Maintenance;Communication networks;Security;Risk management;Research and development","","","","16","IEEE","11 Feb 2025","","","IEEE","IEEE Conferences"
"Software Vulnerability and Functionality Assessment using LLMs","R. I. T. Jensen; V. Tawosi; S. Alamir","JP Morgan AI Research, London, UK; JP Morgan AI Research, London, UK; JP Morgan AI Research, London, UK",2024 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE),"29 Aug 2024","2024","","","25","28","While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final “approve or reject” recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that $36.7 \%$ of LLM-generated descriptions can be associated with true CWE vulnerabilities.CCS CONCEPTS• Software and its engineering → Software verification and validation; Software development techniques.","","979-8-4007-0576-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10647161","Software Security;Functional Validation;Large Language Models","Codes;Reviews;Large language models;Conferences;Software;Security;Task analysis","","4","","22","","29 Aug 2024","","","IEEE","IEEE Conferences"
"Multi-Role Consensus Through LLMs Discussions for Vulnerability Detection","Z. Mao; J. Li; D. Jin; M. Li; K. Tei","Waseda University, Tokyo, Japan; Waseda University, Tokyo, Japan; Peking University, Beijing, China; Dalian Maritime University, Dalian, China; Tokyo Institute of Technology, Tokyo, Japan","2024 IEEE 24th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","29 Oct 2024","2024","","","1318","1319","Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability de-tection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software de-velopment life-cycle, including both developers and testers. To this end, this paper introduces a multi-role approach to employ LLMs to act as different roles simulating a real-life code review process and engaging in discussions toward a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of this approach indicates a 13.48% increase in the precision rate, an 18.25% increase in the recall rate, and a 16.13% increase in the F1 score.","2693-9371","979-8-3503-6565-8","10.1109/QRS-C63300.2024.00173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10726971","large language models;vulnerability detection;prompt engineering;software quality assurance","Codes;Quality assurance;Reviews;Large language models;Software quality;Reliability engineering;Software reliability;Security","","4","","6","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"Using Large Language Models to Generate Concise and Understandable Test Case Summaries","N. Djajadi; A. Deljouyi; A. Zaidman","Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","322","326","Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025920","Automated Test Generation;Large Language Models;Unit Testing;Readability","Software testing;Large language models;Documentation;Software;Test pattern generators","","","","36","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Web Content Filtering Through Knowledge Distillation of Large Language Models","T. Vörös; S. P. Bergeron; K. Berlin","Sophos AI, Budapest, Hungary; Sophos AI, Budapest, Hungary; Sophos AI, Washington, United States",2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),"19 Dec 2023","2023","","","357","361","We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9 % accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large volumes of URLs, and requires 3 orders of magnitude less manually labeled training data than the current state-of-the-art approach. Depending on the specific use case, the output generated by our approach can either be directly returned or employed as a pre-filter for more resource-intensive operations involving website images or HTML.","","979-8-3503-0918-8","10.1109/WI-IAT59888.2023.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350067","web content filtering;machine learning;large language models","Training;Knowledge engineering;Uniform resource locators;Adaptation models;Training data;Tail;Organizations","","9","","23","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"Applying Formal Methods to Specify Security Requirements in Multi-Agent Systems","V. H. Subburaj; J. E. Urban","Computer Science and Mathematics, West Texas A&M University Canyon, USA; Arizona State University Tempe, USA",2018 Federated Conference on Computer Science and Information Systems (FedCSIS),"28 Oct 2018","2018","","","707","714","Security has become an important concern with the development of large scale distributed and heterogeneous multi-agent systems (MAS). One of the main problems in addressing security during the development of MAS is that security is often an afterthought. The cost involved to patch existing systems against vulnerabilities and attacks after deployment is high. If developers and designers can spend some quality time investigating security aspects before beginning to code then this cost can be reduced significantly. Also, using formal methods to specify the complex behavior of large scale software systems has resulted in reliable software systems. This research effort was focused on using formal methods early in the development life cycle to specify security requirements for MAS. New solutions are emerging to fix security related issues, but how much thought gets in during the early phases of development in terms of security needs to be answered. In this paper, analysis of security requirements for MAS, existing solutions to secure MAS, and the use of formal methods to specify security requirements has been studied. Descartes - Agent, a formal specification language for specifying agent systems has been taken into study to model the security requirements of MAS early on in the development process. Functional specifications of MAS are modelled along with the non-functional security requirements using the Descartes - Agent specification language. A case study example is used to illustrate the specification of security requirements in MAS using the Descartes - Agent.","","978-83-949419-5-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8511238","multi-agent systems;security requirements;formal methods;Descartes - Agent","Security;Unified modeling language;Software systems;Formal specifications;Task analysis","","2","","23","","28 Oct 2018","","","IEEE","IEEE Conferences"
"Semantic-Guided Pruning Approach for Large Language Model-based Class-Level Code Generation","M. Geng; B. Duan; Y. Guo; H. Chen; Z. Shi","Satellite Launch Center of Xichang, Sichuan Xichang, China; Satellite Launch Center of Xichang, Sichuan Xichang, China; Satellite Launch Center of Xichang, Sichuan Xichang, China; Satellite Launch Center of Xichang, Sichuan Xichang, China; Satellite Launch Center of Xichang, Sichuan Xichang, China","2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)","13 Mar 2025","2024","","","1276","1280","Recent advancements in code generation from natural language have been driven by large language models (LLMs) like GPT-4 and WizardCoder, but current methods often underutilize their capabilities, especially for class-level code generation. The ClassEval benchmark addresses this by focusing on longer, more complex code snippets, but LLMs face challenges with input length constraints. Traditional approaches like zero-shot and few-shot learning struggle with the complexity and length of class-level code, limiting their effectiveness. To overcome this, we propose a semantic-guided pruning method, which optimizes input length through coarse- and fine-grained pruning, retaining only the most semantically relevant tokens. Our method improves LLM performance, increasing pass@1 accuracy by over 4% on the ClassEval benchmark.","","979-8-3315-4169-9","10.1109/RICAI64321.2024.10911408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911408","large language models;class-level code generation;pruning strategy","Codes;Accuracy;Limiting;Large language models;Semantics;Natural languages;Reinforcement learning;Benchmark testing;Robots;Optimization","","","","12","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Analysis of Reviews of Tourist Objects Based on Large Language Models","O. A. Nikolaychuk; I. A. Poddubniy","Laboratory of Information and Telecommunication Technologies for Technogenic Safety Research, Siberian Branch of the Russian Academy of Sciences, Matrosov Institute for System Dynamics and Control Theory (ISDCT SB RAS), Irkutsk, Russia; Laboratory of Information and Telecommunication Technologies for Technogenic Safety Research, Siberian Branch of the Russian Academy of Sciences, Matrosov Institute for System Dynamics and Control Theory (ISDCT SB RAS), Irkutsk, Russia","2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)","25 Dec 2024","2024","","","1070","1074","The article is devoted to solving the problem of identifying issues in the field of regional tourism. To solve this task, the method of aspect-based analysis of reviews of tourist objects using of large language models and prompt-engineering has been developed and implemented. Accommodation facilities, dining establishments, services and attractions are considered as tourist objects. The reviews are obtained from tourist information aggregators using web-scraping method. The initial dataset consisted of 1300 tourist reviews. A large language model, Saiga2, with 70 billion parameters was chosen for the implementation of prompt engineering. The problem analysis task was divided into subtasks of extracting and classifying facts (advantages and disadvantages). For each subtask, prompts have been developed with an indication of the role, a description of classes and examples of solutions. The description of classes is based on the literature analysis of issues in tourism. The method is implemented on the Yandex. Cloud platform, tested and validated to search for accommodation facilities satisfying tourist's requests for Olkhon Island. The results of the method's accuracy evaluation are described using metrics f1, micro f1, macro f1 and weighted f1. To test the proposed method validation, tests describing information about different categories of tourists were developed. The performance of the method was evaluated by comparing the solutions provided by the tourism expert and the created method implementation. The experiment showed 80 percent coincidence of the solutions. The method is planned to be implemented in the service of regional tourism monitoring of the Baikal region.","","979-8-3315-1632-1","10.1109/PIERE62470.2024.10805064","Russian Science Foundation(grant numbers:23-28-00844); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10805064","tourist profile of the territory;collection of data from open sources;reviews of tourists;large language models;aspect-based sentiment analysis","Measurement;Solid modeling;Sentiment analysis;Reviews;Large language models;Natural languages;Decision making;Prompt engineering;Informatics;Monitoring","","","","17","IEEE","25 Dec 2024","","","IEEE","IEEE Conferences"
"TelecomGPT: A Framework to Build Telecom-Specific Large Language Models","H. Zou; Q. Zhao; Y. Tian; L. Bariah; F. Bader; T. Lestable; M. Debbah","Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE; Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE; Department of Computer and Information Engineering, KU6G Research Center, Khalifa University, Abu Dhabi, UAE; Department of Computer and Information Engineering, KU6G Research Center, Khalifa University, Abu Dhabi, UAE; Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE; Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE; Department of Computer and Information Engineering, KU6G Research Center, Khalifa University, Abu Dhabi, UAE",IEEE Transactions on Machine Learning in Communications and Networking,"","2025","PP","99","1","1","The emergent field of Large Language Models (LLMs) has significant potential to revolutionize how future telecom networks are designed and operated. However, mainstream LLMs lack the specialized knowledge required to understand and operate within the highly technical telecom domain. In this paper, we introduce TelecomGPT, the first telecom-specific LLM, built through a systematic adaptation pipeline designed to enhance general-purpose LLMs for telecom applications. To achieve this, we curate comprehensive telecom-specific datasets, including pre-training datasets, instruction datasets, and preference datasets. These datasets are leveraged for continual pre-training, instruction tuning, and alignment tuning, respectively. Additionally, due to the lack of widely accepted evaluation benchmarks that are tailored for the telecom domain, we proposed three novel LLM-Telecom evaluation benchmarks, namely, Telecom Math Modeling, Telecom Open QnA, and Telecom Code Tasks. These new benchmarks provide a holistic evaluation of the capabilities of LLMs in telecom math modeling, open-ended question answering, code generation, infilling, summarization and analysis. Using the curated datasets, our fine-tuned LLM, TelecomGPT, significantly outperforms general-purpose state of the art (SOTA) LLMs, including GPT-4, Llama-3 and Mistral, particularly in Telecom Math Modeling benchmarks. Additionally, it achieves comparable performance across various evaluation benchmarks, such as TeleQnA, 3GPP technical document classification, telecom code summarization, generation, and infilling. This work establishes a new foundation for integrating LLMs into telecom systems, paving the way for AI-powered advancements in network operations.","2831-316X","","10.1109/TMLCN.2025.3593184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11097898","Generative AI;Large Language Models;3GPP;Telecom Foundation Models","Telecommunications;Training;Tuning;Codes;Benchmark testing;Adaptation models;Standards;Retrieval augmented generation;Finance;Computational modeling","","1","","","CCBY","28 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Poster: Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency","R. Ramler; P. Straubinger; R. Plösch; D. Winkler","Software Competence, Hagenberg GmbH (SCCH), Hagenberg, Austria; University of Passau, Passau, Germany; Johannes Kepler University Linz, Linz, Austria; Austrian Center for Digital Production and TU Wien, Vienna, Austria","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","733","736","The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988960","Large Language Models;Software Testing;Defect Detection","Software testing;Productivity;Large language models;Buildings;Manuals;Chatbots;Defect detection;Software engineering;Software development management","","1","","4","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Detecting Covert Channels in Cloud Access Control Policies Using Large Language Models","H. Karmarkar; V. Joshi; R. Venkatesh","TCS Research, Mumbai, India; TCS Research, Pune, India; TCS Research, Pune, India",2024 IEEE International Conference on Cyber Security and Resilience (CSR),"24 Sep 2024","2024","","","241","246","In the realm of cloud computing, the task of configuring access control policies is a critical aspect of ensuring security of cloud resources. However, policy configuration remains a complex task with a high cognitive load as it requires a simultaneous understanding of the cloud environment and security requirements of the organization. This often creates gaps between intended and actual policy configuration leading to misconfigurations of policies. A misconfigured policy can introduce subtle and unexpected vulnerabilities that can be exploited by malicious entities to gain unauthorized access to cloud resources. In this paper, we model and analyze a particular class of access control vulnerabilities that arise due to the creation of covert channels in role-based access control policies. We present a tool CovertHunter that uses a Large Language Model to recognize intent behind policy configuration described in natural language and check it against actual cloud policies to automatically detect vulnerabilities arising due to the presence of covert channels.","","979-8-3503-7536-7","10.1109/CSR61664.2024.10679435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10679435","access control;cloud computing;security;theft;role based access control;covert channels;large language models","Access control;Cloud computing;Large language models;Computational modeling;Natural languages;Organizations;Predictive models","","","","22","IEEE","24 Sep 2024","","","IEEE","IEEE Conferences"
"Prompt Engineering Adversarial Attack Against Image Captioning Models","H. Vo; S. Yu; X. Zheng","School of Computer Science, University of Technology Sydney, Sydney, Australia; School of Computer Science, University of Technology Sydney, Sydney, Australia; Faculty of Science and Engineering, Macquarie University, Sydney, Australia",2024 17th International Conference on Security of Information and Networks (SIN),"13 Feb 2025","2024","","","1","7","This work presents a highly effective strategy for attacking image captioning models through the use of prompt engineering. The objective of this approach is to deliberately guiding the output of LLMs and introduce dynamic noise into the original clean image captions, causing them to be categorized as a different class. Consequently, when the image captioning model is fine-tuned using adversarial captions, it will deteriorate and produce inaccurate captions for clean photos. The novelty of this attack is that it does not require the attacker to perform any model training and only require to prompt the LLMs to generate only a small amount of captions for the attack to be effective. Comprehensive experiments using GPT-3.5 indicate that with only 100 captions created by LLMs with malicious intent can significantly worsen picture captioning model performance by up to over 50% in BLEU metric and over 25% in ROUGE-L and METEOR scores.","","979-8-3315-0973-6","10.1109/SIN63213.2024.10871522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871522","Adversarial Attack;Large Language Models;Prompt Engineering;Image Captioning","Measurement;Training;Large language models;Noise;Prompt engineering;Meteors;Security;Context modeling","","","","19","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"RMCBench: Benchmarking Large Language Models’ Resistance to Malicious Code","J. Chen; Q. Zhong; Y. Wang; K. Ning; Y. Liu; Z. Xu; Z. Zhao; T. Chen; Z. Zheng","Sun Yat-sen University, Zhuhai, China; Sun Yat-sen University, Zhuhai, China; Sun Yat-sen University, Zhuhai, China; Sun Yat-sen University & Peng Cheng Laboratory, China; Sun Yat-sen University, Zhuhai, China; Tencent AI Lab, China; Tencent AI Lab, China; University of Electronic Science and Technology of China, China; Sun Yat-sen University, Zhuhai, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","995","1006","Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36% in text-to-code scenario and 11.52% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71%; ChatGPT-4 has a refusal rate of only 35.73%. We also analyze the factors that affect LLM’s ability to resist malicious code generation and provide implications for developers to enhance model robustness.CCS CONCEPTS• Security and privacy → Software security engineering.","2643-1572","979-8-4007-1248-7","","Tencent; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764828","large language models;malicious code;code generation","Analytical models;Codes;Resists;Benchmark testing;Malware;Robustness;Security;Standards;Software engineering;Software development management","","1","","78","","29 Nov 2024","","","IEEE","IEEE Conferences"
"GPT-4 versus Bard and Bing: LLMs for Fake Image Detection","O. M. Al-Janabi; O. M. Alyasiri; E. A. Jebur","College of Medicine, University of Baghdad, Baghdad, Iraq; Karbala Technical Institute, Al-Furat Al-Awsat Technical University, Karbala, Iraq; College of Medicine, University of Baghdad, Baghdad, Iraq",2023 3rd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA),"13 Feb 2024","2023","","","249","254","The recent emergence of sophisticated Large Language Models (LLMs) such as GPT-4, Bard, and Bing has revolutionized the domain of scientific inquiry, particularly in the realm of large pre-trained vision-language models. This pivotal transformation is driving new frontiers in various fields, including image processing and digital media verification. In the heart of this evolution, our research focuses on the rapidly growing area of image authenticity verification, a field gaining immense relevance in the digital era. The study is specifically geared towards addressing the emerging challenge of distinguishing between authentic images and deepfakes – a task that has become critically important in a world increasingly reliant on digital media. Our investigation rigorously assesses the capabilities of these advanced LLMs in identifying and differentiating manipulated imagery. We explore how these models process visual data, their effectiveness in recognizing subtle alterations, and their potential in safeguarding against misleading representations. The implications of our findings are far-reaching, impacting areas such as security, media integrity, and the trustworthiness of information in digital platforms. Moreover, the study sheds light on the limitations and strengths of current LLMs in handling complex tasks like image verification, thereby contributing valuable insights to the ongoing discourse on AI ethics and digital media reliability.","","979-8-3503-9455-9","10.1109/ICICyTA60173.2023.10429022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10429022","deepfake detection;large language models (LLMs);GPT-4;bard;bing chat","Deepfakes;Visualization;Image processing;Media;Security;Reliability;Task analysis","","3","","26","IEEE","13 Feb 2024","","","IEEE","IEEE Conferences"
"Design and Implementation of an Intelligent Ancient Chinese Poetry Search System Based on LLMs","W. Chen; R. Dai; K. Chen","Shenzhen College Of International Education, Shenzhen, China; Bellaire High School, Houston, USA; Dept. of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China",2024 10th International Conference on Systems and Informatics (ICSAI),"26 Feb 2025","2024","","","1","5","This paper explores the design of an intelligent ancient Chinese poetry search system using large language models (LLMs) to address limitations in existing methods regarding search intent, authority, and completeness. We built a dataset of 220,000 poems with structured information, designed a two-stage system leveraging LLMs for semantic extraction and query transformation, and proposed prompt engineering techniques combining chain-of-thought reasoning and external knowledge. A prototype achieved 75% accuracy, demonstrating effective intent understanding and semantic extraction without custom training. This study lays a foundation for intelligent poetry search systems, with all code and datasets available on GitHub.","2689-7148","979-8-3315-1013-8","10.1109/ICSAI65059.2024.10893842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893842","Ancient Chinese poetry;Large language models (LLMs);Intelligent search system","Training;Knowledge engineering;Large language models;Semantics;Prototypes;Cognition;Prompt engineering;Data mining;Informatics;Software development management","","","","23","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Neuro-symbolic Generative AI Assistant for System Design","S. Jha; S. K. Jha; A. Velasquez","SRI International; Florida International University; University of Colorado, Boulder",2024 22nd ACM-IEEE International Symposium on Formal Methods and Models for System Design (MEMOCODE),"6 Nov 2024","2024","","","75","76","The design of complex cyber-physical systems involves balancing multiple, often conflicting performance objectives. In practice, some design requirements remain implicit, embedded in the intuition and expertise of seasoned designers who have worked on similar systems for years. These designers rely on their experience to explore a limited set of promising design candidates, evaluating or simulating them with detailed but computationally slow scientific models. The typical goal is to produce a diverse array of high-performing configurations that offer flexibility in trade-offs and avoid premature commitment to a specific design. In this invited talk, we describe an AI assistant that leverages neuro-symbolic machine learning to automate parts of the system design process. Our approach extends oracle-guided inductive synthesis by integrating a hierarchy of oracles, ranging from slow, detailed scientific models to faster but lower-fidelity deep neural network surrogates and symbolic rules. This approach accelerates design iterations, especially during early design phases. We employ deep generative models in the form of fine-tuned large language models to learn the valid design space, followed by joint exploration and optimization across this learned manifold. This allows the generation of a diverse set of optimal designs based on specified performance objectives.","2832-6520","979-8-3503-7802-3","10.1109/MEMOCODE63347.2024.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740777","formal synthesis;generative ai;system design;surrogate models;design space exploration","Manifolds;Generative AI;Large language models;Computational modeling;Machine learning;Artificial neural networks;Cyber-physical systems;Distance measurement;System analysis and design;Optimization","","1","","15","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Translating Natural Language Specifications into Access Control Policies by Leveraging Large Language Models","S. Lawal; X. Zhao; A. Rios; R. Krishnan; D. Ferraiolo","Institute for Cyber Security (ICS), University of Texas, San Antonio, Texas, United States; Department of Information Systems and Cyber Security, University of Texas, San Antonio, Texas, United States; Department of Information Systems and Cyber Security, University of Texas, San Antonio, Texas, United States; Department of Electrical & Computer Engineering, Institute for Cyber Security (ICS), University of Texas, San Antonio, Texas, United States; National Institute of Standards and Technology, Gaithersburg, Maryland, United States","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","361","370","This paper investigates the application of large language models (LLMs) for the automated translation and information extraction of access control policies from a natural language source. Prior research in this domain have predominantly relied on manual methods, traditional natural language processing (NLP), or a hybrid approach involving machine learning and artificial neural networks combined with NLP techniques. We demonstrate a significant advancement by leveraging the power of LLMs to achieve improved efficiency and accuracy in these tasks. Our study focuses on applying cutting-edge prompt engineering techniques designed to optimize LLM performance in the specific context of access control policy information extraction. The findings highlight the potential of LLMs to streamline the process of converting human-readable requirements into formal, machine-interpretable policies, ultimately contributing to the automation and security of access control systems.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00048","National Institute of Standards and Technology; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835708","Natural Language Specification;Access Control Policies;large language models (LLMs);Prompt Engineering;Attribute-Based Access Control (ABAC)","Access control;Translation;Accuracy;Large language models;Refining;Process control;Information retrieval;Natural language processing;Prompt engineering;Context modeling","","","","20","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Automated Program Repair with the GPT Family, Including GPT-2, GPT-3 and CodeX","M. Lajkó; V. Csuvik; T. Gyimothy; L. Vidács","Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, MTA-SZTE Research Group on Artificial Intelligence, University of Szeged, Szeged, Hungary",2024 IEEE/ACM International Workshop on Automated Program Repair (APR),"2 Sep 2024","2024","","","34","41","Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).","","979-8-4007-0577-9","10.1145/3643788.3648021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10653250","Automated Program Repair;Transformers;GPT-3;Codex;JavaScript","Source coding;Large language models;Computer bugs;Neural networks;Static analysis;Machine learning;Maintenance engineering","","","","46","","2 Sep 2024","","","IEEE","IEEE Conferences"
"Exploration of Computer Programming Teaching Reform Based on Large Language Models","Y. Zhao; X. Ling; H. Zhu; F. Zhou; Z. Deng","School of Artificial Intelligence, Shanghai Normal University, Tianhua College, Shanghai, China; School of Artificial Intelligence, Shanghai Normal University, Tianhua College, Shanghai, China; School of Artificial Intelligence, Shanghai Normal University, Tianhua College, Shanghai, China; School of Artificial Intelligence, Shanghai Normal University, Tianhua College, Shanghai, China; School of Artificial Intelligence, Shanghai Normal University, Tianhua College, Shanghai, China",2025 11th International Conference on Computing and Artificial Intelligence (ICCAI),"11 Aug 2025","2025","","","359","363","With the rapid development of artificial intelligence and natural language processing technology, the traditional programming teaching mode faces new opportunities and challenges. This study designs and implements a programming teaching system based on a large language models for improving students’ programming ability, enhancing the learning effect, and promoting the reform and innovation of programming education. The system combines automated code generation, code analysis, error detection, personalized learning suggestions, and exercise recommendations of the large language models to help students solve programming learning challenges. And the system manages students’ learning progress more efficiently and optimizes teaching strategies through intelligent data analysis and automatic scoring functions. Practical application has demonstrated the system’s effectiveness in improving students’ learning outcomes. In the future, with the advancement of technology, the system will be further expanded to promote the intelligent and personalized development of programming education continuously.","","979-8-3315-2491-3","10.1109/ICCAI66501.2025.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105994","large language models;programming education;intelligent teaching;personalized learning;education reform","Technological innovation;Codes;Data analysis;Large language models;Education;Learning (artificial intelligence);Natural language processing;Logic;Programming profession;Faces","","","","20","IEEE","11 Aug 2025","","","IEEE","IEEE Conferences"
"Recovering from Privacy-Preserving Masking with Large Language Models","A. Vats; Z. Liu; P. Su; D. Paul; Y. Ma; Y. Pang; Z. Ahmed; O. Kalinli","Santa Clara University, Santa Clara, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA; Meta, Menlo Park, CA","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","10771","10775","Model adaptation is crucial to handle the discrepancy between proxy training data and actual users’ data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10448234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448234","Privacy-preserving machine learning;language modeling;large language models;automatic speech recognition","Adaptation models;Training data;Speech recognition;Signal processing;Natural language processing;Data models;Servers","","9","","34","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Designing and Implementing Backdoor Attacks on Large Language Models: A Systematic Weight Manipulation Approach","N. K. M; R. Mathew; K. K","SRM Institute of Science and Technology, Chennai, India; SRM Institute of Science and Technology, Chennai, India; SRM Institute of Science and Technology, Chennai, India",2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),"21 Jul 2025","2025","","","1591","1596","Large language models (LLMs) represent a crucial component of contemporary communication and information systems. With their deployment scaled up, an understanding of potential security vulnerabilities assumes great significance. This paper proposes a novel backdoor attack implementation on LLMs, targeting the Qwen2-7B model architecture. We illustrate how slight changes made in the first transformer layer can induce behavior that seems normal for standard inputs while producing malicious outputs when triggered by specific patterns. By mathematically analyzing the activation functions and attention mechanisms, we provide a formal framework identifying optimal weight perturbations, thus maximizing the backdoor effectiveness for the least detectability. Our implementation encompasses a full-fledged framework for designing, testing, and visualizing these backdoors, proving that such attacks are feasible with minimal model weights’ modifications (average L2 norm of modifications < 0.002). Extensive experiments conducted on benchmarks including MMLU, GSM8K, and HumanEval show less than 0.1% degradation in performance on standard metrics while attaining 100% success rate on triggered inputs. Our backdoor attacks persist even after additional fine-tuning across different domains, maintaining 98-100% trigger success rate. We formalize possible detection mechanisms along with a theoretical bound on the effectiveness of any cryptographic and statistical defenses against such attacks.","","979-8-3315-3884-2","10.1109/ICSSAS66150.2025.11080882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11080882","large language models;backdoor attacks;artificial intelligence security;machine learning;neural network security;weight perturbation analysis;transformer models;attention mechanism manipulation","Measurement;Visualization;Attention mechanisms;Systematics;Large language models;Perturbation methods;Neural networks;Transformers;Mathematical models;Standards","","","","13","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Functional and Non-functional Requirements Classification: A Comparative Evaluation of Pre-trained LLMs and ML Techniques","R. Batoool; A. Naseer","Dept. of Computer Software Engineering, MCS NUST, Rawalpindi, Pakistan; Dept. of Computer Software Engineering, MCS NUST, Rawalpindi, Pakistan",2025 International Conference on Communication Technologies (ComTech),"19 Jun 2025","2025","","","1","6","Identifying functional and non-functional requirements at an early phase is essential for project success. However, the requirements engineering community still lacks a comprehensive understanding of these requirements, which are often intertwined and expressed in natural language. Requirements classification is crucial for correctly extracting and organizing functional and non-functional requirements into specified categories. Automated classification reduces development costs, uncertainty, and misunderstanding. Machine learning (ML) and deep learning approaches have been applied for automatic classification in recent studies. This study conducts a comparative analysis by combining two publicly available PROMISE_exp and DOSSPRE datasets, which was classified as functional and non-functional classes of software requirements. First of all, we applied natural language processing (NLP) techniques to the requirements text to extract feature embeddings, followed by training four popular machine learning (ML) algorithms on these requirements. Inspired by the success of large language models (LLMs) in various tasks, we also fine-tuned four text-based pretrained (LLMs) and compared their performance with traditional ML models. Our empirical analysis shows that these models outperform traditional ML models on the combined dataset, offering developers an efficient method to detect and classify software requirements early.","2996-3621","979-8-3315-1533-1","10.1109/ComTech65062.2025.11034464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11034464","Requirements Engineering;Non-functional Requirements;Functional Requirements;Requirements Classification;Machine Learning;Large language models","Training;Analytical models;Uncertainty;Machine learning algorithms;Large language models;Software algorithms;Feature extraction;Software;Natural language processing;Requirements engineering","","","","18","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"SCD: Strengthen Character Development in Long Stories with LLMs via Prompt Engineering","L. Yao","School of Computer Science, Hangzhou Dianzi University, Hangzhou, China","2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","15 Jul 2025","2025","","","564","567","Although large language models (LLMs) have achieved remarkable results in the field of natural language processing, they still exhibit shortcomings in character portrayal when generating long stories. To address this issue, this paper proposes SCD (Strengthen Character Development) framework, which extracts, stores, and analyzes key behavioral information to enrich character details. This method does not require modifications to the internal structure of LLMs and is compatible with various open-source and proprietary models. We conducted experiments comparing our approach with some of the best-performing long-story models currently available, and the results demonstrate that our model effectively improves the quality of long stories.","","979-8-3315-0796-1","10.1109/NNICE64954.2025.11063825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11063825","NLP;Story Generation;LLM;Character Development;Prompt Engineering","Large language models;Coherence;Artificial neural networks;Natural language processing;Prompt engineering;Data mining;Faces","","","","10","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Systematic Testing of Security-Related Vulnerabilities in LLM-Based Applications","H. Kaplan","Jheronimus Academy of Data Science, Tilburg University, ’s-Hertogenbosch, Netherlands",2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN),"16 Jun 2025","2025","","","264","266","Large Language Models (LLMs) have emerged as transformative tools in natural language understanding and generation. They possess billions of parameters, which enable them to generate coherent and contextually rich text [1]. These capabilities have made LLMs vital in domains such as customer service, content creation, and programming assistance [2], [3]. However, these advances come with significant risks. For example, a recent study showed that LLM responses contain private or sensitive information accidentally exposed during training [4], [5]. Furthermore, adversarial attacks have been shown to reduce system accuracy by as much as under controlled conditions [6]. A high-profile example is the misuse of LLMs to generate biased or harmful text when manipulated through adversarial prompts [7].","","979-8-3315-0219-5","10.1109/CAIN66642.2025.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030024","large language models (llms);security risks;adversarial attacks;prompts;data leakage;devsecops","Training;Systematics;Accuracy;Large language models;Customer services;Programming;Natural language processing;Application software;Testing","","","","23","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models in Class-Level Code Generation","X. Du; M. Liu; K. Wang; H. Wang; J. Liu; Y. Chen; J. Feng; C. Sha; X. Peng; Y. Lou","School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","982","994","Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a sim-ple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios. To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3639219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549472","Class-level Code Generation;Large Language Model;Benchmark","Codes;Natural languages;Benchmark testing;Task analysis;Software development management;Software engineering;Python","","17","","76","","14 Jun 2024","","","IEEE","IEEE Conferences"
"KareCoder: A New Knowledge-Enriched Code Generation System","T. Huang; Z. Sun; Z. Jin; G. Li; C. Lyu","Shandong Normal University, China; Shandong Normal University, China; Peking University, China; Peking University, China; Peking University, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","270","271","Large Language Models (LLMs) demonstrate proficiency in handling fundamental programming problems but struggle with complex programming in new types. The study presents KareCoder, integrating programming knowledge into code generation. Initial tests reveal KareCoder's significant success in the Pass@l metric for complex competitive programming problems.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3643076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554822","Code Generation;Large Language Models;Knowledge Library","Measurement;Codes;Programming;Software engineering","","1","","7","","20 Jun 2024","","","IEEE","IEEE Conferences"
"Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models","Z. Yang; Y. Jin; J. Liu; X. Xu; Y. Zhang; S. Ji","University of Illinois at Urbana-Champaign, Champaign, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA; Computer Science Department, University of Illinois Urbana-Champaign, Champaign, IL, USA; Computer Science Department, University of Illinois Urbana-Champaign, Champaign, IL, USA; Computer Science Department, University of Illinois Urbana-Champaign, Champaign, IL, USA; Computer Science Department, University of Illinois Urbana-Champaign, Champaign, IL, USA","2025 IEEE 7th International Conference on Communications, Information System and Computer Engineering (CISCE)","10 Jul 2025","2025","","","1029","1032","The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection system. In addition to existing models such as autoencoders and decision trees, we harness the power of large language models for processing sequence data from network traffic, which allows us a better capture of underlying complex patterns, as well as slight fluctuations in the dataset. We show for a given detection task, the need for a hybrid model that incorporates the attention mechanism of the transformer architecture into a supervised learning framework in order to achieve better accuracy. A pre-trained large language model analyzes and predicts the probable network traffic, and an anomaly detection layer that considers temporality and context is added. Moreover, we present a novel transfer learning-based methodology to enhance the model's effectiveness to quickly adapt to unknown network structures and adversarial conditions without requiring extensive labeled datasets. Actual results show that the designed model outperforms traditional methods in detection accuracy and computational efficiency, effectively identify various network anomalies such as zero-day attacks and traffic congestion pattern, and significantly reduce the false positive rate.","2833-2423","979-8-3315-0161-7","10.1109/CISCE65916.2025.11065413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11065413","Large Language Models;Network Traffic Monitoring;Anomaly Detection;Transfer Learning","Training;Adaptation models;Accuracy;Large language models;Computational modeling;Autoencoders;Telecommunication traffic;Transformers;Monitoring;Anomaly detection","","","","12","IEEE","10 Jul 2025","","","IEEE","IEEE Conferences"
"Software Testing With Large Language Models: Survey, Landscape, and Vision","J. Wang; Y. Huang; C. Chen; Z. Liu; S. Wang; Q. Wang","State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Technical University of Munich, Munich, Germany; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; York University, Toronto, ON, Canada; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Software Engineering,"17 Apr 2024","2024","50","4","911","936","Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.","1939-3520","","10.1109/TSE.2024.3368208","National Natural Science Foundation of China(grant numbers:62232016,62072442,62272445); Youth Innovation Promotion Association Chinese Academy of Sciences, Basic Research Program of ISCAS(grant numbers:ISCAS-JCZD-202304); Major Program of ISCAS(grant numbers:ISCAS-ZD-202302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440574","Pre-trained large language model;software testing;LLM;GPT","Software testing;Task analysis;Computational modeling;Codes;Software systems;Natural language processing;Reviews","","161","","166","IEEE","20 Feb 2024","","","IEEE","IEEE Journals"
"MUARF: Leveraging Multi-Agent Workflows for Automated Code Refactoring","Y. Xu","Gina Cody School of Engineering and Computer, Science Concordia University, Montreal, Canada",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","226","227","Refactoring is crucial for maintaining a project, but it requires developers to understand code structure and system design principles well. Recent research on Large Language Models(LLMs) has shown their great capability for handling complex tasks, making them a possible solution for overcoming these challenges. In this paper, we propose MUARF, an LLM-based solution designed to automate method-level code refactoring, aiming to generate correct, high-quality, and human-like refactored code. MUARF leverages Contextual Retrieval-Augmented Generation to search for similar refactoring samples for few-shot learning, uses Multi-Agent Workflow to simulate the human refactoring process, and integrates advanced software engineering tools (e.g., RefactoringMiner, PurityChecker, StyleChecker) to assist refactoring. Evaluation results show that MUARF achieves a compilation pass rate of 86.5% and a test success rate of 83.8% for the refactored code it generates. Additionally, metrics such as CodeBLEU score and AST Diff accuracy-which compare human-refactored code with the output of MUARF -highlight the generated code is human-like. The ablation results show that RefactoringMiner and Agentware made the greatest contribution to MUARF.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024270","Code Refactoring;Large Language Model;Multi-Agent Communication;Contextual Retrieval-Augmented Generation;Prompt Engineering","Measurement;Codes;Large language models;Retrieval augmented generation;Prompt engineering;Few shot learning;System analysis and design;Software engineering","","","","4","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"On Hardware Security Bug Code Fixes by Prompting Large Language Models","B. Ahmad; S. Thakur; B. Tan; R. Karri; H. Pearce","Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Software Engineering, University of Calgary, Calgary, AB, Canada; Department of Electrical and Computer Engineering, New York University Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, University of New South Wales, Sydney, NSW, Australia",IEEE Transactions on Information Forensics and Security,"2 May 2024","2024","19","","4043","4057","Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI’s Codex have demonstrated capabilities in many coding-adjacent domains. In this work, we consider how LLMs may be leveraged to automatically repair identified security-relevant bugs present in hardware designs by generating replacement code. We focus on bug repair in code written in Verilog. For this study, we curate a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all fifteen of our benchmarks. This ensemble outperforms a state-of-the-art automated hardware bug repair tool on its own suite of bugs. These results show that LLMs have the ability to repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair tool.","1556-6021","","10.1109/TIFS.2024.3374558","Intel Corporation; Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2022-03027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462177","Hardware security;large language models;bug repair","Maintenance engineering;Computer bugs;Codes;Hardware;Security;Software;Registers","","31","","38","IEEE","7 Mar 2024","","","IEEE","IEEE Journals"
"Large Language Models for Indian Legal Text Summarisation","H. K. M; J. P; A. K. M","Department of Information Technology, Artificial Intelligence, National Institute of Technology Karnataka, India; Department of Information Technology, Artificial Intelligence, National Institute of Technology Karnataka, India; Department of Information Technology, Artificial Intelligence, National Institute of Technology Karnataka, India","2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)","20 Sep 2024","2024","","","1","5","Summarizing legal case judgments is a complex task in Legal Natural Language Processing (NLP), with a gap in understanding how various summarization models, including extractive and abstractive approaches and analysing the perform within the domain of legal documents. Since there are around 4 crore pending cases in the Indian court system, this study addresses the challenge of laborious task of manually summarizing legal documents. It introduces both supervised and unsupervised models for both extractive and abstractive summarization, showcasing their effective performance through evaluations using ROUGE metrics and BERT score. BART, T5, PEGASUS, ROBERTA, Legal-PEGASUS, Legal-BERT models are used for abstractive summarisation. TextRank, LexRank, LSA, Summarizer BERT, KL-Summ are used in case of extractive summarisation. Longformer, Bert - Legal Pegasus are also considered for the task of Summarisation. In the domain of legal document summarization, we used GPT-4 and LLAMA-2, employing prompt engineering with both Zero-shot and Oneshot prompts to extract summaries. As far of our knowledge, this is the first paper that used Large Language Models like GPT-4 and LLama-2 for the task of Legal Text summarisation. Along with that a user-friendly chatbot has been developed utilizing the Llama model and specifically designed to respond for queries related to legal texts. Additionally, a web application has been created, allowing users to upload legal documents for summarization. An option is given to users to select from various languages including Telugu, Tamil, Kannada, Malayalam, and Hindi. As a result the summarised text is converted into respective language.","2766-2101","979-8-3503-8592-2","10.1109/CONECCT62155.2024.10677065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10677065","Legal;Summarisation;NLP;Large language models;GPT-4;LLAMA-2","Measurement;Analytical models;Law;Large language models;Computational modeling;Chatbots;Communications technology","","1","","14","IEEE","20 Sep 2024","","","IEEE","IEEE Conferences"
"Comparative Analysis of Large Language Models in Solidity Smart Contract Vulnerability Detection: Review","A. Almaghthawi; W. M. S. Yafooz","Computer Science Department, College of computer Science and Engineering, Taibah University, Madinah, Saudi Arabia; Computer Science Department, College of computer Science and Engineering, Taibah University, Madinah, Saudi Arabia",2025 2nd International Conference on Advanced Innovations in Smart Cities (ICAISC),"21 Apr 2025","2025","","","1","5","Blockchain (BC) technology and Artificial Intelligence (AI) are promising areas with potential for powerful, secure, and decentralized applications in various sectors. Also, LLMs, are essential advanced machine learning frameworks, which are now used in various applications, including customer service, chatbots, code generation, and language translation. In terms of software development, LLMs are being investigated as a potential way to create smart contracts, focusing on their ability to emulate human programming techniques. For that purpose, this study reviews the capabilities and limitations of Large Language Models (LLMs) like GPT-3.5, PaLM2, and Code Llama in detecting vulnerabilities in Solidity smart contract code.","","979-8-3315-0699-5","10.1109/ICAISC64594.2025.10959494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10959494","Large Language Models (LLMs);Vulnerability Detection;Smart Contract;GPT-3.5;Code Llama;Palm 2;Solidity;Prompt Engineering","Technological innovation;Codes;Translation;Reviews;Large language models;Smart contracts;Focusing;Static analysis;Blockchains;Software development management","","","","40","IEEE","21 Apr 2025","","","IEEE","IEEE Conferences"
"Prompting Large Language Models for Aerial Navigation","E. Balcı; M. Sarıgül; B. Ata","Department of Computer Engineering, Adana Science and Technology University, Adana, Türkiye; Department of Computer Engineering, Çukurova University, Adana, Türkiye; Department of Computer Engineering, Çukurova University, Adana, Türkiye",2024 9th International Conference on Computer Science and Engineering (UBMK),"11 Dec 2024","2024","","","304","309","Robots are becoming more prevalent and consequently utilized in numerous fields due to the latest advancements in artificial intelligence. Recent studies have shown promise in the human-robot interaction where non-experts are capable of handling the collaboration with robots. Whereas traditional interaction approaches are compact and rigid, natural language communication offers a coherent approach that allows interaction to be more versatile. The utilization of large language models (LLMs) makes it possible for non-expert users to take place in human-robot communications and manipulate robots to perform complex tasks such as aerial navigation, obstacle avoidance, and pathfinding. In t his paper, we performed an experimental study to compare the performances of LLMs based on the generated source code from prompts to perform aerial navigation tasks in a simulated environment. The few-shot prompting technique is applied to LLMs such as ChatGPT, Gemini, Mistral, and Claude on Microsoft's AirSim drone simulation. We defined three test cases based on UAV-based aerial navigation, specified model prompts for each test, and extracted ground-truth trajectories for the test cases. Finally, we tested the models on the simulator with predefined prompts to compare the predicted trajectories with the ground truth. Our findings indicate that no single model surpasses all test cases, using LLMs for aerial navigation remains a challenging task in robotic applications. The source code can be found at https://github.com/cukurovaai/Prompting-LLMs-for-Aerial-Navigation/.","2521-1641","979-8-3503-6588-7","10.1109/UBMK63289.2024.10773467","TUBITAK (Scientific and Technological Research Council of Türkiye)(grant numbers:123E694); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773467","Prompt Engineering;Large Language Models (LLMs);Robotics;Aerial Navigation","Navigation;Source coding;Large language models;Computational modeling;Atmospheric modeling;Chatbots;Reliability engineering;Trajectory;Prompt engineering;Robots","","","","34","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"A Bibliometric Exposition and Review on Leveraging LLMs for Programming Education","J. Pwanedo Amos; O. Ahmed Amodu; R. Azlina Raja Mahmood; A. Bolakale Abdulqudus; A. F. Zakaria; A. Rhoda Iyanda; U. Ali Bukar; Z. Mohd Hanapi","Information and Communication Engineering Department, Elizade University, Ilara-Mokin, Nigeria; Information and Communication Engineering Department, Elizade University, Ilara-Mokin, Nigeria; Department of Communication Technology and Network, Universiti Putra Malaysia (UPM), Serdang, Selangor, Malaysia; Department of Mathematics and Computer Science, Elizade University, Ilara-Mokin, Nigeria; Department of Engineering Education, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Computer Science and Engineering Department, Obafemi Awolowo University, Ile-Ife, Nigeria; Department of Computer Science, Faculty of Computing and Artificial Intelligence, Taraba State University, Jalingo, Nigeria; Department of Communication Technology and Network, Universiti Putra Malaysia (UPM), Serdang, Selangor, Malaysia",IEEE Access,"7 Apr 2025","2025","13","","58364","58393","The world is experiencing an AI revolution, with large language models (LLMs) transforming various industries, including education. Academics are striving to harness the potential of LLMs while also contending with their risks. This paper presents the first bibliometric analysis focused on LLM research in programming education, identifying leading countries, authors, and institutions while analyzing key terms and popular keywords in this field. Additionally, it highlights influential studies on topics such as introductory programming, computer science, computing, programming education, and prompt engineering, discussing key insights from these works. Findings indicate that LLMs could play a significant role in programming education and may be integrated into computer science curricula. However, careful consideration is needed to ensure their benefits outweigh their risks across various use cases. This study specifically examines ChatGPT as a representative LLM, exploring its benefits and limitations as both a learning aid for students and a support tool for professionals. It also evaluates the quality of ChatGPT-generated code and its effectiveness in simplifying programming concepts for beginners. Furthermore, the ethical implications of increasing reliance on LLMs for programming tasks, including concerns about dependency, plagiarism, and potential effects on critical thinking, are addressed. By contributing to the ongoing discourse on integrating AI tools like ChatGPT in programming education, this research emphasizes the importance of responsible and ethical usage to maximize benefits for students, educators, and the broader educational community.","2169-3536","","10.1109/ACCESS.2025.3554627","Geran Universiti Penyelidikan, Universiti Kebangsaan Malaysia(grant numbers:GUP-2024-114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938596","ChatGPT;code generation;ethical concerns;large language models (LLMs);introductory programming;programming education;prompt engineering","Chatbots;Programming profession;Education;Bibliometrics;Market research;Large language models;Codes;Ethics;Requirements engineering;Mathematics","","2","","130","CCBYNCND","26 Mar 2025","","","IEEE","IEEE Journals"
"Impact of Generative AI Adoption in Academia and How it Influences Ethics, Cognitive Thinking and AI Singularity","D. Marimekala; J. Lamb","Irving A. Robbins Middle School, CT, USA; Adjunct Faculty, Mathematics, Pace University, Pleasantville, NY, USA",2024 IEEE Integrated STEM Education Conference (ISEC),"17 Sep 2024","2024","","","1","4","Our Gen Alpha, born between year 2010 and 2024 see and experience more of AI and they adapt quickly to AI and will have less impact on advancements in Technology when compared to Generation X (19651980); Generation Y (Millennials) born from 1980 to 1994; Generation Z from 1995 to 2009. The reason is that Gen Alpha constantly uses electronic gadgets either in gaming, learning or for social media. They are susceptible quickly to Generative AI when compared to Gen X, Gen Y or Gen Z. Especially in Academia, where Gen Alpha is more encouraged to Generative AI such as ChatGPT in homework, assignments, and in research. Well, this is a good approach for those who are struggling to complete their work or for those who do not have a clue on how to complete their homework, assignment, or research. On other hand, the Generative AI tools such as ChatGPT are slowly becoming a part of the eco system that students lean on the Generative AI tools instead of doing research or thinking critically. As a result, there will be some behavioral changes that will be developed over a period. These behavioral changes are, impatient for answers to the problems, express panic while solving a problem, shows anxiety, low self-esteem in solving a problem, and low confidence factor. There is always a positive side of Generative AI, if we look at it from a different angle. Some of them are, it helps an individual and guides them with possible answers, all individuals can ask questions using prompt engineering and get responses. But the fact of the matter is how authentic the response from Generative AI is a question? Can the author quote the responses he/she received from ChatGPT and How can we avoid plagiarism? How can we reduce bias? How can we avoid AI singularity?","2473-7623","979-8-3503-5280-1","10.1109/ISEC61299.2024.10664756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664756","Generative AI;ChatGPT;Guardrails;Singularity;AI Models;Large Language Models;ML","Technological innovation;Generative AI;Social networking (online);Shape;Plagiarism;Writing;Chatbots","","2","","6","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Creating UML Class Diagrams with General-Purpose LLMs","M. Shehata; B. Lepore; H. Cummings; E. Parra","Dept. of Math, Computer Science, and Data Science Belmont University, Nashville, TN, USA; Dept. of Math, Computer Science, and Data Science Belmont University, Nashville, TN, USA; Dept. of Math, Computer Science, and Data Science Belmont University, Nashville, TN, USA; Dept. of Math, Computer Science, and Data Science Belmont University, Nashville, TN, USA",2024 IEEE Working Conference on Software Visualization (VISSOFT),"18 Dec 2024","2024","","","157","158","General-purpose large language models (LLMs) have become a versatile tool in software development and maintenance. These models offer support in tasks such as under-standing, writing, and summarizing code. While these LLMs can generate code quickly their use in software modeling is relatively under-explored. This research aims to study ChatGPT's ability to generate class diagrams from source code. To do this, we engineered a prompt that takes in source code to create a UML class diagram for that system. We used this prompt to create class diagrams for 40 systems and assessed the diagrams on their correctness and structure. Our results show that ChatGPT creates class diagrams that accurately capture 90% of the classes and their attributes and 66 % of the associations. While ChatGPT performed nearly flawlessly for smaller projects, the diagrams for larger projects had more issues. We conclude that ChatGPT is best utilized as a complementary tool rather than the sole resource for software modeling and maintenance.","2832-6555","979-8-3315-2848-5","10.1109/VISSOFT64034.2024.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794946","—UML;Large Language Models;LLM;ChatGPT;open source;class diagrams;software maintenance","Visualization;Codes;Source coding;Scalability;Unified modeling language;Writing;Chatbots;Software systems;Maintenance;Software development management","","","","4","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"Exploring Large Language Models for Automated Essay Grading in Finance Domain","G. Malik; M. Cevik; S. Lee","Data Science Lab, Toronto Metropolitan University, Toronto, Canada; Data Science Lab, Toronto Metropolitan University, Toronto, Canada; Blees Technologies Inc. (Blees AI), Toronto, Canada",2024 34th International Conference on Collaborative Advances in Software and COmputiNg (CASCON),"17 Jan 2025","2024","","","1","10","This study explores the application of large language models (LLMs) for the automated grading of essays in the finance domain. The focus is on generating grades for six Assessment Indicators (AIs) related to finance and accounting for each essay. Our research highlights the potential of LLMs and showcases custom prompt engineering's effectiveness in a domain-specific Automated Essay Scoring (AES) task. We propose two distinct prompting techniques: unified and discrete. The unified technique generates grades for all AIs using a single comprehensive prompt, while the discrete technique employs separate prompts for each AI. To enhance the effectiveness of these models, we apply In-Context learning through One-shot and Few-shot methods. Through extensive experimentation, we show that LLMs outperform fine-tuned BERT-like baselines, demonstrating consistency and generalizability in their results. However, challenges remain with output post-processing and the cost of processing input tokens.","","979-8-3315-0483-0","10.1109/CASCON62161.2024.10838105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838105","Large Language Models (LLMs);Automated Essay Grading;In-Context Learning;Generative AI;Natural Language Processing (NLP)","Analytical models;Costs;Large language models;Finance;One shot learning;Software;Robustness;Prompt engineering;Few shot learning;Standards","","","","25","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Applying Large Language Models to Enhance the Assessment of Parallel Functional Programming Assignments","S. Grandel; D. C. Schmidt; K. Leach","Vanderbilt University, Nashville, Tennessee, USA; Vanderbilt University, Nashville, Tennessee, USA; Vanderbilt University, Nashville, Tennessee, USA",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","102","110","Courses in computer science (CS) often assess student programming assignments manually, with the intent of providing in-depth feedback to each student regarding correctness, style, efficiency, and other quality attributes. As class sizes increase, however, it is hard to provide detailed feedback consistently, especially when multiple assessors are required to handle a larger number of assignment submissions. Large language models (LLMs), such as ChatGPT, offer a promising alternative to help automate this process in a consistent, scalable, and minimally-biased manner.This paper explores ChatGPT-4’s scalablility and accuracy in assessing programming assignments based on predefined rubrics in the context of a case study we conducted in an upper-level undergraduate and graduate CS course at Vanderbilt University. In this case study, we employed a method that compared assessments generated by ChatGPT-4 against human graders to measure the accuracy, precision, and recall associated with identifying programming mistakes. Our results show that when ChatGPT-4 is used properly (e.g., with appropriate prompt engineering and feature selection) it can improve objectivity and grading efficiency, thereby acting as a complementary tool to human graders for advanced computer science graduate and undergraduate students.CCS CONCEPTS• Software and its engineering → Software maintenance tools; • Applied computing → Computer-assisted instruction.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734431","ChatGPT;Education;Generative AI;Large Language Models;Prompt Engineering;Automated Grading","Software maintenance;Accuracy;Codes;Large language models;Conferences;Computational modeling;Feature extraction;Chatbots;Prompt engineering;Functional programming","","","","27","","30 Oct 2024","","","IEEE","IEEE Conferences"
"Privacy and Security Challenges in Large Language Models","V. Rathod; S. Nabavirazavi; S. Zad; S. S. Iyengar",Florida International University; Florida International University; Florida International University; Florida International University,2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00746","00752","Large Language Models (LLMs) are at the forefront of artificial intelligence advancements, demonstrating exceptional capabilities in natural language understanding and generation across diverse domains such as healthcare, finance, and customer service. However, their deployment introduces substantial secu-rity and privacy risks, including prompt injection, data leakage, and unauthorized data disclosures. These vulnerabilities highlight the need for robust frameworks to safeguard sensitive data and prevent misuse. This paper provides a comprehensive analysis of the security and privacy challenges in LLMs, examines existing mitigation strategies such as intelligent LLM firewalls, differen-tial privacy, and OW ASP-based security principles, and discusses future directions for ethical and secure LLM deployment. By addressing these challenges in detail, we identify gaps in current practices and propose a roadmap for the secure and responsible deployment of LLMs in high-stakes applications. Our findings underscore the importance of tailored security frameworks and privacy-preserving techniques to ensure the ethical and reliable use of LLMs in sensitive environments. Additionally, this pa-per emphasizes the significance of a human-in-the-loop (HITL) approach to ensure accountability and accuracy, particularly in critical domains. The discussion extends to emerging technologies such as retrieval-augmented generation (RAG) and adaptive threat detection systems, which hold promise for enhancing the security and ethical deployment of LLMs.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903912","Army Research Office(grant numbers:W911NF-21-1-0264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903912","Artificial intelligence;Natural language processing;Large language models (LLM);Privacy;OWASP;Data Protection;AI Ethics;Firewall;Threat Modeling;Data Leakage;Ethical Bias Mitigation;Federated Learning;Healthcare AI;Human-in-the-Loop (HITL);Adaptive Security Frameworks;Privacy-Preserving Computation","Industries;Ethics;Privacy;Technological innovation;Firewalls (computing);Large language models;Computational modeling;Medical services;Threat assessment;Security","","3","","33","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"A Novel Approach to Generative AI-based Optimized Code Generation for Semiconductor Equipment Interfaces","H. Lee; M. Yang; J. Jeong","Department of Smart Factory Convergence, Sungkyunkwan University, Suwon, Korea; Research & Development Team, THiRA-UTECH Co., Ltd, Seoul, Korea; Department of Smart Factory Convergence, Sungkyunkwan University, Suwon, Korea",2025 27th International Conference on Advanced Communications Technology (ICACT),"28 Mar 2025","2025","","","1","6","Generally, standardization defined by the SEMI association is well established and utilized in the semiconductor industry. In particular, most semiconductor equipment supports SECS / GEM communication protocols, and the automation and smart factory construction consist of equipment communication control programs using these standard protocols. We propose to improve development efficiency by automatically generating control program code using generative artificial intelligence technology to develop interface programs that control these semiconductor facilities. In addition, to improve the completeness and utilization of the automatically generated code, this paper presents a method to automatically generate semiconductor equipment control interface codes through generative artificial intelligence based on existing codes and minimize the constraints that may occur due to the hallucination effect, which is a significant weakness of generative artificial intelligence.","1738-9445","979-11-88428-13-7","10.23919/ICACT63878.2025.10936654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10936654","Generative AI;Code Generation;LLM (Large Language Model);RAG (Retrieval Augmented Generation);Prompt","Codes;Protocols;Generative AI;Databases;Large language models;Retrieval augmented generation;Electronics industry;Documentation;Standards;Smart manufacturing","","","","11","","28 Mar 2025","","","IEEE","IEEE Conferences"
"The Colossal Defense: Security Challenges of Large Language Models","S. Maity; J. Arora","Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India",2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON),"11 Feb 2025","2024","","","1","5","Large Language Models (LLMs) have become increasingly popular because of their strong capabilities and abilities in multiple fields, such as text generation, language translation, image, video, audio, and other forms of output generation. The ability of the model to analyze a complex input prompt and provide relevant and simplified responses, depending on its usage, elevates its desirability. LLM not only has a transformative effect on the interaction between humans and models and the usage of the models in different situations but also has a significant drawback. Although the main benefits of LLM are apparent, there is a potential landmine of security threats accompanying its existence. Exposure to artificial intelligence increases the security risk throughout its design, development, infrastructure, and incorporation. Threat actors can exploit existing vulnerabilities to control models and execute uncontrollable and divisive actions. This study thoroughly examines the fundamental security problems of prompt injection, data poisoning, and sensitive information extraction. The attacks and defenses arising from these issues related to LLM are summarized. Finally, we suggest future research directions to secure LLMs.","","979-8-3315-1859-2","10.1109/DELCON64804.2024.10866433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10866433","Natural Language Processing (NLP);Large Language Models (LLMs);ChatGPT;Llama;Gemini;Transformer","Analytical models;Translation;Landmine detection;Prevention and mitigation;Large language models;Information retrieval;Natural language processing;Security","","","","38","IEEE","11 Feb 2025","","","IEEE","IEEE Conferences"
"Understanding Defects in Generated Codes by Language Models","A. M. Esfahani; N. Kahani; S. A. Ajila","Systems and Computer Engineering, Carleton University, Ottawa, Canada; Systems and Computer Engineering, Carleton University, Ottawa, Canada; Systems and Computer Engineering, Carleton University, Ottawa, Canada",2024 34th International Conference on Collaborative Advances in Software and COmputiNg (CASCON),"17 Jan 2025","2024","","","1","10","This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the accuracy and functionality of the output remains a significant challenge. By using a structured defect classification method to understand their nature and origins this study categorizes and analyzes 367 identified defects from code snippets generated by LLMs, with a significant proportion being functionality and algorithm errors. These error categories indicate key areas where LLMs frequently fail, underscoring the need for targeted improvements. To enhance the accuracy of code generation, this paper implemented five prompt engineering techniques, including Scratchpad Prompting, Program of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code Prompting, and Structured Chain-of-Thought Prompting. These techniques were applied to refine the input prompts, aiming to reduce ambiguities and improve the models' accuracy rate. The research findings suggest that precise and structured prompting significantly miti-gates common defects, thereby increasing the reliability of LLM-aenerated code.","","979-8-3315-0483-0","10.1109/CASCON62161.2024.10837857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837857","Large Language Models (LLMs);Code Generation;Defect Classification;Software Testing;Prompt Engineering","Codes;Accuracy;Large language models;Focusing;Collaboration;Reliability engineering;Software;Encoding;Software reliability;Prompt engineering","","","","44","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Smart Design Evolution with GenAI and 3D Printing","A. Ray","Huber Street Elementary School, New Jersey, USA",2024 IEEE Integrated STEM Education Conference (ISEC),"17 Sep 2024","2024","","","1","4","This paper presents a system design environment (SDE) that leverages the power of generative AI and rapid prototyping to revolutionize product design iteration. The core of the SDE lies in the GenAI Analysis Engine, an intelligent system that extracts actionable insights from diverse user feedback data using large language models (LLMs). Through a data preparation pipeline, prompt engineering module, and LLM-powered analysis, the GenAI Engine identifies recurring themes, user pain points, reported flaws and feature sentiment, informing subsequent design iterations. To bridge the gap between insights and tangible solutions, the SDE seamlessly integrates with popular CAD software and offers robust parametric design tools. This empowers designers to directly translate AI-driven insights into real-time design modifications, visualized within their familiar CAD environment. Furthermore, integrated material and functionality simulators provide valuable predictive insights, enabling informed design refinement and minimizing the need for physical prototypes. Closing the loop on user feedback, the SDE leverages a cloud-based 3D printing network to rapidly produce and deliver prototypes for real-world testing. User feedback from these prototypes is then fed back into the GenAI Engine, prompting further design refinements, and ensuring a continuous cycle of improvement. This paper demonstrates the SDE's potential to significantly shorten design cycles, reduce costs, and enhance product quality by harnessing the synergy between generative AI, rapid prototyping, and human expertise. This novel approach paves the way for a future of data-driven design, where user insights seamlessly guide product evolution, leading to superior user experiences and market success.","2473-7623","979-8-3503-5280-1","10.1109/ISEC61299.2024.10665095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10665095","Generative AI for product design;3D printing for rapid prototyping;Customer feedback analysis;User-centric design;Smart design evolution;Large Language Models (LLMs)","Analytical models;Technological innovation;Sentiment analysis;Reviews;Three-dimensional printing;Product design;Data models","","3","","7","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Refactoring Programs Using Large Language Models with Few-Shot Examples","A. Shirafuji; Y. Oda; J. Suzuki; M. Morishita; Y. Watanobe","University of Aizu, Japan; Tohoku University, Japan; Tohoku University, Japan; NTT Communication Science Laboratories, Japan; University of Aizu, Japan",2023 30th Asia-Pacific Software Engineering Conference (APSEC),"2 Apr 2024","2023","","","151","160","A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Further-more, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.","2640-0715","979-8-3503-4417-2","10.1109/APSEC60848.2023.00025","Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:JP23H03508); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479398","code refactoring;large language models;few-shot prompting;software complexity;programming education","Measurement;Codes;Filtering;Education;Writing;Programming;Complexity theory","","18","","48","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Gender Fairness Analysis in LLMs using Zero-Shot and Few-Shot Experiments","K. Gupta; K. Jain; L. Solanki","Department of Computer Science & Engineering, Delhi Technological University, Delhi, India; Department of Computer Science & Engineering, Delhi Technological University, Delhi, India; Department of Computer Science & Engineering, Delhi Technological University, Delhi, India",2024 Third International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN),"13 Sep 2024","2024","","","1","6","This research delves into the critical domain of fairness analysis for various prominent large language models (LLMs). The proposed work aims to establish a comprehensive fairness analysis in the gender domain for six models, namely, GPT 3.5, Gemini, Gemini-Flash, Llama 2, Llama 3 and Gemma. This research focuses on evaluating gender biases using zero-shot and few-shot experiments on a dataset covering generalized sentences based on day-to-day instances and conversations. The few shot experiments, furthermore, help us understand how these models respond to instructions and system prompting. The motivation behind this investigation arises from escalating ethical concerns associated with biases and potential discrimination in the outputs generated by these LLMs. The findings and insights gained from this research will have direct implications for real-world applications. By highlighting potential biases, the research directly contributes to the ongoing AI development.","","979-8-3503-9156-5","10.1109/ICSTSN61422.2024.10671001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10671001","Gender Bias;Large Language Models;Zero Shot Experiments;Few Shot Experiments;Prompt Engineering","Ethics;Analytical models;Large language models;Computational modeling;Oral communication;Benchmark testing;Next generation networking","","","","12","IEEE","13 Sep 2024","","","IEEE","IEEE Conferences"
"A Comprehensive Framework and Empirical Analysis for Evaluating Large Language Models in Arabic Dialect Identification","S. Al-Azani; N. Alturayeif; H. Abouelresh; A. Alhunief","SDAIA-KFUPM Joint Research Center for Artificial Intelligence, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia; Department of Computer Science, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; SDAIA-KFUPM Joint Research Center for Artificial Intelligence, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia; SDAIA-KFUPM Joint Research Center for Artificial Intelligence, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia",2024 International Joint Conference on Neural Networks (IJCNN),"9 Sep 2024","2024","","","1","7","The widespread interest in large language models (LLMs) is rooted in their remarkable capacity to generate human-like and contextually relevant responses. However, the precision of LLMs within specific domains or intricate tasks, such as Arabic dialect identification, remains largely unexplored. This task presents a substantial challenge in Arabic natural language processing, given its language-dependent nature. This paper provides a framework for evaluating LLMs for Arabic dialect identification and conducts a comprehensive evaluation of LLMs, employing both tuning-free and fine-tuning-based learning paradigms. The evaluation encompasses GPT-3.5, chatGPT, GPT-4, and Google BARD for Arabic dialect identification under the tuning-free learning paradigm. Furthermore, it assesses the performance of GPT-3.5 along with AraBERT and MARBERT using the fine-tuning learning paradigm. In the tuning-free approach, GPT-4 achieves the most favorable results, reporting an F1MAC of 45.60%. Under the fine-tuning learning paradigm, both AraBERT and MARBERT exhibit comparable performance (around 50% F 1MAC) to GPT-3.5, without incurring any financial costs, in contrast to the expenses associated with GPT-3.5.","2161-4407","979-8-3503-5931-2","10.1109/IJCNN60899.2024.10651099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10651099","Large language models;Arabic dialect identification;Prompt engineering;Instruction fine-tuning;Evaluation LLMs","Analytical models;Costs;Accuracy;Large language models;Neural networks;Focusing;Chatbots","","1","","27","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Requirements Engineering and Large Language Models: Insights From a Panel","M. Borg","CodeScene AB, Malmö, Sweden",IEEE Software,"28 Feb 2024","2024","41","2","6","10","As a general-purpose technology, large language models promise to enhance various software engineering tasks. But how will they impact requirements engineering? This column offers a summary of an expert panel discussion from the 2023 International Requirements Engineering Conference in Hanover, Germany.","1937-4194","","10.1109/MS.2023.3339934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452860","","Requirements engineering;Task analysis;Software engineering","","9","","5","IEEE","28 Feb 2024","","","IEEE","IEEE Magazines"
"An Extended Review: LLM Prompt Engineering in Cyber Defense","N. Shenoy; A. V. Mbaziira","School of Technology and Innovation, Marymount University, Arlington, USA; School of Technology and Innovation, Marymount University, Arlington, USA","2024 International Conference on Electrical, Computer and Energy Technologies (ICECET","8 Oct 2024","2024","","","1","6","The launch of ChatGPT in November 2022 marked a significant advancement in the field of artificial intelligence, particularly in the realm of generative AI (GAI). ChatGPT is based on the Large Language Model (LLM). Use of AI in Cybersecurity can prove to be beneficial as the security analysts are employing AI models for improved detection of threats and quicker response to incidents. The interaction with LLMs needs to be skillfully and tactfully created to get precise and concise response. This technique of crafting queries to interact with the LLM is called prompt engineering. Prompt engineering in vulnerability detection and management is a new trend in the industry to manage cybersecurity threats and weaknesses proactively and effectively. This paper presents an extensive review of the field of prompt engineering in cybersecurity. It is primarily based on a comprehensive analysis of existing literature, encompassing a wide range of sources to provide a thorough overview of the current state and advancements in AI. The review delves into various aspects of prompt engineering, synthesizing key findings and theories from a multitude of scholarly articles and industry reports. This approach ensures a holistic understanding of the AI models including LLMs and how generative AI, in terms of prompt engineering, can be used for cyber-defense.","","979-8-3503-9591-4","10.1109/ICECET61485.2024.10698605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698605","Artificial Intelligence;Machine Learning;Natural Language Processing;Generative AI;Large Language Models;Prompt Engineering;ChatGPT;Cyber-Defense;Cybersecurity;Vulnerability Management","Industries;Training;Analytical models;Reviews;Chatbots;Market research;Prompt engineering;Artificial intelligence;Computer security;Monitoring","","2","","3","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"From Specifications to Prompts: On the Future of Generative Large Language Models in Requirements Engineering","A. Vogelsang","University of Cologne, Cologne, Germany",IEEE Software,"7 Aug 2024","2024","41","5","9","13","Generative LLMs, such as GPT, have the potential to revolutionize Requirements Engineering (RE) by automating tasks in new ways. This column explores the novelties and introduces the importance of precise prompts for effective interactions. Human evaluation and prompt engineering are essential in leveraging LLM capabilities.","1937-4194","","10.1109/MS.2024.3410712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10629163","","Large language models;Requirements engineering;Prompt engineering;Task analysis;Software development management","","5","","10","IEEE","7 Aug 2024","","","IEEE","IEEE Magazines"
"Large Language Models (LLMs) for Verification, Testing, and Design","C. K. Jha; M. Hassan; K. Qayyum; S. Ahmadi-Pour; K. Xu; R. Qiu; J. Blocklove; L. Collini; A. Nakkab; U. Schlichtmann; G. Li Zhang; R. Karri; B. Li; S. Garg; R. Drechsler","Institute of Computer Science, University of Bremen, Germany; Institute of Computer Science, University of Bremen, Germany; Cyber-Physical Systems, DFKI GmbH, Germany; Institute of Computer Science, University of Bremen, Germany; Chair of Electronic Design Automation, Technical University of Munich, Germany; Chair of Electronic Design Automation, Technical University of Munich, Germany; Tandon School of Engineering, New York University, USA; Tandon School of Engineering, New York University, USA; Tandon School of Engineering, New York University, USA; Chair of Electronic Design Automation, Technical University of Munich, Germany; Hardware for Artificial Intelligence Group, Technical University of Darmstadt, Germany; Tandon School of Engineering, New York University, USA; Research Group of Digital Integrated Systems, University of Siegen, Germany; Tandon School of Engineering, New York University, USA; Institute of Computer Science, University of Bremen, Germany",2025 IEEE European Test Symposium (ETS),"1 Jul 2025","2025","","","1","10","Large Language Models (LLMs) are being explored for their use in the domain of Electronic Design Automation (EDA). In this paper, we discuss state-of-the-art works showing the use of LLMs in verification, testing, and design generation. We provide a summary of the existing works and highlight the methods that have been used to enhance the quality of the output of the LLMs, like prompt engineering, Retrieval Augmented Generation (RAG), fine-tuning, multi-shot prompting, etc. We show that LLMs can aid in the domain of EDA, however, several challenges need to be addressed, such as data availability for fine-tuning the LLMs, integration with EDA tools, scalability, etc. This paper aims to highlight the use of LLMs in EDA, improve the output quality when using LLMs, and highlight the challenges and future directions that can be useful for further research.","1558-1780","979-8-3315-9450-3","10.1109/ETS63895.2025.11049311","Ministry of Education; Deutsche Forschungsgemeinschaft; Synopsys; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11049311","Large language models;formal verification;testing;Verilog;assertion based verification","Design automation;Large language models;Scalability;Retrieval augmented generation;Europe;Prompt engineering;Hardware design languages;Testing;Formal verification","","","","78","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Harnessing Large Language Models for Curated Code Reviews","O. B. Sghaier; M. Weyssow; H. Sahraoui","Université de Montréal, Montréal, Canada; Singapore Management University, Singapore; Université de Montréal, Montréal, Canada",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","187","198","In code review, generating structured and relevant comments is crucial for identifying code issues and facilitating accurate code changes that ensure an efficient code review process. Well-crafted comments not only streamline the code review itself but are also essential for subsequent tasks like code refinement, where the code is modified to satisfy the input review comment. Although various AI-based approaches aimed to automate comment generation, their effectiveness remains limited by the quality of the training data. Existing code review datasets are often noisy and unrefined, posing limitations to the learning potential of AI models and hindering the automation process. To address these challenges, we propose a curation pipeline designed to enhance the quality of the largest publicly available code review dataset. We begin by establishing an evaluation framework, incorporating specific criteria and categories to empirically study the initial quality of the dataset. Using a large language model (LLM)-driven approach, we then apply our curation pipeline to refine the dataset. A comparative analysis of the newly curated dataset, based on the same evaluation framework, demonstrates substantial improvements in the clarity and conciseness of the comments. Additionally, we assess the impact of the curated dataset on automating downstream tasks, specifically comment generation and code refinement. Our findings show that the curated dataset leads to enhanced model performance in generating more accurate comments. Curated comments are also more useful as they lead to more accurate code refinement.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025570","Code review;large language models;software maintenance;empirical software engineering","Training;Software maintenance;Codes;Accuracy;Reviews;Large language models;Pipelines;Training data;Predictive models;Software engineering","","","","42","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Enhanced Database Interaction Using Large Language Models for Improved Data Retrieval and Analysis","V. Usha; N. C. Abhinash; S. N. Chowdary; V. Sathya; E. R. Reddy; S. P. S","Dept.of CSE, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India; Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India; Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India; Dept of Information Technology, Velammal Engineering College, Chennai, India; Dept.of CSE, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India; Dept.of CSE, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Chennai, India",2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI),"4 Oct 2024","2024","","","1302","1306","One of the difficult task for many users on SQL is to write the SQL Query due to its syntax and structure. If a person needs to query a database, they should know everything about how data is distributed and what the internal dependencies are. For this reason, it is not easy for everyone to access data in database without proper knowledge. This paper presents a novel application that leverages generative AI and natural language processing (NLP) to enable users to interact with databases using natural language queries. Built on the Gemini API, the application translates user queries into SQL queries, simplifying database interactions for non-technical users. The Python-based backend, SQLite database management, and Streamlit frontend provide a comprehensive solution for database querying and analysis. This approach democratizes data retrieval and analysis, offering automated insights and visualizations to users of all skill levels. The app also features automated data analysis, which boosts insight generation for users of all skill levels. Further, the traditional ways of querying SQL generally require specialized knowledge and are only accessible to those with technical backgrounds. When the application takes charge of the query construction process and offers data as UI elements, it can invigorate users to have a higher degree of insight into what their data actually is and explore it even more efficiently. The Python software stack combines Python for backend processing, SQLite for database management and a web-based frontend for user interaction to provide an all-encompassing database querying and analysis solution.","","979-8-3315-4066-1","10.1109/ICoICI62503.2024.10696623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696623","Generative AI;Natural Language Processing;Text to SQL Generation;Gemini API;Large Language Model;Machine Learning;Deep Learning;Google LLM;SQLQuery Generation;Prompt Engineering;Data Retrieval","Structured Query Language;Databases;Generative AI;Large language models;Distributed databases;Syntactics;Natural language processing;User experience;Internet of Things;Python","","2","","12","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Software Development Automation Using Generative AI","K. R. Raghi; K. Sudha; S. A. M; S. Joshua S","Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India; Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India; Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India; Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India",2024 International Conference on Emerging Research in Computational Science (ICERCS),"27 Feb 2025","2024","","","1","6","The Software Development Lifecycle (SDLC) is a structured process that guides the development of software projects, encompassing phases from planning to deployment. Traditionally, the SDLC has relied on manual input, making it prone to delays, errors, and inefficiencies. With the recent advancements in Generative AI (GenAI) and Large Language Models (LLMs) such as GPT, it is now feasible to automate substantial portions of the SDLC. This paper presents a novel approach to automating the SDLC using LLMs and the Langchain framework, aiming to streamline the entire software development process. By au-tomating key phases, including project planning, requirements gathering, code generation, testing, and deployment, this research explores how AI can minimize human intervention and accelerate software development timelines. The paper also discusses the potential advantages of AI-driven SDLC automation, such as improved efficiency, consistency, and scalability, while addressing challenges related to its integration. The proposed approach offers a glimpse into the future of software engineering, where AI plays a central role in transforming how software is developed and delivered.","","979-8-3315-3496-7","10.1109/ICERCS63125.2024.10894980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894980","Software Development Lifecycle (SDLC);Generative AI (GenAI);Large Language Models (LLMs);GPT;Langchain;SDLC automation;AI-driven software development;code generation","Automation;Generative AI;Scientific computing;Scalability;Manuals;Software;Planning;Software development management;Testing;Software engineering","","1","","16","IEEE","27 Feb 2025","","","IEEE","IEEE Conferences"
"Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis","A. M. Dumitran; A. C. Badea; S. -G. Muscalu","Computer Science Department, University of Bucharest, Romania; Computer Science Department, University of Bucharest, Romania; Independent Researcher, It Just Works Inc., Bucharest, Romania",2024 International Conference on INnovations in Intelligent SysTems and Applications (INISTA),"24 Sep 2024","2024","","","1","7","This study explores the performance of large language models (LLMs) in solving competitive programming problems from the Romanian Informatics Olympiad at the county level. Romania, a leading nation in computer science competitions, provides an ideal environment for evaluating LLM capabilities due to its rich history and stringent competition standards. We collected and analyzed a dataset comprising 304 challenges from 2002 to 2023, focusing on solutions written by LLMs in C++ and Python for these problems. Our primary goal is to understand why LLMs perform well or poorly on different tasks. We evaluated various models, including closed-source models like GPT-4 and open-weight models such as CodeLlama and RoMistral, using a standardized process involving multiple attempts and feedback rounds. The analysis revealed significant variations in LLM performance across different grades and problem types. Notably, GPT-4 showed strong performance, indicating its potential use as an educational tool for middle school students. We also observed differences in code quality and style across various LLMs.","2768-7295","979-8-3503-6813-0","10.1109/INISTA62901.2024.10683837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10683837","Large Language Models (LLMs);Benchmark;IOI;Code Generation;AI in Education;C++;Python","Technological innovation;Codes;Large language models;Focusing;History;Intelligent systems;Informatics","","","","19","IEEE","24 Sep 2024","","","IEEE","IEEE Conferences"
"Studying and Understanding the Effectiveness and Failures of Conversational LLM-Based Repair","A. Chen; H. Wu; Q. Xin; S. P. Reiss; J. Xuan","School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; Department of Computer Science, Brown University, USA; School of Computer Science, Wuhan University, China",2025 IEEE/ACM International Workshop on Automated Program Repair (APR),"13 Jun 2025","2025","","","56","59","Automated program repair (APR) is designed to automate the process of bug-fixing. In recent years, thanks to the rapid development of large language models (LLMs), automated repair has achieved remarkable progress. Advanced APR techniques powered by conversational LLMs, most notably ChatGPT, have exhibited impressive repair abilities and gained increasing popularity due to the capabilities of the underlying LLMs in providing repair feedback and performing iterative patch improvement. Despite the superiority, conversational APR techniques still fail to repair a large number of bugs. For example, a state-of-the-art conversational technique Chatrepair does not correctly repair over half of the single-function bugs in the Defects4J dataset. To understand the effectiveness and failures of conversational LLM-based repair and provide possible directions for improvement, we studied the exemplary Chatrepair with a focus on comparing the effectiveness of its cloze-style and full-function repair strategies, assessing its key iterative component for patch improvement, and analyzing the repair failures. Our study has led to a series of findings, which we believe provide key implications for future research.","","979-8-3315-2585-9","10.1109/APR66717.2025.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029649","Large language models;conversational APR","Large language models;Conferences;Computer bugs;Maintenance engineering;Chatbots;Iterative methods","","","","5","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Chat GPT and Cyber Risks: Need of Custom Large Language Models","P. Vajpayee; C. Karuppiah; G. Hossain","Dept. of Informtation Science, University of North Texas, Denton, TX, USA; Dept. of Information Scicence, Univesity of North Texas, Denton, TX, USA; Dept. of Information Scinece, University of North Texas, Denton, TX, USA","2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)","12 Mar 2025","2024","","","1","4","Due to growing utilization of Artificial Intelligence (AI) for small, medium, and large business organizations cybersecurity risks are increasing. While AI provides enormous opportunities to generate great insight, it also enables risks on data privacy or network security. Organizations are integrating Chat GPT to enhance customer experience by providing personalization, product recommendations, and immediate responses of users’ queries. However, if underline AI models have not been trained appropriately or default ready to use models have been used to generate responses, the cybersecurity risk will be increased. The paper will discuss about the prompt injection attack in which an attacker can exploit the vulnerabilities of large language models (LLM) to get sensitive data or system specific information to incorporate data breach or other cyber-attacks. The study compares the response of cybersecurity related questions for Chat GPT and Ollama LLM models to discuss the possible cyber risks and suggest the need of custom LLM models for organizations. The paper also provides an approach how these risks can be quantified using concept of Cyber Value at Risk (CVaR).","","979-8-3503-6389-0","10.1109/ICDDS62937.2024.10910783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10910783","Artificial Intelligence;Large Language Models;Cyber Risk;Data Privacy;Cyber-attacks;Cyber Value at Risk","Training;Data privacy;Reactive power;Protocols;Large language models;Organizations;Network security;Data models;Organizational aspects;Cyberattack","","","","20","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG","K. Bourne; S. Es",NA; NA,Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG,"","2024","","","","","Leverage cutting-edge generative AI techniques such as RAG to realize the potential of your data and drive innovation as well as gain strategic advantageKey FeaturesOptimize data retrieval and generation using vector databasesBoost decision-making and automate workflows with AI agentsOvercome common challenges in implementing real-world RAG systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI is helping organizations tap into their data in new ways, with retrieval-augmented generation (RAG) combining the strengths of large language models (LLMs) with internal data for more intelligent and relevant AI applications. The author harnesses his decade of ML experience in this book to equip you with the strategic insights and technical expertise needed when using RAG to drive transformative outcomes. The book explores RAG’s role in enhancing organizational operations by blending theoretical foundations with practical techniques. You’ll work with detailed coding examples using tools such as LangChain and Chroma’s vector database to gain hands-on experience in integrating RAG into AI systems. The chapters contain real-world case studies and sample applications that highlight RAG’s diverse use cases, from search engines to chatbots. You’ll learn proven methods for managing vector databases, optimizing data retrieval, effective prompt engineering, and quantitatively evaluating performance. The book also takes you through advanced integrations of RAG with cutting-edge AI agents and emerging non-LLM technologies. By the end of this book, you’ll be able to successfully deploy RAG in business settings, address common challenges, and push the boundaries of what’s possible with this revolutionary AI technique.What you will learnUnderstand RAG principles and their significance in generative AIIntegrate LLMs with internal data for enhanced operationsMaster vectorization, vector databases, and vector search techniquesDevelop skills in prompt engineering specific to RAG and design for precise AI responsesFamiliarize yourself with AI agents' roles in facilitating sophisticated RAG applicationsOvercome scalability, data quality, and integration issuesDiscover strategies for optimizing data retrieval and AI interpretabilityWho this book is forThis book is for AI researchers, data scientists, software developers, and business analysts looking to leverage RAG and generative AI to enhance data retrieval, improve AI accuracy, and drive innovation. It is particularly suited for anyone with a foundational understanding of AI who seeks practical, hands-on learning. The book offers real-world coding examples and strategies for implementing RAG effectively, making it accessible to both technical and non-technical audiences. A basic understanding of Python and Jupyter Notebooks is required.","","9781835887912","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769241.pdf&bkn=10769240&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Fine-Tuning Large Language Models for Network Traffic Analysis in Cyber Security","O. Lavi; O. Manor; T. Schwartz; A. F. Murillo; A. Messous; M. Sekiya; J. Suga; K. Hikichi; Y. Unno","Fujitsu Research of Europe, Tel Aviv, Israel; Fujitsu Research of Europe, Tel Aviv, Israel; Fujitsu Research of Europe, Tel Aviv, Israel; Fujitsu Research of Europe, Slough, UK; Fujitsu Research of Europe, Slough, UK; Fujitsu Research of Europe, Slough, UK; Fujitsu Laboratories, Kawasaki, Japan; Fujitsu Laboratories, Kawasaki, Japan; Fujitsu Laboratories, Kawasaki, Japan",2024 IEEE Conference on Dependable and Secure Computing (DSC),"29 Nov 2024","2024","","","45","50","Language modelling has demonstrated the exceptional interpretation and analysis capabilities of Large Language Models (LLMs) in addressing a wide range of Natural Language Processings (NLPs) tasks. However, LLMs still face challenges in technical network text analysis due to its special terminology, syntax, and protocols which are not consistent with conventional human language. In this paper, we introduce a method for fine-tuning NLP-based LLMs on technical network text intended for machine comprehension. We apply our method to detect and classify different types of attacks in network traffic flows. To validate our method, we used the Kitsune Network Attack Dataset and our results show that our method is able to correctly classify the network attacks, even in adverse conditions, when the model receives irrelevant context for classification. This study highlights how the capabilities of LLMs can be extended across different unknown domains, specifically cyber security, thereby enhancing their ability to detect anomalies, classify them, and distinguish between various types of attacks. Our experiments show that fine-tuned models significantly outperform non-fine-tuned models, underlining the effectiveness of fine-tuning LLMs for cyber security applications involving the analysis of technical text data.","","979-8-3315-4028-9","10.1109/DSC63325.2024.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763746","large language models;cyber security;network anomaly classification","Analytical models;Text analysis;Protocols;Terminology;Large language models;Computational modeling;Telecommunication traffic;Syntactics;Data models;Computer crime","","1","","18","IEEE","29 Nov 2024","","","IEEE","IEEE Conferences"
"Sentiment Bias and Security Analysis in Training Datasets of Large Language Models","K. Inoshita; X. Zhou","Faculty of Data Science, Shiga University, Hikone, Japan; Faculty of Business Data Science, Kansai University, Osaka, Japan",2024 IEEE International Conference on Big Data and Cloud Computing (BDCloud),"11 Mar 2025","2024","","","1","8","The development of large language models (LLMs) has led to improved predictive performance and efficiency in various fields. However, the datasets used for training LLMs contain country-specific sentiment biases, which could pose serious issues in the field of security. This study aims to analyze the country-specific sentiment biases in the datasets used for training LLMs and evaluate their impact. In this study, we analyzed the sentiment tendencies associated with country names using VADER across four major large-scale training datasets: C4, RedPajama, OSCAR, and RefinedWeb. Specifically, we calculated the frequency of appearance and sentiment tendencies for each country, and further assessed the sentiment tendencies in bilateral relationships. The results revealed that the USA had a significantly high frequency of appearances and exhibited greater robustness compared to other countries. In contrast, negative sentiments towards Russia and Iran were prominently emphasized. Additionally, notable sentiment biases were confirmed in specific combinations of bilateral relationships. These findings indicate the presence of substantial biases in the datasets used for training LLMs. Particularly in the field of security, there is a risk of misunderstandings and conflicts arising in international relations. To address this, it is necessary to create datasets that evaluate LLMs’ perceptions of national conflicts and establish bias-removal standards in collaboration with international organizations. This will facilitate the development of more fair and reliable AI systems.","","979-8-3315-1026-8","10.1109/BDCloud63169.2024.00008","Japan Society for the Promotion of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10918029","Sentiment Bias;Large Language Models;Dataset Analysis;Security","Training;Cloud computing;Large language models;Standards organizations;Collaboration;Organizations;International relations;Robustness;Security;High frequency","","","","33","IEEE","11 Mar 2025","","","IEEE","IEEE Conferences"
"Evaluation of Large Language Models for Unit Test Generation","M. Konuk; C. Baglum; U. Yayan","Computer Engineering Department, Center of Intelligent Systems, Applications Research (CISAR) Eskisehir Osmangazi University, Eskisehir, Türkiye; Computer Engineering Department, Center of Intelligent Systems, Applications Research (CISAR) Eskisehir Osmangazi University, Eskisehir, Türkiye; Computer Engineering Department, Center of Intelligent Systems, Applications Research (CISAR) Eskisehir Osmangazi University, Eskisehir, Türkiye",2024 Innovations in Intelligent Systems and Applications Conference (ASYU),"28 Nov 2024","2024","","","1","6","In recent years, Artificial Intelligence (AI) has significantly transformed various industries, especially software development, through automation and enhanced decision-making processes. Traditional software testing, often manual and error-prone, cannot keep up with rapid development cycles and complex systems, leading to extended development times, higher costs, and undetected bugs. This study develops an AI-based platform using OpenAI models to generate and execute unit tests across multiple programming languages. By leveraging Large Language Models (LLMs) like GPT, we automate unit test creation, demonstrating proficiency in understanding and generating natural language to interpret code. Our web-based system architecture ensures efficient test generation and execution, significantly reducing manual effort and mitigating human error, thus revolutionizing software testing. Furthermore, we introduce unique evaluation metrics such as “Is Executable” and “Assertion Count” to assess the performance and effectiveness of the generated unit tests, providing a comprehensive measure of the models' capabilities.","2770-7946","979-8-3503-7943-3","10.1109/ASYU62119.2024.10756954","Scientific and Technological Research Council of Turkey (TÜBiTAK)(grant numbers:2209/B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756954","AI-based Software Testing;Large Language Models (LLMs);Unit Test Automation;Web-based Testing Platforms;Model Performance Evaluation","Software testing;Performance evaluation;Technological innovation;Large language models;Natural languages;Systems architecture;Manuals;Software;Test pattern generators;Software development management","","","","16","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"On the Performance of Large Language Models for Code Change Intent Classification","I. Oukhay; M. Chouchen; A. Ouni; F. H. Fard","ÉTS Montreal, QC, Canada; Concordia University, QC, Canada; ÉTS Montreal, QC, Canada; University of British Columbia, Canada","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","598","603","Modern Code Review (MCR) is an essential practice in software engineering, supporting early defect detection, enhancing code quality, and fostering knowledge. To manage code review tasks effectively, developers need to understand the intent behind code changes, such as a bug fix, test, refactoring, or new feature. Traditional methods for categorizing code changes in MCR rely on rule-based heuristics with predefined keywords. However, these methods lack context regarding the code changes, leading to limited generalizability, particularly when dealing with sparsely documented changes. This paper addresses these limitations by investigating the potential of Large Language Models (LLMs) for changes' intent classification. We introduce LLM Change Classifier (LLMCC), an LLM-based approach that classifies code changes based on their underlying intent. We evaluate the effectiveness of LLMCC by conducting an empirical study on three open-source projects: Android, OpenS tack, and Qt. The performance of LLMCC was benchmarked against traditional heuristic methods, conventional machine learning algorithms (including Decision Trees and Random Forests), and state-of-the-art transformer models (including BERT and RoBERTa). Results show that LLMCC significantly enhances code change intent classification accuracy, achieving up to a 33 % improvement in F1 score over heuristic-based methods. Additionally, LLMCC outperformed both traditional machine learning and transformer models, achieving an average 77% improvement in terms of Matthew Correlation Coefficient (MCC). These findings underscore the potential of LLMCC to streamline code change intent classification.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992584","Modern Code Review;Large Language Models;Machine Learning;Change Intent Analysis","Knowledge engineering;Codes;Machine learning algorithms;Intent recognition;Reviews;Large language models;Transformers;Software;Random forests;Software engineering","","","","34","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Application of Large Language Models in Cybersecurity: A Systematic Literature Review","I. Hasanov; S. Virtanen; A. Hakkala; J. Isoaho","Department of Computing, University of Turku, Turku, Finland; Department of Computing, University of Turku, Turku, Finland; Department of Computing, University of Turku, Turku, Finland; Department of Computing, University of Turku, Turku, Finland",IEEE Access,"3 Dec 2024","2024","12","","176751","176778","The emergence of Large Language Models (LLMs) is currently creating a major paradigm shift in societies and businesses in the way digital technologies are used. While the disruptive effect is especially observable in the information and communication technology field, there is a clear lack of systematic studies focusing on the application and impact of LLMs in cybersecurity holistically. This article presents an exhaustive systematic literature review of 177 articles published in 2018-2024 on the application of LLMs and the use of Artificial Intelligence (AI) as a defensive measure in cybersecurity. This article contributes an analytical compendium of the recent research on the application of LLMs in offensive and defensive cybersecurity as well as in research on cyberethics, current legal frameworks, and research regarding the use of LLMs for cybersecurity governance. It also contributes a statistical summary of global research trends in the field. Of the reviewed literature, 68% was published in 2023. Nearly 30% of the articles originate from the USA and 11% from China, with other countries currently having significantly lower contributions to recent research. Most attention in recent research has been given to AI as a defensive measure, accounting for 27% of the reviewed literature. It was observed that LLMs have proven highly effective in phishing attack simulations and in managing cybersecurity administrative aspects, including defending against advanced exploits. Furthermore, LLMs show significant potential in the development of security software, further cementing their role as a powerful tool in cybersecurity innovation.","2169-3536","","10.1109/ACCESS.2024.3505983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767242","Cybersecurity;artificial intelligence;large language models;generative AI;penetration testing;cyberethics;network security;natural language processing;systematic literature review;survey","Computer security;Artificial intelligence;Computer crime;Large language models;Phishing;Electronic mail;Internet;Databases;Companies;Bibliographies","","13","","210","CCBY","25 Nov 2024","","","IEEE","IEEE Journals"
"Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models","Y. Xu; J. Feng; W. Miao","East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","26 Sep 2024","2024","","","204","215","Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4% accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27% to 78% under interactive prompt evolution.","2693-9177","979-8-3503-6563-4","10.1109/QRS62785.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684640","Requirements Engineering;Formal Specification;Large Language Models;Prompt Engineering;Natural Language Processing","Industries;Training;Accuracy;Large language models;Software quality;Software reliability;Logic","","","","63","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Detecting Health Misinformation on Social Networking Sites Using Large Language Models and Deep Learning-based Natural Language Processing","A. M. Alshahrani; H. Farooq Ahmad; J. Hussain","Department of Computer Science, College of Computer Sciences and Information Technology, King Faisal University, Al-Ahsa, Saudi Arabia; Department of Computer Science, College of Computer Sciences and Information Technology, King Faisal University, Al-Ahsa, Saudi Arabia; Department of Computer Science, Sejong University, Seoul, Korea",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","561","568","Health misinformation on social networking sites (SNS) is a critical issue, particularly during health crises like the COVID-19 pandemic. The spread of inaccurate health information can lead to severe outcomes, including reduced vaccine uptake and decreased trust in healthcare. This paper introduces a system using large language models (LLMs) and deep learning-based natural language processing (NLP) techniques to detect and mitigate health misinformation on SNS. The model incorporates transformer-based architectures, zero-shot (ZS) and few-shot (FS) learning, and prompt engineering to classify health-related content as true or false. A key feature is the integration of a knowledge graph that enhances verification capabilities. Applied to both English and Arabic datasets, the system’s performance is evaluated using accuracy, F1-score, BLEU, and METEOR metrics, demonstrating its multilingual effectiveness in real-time misinformation detection.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852503","King Faisal University; King Faisal University; Sejong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852503","Health Misinformation;Large Language Models (LLMs);Deep Learning;Natural Language Processing (NLP);Zero-Shot Learning;Few-Shot Learning;Prompt Engineering;Social Networking Sites (SNS)","Measurement;Adaptation models;Social networking (online);Large language models;Knowledge graphs;Natural language processing;Real-time systems;Multilingual;Meteors;Fake news","","","","26","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"The Security of Using Large Language Models: A Survey with Emphasis on ChatGPT","W. Zhou; X. Zhu; Q. -L. Han; L. Li; X. Chen; S. Wen; Y. Xiang","School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn VIC, Australia; School of Computer and Mathematical Sciences, the University of Adelaide, Adelaide, South Australia, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn VIC, Australia; Faculty of Science and Engineering, Southern Cross University, Gold Coast QLD, Australia; School of Information and Physical Sciences, the University of Newcastle, Callaghan, NSW, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn VIC, Australia; School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Hawthorn VIC, Australia",IEEE/CAA Journal of Automatica Sinica,"21 Jan 2025","2025","12","1","1","26","ChatGPT is a powerful artificial intelligence (AI) language model that has demonstrated significant improvements in various natural language processing (NLP) tasks. However, like any technology, it presents potential security risks that need to be carefully evaluated and addressed. In this survey, we provide an overview of the current state of research on security of using ChatGPT, with aspects of bias, disinformation, ethics, misuse, attacks and privacy. We review and discuss the literature on these topics and highlight open research questions and future directions. Through this survey, we aim to contribute to the academic discourse on AI security, enriching the understanding of potential risks and mitigations. We anticipate that this survey will be valuable for various stakeholders involved in AI development and usage, including AI researchers, developers, policy makers, and end-users.","2329-9274","","10.1109/JAS.2024.124983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10751746","Artificial intelligence (AI);ChatGPT;large language models (LLMs);security","Artificial intelligence;Security;Surveys;Chatbots;Ethics;Data models;Large language models;Fake news;Safety;Privacy","","19","","177","","12 Nov 2024","","","IEEE","IEEE Journals"
"Assessing LLMs for Front-end Software Architecture Knowledge","L. P. F. Guerra; N. Ernst","Computer Science, University of Victoria, Victoria, Canada; Computer Science, University of Victoria, Victoria, Canada",2025 IEEE/ACM International Workshop on Designing Software (Designing),"16 Jun 2025","2025","","","6","10","Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools.","","979-8-3315-1491-4","10.1109/Designing66910.2025.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029550","Large Language Model;Software Architecture;VIPER Architecture;Front-end;Bloom's Taxonomy","Training;Software design;Costs;Software architecture;Large language models;Conferences;Taxonomy;Computer architecture;Software;Software development management","","","","23","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Cheap-Fake Detection with LLM Using Prompt Engineering","G. Wu; W. Wu; X. Liu; K. Xu; T. Wan; W. Wang","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Shanghai Jiao Tong University, Shanghai, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; University of Electronic Science and Technology of China, Chengdu, China",2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW),"28 Aug 2023","2023","","","105","109","The misuse of real photographs with conflicting image captions in news items is an example of the out-of-context (OOC) misuse of media. In order to detect OOC media, individuals must determine the accuracy of the statement and evaluate whether the triplet (i.e., the image and two captions) relates to the same event. This paper presents a novel learnable approach for detecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The proposed method is based on the COSMOS structure, which assesses the coherence between an image and captions, as well as between two captions. We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor. Specifically, we propose an innovative approach to feature extraction utilizing prompt engineering to develop a robust and reliable feature extractor with GPT3.5 model. The proposed method captures the correlation between two captions and effectively integrates this module into the COSMOS baseline model, which allows for a deeper understanding of the relationship between captions. By incorporating this module, we demonstrate the potential for significant improvements in cheap-fakes detection performance. The proposed methodology holds promising implications for various applications such as natural language processing, image captioning, and text-to-image synthesis. Docker for submission is available at https://hub.docker.com/repository/docker/mulns/acmmmcheapfakes.","","979-8-3503-1315-4","10.1109/ICMEW59549.2023.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221967","Large Language Models;Prompt Engineering;Cheap-fakes","Correlation;Conferences;Semantics;Coherence;Media;Feature extraction;Reliability engineering","","13","","11","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Prompt Engineering in Medical Image Segmentation: An Overview of the Paradigm Shift","H. Ali; M. F. Bulbul; Z. Shah","College of Science and Engineering, Hamad Bin Khalifa University Qatar Foundation, Doha, Qatar; Department of Computer Science and Engineering, Pohang University of Science and Technology (POSTECH), Pohang, Korea; College of Science and Engineering, Hamad Bin Khalifa University Qatar Foundation, Doha, Qatar","2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)","30 Oct 2023","2023","","","1","4","Foundation AI models have emerged as powerful pre-trained models on a large scale, capable of seamlessly handling diverse tasks across multiple domains with minimal or no fine-tuning. These models, exemplified by the impressive achievements of GPT-3 and BERT in natural language processing (NLP), as well as CLIP and DALL-E in computer vision, have garnered considerable attention for their exceptional performance. A noteworthy addition to the realm of image segmentation is the Segment Anything Model (SAM), a foundation AI model that revolutionizes image segmentation. With a single click or a natural language prompt, SAM exhibits the remarkable ability to segment any object within an image, marking a significant paradigm shift in medical image segmentation. Unlike conventional approaches that rely on labeled data and domain-specific knowledge, SAM breaks free from these constraints. Deep convolutional neural network (DCNN)-based, SAM comprises an image encoder, a prompt encoder, and a mask decoder, showcasing its efficient and flexible architecture. Medical image segmentation, in particular, benefits from SAM’s exceptional speed and high-quality segmentation. In this paper, we delve into the effectiveness of SAM for medical image segmentation shedding light on its capabilities. Moreover, our investigation explores the strengths and limitations of prompt engineering in medical computer vision applications, not only encompassing SAM but also other foundation AI models. Through this exploration, we unravel their immense potential in catalyzing a paradigm shift in the field of medical imaging.","","979-8-3503-2234-7","10.1109/AIBThings58340.2023.10292475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292475","Foundation AI;Prompt Engineering;Segmentation;Medical Imaging;Healthcare;Large Language Models;Medical Artificial Intelligence;Natural Language Processing","Image segmentation;Computer vision;Analytical models;Computational modeling;Natural language processing;Lesions;Artificial intelligence","","6","","13","IEEE","30 Oct 2023","","","IEEE","IEEE Conferences"
"GeoPet: Interactive Prompt Engineering for Enhancing Tool Calling of Large Language Models in Geospatial Tasks","X. Guo; C. Gao; W. Niu; X. Wei; C. Hua; J. Liu; M. Xu","School of Computer and Arifcial intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Arifcial intelligence, Zhengzhou University, Zhengzhou, China; School of Earth Science and Technology, Zhengzhou University, Zhengzhou, China; School of Computer and Arifcial intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Arifcial intelligence, Zhengzhou University, Zhengzhou, China; School of Earth Science and Technology, Zhengzhou University, Zhengzhou, China; School of Computer and Arifcial intelligence, Zhengzhou University, Zhengzhou, China",2025 IEEE 18th Pacific Visualization Conference (PacificVis),"16 Jun 2025","2025","","","47","57","Geospatial tasks often require the coordination of various spatial algorithms and operations, which are usually performed through tool calling guided by natural language prompts. Crafting effective prompts is challenging due to the inherent complexity and ambiguity of natural language. In this paper, we present GeoPet, a visual analytics system designed to simplify the process of prompt engineering to improve the performance of geospatial tool callings on large language models (LLMs). At its core, GeoPet is a sophisticated tool recommendation method that accepts geospatial tasks as input, decomposes them into atomic tasks, identifies relevant tools, and extracts the most relevant tool descriptions for these atomic tasks. The system is designed to support interactive prompt engineering of geospatial tool invocations, enabling users to explore the connections between geospatial tasks and established tools and to evaluate their performance. This enables crafting and refining prompts that coordinate LLMs with human expertise. GeoPet's satisfaction, practicality, usability, and visual design are validated through two case studies and a user study. These demonstrate that the system significantly eases the burden of rapid engineering and skillfully guides LLMs in geospatial tool calling capabilities. By providing a visual and interactive system for prompt engineering, GeoPet helps users navigate complex geospatial tasks and improves the overall efficiency and accuracy of tool calling for LLMs.","2165-8773","979-8-3315-0581-3","10.1109/PacificVis64226.2025.00011","National Natural Science Foundation of China; Zhejiang University; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11021077","Prompt engineering;tool calling;geospatial task;large language model","Navigation;Large language models;Visual analytics;Interactive systems;Natural languages;Refining;Geospatial analysis;Complexity theory;Prompt engineering;Usability","","","","69","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"WIP: Focusing on Programmer Literacy in the Time of AI-Aided Code Generation","J. A. Rosiene; C. P. Rosiene","Department of Computer Science, Eastern Connecticut State University, Windham, CT, USA; Department of Computing Sciences, University of Hartford, W. Hartford, CT, USA",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","4","This work-in-progress, innovative-practice, full paper describes an approach to the training of computer scientists and engineers focused on the reading of programs generated by AI-based Large Language Models (LLM). In contrast to the practice of Literate Programming, in which the coder crafts a solution which reads as English, the proposed approach treats an existing programming as a “shorthand”, which, when articulated, is explanatory. LLMs amount to enhanced search, having digested numerous examples with attention to context in order to provide a most “desired” result for a given prompt. How well the LLM generates a solution for a particular language varies substantially. The paper gives numerous LLM-generated code examples of how the proper reading of programming statements conveys their function and requires little or no commentary. The Sieve of Eratosthenes will be used to contrast the LLM-generated code against code which adheres to the language's idiom.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893329","Programmer literacy;code generation;Large Language Models","Training;Codes;Large language models;Focusing;Weaving;Programming profession","","","","8","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"SolSearch: An LLM-Driven Framework for Efficient SAT-Solving Code Generation","J. Sheng; Y. Lin; J. Wu; Y. Huang; J. Shi; M. Zhang; X. Wang","School of Computer Science and Technology, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China; School of Computer Science and Technology, Shanghai Formal-Tech Information Technology Co., Lt, East China Normal University, Shanghai, China",2025 IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),"10 Jun 2025","2025","","","6","10","The Satisfiability (SAT) problem is a core challenge with significant applications in software engineering, including automated testing, configuration management, and program verification. This paper presents SolSearch, a novel framework that harnesses large language models (LLMs) to discover and optimize SAT-solving strategies automatically. Leveraging a curriculum-based, trial-and-error process, SolSearch enables the LLM to iteratively modify and generate SAT solver code, thereby improving solving efficiency and performance. This automated SAT-solving paradigm has the advantage of being plug-and-play, allowing integration with any SAT solver and accelerating the development or design process of new SAT solvers (new methods). Our preliminary experimental results are encouraging by demonstrating that the LLM-powered paradigm improves state-of-the-art SAT solvers on general SAT benchmarks and significantly enhances the performance of the widely used Z3 solver (11% on PAR-2 score). These results highlight the potential for using LLM-driven methods to advance solver adaptability and effectiveness in real-world software engineering challenges. Future research directions are discussed to further refine and validate this approach, offering a promising avenue for integrating AI with traditional software engineering tasks.","2832-7632","979-8-3315-3711-1","10.1109/ICSE-NIER66352.2025.00007","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023952","Large Language Models (LLM);SAT Solver;Code Generation;Heuristic Method","Codes;Large language models;Configuration management;Benchmark testing;Software engineering","","","","29","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Automated Refactoring of Non-Idiomatic Python Code: A Differentiated Replication with LLMS","A. Midolo; M. Di Penta","Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","01","11","In the Python ecosystem, the adoption of idiomatic constructs has been fostered because of their expressiveness, increasing productivity and even efficiency, despite controversial arguments concerning familiarity or understandability issues. Recent research contributions have proposed approaches-based on static code analysis and transformation-to automatically identify and enact refactoring opportunities of non-idiomatic code into idiomatic ones. Given the potential recently offered by Large Language Models (LLMs) for code-related tasks, in this paper, we present the results of a replication study in which we investigate GPT-4 effectiveness in recommending and suggesting idiomatic refactoring actions. Our results reveal that GPT-4 not only identifies idiomatic constructs effectively but frequently exceeds the benchmark in proposing refactoring actions where the existing baseline failed. A manual analysis of a random sample shows the correctness of the obtained recommendations. Our findings underscore the potential of LLMs to achieve tasks where, in the past, implementing recommenders based on complex code analyses was required.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025897","Pythonic constructs;Automated Refactoring;Large Language Models","Productivity;Codes;Large language models;Ecosystems;Manuals;Benchmark testing;Python","","","","40","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"GOBEAT: Towards a Methodology to Support MAS Test Case Definition","R. Andrade; A. P. F. M. Mascarenhas; M. A. Simões","dept. Ciencias exatas e da Terra, Universidade do Estado da Bahia (UNEB), Salvador, Brazil; dept. Ciencias exatas e da Terra, Universidade do Estado da Bahia (UNEB), Salvador, Brazil; dept. Ciencias exatas e da Terra, Universidade do Estado da Bahia (UNEB), Salvador, Brazil","2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics (SBR), and 2023 Workshop on Robotics in Education (WRE)","5 Dec 2023","2023","","","218","223","Multi-Agent Systems (MAS) is a branch of Artificial Intelligence (AI) that works with distributed systems whose components are autonomous entities called agents. The soccer game has been used as a test bed to stimulate research in the MAS. A soccer team, i.e., a MAS, is composed of a group of players, i.e., agents, who should coordinate their actions towards a goal. The testing process in MASs has proven to be challenging. Autonomous agents are programmed to learn during their execution. So, running the same test scenario successively can lead to different results. It makes difficult the application of conventional software testing techniques. This paper proposes the Goal Behavioral Agent Testing (GoBeAT) methodology to assist in specifying the MAS test suite applied to the robot soccer domain. GoBeAT was tested in a case study by a robot soccer team of the Robotic World Cup (RoboCup) and showed positive results in the definition of a test suit.","2643-685X","979-8-3503-1538-7","10.1109/LARS/SBR/WRE59448.2023.10332953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10332953","MAS;Software Testing;Robot Soccer","Software testing;Productivity;Three-dimensional displays;Systematics;Robot kinematics;Behavioral sciences;Planning","","","","12","IEEE","5 Dec 2023","","","IEEE","IEEE Conferences"
"Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations","H. Abu-Rasheed; C. Weber; M. Fathi","Knowledge-based Systems and Knowledge Management Inst., University of Siegen, Siegen, Germany; Knowledge-based Systems and Knowledge Management Inst., University of Siegen, Siegen, Germany; Knowledge-based Systems and Knowledge Management Inst., University of Siegen, Siegen, Germany",2024 IEEE Global Engineering Education Conference (EDUCON),"8 Jul 2024","2024","","","1","5","In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.","2165-9567","979-8-3503-9402-3","10.1109/EDUCON60312.2024.10578654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578654","Large language models (LLMs);Knowledge graphs;ChatGPT;Generative AI (GenAI) Learning recommendations;explainable AI (XAI)","Shape;Large language models;Semantics;Knowledge graphs;Prompt engineering;Data mining;Engineering education","","14","","19","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"A Novel LLM enabled Code Snippet Generation Framework","S. Sarkar; S. P. Kushwaha; V. Sharma; N. Mishra; A. Alkhayyat","Kalinga Institute of Industrial Technology, Deemed to be University, Bhuabneswar, Odisha, India; Kalinga Institute of Industrial Technology, Deemed to be University, Bhuabneswar, Odisha, India; Computer Science Department, CHRIST University, Bengaluru, India; VIT Bhopal University, Sehore, Madhya Pradesh, India; College of Technical Engineering, The Islamic University, Najaf, Iraq",2024 International Conference on Intelligent & Innovative Practices in Engineering & Management (IIPEM),"21 Mar 2025","2024","","","1","5","Large Language Models (LLMs) represent a breakthrough in natural language processing (NLP), leveraging deep learning techniques to achieve exceptional proficiency in code generation, analysis and modification of human languages. These models, characterized by their vast scale and parameter count, like Bidirectional Encoder Representations from Transformers (BERT by Google) and the Generative Pre-trained Transformer series (by OpenAI’s GPT), have revolutionized various applications including text generation, translation, summarization, and question answering. In our paper we investigate the practicality,complications, and significance of using LLMs for code generation. We provide a review analysis of existing LLM models in use and compare their proficiency for code generation. This paper examines the underlying mechanisms of LLMs, specially their ability to grasp the code syntax, semantics, and programming logic from large-scale repositories and their documentations. The models’ training techniques include fine-tuning programming-specific datasets and enhancing the models' competency to generate code snippets that are syntactically correct and contextually relevant.","","979-8-3503-9004-9","10.1109/IIPEM62726.2024.10925748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925748","Large Language Models (LLMs);Code Generation;Natural Language Processing (NLP);Code Quality;Software Development","Training;Codes;Translation;Reviews;Semantics;Syntactics;Transformers;Throughput;Software development management;Context modeling","","","","19","IEEE","21 Mar 2025","","","IEEE","IEEE Conferences"
"An Evaluation Method for Large Language Models’ Code Generation Capability","H. Su; J. Ai; D. Yu; H. Zhang","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China",2023 10th International Conference on Dependable Systems and Their Applications (DSA),"14 Nov 2023","2023","","","831","838","Large language models are becoming increasingly popular in various professional fields. One of their applications is providing code suggestions. However, the differences in code generation capabilities of different large language models and the problems they may make in giving code suggestions have not been well studied. This paper proposes a method for evaluating the code generation capabilities of large language models and applies it to several commonly used models, including ChatGPT, Claude, Spark, and Bing AI. Through experimental evaluation and data analysis, we find that search-based large language models, such as Bing AI, exhibit stronger code generation capabilities than pre-trained models, such as ChatGPT, Claude, and Spark. We also find that the current large language models possess strong natural language understanding abilities, and errors in code suggestions are more likely to be due to code problems rather than understanding problems.","2767-6684","979-8-3503-0477-0","10.1109/DSA59317.2023.00118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10314291","Evaluation Method;Large Language Model;Code Generation","Analytical models;Codes;Data analysis;Semantics;Syntactics;Chatbots;Data models","","3","","21","IEEE","14 Nov 2023","","","IEEE","IEEE Conferences"
"Question and Answer Model using LangChain Framework and Generative AI","S. K. Kakkerla; K. Yalla; P. Pathipaka; Y. Irukulla; S. Panthangi; R. Adupa","School of CS & AI, SR University, Warangal, Telangana, India; School of Computer Science & Artificial Intelligence, SR University, Warangal, Telangana, India; School of Computer Science & Artificial Intelligence, SR University, Warangal, Telangana, India; School of Computer Science & Artificial Intelligence, SR University, Warangal, Telangana, India; School of Computer Science & Artificial Intelligence, SR University, Warangal, Telangana, India; School of Computer Science & Artificial Intelligence, SR University, Warangal, Telangana, India",2024 Intelligent Systems and Machine Learning Conference (ISML),"23 May 2025","2024","","","200","205","The newest technology trend in the market is being led by generative AI. A number of businesses created tools and frameworks to enable developers to create creative applications using these LLMs like OpenAI recently lead the big language model wave. In a similar vein, we sought to create a program called ‘Question and Answer model using Generative AI and LangChain Framework’. A number of LLMs on the market can be integrated with LangChain, as well as other data sources. With the use of large language models (LLMs), developers can now create apps that they were before unable to. To build a genuinely effective app, leveraging these LLMs alone is frequently insufficient; you must mix them with additional sources of computation or knowledge. Artificial intelligence that can produce text, graphics, or other stuff that resembles what a person would produce is known as ""generative AI"". It analyses data patterns using deep learning techniques, such as neural networks, and then creates new content based on those patterns. Text generating, visual synthesis, and even music composition are some of the uses for generative AI.","","979-8-3503-4387-8","10.1109/ISML60050.2024.11007304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007304","Machine learning;Generative AI;LangChain;Semantic search;prompt Engineering","Training;Adaptation models;Visualization;Accuracy;Generative AI;Semantic search;Soft sensors;Information retrieval;Data models;Time factors","","","","12","IEEE","23 May 2025","","","IEEE","IEEE Conferences"
"A Taxonomy of Failures in Tool-Augmented LLMs","C. Winston; R. Just","University of Washington, Seattle, USA; University of Washington, Seattle, USA",2025 IEEE/ACM International Conference on Automation of Software Test (AST),"21 Jul 2025","2025","","","125","135","Large language models (LLMs) can perform a variety of tasks given a user prompt that contains a description of the task. To enhance the performance of LLMs, recent research has focused on augmenting LLMs with external tools, such as Python APIs, REST APIs, and other deep learning models. Much of the research on tool-augmented LLMs (TaLLMs) has focused on improving their capabilities and accuracy. However, research on understanding and characterizing the kinds of failures that can occur in these systems is lacking. To address this gap, this paper proposes a taxonomy of failures in TaLLMs and their root causes, details an analysis of the failures that occur in two published TaLLMs (Gorilla and Chameleon), and provides recommendations for testing and repair of TaLLMs.","2833-9061","979-8-3315-0179-2","10.1109/AST66626.2025.00019","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081716","Large language models;tool-augmented LLMs;software testing;fault localization;repair","Software testing;System testing;Large language models;Taxonomy;Social sciences;Reinforcement learning;Maintenance engineering;Robustness;Software;Testing","","","","31","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Heuristic Analysis for Security, Privacy and Bias of Text Generative AI: GhatGPT-3.5 case as of June 2023","T. Mitsunaga","INIAD, Toyo University, The Tokyo Foundation for Policy Research, Tokyo, Japan",2023 IEEE International Conference on Computing (ICOCO),"24 Jan 2024","2023","","","301","305","With the rapid advancement of technology and the expansion of available data, AI has permeated many aspects of people's lives. Large Language Models(LLMs) such as ChatGPT are increasing the accuracy of their response and achieving a high level of communication with humans. These AIs can be used in business to benefit, for example, customer support and documentation tasks, allowing companies to respond to customer inquiries efficiently and consistently. In addition, AI can generate digital content, including texts, images, and a wide range of digital materials based on the training data, and is expected to be used in business. However, the widespread use of AI also raises ethical concerns. The potential for unintentional bias, discrimination, and privacy and security implications must be carefully considered. Therefore, While AI can improve our lives, it has the potential to exacerbate social inequalities and injustices. This paper aims to explore the unintended outputs of AI and assess their impact on society. Developers and users can take appropriate precautions by identifying the potential for unintended output. Such experiments are essential to efforts to minimize the potential negative social impacts of AI transparency, accountability, and use. We will also discuss social and ethical aspects with the aim of finding sustainable solutions regarding AI.","","979-8-3503-0268-4","10.1109/ICOCO59262.2023.10397858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397858","ChatGPT;Generative AI;Privacy;Bias;Security","Privacy;Generative AI;Training data;Security;Artificial intelligence;Task analysis;Business","","4","","11","IEEE","24 Jan 2024","","","IEEE","IEEE Conferences"
"Optimizing Large Language Models for Auto-Generation of Programming Quizzes","Y. Kumar; A. Manikandan; J. J. Li; P. Morreale",Kean University; Kean University; Kean University; Kean University,2024 IEEE Integrated STEM Education Conference (ISEC),"17 Sep 2024","2024","","","1","5","This study analyzes the use of Large Language Models (LLMs) like ChatGPT in creating quizzes for Java programming courses, specifically Object-Oriented Programming (CS1) and Data Structures (CS2). It aims to evaluate the accuracy of LLM-generated assessments, understand the benefits and drawbacks of using LLMs in CS education from educators' viewpoints, and identify effective prompt engineering strategies to enhance the quality of educational materials. The research compares quizzes made by LLMs against human-created content to assess their consistency with Java programming principles, alignment with CS1 and CS2 learning goals, and their impact on student engagement and comprehension, providing insights into LLMs' effectiveness in academic assessment creation for computer science education.","2473-7623","979-8-3503-5280-1","10.1109/ISEC61299.2024.10665141","NSF(grant numbers:1834620,2137791); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10665141","Java programming instruction;AI-Supplemental Instructor (AI-SI);use of LLMs in CS education","Java;Accuracy;Large language models;Education;Data structures;Chatbots;Computer science education","","","","22","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Code LLMs: A Taxonomy-based Survey","N. Raihan; C. Newman; M. Zampieri","George Mason University, Fairfax, VA, USA; Rochester Institute of Technology, Rochester, NY, USA; George Mason University, Fairfax, VA, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5402","5411","Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks and have recently expanded their impact to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). This taxonomy-based survey provides a comprehensive analysis of LLMs in the NL-PL domain, investigating how these models are utilized in coding tasks and examining their methodologies, architectures, and training processes. We propose a taxonomy-based framework that categorizes relevant concepts, providing a unified classification system to facilitate a deeper understanding of this rapidly evolving field. This survey offers insights into the current state and future directions of LLMs in coding tasks, including their applications and limitations.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10826108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10826108","Large Language Models (LLMs);Code-LLMS;Code Generation;Corpora;Benchmark","Surveys;Training;Analytical models;Codes;Reviews;Large language models;Taxonomy;Natural languages;Benchmark testing;Encoding","","","","107","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models to Detect NPM Malicious Packages","N. Zahan; P. Burckhardt; M. Lysenko; F. Aboukhadijeh; L. Williams","North Carolina State University; Socket, Inc; Socket, Inc; Socket, Inc; North Carolina State University",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2625","2637","Existing malicious code detection techniques demand the integration of multiple tools to detect different malware patterns, often suffering from high misclassification rates. Therefore, malicious code detection techniques could be enhanced by adopting advanced, more automated approaches to achieve high accuracy and a low misclassification rate. The goal of this study is to aid security analysts in detecting malicious packages by empirically studying the effectiveness of Large Language Models (LLMs) in detecting malicious code. We present SocketAI, a malicious code review workflow to detect malicious code. To evaluate the effectiveness SocketAI, we leverage a benchmark dataset of $5,115 \text{npm}$ packages, of which 2,180 packages have malicious code. We conducted a baseline comparison of GPT3 and GPT-4 models with the state-of-the-art CodeQL static analysis tool, using 39 custom CodeQL rules developed in prior research to detect malicious Javascript code. We also compare the effectiveness of static analysis as a pre-screener with SocketAI workflow, measuring the number of files that need to be analyzed and the associated costs. Additionally, we performed a qualitative study to understand the types of malicious packages detected or missed by our workflow. Our baseline comparison demonstrates a 16 % and 9 % improvement over static analysis in precision and F1 scores, respectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1 scores, while GPT-3 offers a more cost-effective balance at 91 % precision and 94 % F1 scores. Prescreening files with a static analyzer reduces the number of files requiring LLM analysis by $\mathbf{7 7. 9 \%}$ and decreases costs by $\mathbf{6 0. 9 \%}$ for GPT-3 and $\mathbf{7 6. 1 \%}$ for GPT-4. Our qualitative analysis identified data theft, execution of arbitrary code, and suspicious domain categories as the top detected malicious packages.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00146","National Science Foundation(grant numbers:2207008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029752","malicious code detection;large language models;npm packages;malicious code-review workflow;software supply chain security","Costs;Codes;Accuracy;Reviews;Large language models;Static analysis;Benchmark testing;Malware;Security;Software engineering","","","","59","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"A Security Risk Taxonomy for Prompt-Based Interaction With Large Language Models","E. Derner; K. Batistič; J. Zahálka; R. Babuška","ELLIS Alicante, Alicante, Spain; Independent Researcher, Ljubljana, Slovenia; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic",IEEE Access,"17 Sep 2024","2024","12","","126176","126187","As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by specifically focusing on security risks posed by LLMs within the prompt-based interaction scheme, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline and categorizes the attacks by target and attack type alongside the commonly used confidentiality, integrity, and availability (CIA) triad. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.","2169-3536","","10.1109/ACCESS.2024.3450388","nominal grant received at the ELLIS Unit Alicante Foundation from the Regional Government of Valencia in Spain (Convenio Singular signed with Generalitat Valenciana, Conselleria de Innovación, Industria, Comercio y Turismo, Dirección General de Innovación); Intel Corporation; European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:951847); European Union’s Horizon Europe Research and Innovation Programme(grant numbers:101070254 CORESENSE); European Union through the project ROBOPROX(grant numbers:CZ.02.01.01/00/22_008/0004590); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10648691","Large language models;security;jailbreak;natural language processing","Security;Taxonomy;Chatbots;Large language models;Data models;Codes;Privacy;Natural language processing;Risk analysis","","3","","74","CCBY","26 Aug 2024","","","IEEE","IEEE Journals"
"Improving Few-Shot Code Generation with Prompt Selection and Augmentation Techniques in Large Language Models","O. T. Wu; F. K. S. Chan; Y. N. Law","Department of Computer Science, The University of Hong Kong, Hong Kong; Hong Kong Industrial Artificial Intelligence & Robotics Centre, Hong Kong; Hong Kong Industrial Artificial Intelligence & Robotics Centre, Hong Kong",2024 5th International Conference on Computers and Artificial Intelligence Technology (CAIT),"17 Apr 2025","2024","","","635","641","Few-shot prompting and step-by-step reasoning have enhanced the capabilities of Large Language Models (LLMs) in tackling complex tasks including code generation. In this paper, we introduce a prompt selection and augmentation algorithm aimed at improving mathematical reasoning and robot arm operations. Our approach incorporates a multi-stage example augmentation scheme combined with an example selection scheme. This algorithm improves LLM performance by selecting a set of examples that increase diversity, minimize redundancy, and increase relevance to the question. When combined with the Program-of-Thought prompting, our algorithm demonstrates an improvement in performance on the GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively. Furthermore, in simulated tabletop environments, our algorithm surpasses the Code-as-Policies approach by achieving a 3.4% increase in successful task completions and a decrease of over 70% in the number of examples used. Its ability to discard examples that contribute little to solving the problem reduces the inferencing time of an LLM-powered robotics system. This algorithm also offers important benefits for industrial process automation by streamlining the development and deployment process, reducing manual programming effort, and enhancing code reusability.","","979-8-3315-3089-1","10.1109/CAIT64506.2024.10963264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10963264","Code generation;Prompt engineering;Large Language Model;Robotics","Codes;Automation;Service robots;Large language models;Redundancy;Process control;Manuals;Programming;Cognition;Prompt engineering","","","","24","IEEE","17 Apr 2025","","","IEEE","IEEE Conferences"
"Requirements are All You Need: From Requirements to Code with LLMs","B. Wei","Department of Computer Science, Texas Christian University, Fort Worth, USA",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","416","422","The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a “Progressive Prompting” method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628487","Requirements Engineering;Large Language Models (LLMs);ChatGPT;Code Generation;Use Cases;Software Specification;Automated Software Engineering","Knowledge engineering;Codes;Software design;Object oriented modeling;Refining;Software;Requirements engineering","","6","","8","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Improving Co-Decoding Based Security Hardening of Code LLMs Leveraging Knowledge Distillation","D. Li; S. Shu; M. Yan; Z. Liu; C. Liu; X. Zhang; D. Lo","School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Blockchain and Data Security, Hangzhou, Zhejiang, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Information Systems, Singapore Management University, Singapore",IEEE Transactions on Software Engineering,"","2025","PP","99","1","16","Large Language Models (LLMs) have been widely adopted by developers in software development. However, the massive pretraining code data is not rigorously filtered, allowing LLMs to learn unsafe coding patterns. Several prior studies have demonstrated that code LLMs tend to generate code with potential vulnerabilities. The widespread adoption of intelligent programming assistants poses a significant threat to the software development process. Existing approaches to mitigating this risk primarily involve constructing secure data that are free of vulnerabilities and then retraining or fine-tuning the models. However, such an effort is resource intensive and requires significant manual supervision. When the model parameters are too large (e.g., more than 1 billion) or multiple models with the same parameter scale have the same optimization needs (e.g., to avoid outputting vulnerable code), the above work will become unaffordable. To address this challenge, in previous work, we proposed CoSec, an approach to improve the security of code LLMs with different parameters by utilizing an independent and very small parametric security model as a decoding navigator. Despite CoSec’s excellent performance, we found that there is still room for improving: 1) its ability to maintain the functional correctness of hardened targets, and 2) the security of the generated code. To address the above issues, we propose CoSec+, a hardening framework consisting of three phases: 1) Functional Correctness Alignment, which improves the functional correctness of the security base with knowledge disstillation; 2) Security Training, which yields an independent, but much smaller security model; and 3) Co-decoding, where the security model iteratively reasons about the next token along with the target model. Due to the higher confidence that a well-trained security model places in secure and correct tokens, it guides the target base model to generate more secure code, even as it improves the functional correctness of the target base model. We have conducted extensive experiments in several code LLMs (i.e., CodeGen, StarCoderBase, DeepSeekCoder and Qwen2.5-Coder), and the results show that our approach is effective in improving the functional correctness and security of the models. The evaluation results show that CoSec+ can deliver a 0.8% to 37.7% improvement in security across models of various parameter sizes and families; moreover, it preserves the functional correctness of the target base models—achieving functional-correctness gains of 0.7% to 51.1% for most of those models.","1939-3520","","10.1109/TSE.2025.3591791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11106920","Large Language Models;Code Generation;Software Security;AI Safety","Security;Codes;Training;Software development management;Predictive models;Computational modeling;Maintenance;Pipelines;Optimization;Encoding","","","","","IEEE","1 Aug 2025","","","IEEE","IEEE Early Access Articles"
"Integrating Large Language Models (LLMs) and Vector Databases into Healthcare Operations","R. Patil; M. Abbidi; S. Boit; K. Sutrave; J. Du","College of Computing, Grand Valley State University, Allendale, MI, USA; College of Computing, Grand Valley State University, Allendale, MI, USA; College of Computing, Grand Valley State University, Allendale, MI, USA; College of Computing, Grand Valley State University, Allendale, MI, USA; College of Computing, Grand Valley State University, Allendale, MI, USA",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0608","0613","The medical field, especially in US, has transitioned from paper-based records to Electronic Health Records (EHRs). Switching to EHRs have offered health care providers several advantages over paper-based records, such as coordination, documentation, security, and communication with patients. But as EHRs were primarily designed for billing and documentation, they have led physicians to do more administrative tasks, instead of healthcare related tasks. Despite excellent adoption of EHRs, there have been very few studies addressing the side-effects related to EHR usage, such as - Physician burnout, poor user interface, complex usability, and time incurred to perform clerical tasks. In this study, we propose a prototype where the patients can interact with the LLM enabled system integrated with vector database. This system will allow patients to get initial observations and physician-like results to their queries that don't need immediate escalation to physicians. The experiments and results section demonstrates how the proposed study can help in reducing the clerical burden and cognitive overload of clinicians.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105274","EHRs;LLMs;Vector databases;Healthcare;Clinicians;Electronic Health Records;Large Language Models","","","","","22","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Automated API Docs Generator using Generative AI","P. Dhyani; S. Nautiyal; A. Negi; S. Dhyani; P. Chaudhary","Department of Computer Science and Engineering, Graphic Era Hill University, Dehradun, India; Department of Computer Science and Engineering, Graphic Era Hill University, Dehradun, India; Department of Computer Science and Engineering, Graphic Era Hill University, Dehradun, India; Department of Computer Science and Engineering, Graphic Era Hill University, Dehradun, India; Department of Computer Science and Engineering, Graphic Era Hill University, Dehradun, India","2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)","2 Apr 2024","2024","","","1","6","Our study provides an improvement on the creation of Application Programming Interfaces (APIs) usage documentation using the efficiency and power of Generative AI. APIs play an important role in software integration and software maintenance but the process of API documentation creation has been traditional and did not evolve with time, this paper employs Generative AI to enhance the accuracy, speed, and scale of API documentation generation. The automated API documentation generator is created using natural language processing applied through a large language model (TinyPixel/Llama-2-7B-bf16-sharded model). Training data was created by applying web scraping on various large tech companies' documentation web pages to get a good quality and industry-standard documentation dataset. It was further diversified and increased using the GPT model to handle a wide range of API scenarios. The fine-tuning greatly enhanced the TinyPixel/Llama-2-7B-bf16-sharded model's efficiency and quality of output which is proven by the reduced response time and the accuracy of documentation generated. Our study's comparative study confirms the effectiveness of the approach used. Our study's conclusion offers a comprehensive approach that should improve software development processes and pave the way for additional developments in API documentation.","2688-0288","979-8-3503-4846-0","10.1109/SCEECS61402.2024.10482119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482119","API Documentation;Generative AI;Fine-Tuning;Large Language Models;Web Scraping;Natural Language Processing","Industries;Generative AI;Web pages;Training data;Documentation;Generators;User experience","","7","","14","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Teaching Security in the Era of Generative AI: A Course Design of Security + ChatGPT","Y. Wang; M. McCoey; Q. Hu; M. Jalalitabar","Department of Math and Computer Science, La Salle University, PA; Department of Math and Computer Science, La Salle University, PA; Department of Math and Computer Science, La Salle University, PA; Department of Computer Science, California State University, Northridge, CA",2024 IEEE Integrated STEM Education Conference (ISEC),"17 Sep 2024","2024","","","01","06","The 2023 CS curriculum by ACM, IEEE, and AAAI identifies security as an independent knowledge area that develops the “security mindset” so that students are ready for the “continual changes” in computing. Likewise, the curriculum emphasises the coverage of “uses”, and “shortcomings/pitfalls” of practical AI-tools like ChatGPT. This paper presents our endeavors to approach those goals with the design of an Information Security course. Our course design bears the following distinct features: Certificate-readiness, where we align the knowledge areas with major security/ethical hacking certificates; Coverage of ChatGPT, where the uses of ChatGPT for assisting security tasks and security issues caused by ChatGPT usage are both addressed for the first time in the teaching; “Learn defending from attackers' perspective”, where labs of both offensive and defensive natures are developed to equally sharpen ethical hacking and hardening skills, and to facilitate the discussion on legal/ethical implications; Current and Representative, where ajust-enough set of representative and/or current security topics are selected in order and covered in respective modules in the most current form. In addition, we generalize our design principles and strategies, with the hope to shed lights on similar efforts in other institutions.","2473-7623","979-8-3503-5280-1","10.1109/ISEC61299.2024.10664905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664905","Security;Generative AI;ChatGPT","Ethics;Generative AI;Education;Information security;Chatbots;Security;Computer crime","","","","19","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Analyzing the impact of prompt engineering on efficiency, code quality, and security in CRUD application development","K. A. Ashen Shanuka; J. Wijayanayake; K. Vidanage","Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka; Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka; Department of Computer Science, Kotelawala Defence University, Sri Lanka",2025 5th International Conference on Advanced Research in Computing (ICARC),"16 Apr 2025","2025","","","1","6","This research evaluates the capabilities of Large Language Models (LLMs) in generating CRUD applications using Python Flask framework, focusing on code quality, security, and UI design. The study analyzes five prominent LLMs: Claude 3.5 Sonnet, Gemini, GitHub Copilot, GPT-4, and Perplexity, through automated static code review tools including Code Factor, Codacy, and Code Scene. The evaluation reveals consistently high performance across models, with GitHub Copilot and Gemini achieving superior code health metrics (9.5-10.0) and 100% green code ratings. While all models maintained Grade A ratings in Code Factor, common security vulnerabilities, particularly in Flask debug mode configuration, persisted across implementations. UI evaluation through professional interviews indicated that while generated interfaces were functionally complete, they lagged behind human-designed UIs in aesthetic appeal. The research demonstrates that LLMs can automate 60-90% of CRUD development tasks, though human intervention remains essential for environment setup and security configuration. The findings suggest a shift toward human-AI hybrid development approaches, where entry-level developers can effectively manage LLM-generated code while focusing on strategic implementation decisions.","","979-8-3315-3098-3","10.1109/ICARC64760.2025.10963005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10963005","crud operations;large language models;programming;prompt engineering;code quality","Measurement;Codes;Large language models;Focusing;Collaboration;User experience;Security;Prompt engineering;Software development management;Testing","","","","26","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"A Multi-Agent LLM Environment for Software Design and Refactoring: A Conceptual Framework","V. Rajendran; D. Besiahgari; S. C. Patil; M. Chandrashekaraiah; V. Challagulla","Amazon, Seattle, USA; University of Cincinnati, Cincinnati, USA; Independent Researcher, West Chester, USA; ALAB, San Jose, USA; Egen.ai, Naperville, USA",SoutheastCon 2025,"25 Apr 2025","2025","","","488","493","Modern software systems demand continuous evolution to maintain performance, scalability, and security. Traditional single-agent AI-driven code refactoring approaches are often limited in addressing the multi-faceted constraints (e.g., performance, security, maintainability) that emerge during complex software design tasks. In this paper, we propose a novel Multi-Agent Large Language Model (LLM) Environment for automated software design and refactoring. Our conceptual framework comprises specialized LLM “experts,” each trained or fine-tuned on a different aspect of software engineering (performance optimization, security hardening, UI/UX, maintainability). These agents collaborate in a cooperative or competitive fashion-using coordination protocols akin to consensus or auction mechanisms-to synthesize design insights and refactoring recommendations. We present formal definitions of agent interactions (including mathematical notation for termination conditions), a sequence diagram demonstrating agent collaboration, a complexity analysis of the coordination mechanism, and an expanded reference list. Preliminary experimental design is outlined to demonstrate how multi-agent interactions may resolve conflicting design goals more effectively than a single-agent approach. Our aim is to provide a roadmap for integrating multi-agent LLMs into the software development lifecycle, thereby improving development efficiency, reducing technical debt, and enhancing software quality.","1558-058X","979-8-3315-0484-7","10.1109/SoutheastCon56624.2025.10971563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971563","Multi-agent systems;Large Language Models;Software refactoring;Agent specialization;Consensus protocols;Auction mechanisms;Code quality","Software design;Codes;Large language models;Scalability;Software quality;Software systems;Security;Optimization;Software engineering;Software development management","","","","16","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Reimagining Self-Adaptation in the Age of Large Language Models","R. Donakanti; P. Jain; S. Kulkarni; K. Vaidhyanathan","Software Engineering Research Center, IIIT Hyderabad, India; Software Engineering Research Center, IIIT Hyderabad, India; Software Engineering Research Center, IIIT Hyderabad, India; Software Engineering Research Center, IIIT Hyderabad, India",2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C),"21 Aug 2024","2024","","","171","174","Modern software systems are subjected to various types of uncertainties arising from context, environment, etc. To this end, self-adaptation techniques have been sought out as potential solutions. Although recent advances in self-adaptation through the use of ML techniques have demonstrated promising results, the capabilities are limited by constraints imposed by the ML techniques, such as the need for training samples, the ability to generalize, etc. Recent advancements in Generative AI (GenAI) open up new possibilities as it is trained on massive amounts of data, potentially enabling the interpretation of uncertainties and synthesis of adaptation strategies. In this context, this paper presents a vision for using GenAI, particularly Large Language Models (LLMs), to enhance the effectiveness and efficiency of architectural adaptation. Drawing parallels with human operators, we propose that LLMs can autonomously gen-erate similar, context-sensitive adaptation strategies through its advanced natural language processing capabilities. This method allows software systems to understand their operational state and implement adaptations that align with their architectural requirements and environmental changes. By integrating LLMs into the self-adaptive system architecture, we facilitate nuanced decision-making that mirrors human-like adaptive reasoning. A case study with the SWIM exemplar system provides promising results, indicating that LLMs can potentially handle different adaptation scenarios. Our findings suggest that GenAI has significant potential to improve software systems' dynamic adaptability and resilience.","2768-4288","979-8-3503-6625-9","10.1109/ICSA-C63560.2024.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628416","Self-Adaptation;Software Architecture;Gener-ative AI;LLM;Software Engineering","Training;Uncertainty;Software architecture;Large language models;Decision making;Systems architecture;Software systems","","1","","12","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"OpenAssert: Towards Secure Assertion Generation using Large Language Models","A. Menon; S. S. Miftah; A. Srivastava; S. Kundu; S. Kundu; A. Raha; S. Banerjee; D. Mathaikutty; K. Basu","University of Texas at Dallas, TX, USA; University of Texas at Dallas, TX, USA; University of Texas at Dallas, TX, USA; Intel Labs, USA; Intel Labs, USA; Intel Labs, USA; Intel Labs, USA; Intel Labs, USA; University of Texas at Dallas, TX, USA",2025 IEEE 43rd VLSI Test Symposium (VTS),"10 Jun 2025","2025","","","1","5","Assertions are critical components used in hardware verification, ensuring robust functionality, fortifying design security, and providing essential verification features. Traditional hardware assertion methods are not automated, complicate security audits, and require effort, causing prolonged development cycles. Recent studies have highlighted the potential of commercial Large Language Models (LLMs) to generate security-focused assertions by leveraging textual data from design specifications. However, reliance on proprietary models like GPT-4 severely jeopardizes IP privacy and data confidentiality, undermining transparency and accountability in data handling practices. In this paper, we address secure hardware assertion generation by proposing a practical approach to significantly enhance the feasibility of open-source LLMs. Our proposed method, OpenAssert, involves fine-tuning existing models to be utilized locally at the user’s end without compromising confidentiality. Additionally, we employ Retrieval Augmentation Generation to refine these models, mitigating hallucinations and security-related errors. OpenAssert demonstrates improvements, achieving up to a 44% increase in rouge-1 score, a 49% improvement in cosine similarity, and a 43.4% reduction in word error rate for security-critical designs compared to open-source models.","2375-1053","979-8-3315-2144-8","10.1109/VTS65138.2025.11022798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022798","LLMs;Assertion;Verification","Data privacy;Error analysis;Databases;Large language models;Very large scale integration;Hardware;Data models;Security;IP networks;Context modeling","","","","15","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Comprehensive overview of multi-agent systems for controlling smart grids","O. P. Mahela; M. Khosravy; N. Gupta; B. Khan; H. H. Alhelou; R. Mahla; N. Patel; P. Siano","Power System Planning Division, Rajasthan Rajya Vidyut Prasaran Nigam Ltd., Jaipur, India; Media Integrated Communication Laboratory, Osaka University, Osaka, Japan; Computer Science and Engineering Department, Oakland University, Rochester, NY, USA; Department of Electrical and Computer Engineering, Hawassa University, Hawassa, Ethiopia; Faculty of Mechanical and Electrical Engineering, Tishreen University, Latakia, Syria; Department of Electrical Engineering, National Institute of Technology, Kurukshetra, Kurukshetra, India; Computer Science and Engineering Department, Oakland University, Rochester, NY, USA; Department of Management & Innovation Systems, University of Salerno, Salerno, Italy",CSEE Journal of Power and Energy Systems,"24 Jan 2022","2022","8","1","115","131","Agents are intelligent entities that act flexibly and autonomously and make wise decisions based on their intelligence and experience. A multi-agent system (MAS) contains multiple, intelligent, and interconnected collaborating agents for solving a problem beyond the ability of a single agent. A smart grid (SG) combines advanced intelligent systems, control techniques, and sensing methods with an existing utility power network. For controlling smart grids, various control systems with different architectures have already been developed. MAS-based control of power system operations has been shown to overcome the limitations of time required for analysis, relaying, and protection; transmission switching; communication protocols; and management of plant control. These systems provide an alternative for fast and accurate power network control. This paper provides a comprehensive overview of MASs used for the control of smart grids. The paper provides a wide-spectrum view of the status of smart grids, MAS-based control techniques and their implementation for the control of smart grids. Use of MASs in the control of various aspects of smart grids—including the management of energy, marketing energy, pricing, scheduling energy, reliability, network security, fault handling capability, communication between agents, SG-electrical vehicles, SG-building energy systems, and soft grids—have been critically reviewed. More than a hundred publications on the topic of MAS-based control of smart grids have been critically examined, classified, and arranged for fast reference.","2096-0042","","10.17775/CSEEJPES.2020.03390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9299490","Coordinated control;multi-agent systems;renewable energy sources;smart energy infrastructure;smart grid","Smart grids;Standards;Control systems;Protocols;Power system dynamics;Reliability;Economics","","38","","","","21 Dec 2020","","","CSEE","CSEE Journals"
"Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code","S. Honarvar; M. van der Wilk; A. F. Donaldson","Department of Computing, Imperial College London, London, UK; Department of Computer Science, University of Oxford, Oxford, UK; Department of Computing, Imperial College London, London, UK","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","80","91","We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language question templates, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated test oracle that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a neighbourhood of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including anomalies where the LLM correctly solves almost all questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Cohere and Meta, each at two temperature configurations. Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond merely highlighting that LLMs sometimes produce wrong code (which is no surprise): by systematically identifying cases where LLMs are able to solve some problems in a neighbourhood but do not manage to generalise to solve the whole neighbourhood, our method is effective at highlighting robustness issues. We present data and examples that shed light on the kinds of mistakes that LLMs make when they return incorrect code results.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989005","Large language models;correctness;robustness;AI evaluation;code generation","Software testing;Codes;Large language models;Natural languages;Programming;Benchmark testing;Robustness;Cognition","","","","71","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Applications of LLMs for Generating Cyber Security Exercise Scenarios","M. Mudassar Yamin; E. Hashmi; M. Ullah; B. Katt","Department of Information Security and Communication Technology (IIK), Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Department of Information Security and Communication Technology (IIK), Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Department of Computer Science (IDI), Norwegian University of Science and Technology (NTNU), Gjøvik, Norway; Department of Information Security and Communication Technology (IIK), Norwegian University of Science and Technology (NTNU), Gjøvik, Norway",IEEE Access,"8 Oct 2024","2024","12","","143806","143822","This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing’s seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‘hallucination’ inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‘CyExec,’ a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing’s exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.","2169-3536","","10.1109/ACCESS.2024.3468914","Norwegian Research Council through the ASCERT Project 3290; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695083","Cyber security exercise scenarios;large language models;bounded rationality;generative configurations;Halluciation in LLMs","Training;Computer security;Computer crime;Security;Manuals;Transformers;Testing;Organizations;Ethics;Data models","","14","","55","CCBY","26 Sep 2024","","","IEEE","IEEE Journals"
"Green-Code: Learning to Optimize Energy Efficiency in Llm-Based Code Generation","S. Ilager; L. F. Briem; I. Brandic","University of Amsterdam, The Netherlands; TU Wien, Austria; TU Wien, Austria","2025 IEEE 25th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","30 Jun 2025","2025","","","559","569","Large Language Models (LLMs) are becoming integral to daily life, showcasing their vast potential across various Natural Language Processing (NLP) tasks. Beyond NLP, LLMs are increasingly used in software development tasks, such as code completion, modification, bug fixing, and code translation. Software engineers widely use tools like GitHub Copilot and Amazon Q, streamlining workflows and automating tasks with high accuracy. While the resource and energy intensity of LLM training is often highlighted, inference can be even more resourceintensive over time, as it's a continuous process with a high number of invocations. Therefore, developing resource-efficient alternatives for LLM inference is crucial for sustainability. This work proposes GREEN-CODE, a framework for energy-aware code generation in LLMs. GREEN-CODE performs dynamic early exit during LLM inference. We train a Reinforcement Learning (RL) agent that learns to balance the trade-offs between accuracy, latency, and energy consumption. Our approach is evaluated on two open-source LLMs, Llama 3.2 3B and OPT 2.7 B, using the JavaCorpus and PY150 datasets. Results show that our method reduces the energy consumption between 2350 % on average for code generation tasks without significantly affecting accuracy.","2993-2114","979-8-3315-0934-7","10.1109/CCGRID64434.2025.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11044793","Large Language Models (LLMs);Green AI;Sustainable AI;Code Generation;Pruning;Early Exits","Training;Energy consumption;Adaptation models;Codes;Accuracy;Translation;Computational modeling;Large language models;Natural language processing;Software development management","","","","38","IEEE","30 Jun 2025","","","IEEE","IEEE Conferences"
"Chain-of-Thoughts Prompting with Language Models for Accurate Math Problem-Solving","S. C. E. Fung; M. F. Wong; C. W. Tan","Diocesan Girls' School, Hong Kong, China; City University of Hong Kong, Hong Kong, China; Nanyang Technological University, Singapore",2023 IEEE MIT Undergraduate Research Technology Conference (URTC),"24 May 2024","2023","","","1","5","Large Language Models (LLMs) have gained usage across various domains, especially in education. However, the current state-of-the-art LLMs fail in numerical calculations due to their reliance on the pre-trained dataset that does not focus on mathematical oversight. Prompting is crucial to guide LLMs to yield desired outputs for mathematical problems. This paper explores a new Chain-of-Thoughts (CoT) prompting framework, leveraging Python-based tools like LLM Math, LLM symbolic math, and SerpAPI. We also evaluate the existing works with the CoT prompting framework for math problem-solving. Students can utilize this framework to obtain more precise solutions and comprehensive explanations for their queries.","","979-8-3503-0965-2","10.1109/URTC60662.2023.10534945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534945","Large Language Models;Prompt Engineering;LangChain;Mathematical reasoning;Conversational AI","Education;Mathematical models;Problem-solving","","","","23","IEEE","24 May 2024","","","IEEE","IEEE Conferences"
"FlexFL: Flexible and Effective Fault Localization With Open-Source Large Language Models","C. Xu; Z. Liu; X. Ren; G. Zhang; M. Liang; D. Lo","State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; School of Computing and Information Systems, Singapore Management University, Singapore",IEEE Transactions on Software Engineering,"16 May 2025","2025","51","5","1455","1471","Fault localization (FL) targets identifying bug locations within a software system, which can enhance debugging efficiency and improve software quality. Due to the impressive code comprehension ability of Large Language Models (LLMs), a few studies have proposed to leverage LLMs to locate bugs, i.e., LLM-based FL, and demonstrated promising performance. However, first, these methods are limited in flexibility. They rely on bug-triggering test cases to perform FL and cannot make use of other available bug-related information, e.g., bug reports. Second, they are built upon proprietary LLMs, which are, although powerful, confronted with risks in data privacy. To address these limitations, we propose a novel LLM-based FL framework named FlexFL, which can flexibly leverage different types of bug-related information and effectively work with open-source LLMs. FlexFL is composed of two stages. In the first stage, FlexFL reduces the search space of buggy code using state-of-the-art FL techniques of different families and provides a candidate list of bug-related methods. In the second stage, FlexFL leverages LLMs to delve deeper to double-check the code snippets of methods suggested by the first stage and refine fault localization results. In each stage, FlexFL constructs agents based on open-source LLMs, which share the same pipeline that does not postulate any type of bug-related information and can interact with function calls without the out-of-the-box capability. Extensive experimental results on Defects4J demonstrate that FlexFL outperforms the baselines and can work with different open-source LLMs. Specifically, FlexFL with a lightweight open-source LLM Llama3-8B can locate 42 and 63 more bugs than two state-of-the-art LLM-based FL approaches AutoFL and AgentFL that both use GPT-3.5. In addition, FlexFL can localize 93 bugs that cannot be localized by non-LLM-based FL techniques at the top 1. Furthermore, to mitigate potential data contamination, we conduct experiments on a dataset which Llama3-8B has not seen before, and the evaluation results show that FlexFL can also achieve good performance.","1939-3520","","10.1109/TSE.2025.3553363","Zhejiang Provincial Natural Science Foundation of China(grant numbers:LZ25F020003); National Natural Science Foundation of China(grant numbers:62302437); National Research Foundation(grant numbers:NRF-NRFI08-2022-0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934742","Fault localization;large language model;LLM-based agent","Computer bugs;Location awareness;Codes;Debugging;Pipelines;Large language models;Training;Data privacy;Source coding;Software systems","","","","66","IEEE","19 Mar 2025","","","IEEE","IEEE Journals"
"Jailbreak Attacks on Large Language Models and Possible Defenses: Present Status and Future Possibilities","S. S. Ahmed; J. Angel Arul Jothi","Department of Computer Science, Birla Institute of Technology and Science, Dubai, UAE; Department of Computer Science, Birla Institute of Technology and Science, Dubai, UAE",2024 IEEE International Symposium on Technology and Society (ISTAS),"29 Oct 2024","2024","","","1","7","Artificial intelligence (AI) is rapidly advancing with the emergence of large language models (LLMs) as potent tools. However, this potential is accompanied by significant challenges, particularly concerning the security and integrity of these models. AI ethics experts raise serious concerns about the misuse of LLMs like GPT-3, citing risks such as misinformation and harmful content generation through jailbreak attacks. Jailbreak attacks manipulate models to violate ethical guidelines, exposing critical flaws and legal consequences for businesses. This paper analyzes the various jailbreak attack models, categorizes them, and discusses the defense strategies proposed to mitigate these attacks. Moving forward, the paper elucidates the popular datasets, evaluation metrics and finally provides the current state of the research and future challenges in this area.","2158-3412","979-8-3315-4070-8","10.1109/ISTAS61960.2024.10732418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10732418","Artificial intelligence (AI);large language models (LLMs);security;ethics;adversarial attacks;jailbreak attacks","Measurement;Ethics;Reviews;Law;Large language models;Safety;Security;Fake news;Guidelines;Business","","","","21","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"PCEBench: A Multi-Dimensional Benchmark for Evaluating Large Language Models in Parallel Code Generation","L. Chen; N. Ahmed; M. Capotă; T. Willke; N. Hasabnis; A. Jannesari","Iowa State University, Ames, USA; Cisco AI Research, San Jose, USA; Intel Labs, Hillsboro, USA; DataStax, Santa Clara, USA; Code Metal, San Jose, USA; Iowa State University, Ames, USA",2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"23 Jul 2025","2025","","","546","557","The increasing complexity of software systems and advancements in hardware architectures have intensified the demand for efficient parallel code generation. While parallel programming offers significant performance benefits, it requires extensive expertise and effort due to the intricacies of synchronization, data management, and optimizations. To address these challenges, recent studies have explored the application of machine learning (ML) techniques in parallel code generation, aiming to reduce manual efforts and enhance performance outcomes. Large language models (LLMs) have recently revolutionized natural language processing (NLP) and demonstrated remarkable capabilities in code generation. However, evaluating their ability to generate high-performance parallel code presents unique challenges. Unlike sequential code evaluation, the evaluation of LLM-generated parallel code requires consideration of not only correctness but also efficiency and scalability in utilizing parallel resources. Concretely, existing benchmarks for LLM-generated parallel code evaluation are limited in size and scope compared to their sequential counterparts. To address this evaluation gap, we introduce PCEBench, a novel benchmark designed to assess LLMs' capabilities in generating parallel code. PCEBench focuses on multi-tasking and multidimensional performance evaluation, leveraging an LLM-based approach to generate verified prompts for parallel code generation. The benchmark incorporates scripts compatible with compilers and data race checkers, enabling comprehensive testing across critical dimensions such as compilability, executability, code self-correctness, functional correctness, data race detection, and speedup over serial implementations. By examining these multiple dimensions, PCEBench not only facilitates a thorough evaluation of LLMs in parallel code generation but also provides valuable insights for developers to enhance model performance in this challenging task. This comprehensive approach contributes to advancing the field of automated parallel programming and supports the development of more efficient and scalable software systems.","1530-2075","979-8-3315-3237-6","10.1109/IPDPS64566.2025.00055","NSF(grant numbers:2211982); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11078564","large language model;parallel code generation;benchmark;evaluation;LLM agent","Technological innovation;Codes;Parallel programming;Large language models;Scalability;Benchmark testing;Software systems;Multitasking;Natural language processing;Synchronization","","","","27","IEEE","23 Jul 2025","","","IEEE","IEEE Conferences"
"MaRV: A Manually Validated Refactoring Dataset","H. Nunes; T. Sharma; E. Figueiredo","Federal University of Minas Gerais, Belo Horizonte, Brazil; Dalhousie University, Halifax, Canada; Federal University of Minas Gerais, Belo Horizonte, Brazil",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","141","145","Despite the existence of traditional refactoring tools that offer semi-automated assistance, machine learning-based models have shown significant potential to generate refactored code. A comprehensive, manually validated refactoring dataset could help the software engineering community to train such models for effective refactorings. However, the community lacks a manually validated refactoring dataset. This paper introduces the MaRV dataset containing 693 manually evaluated code pairs extracted out of 126 GitHub Java repositories, representing four types of refactoring. In addition, the metadata describing the supposedly refactored elements was collected. Each code pair was manually evaluated by two reviewers out of 40 participants. MaRV dataset is constantly evolving with a web-based tool available for evaluating refactoring representations. The potential application of this dataset is to improve the accuracy and reliability of state-of-the-art models in refactoring tasks (e.g., refactoring candidate identification and refactoring code generation) by providing high-quality data.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00023","EMI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052808","refactoring;dataset;manually validated;foundation models","Java;Codes;Accuracy;Foundation models;Metadata;Software reliability;Software engineering;Software development management","","","","29","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"From Chatbots to Phishbots?: Phishing Scam Generation in Commercial Large Language Models","S. S. Roy; P. Thota; K. V. Naragam; S. Nilizadeh",The University of Texas at Arlington; The University of Texas at Arlington; The University of Texas at Arlington; The University of Texas at Arlington,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","36","54","The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs, i.e., ChatGPT (GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing websites and emails that can convincingly imitate well-known brands and also deploy a range of evasive tactics that are used to elude detection mechanisms employed by anti-phishing systems. These attacks can be generated using unmodified or ""vanilla"" versions of these LLMs without requiring any prior adversarial exploits such as jailbreaking. We evaluate the performance of the LLMs towards generating these attacks and find that they can also be utilized to create malicious prompts that, in turn, can be fed back to the model to generate phishing scams - thus massively reducing the prompt-engineering effort required by attackers to scale these threats. As a countermeasure, we build a BERT-based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content. Our model is transferable across all four commercial LLMs, attaining an average accuracy of 96% for phishing website prompts and 94% for phishing email prompts. We also disclose the vulnerabilities to the concerned LLMs, with Google acknowledging it as a severe issue. Our detection model is available for use at Hugging Face, as well as a ChatGPT Actions plugin.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00182","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646856","Phishing;Large Language Models;Prompt Engineering;ChatGPT;vulnerabilities;Detection tool","Training;Technological innovation;Phishing;Large language models;Machine learning;Chatbots;Electronic mail","","13","","123","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"Leveraging LLMs to Automate Software Architecture Design from Informal Specifications","A. Tagliaferro; S. Corbo; B. Guindani","Department of Electronics, Information and Bioengineering, Politecnico di Milano, Italy; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Italy; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Italy",2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C),"30 May 2025","2025","","","291","299","Designing a software architecture starting from specifications and requirements is a time-consuming and errorprone process that demands domain expertise. Automating this process has become a significant research focus in software engineering. Traditional approaches rely on rule-based mechanisms to translate manually derived, standardized requirements into the desired architecture. However, these methods struggle to identify implicit patterns without expert intervention. Recently, approaches leveraging Large Language Models (LLMs) have gained attention. This study evaluates the performance of LLMs in generating software architecture blueprints, specifically UML component diagrams, from informal natural-language specifications. We develop a formal characterization of component diagrams to derive quantitative metrics for analyzing LLM-generated diagrams, comparing them against expert-drawn ground truths associated with the specifications. Our findings indicate that while LLM-based approaches show promise in addressing the flaws of rule-based methods, they currently lack the accuracy needed for deployment in real-world scenarios.","2768-4288","979-8-3315-3336-6","10.1109/ICSA-C65153.2025.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11014997","Software Engineering;Requirements Engineering;Unified Modeling Language;Large Language Models;Computing Architecture Design","Measurement;Translation;Software architecture;Large language models;Unified modeling language;Semantics;Stochastic processes;Computer architecture;Syntactics;Software development management","","","","28","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"RefleXGen:The unexamined code is not worth using","B. Wang; H. Li; A. Liu; B. Yang; A. Yang; Y. Zhong; W. Huang; R. Huang; W. Zeng; Y. Zhang","School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; Capability & Platform Business Dept., China Mobile Internet Co., Beijing, China; China Telecom Cloud Technology Co., Ltd., Guangzhou, China; China Telecom Cloud Technology Co., Ltd., Guangzhou, China; Capability & Platform Business Dept., China Mobile Internet Co., Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Security in code generation remains a pivotal challenge when applying large language models (LLMs). This paper introduces RefleXGen, an innovative method that significantly enhances code security by integrating Retrieval-Augmented Generation (RAG) techniques with guided self-reflection mechanisms inherent in LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing specialized secure code datasets—processes that can be resource-intensive—RefleXGen iteratively optimizes the code generation process through self-assessment and reflection without the need for extensive resources. Within this framework, the model continuously accumulates and refines its knowledge base, thereby progressively improving the security of the generated code. Experimental results demonstrate that RefleXGen substantially enhances code security across multiple models, achieving a 13.6% improvement with GPT-3.5 Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a 5.8% improvement with Gemini.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890824","code generation;security;large language models;RAG","Codes;Speech coding;Large language models;Knowledge based systems;Retrieval augmented generation;Signal processing;Reflection;Security;Speech processing;Investment","","","","21","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Exploring Large Language Models in Active Learning for Annotating Physical Sensing Data","A. Hota; S. Chatterjee; S. Chakraborty","IIT Kharagpur, India; Nokia Bell Labs, Cambridge, UK; IIT Kharagpur, India",2025 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),"19 Jun 2025","2025","","","681","684","Active learning minimizes annotation costs by lever-aging a model trained using a small bootstrap amount of labeled data to quantify the uncertainty of predictions and then delegate those samples for annotation to a human oracle. This work explores the idea of replacing the human-in-the-loop with Large Language Models (LLMs) for physical sensing data. In this paper, we systematically study the uncertainty of models and the corresponding accuracy of responses from the LLM. To perform the study in a principled manner, we used two state-of-the-art LLMs, GPT-4 and Llama-3.2, without fine-tuning or sophisticated prompt engineering. Experiments on benchmark HAR datasets, MotionSense and HHAR, demonstrate the overall idea’s viability and highlight the key challenges.","2766-8576","979-8-3315-3553-7","10.1109/PerComWorkshops65533.2025.00162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11038626","Active Learning;LLMs;Activity Detection","Pervasive computing;Uncertainty;Accuracy;Annotations;Large language models;Conferences;Active learning;Predictive models;Sensors;Prompt engineering","","","","10","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Large Language Models as a Tool for Cognitive Stimulation: Chatbot Book Clubs for Seniors","H. Zhou; E. Chen; S. Wen; Y. Wang; R. Norel","Cornell University | ‘27; Newton North High School | ‘26; Robert E. Bell Middle School | ’24; NYC Q300 Gifted and Talented School | ‘24; IBM TJ Watson Research Center, USA",2024 IEEE International Conference on Digital Health (ICDH),"28 Aug 2024","2024","","","123","125","This paper introduces a unique Chatbot book club application, harnessing Large Language Models (LLMs) to prevent cognitive decline in seniors, as part of a pilot study conducted by IBM and Harvard Medical School. The study will follow two groups of 70+ years old in assisted living facilities - a control arm and a group encouraged to read books. The Chatbot engages participants in the reading group in conversations about the books to promote continued engaged reading, with the hypothesis that this will measurably improve cognitive function. Utilizing prompt engineering and user profiles, it generates personalized, thought-provoking questions. The friendly user interface with large fonts and voice interactivity encourages cognitive engagement. Future work will deploy the Chatbot in the year-long study, iterate on improvements, and explore broader applications.","","979-8-3503-6857-4","10.1109/ICDH62654.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645937","large language model (LLM);chatbot;cognitive decline;prompt engineering;user experience","Atmospheric measurements;Large language models;Oral communication;User interfaces;Chatbots;Particle measurements;Electronic healthcare","","1","","15","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Distributed Fault Detection for Second-Order Delayed Multi-Agent Systems With Adversaries","Y. Quan; W. Chen; Z. Wu; L. Peng","School of Internet of Things Engineering, Jiangnan University, Wuxi, China; Division of Engineering Technology, Wayne State University, Detroit, MI, USA; School of Internet of Things Engineering, Jiangnan University, Wuxi, China; School of Internet of Things Engineering, Jiangnan University, Wuxi, China",IEEE Access,"6 Sep 2017","2017","5","","16478","16483","In this paper, the problem of distributed fault detection and isolation is considered for multiagent systems with time delays. An adversarial observer is designed for each agent, using local information of the agent, to estimate the external adversaries. By utilizing the adversarial observer, the detection of the faults caused by external incipient adversaries can be handled. Moreover, a software architecture is proposed for each agent; this can guarantee the multi-agent systems immune to the agents intruded by external adversaries. At last, numerical simulations are employed to verify the validity of the theoretical results.","2169-3536","","10.1109/ACCESS.2017.2723906","Natural Science Foundation of the Anhui Provincial Department of the Education of China(grant numbers:KJ2016A170); U.S. National Science Foundation(grant numbers:EPCN 1507096); National Natural Science Foundation of China(grant numbers:61203147,61374047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7970117","Distributed detection and isolation;external adversaries;second-order discrete-time multi-agent systems;time delays","Observers;Delay effects;Multi-agent systems;Fault detection;Software architecture;Monitoring;Interconnected systems","","18","","23","OAPA","6 Jul 2017","","","IEEE","IEEE Journals"
"Defending Large Language Models Against Jailbreak Attacks Through Chain of Thought Prompting","Y. Cao; N. Gu; X. Shen; D. Yang; X. Zhang","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Engineering, Anhui Agricultural University, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China",2024 International Conference on Networking and Network Applications (NaNA),"20 Sep 2024","2024","","","125","130","With the deep research and widespread application of Large Language Models (LLMs), the security and privacy issues inherent in them have gradually become prominent, posing new challenges in the field of network security. Elaborately designed jailbreak attack prompts may induce LLMs to act against human values and preferences and generate harmful responses. To address this issue, we introduce a straightforward yet effective defence mechanism: Chain of Thought Prompting. This defence mechanism aims to emulate human thought processess. Chain of Thought Prompting method fully harnesses the inherent reasoning ability of the LLMs via five stages. It encourages Large Language Models (LLMs) to engage in self-thought, self-reflection, and self-refinement. Our experiments on ChatGLM-3-6B and Llama-2-7B-Chat, utilizing the JADE and DAN datasets, demonstrate a significant reduction in the average Attack Success Rate (ASR) for jailbreak attacks. This reduction is from 65.01% to 13.40% for ChatGLM-3-6B, and from 49.06% to 0.13% for Llama-2-7B-Chat. Our findings suggest that Chain of Thought Prompting can significantly reduce the generation of harmful content. Our work provides an effective method for LLMs to counter jailbreak attacks, enhancing the safety of LLMs without the need for additional training.","","979-8-3503-7677-7","10.1109/NaNA63151.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10679816","network security;large language models;jailbreak attack;chain of thought","Training;Privacy;Large language models;Network security;Cognition;Safety","","","","40","IEEE","20 Sep 2024","","","IEEE","IEEE Conferences"
"Semantic Compression with Large Language Models","H. Gilbert; M. Sandborn; D. C. Schmidt; J. Spencer-Smith; J. White","Dept. of Computer Science, Vanderbilt University, Nashville, TN, USA; Dept. of Computer Science, Vanderbilt University, Nashville, TN, USA; Dept. of Computer Science, Vanderbilt University, Nashville, TN, USA; Dept. of Computer Science, Vanderbilt University, Nashville, TN, USA; Dept. of Computer Science, Vanderbilt University, Nashville, TN, USA","2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS)","2 Jan 2024","2023","","","1","8","The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as “hallucinations”), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed. This paper presents three contributions to research on LLMs. First, we present the results from experiments exploring the viability of “approximate compression” using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT interfaces. Second, we investigate and quantify the capability of LLMs to compress text. Third, we present two novel metrics-Exact Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness (SRE)-that quantify the level of preserved intent between text compressed and decompressed by the LLMs we studied. Our initial results indicate that GPT-4 can effectively compress and reconstruct text while preserving the semantic essence of the original text, providing a path to leverage more tokens than current limits allow.","2831-7343","979-8-3503-1890-6","10.1109/SNAMS60348.2023.10375400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375400","large language models;data compression;prompt engineering;text generation","Analytical models;Codes;Social networking (online);Semantics;Focusing;Chatbots;Question answering (information retrieval)","","16","","26","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"Towards Continuous Security Assessment: Integrating Model-Based Risk Assessment and Large Language Models","M. Werthwein; B. Annighoefer; Z. Daw","Institute for aircraft systems, University of Stuttgart, Stuttgart, Germany; Institute for aircraft systems, University of Stuttgart, Stuttgart, Germany; Institute for aircraft systems, University of Stuttgart, Stuttgart, Germany","2025 Integrated Communications, Navigation and Surveillance Conference (ICNS)","6 May 2025","2025","","","1","12","Cyber threats and attacks on aircraft ground infrastructure as well as communication, navigation, and surveillance service providers have increased significantly in recent years. Due to the dynamic nature of cyber threats, a once securely designed architecture will not stay secure throughout its life-cycle. Ensuring the safety and security of large systems of systems requires continuous reassessment of security risks. Currently, while useful building blocks exist, no comprehensive framework supports the continuous and automated risk reassessment in aviation. To address this gap, we propose a conceptual framework that integrates model-based system specification, model-based security assessment, and Large Language Models (LLMs) to automate the continuous reassessment of residual risks in cyber-physical systems. This framework dynamically analyzes and adapts to emerging threats, enabling proactive and up-to-date risk management. This paper outlines the high-level requirements of the proposed framework and presents a state-of-the-art analysis of technology readiness and existing gaps necessary for its implementation. To demonstrate its feasibility and effectiveness, we apply the framework to an Air Traffic Management use case.","2155-4951","979-8-3315-3473-8","10.1109/ICNS65417.2025.10976786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976786","Security;Risk Assessment;Large Language Models;Model-based system engineering","Adaptation models;Large language models;Surveillance;Atmospheric modeling;Cyber-physical systems;Reliability engineering;Regulation;Safety;Risk management;Air traffic control","","","","44","IEEE","6 May 2025","","","IEEE","IEEE Conferences"
"Prompt Engineering to Classify Components of Standard Operating Procedure Steps Using Large Language Model (LLM)-Based Chatbots","J. Bashatah; L. Sherry","Center for Air Transportation Systems Research/System Engineering & Operations Research, George Mason University, Fairfax, Va, U.S.A; Center for Air Transportation Systems Research/System Engineering & Operations Research, George Mason University, Fairfax, Va, U.S.A","2024 Integrated Communications, Navigation and Surveillance Conference (ICNS)","11 Jun 2024","2024","","","1","8","Mission safety and efficiency is achieved when flight crews can complete Standard Operating Procedures (SOPs) within the Available Operational Time Window (AOTW). This is especially the case for abnormal and emergency procedures. To ensure SOPs can be completed within the AOTW, SOP Designers use Monte Carlo simulations to model the SOP performance. These simulations necessitate the decomposition of an SOP step into its constituent components, a process traditionally reliant on labor-intensive manual classification. Recent advancements in Large Language Models (LLMs) and the accompanying automation facilitated by Prompt Engineering (PE) present promising avenues for streamlining the classification of SOP steps, thereby potentially expediting the SOP design process.This paper describes a method for Prompt Engineering (PE) aimed at the automation of component classification within Large Language Model (LLM)-enabled chatbots. The classified components can be assigned time distributions for a Monte Carlo simulation to evaluate SOP performance. A case study is described in which the LLM correctly classified 80% of SOP step components, saving 36% of the time to conduct the analysis. The implications of the method are discussed.","2155-4951","979-8-3503-9309-5","10.1109/ICNS60906.2024.10550703","University of Oregon; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550703","Standard Operating Procedures (SOPs);Large Language Models (LLMs);Prompt Engineering (PE);Procedure Analysis;Human Performance Simulation","Analytical models;Monte Carlo methods;Automation;Navigation;Surveillance;Manuals;Chatbots","","1","","16","IEEE","11 Jun 2024","","","IEEE","IEEE Conferences"
"Examine the Role of Generative AI in Enhancing Threat Intelligence and Cyber Security Measures","V. R. Saddi; S. K. Gopal; A. S. Mohammed; S. Dhanasekaran; M. S. Naruka","Technology Lead, Raleigh, NC, USA; Technology Lead, Charlotte, NC, USA; School of Computer and Information Sciences, University of the Cumberlands, Kentucky, USA; Department of ECE, Sri Eshwar College of Engineering, Coimbatore, Tamil Nadu, India; G.L. Bajaj Institute of Management, Greater Noida, India",2024 2nd International Conference on Disruptive Technologies (ICDT),"11 Apr 2024","2024","","","537","542","Generative Artificial Intelligence (AI) has increasingly been used to enhance threat intelligence and cyber security measures for organizations. Generative AI is a form of AI that creates new data without relying on existing data or expert knowledge. This technology provides decision support systems with the ability to automatically and quickly identify threats posed by hackers or malicious actors by taking into account various sources and data points. In addition, generative AI can help identify vulnerabilities within an organization's infrastructure, further reducing the potential for a successful attack. This technology is especially well-suited for security operations centers (SOCs), which require rapid identification of threats and defense measures. By incorporating interesting and valuable data points that previously would have been missed, generative AI can provide organizations with an additional layer of defense against increasingly sophisticated attacks.","","979-8-3503-7105-5","10.1109/ICDT61202.2024.10489766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489766","Generative AI;Threat Intelligence;Cyber security;Enhancing;Measures","Generative AI;Organizations;Market research;Time measurement;Threat assessment;Malware;Security","","12","","17","IEEE","11 Apr 2024","","","IEEE","IEEE Conferences"
"RTL-Breaker: Assessing the Security of LLMs Against Backdoor Attacks on HDL Code Generation","L. L. Mankali; J. Bhandari; M. Alam; R. Karri; M. Maniatakos; O. Sinanoglu; J. Knechtel",New York University Tandon School of Engineering; New York University Tandon School of Engineering; New York University Abu Dhabi; New York University Tandon School of Engineering; New York University Abu Dhabi; New York University Abu Dhabi; New York University Abu Dhabi,"2025 Design, Automation & Test in Europe Conference (DATE)","21 May 2025","2025","","","1","7","Large language models (LLMs) have demonstrated remarkable potential with code generation/completion tasks for hardware design. However, the reliance on such automation introduces critical security risks. Notably, given that LLMs have to be trained on vast datasets of codes that are typically sourced from publicly available repositories, often without thorough validation, LLMs are susceptible to so-called data poisoning or backdoor attacks. Here, attackers inject malicious code for the training data, which can be carried over into the hardware description code (HDL) generated by LLMs. This threat vector can compromise the security and integrity of entire hardware systems. In this work, we propose RTL-Breaker, a novel backdoor attack framework on LLM-based HDL code generation. RTL-Breaker provides an indepth analysis of essential aspects of this novel problem: 1) various trigger mechanisms versus their effectiveness for inserting malicious modifications, and 2) side-effects by backdoor attacks on code generation in general, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need for more robust measures to safeguard against such attacks. Toward that end, we open-source our framework and all data.","1558-1101","978-3-9826741-0-0","10.23919/DATE64628.2025.10993260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10993260","LLM;HDL Code Generation;Backdoor Attacks","Codes;Systematics;Semantics;Training data;Syntactics;Hardware;Vectors;Security;Hardware design languages;Payloads","","","","39","","21 May 2025","","","IEEE","IEEE Conferences"
"POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models","J. He; X. Wang; S. Liu; G. Wu; C. Silva; H. Qu","Hong Kong University of Science and Technology; Bosch Center for Artificial Intelligence (BCAI), Bosch Research North America; Arizona State University; New York University; New York University; Hong Kong University of Science and Technology",2025 IEEE 18th Pacific Visualization Conference (PacificVis),"16 Jun 2025","2025","","","36","46","Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities in multimodal inputs. This oversight hinders the development of effective prompts that guide models’ multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities. In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for steering the multimodal reasoning performance of LLMs. The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights. The effectiveness and efficiency of our system are validated through quantitative and qualitative evaluations with experts.","2165-8773","979-8-3315-0581-3","10.1109/PacificVis64226.2025.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11021043","prompt engineering;multimodal reasoning;multimodal large language models","Large language models;Visual analytics;Interactive systems;Refining;Cognition;Prompt engineering;Usability;Optimization;Context modeling","","","","79","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Application of Vector Based Memory Prompt Engineering Using Personality Psychology in Language Model of Conservational Interfaces","E. Dağlarlı; E. Arıbaş","Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, Turkey; Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, Turkey",2024 32nd Signal Processing and Communications Applications Conference (SIU),"23 Jul 2024","2024","","","1","4","The psychology of personality is a part of psychology analyzing personality and its diversity for the people. It’s purpose is to depict how individuals are pesonally various due to psychological motives. Personality type refers to the psychological classification of people into various classes. Personality traits are distinguished from personality types, that come in various degrees. Many theories related to personality are available, but most of them include several and sometimes many sub theories. Constructing the Myers–Briggs Type Indicator (MBTI) and The Big Five (BF) personality traits is crucial in personality pyscology. Our objective is twofold. Firstly, it is planned to develop a correlation model between MBTI and BF personality traits and apply it to conversational interfaces’ language models. Secondly, it is aimed to extend this approach to time-sensitive design by utilizing fixed-effect analysis, which is particularly useful when analyzing data from multiple individuals across various time points. The results show distinct observations, as seen in popular language models like ChatGPT.","2165-0608","979-8-3503-8896-1","10.1109/SIU61531.2024.10601097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601097","personality psychology;prompt engineering;MBTI;BF;ChatGPT;Large language models (LLM)","Correlation;Psychology;Signal processing;Chatbots;Vectors;Prompt engineering","","1","","0","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Event-Triggered Control for Human-in-the-Loop Multi-agent Systems under DoS Attacks","Y. Xu; K. Liu; H. Liang; T. Li; Y. Long; Q. Liu; X. Yang; Z. Hang","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Engineering, China University of Petroleum-Beijing at Karamay, Karamay, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China",2024 14th International Conference on Information Science and Technology (ICIST),"30 Dec 2024","2024","","","292","297","This paper introduces an event-triggered secure control scheme for human-in-the-loop multi-agent systems in the context of DoS attacks. The integration of human intelligence and decision-making significantly enhances system security, as a human provides command signals to a non-autonomous leader agent. To determine unknown states, an adaptive neural state observer utilizes neural networks to approximate nonlinear functions, while a relative threshold-based event-triggered con-trol strategy is introduced to optimize communication resource usage. At the same time, a predictor is developed to monitor potential compromises in the edges of the multi-agent network to counteract attacks. Using Lyapunov analysis, it is shown that the proposed secure control protocol is capable of maintaining bounded closed-loop signals despite the occurrence of attacks. Finally, the effectiveness of the proposed scheme is validated by the simulation results.","2573-3311","979-8-3503-5333-4","10.1109/ICIST63249.2024.10805436","National Natural Science Foundation of China(grant numbers:51939001,62322307,62273072); China Postdoctoral Science Foundation(grant numbers:2023TQ0049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10805436","Multi-agent systems;DoS attacks;event-triggered control;Human in the loop","Protocols;Event detection;Human intelligence;Decision making;Neural networks;Human in the loop;Security;State estimation;Monitoring;Multi-agent systems","","","","26","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Beyond Accuracy and Robustness Metrics for Large Language Models for Code","D. Rodriguez-Cardenas","William and Mary, Williamsburg, Virginia, USA",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","159","161","In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot [31] and Google's CodeBot [21] exemplify how LLMc contributes to substantial time and effort savings in software development. However, despite their widespread use, there is a growing need to thoroughly assess LLMc, as current evaluation processes heavily rely on accuracy and robustness metrics, lacking consensus on additional influential factors in code generation. This gap hinders a holistic understanding of LLMc performance, impacting interpretability, efficiency, bias, fair-ness, and robustness. The challenges in benchmarking and data maintenance compound this issue, underscoring the necessity for a comprehensive evaluation approach. To address these issues, this dissertation proposes the development of a benchmarking infras-tructure, named HolBench, aimed at overcoming gaps in evaluating LLMc quality. The goal is to standardize testing scenarios, facilitate meaningful comparisons across LLMc, and provide multi-metric measurements beyond a sole focus on accuracy. This approach aims to decrease the costs associated with advancing LLMc research, en-hancing their reliability for adoption in academia and industry.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3639792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554900","deep learning;code generation;interpretability;transformers","Measurement;Codes;Accuracy;Reviews;Benchmark testing;Robustness;Software reliability","","","","38","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Beyond Text: Nefarious Actors Harnessing LLMs for Strategic Advantage","D. Majumdar; A. S; P. Boyina; S. S. P. Rayidi; Y. R. Sai; S. V. Gangashetty","Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India; Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India; Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India; Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India; Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India; Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, Andhra Pradesh, India",2024 International Conference on Intelligent Systems for Cybersecurity (ISCS),"12 Jul 2024","2024","","","1","7","In the pervasive landscape of digital advancements, Large Language Models (LLMs) emerge as an indispensable force, orchestrating efficiency and productivity. Yet, as the influence of LLMs extends, so does the array of emerging threats. Their potential for misuse by nefarious actors poses significant challenges to cybersecurity and societal well-being. This paper investigates the phenomenon of malicious actors harnessing LLMs for strategic advantage, going beyond conventional uses of these models. Through a comprehensive review of literature, case studies, and analysis of real-world examples, we scrutinize the multifaceted ways in which LLMs are leveraged for malicious intent. This includes the manipulation of public opinion through the generation of deceptive content, the orchestration of social engineering attacks using sophisticated language-based techniques, and the facilitation of cybercrimes such as phishing, password cracking, typosquatting, and malware propagation.","","979-8-3503-7523-7","10.1109/ISCS61804.2024.10581181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581181","Large Language Models;LLMs;AI;Package Hallucination;Deceptive Content;Exploitation;Threat Mitigation;Code Generation;Security Risks;Natural Language Processing;Cybersecurity;Threat Intelligence;Social Engineering","Training;Technological innovation;Reviews;Phishing;Large language models;Force;Passwords","","3","","34","IEEE","12 Jul 2024","","","IEEE","IEEE Conferences"
"Improving Cybersecurity Named Entity Recognition with Large Language Models","Z. Qiao; C. Zhang; G. Du","Information Security management and operation Center China Mobile Ltd, Beijing, China; China Mobile Group Design Institute Co., Ltd, Beijing, China; China Mobile Group Design Institute Co., Ltd, Beijing, China",2023 6th International Conference on Software Engineering and Computer Science (CSECS),"16 Feb 2024","2023","","","01","06","A lot of attention has been paid to cybersecurity threat intelligence analysis based on security knowledge graphs because they can evaluate multi-source threat intelligence data at a fine-grained level. The primary effort in building knowledge graphs is named entity recognition (NER). However, the retrieved features from classic NER algorithms are insufficient to identify novel security entities in the cybersecurity field. Currently, the natural language processing(NLP) field has made unprecedented progress, driven by large language models(LLMs). In this paper, we present a BERT- BiLSTM-CRF cybersecurity NER method based on BiLSTM-CRF that combines the pre-trained large language model BERT. The preprocessed annotated corpus is fed into the Bi-LSTM and CRF models, which employ the word vectors to extract contextual features and annotate 9 different categories of named entities. The proposed cyberecurity NER method is effective in identifying cybersecurity entities on large-scale cybersecurity datasets, and the relevant evaluation indexes are superior to other algorithms, according to comparative experiments carried out within the cyberspace security corpus.","","979-8-3503-0637-8","10.1109/CSECS60003.2023.10428218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428218","NER;cybersecurity;BERT;BiLSTM;LLMs","Natural languages;Knowledge graphs;Feature extraction;Labeling;Indexes;Computer security;Software engineering","","1","","20","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"Evaluating the Generalizability of LLMs in Automated Program Repair","F. Li; J. Jiang; J. Sun; H. Zhang","College of Intelligence and and Computing, Tianjin University, Tianjin, China; College of Intelligence and and Computing, Tianjin University, Tianjin, China; College of Intelligence and and Computing, Tianjin University, Tianjin, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China",2025 IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),"10 Jun 2025","2025","","","91","95","LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well-known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs’ capabilities (increasing the number of correct and plausible patches by up to 136.67% and 121.82%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs’ repair capabilities. Based on our study, we also offer several recommendations for future research.","2832-7632","979-8-3315-3711-1","10.1109/ICSE-NIER66352.2025.00024","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023968","Program Repair;LLM;Generalizability of LLM","Semantics;Maintenance engineering;Prompt engineering;Software engineering","","","","43","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Towards Autonomous Testing Agents via Conversational Large Language Models","R. Feldt; S. Kang; J. Yoon; S. Yoo",Chalmers University of Technology; KAIST; KAIST; KAIST,2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","1688","1693","Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298360","software testing;machine learning;large language model;artificial intelligence, test automation","Software testing;Automation;Taxonomy;Oral communication;Drives;Middleware;Testing","","22","","36","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"LLM4PlC: Harnessing large Language Models for Verifiable Programming of PlCs in Industrial Control Systems","M. Fakih; R. Dharmaji; Y. Moghaddas; G. Q. Araya; O. Ogundare; M. A. Al Faruque","Dept. of Electrical Engineering and Computer Science, University of California, Irvine, CA, USA; Dept. of Electrical Engineering and Computer Science, University of California, Irvine, CA, USA; Dept. of Electrical Engineering and Computer Science, University of California, Irvine, CA, USA; Siemens Technology, Princeton, NJ, USA; Siemens Technology, Princeton, NJ, USA; Dept. of Electrical Engineering and Computer Science, University of California, Irvine, CA, USA",2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),"18 Jun 2024","2024","","","192","203","Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainabil-ity, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) op-erated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feed-back and external verification tools - including grammar checkers, compilers and SMV verifiers - to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTech-nik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally-flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open research, we share the complete experi-mental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage11https://sites.google.com/uci.edu/llm4plc/home.","2832-7659","979-8-4007-0501-4","10.1145/3639477.3639743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554746","Industrial Control;Verifiable Synthesis;Large Language Models;Prompt Engineering","Codes;Industrial control;Pipelines;Programmable logic devices;Programming;Reliability engineering;Manufacturing","","6","","63","CCBY","18 Jun 2024","","","IEEE","IEEE Conferences"
"Is ChatGPT a Competent Teacher? Systematic Evaluation of Large Language Models on the Competency Model","L. Gong; J. Chen; F. Wu","School of Public Affairs, Zhejiang University, Hangzhou, China; College of Education, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China",IEEE Transactions on Learning Technologies,"20 May 2025","2025","18","","530","541","The capabilities of large language models (LLMs) in language comprehension, conversational interaction, and content generation have led to their widespread adoption across various educational stages and contexts. Given the fundamental role of education, concerns are rising about whether LLMs can serve as competent teachers. To address the challenge of comprehensively evaluating the competencies of LLMs as teachers, a systematic quantitative evaluation based on the competency model has emerged as a valuable approach. Our study, grounded in the teacher competency model and drawing from 14 existing scales, constructed an evaluation framework called TeacherComp. Based on TeacherComp, we evaluated six LLMs from OpenAI across four dimensions: knowledge, skills, values, and traits. Through comparisons between LLMs’ responses and human norms, we found that: 1) with each successive update, LLMs have shown overall improvements in knowledge, while their skills dimension scores have increasingly aligned with human norms; 2) there are both commonalities and differences in the performance of various LLMs regarding values and traits. For instance, while they all tend to exhibit more negative traits than humans, their morals can vary; and 3) LLMs with reduced security, constructed using jailbreak techniques, exhibit values and traits more closely aligned with human norms. Building on these findings, we provided interpretations and suggestions for the application of LLMs in various educational contexts. Overall, this study helps teachers and students use LLMs in appropriate contexts and provides developers with guidance for future iterations, thereby advancing the role of LLMs in empowering education.","1939-1382","","10.1109/TLT.2025.3564177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976353","Evaluation;large language models (LLMs);teacher competency","Education;Ethics;Chatbots;Systematics;Standards;Security;Psychology;Large language models;Generative AI;Ciphers","","","","92","IEEE","24 Apr 2025","","","IEEE","IEEE Journals"
"Generating Abuse Stories and Misuse Cases using Large Language Models","C. Cheh; N. S. Kham Shing; R. Lim; B. Chen","Illinois Advanced Research Center at Singapore Ltd., Singapore, Singapore; Singapore University of Technology and Design Singapore, Singapore; Illinois Advanced Research Center at Singapore Ltd., Singapore, Singapore; Singapore University of Technology and Design Singapore, Singapore",2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops),"18 Mar 2025","2024","","","208","215","Security-by-design aims to build in security into software systems during the design stage in order to achieve a higher level of system resilience. However, traditional approaches to integrate security into software design requires non-security experts to manually brainstorm potential ways that attackers can target the software system. With the remarkable reasoning and generative capabilities of Large Language Models (LLMs), we believe that the initial effort to brainstorming attack strategies on a system can be performed by the LLMs to aid the non-security expert. In this paper, we provide two design artifacts — use case scenarios and user stories — that are produced at different stages of software development to an ensemble of five LLMs and instruct those LLMs to generate misuse case scenarios and abuse stories following a strict set of requirements. Our results show that the LLMs are able to produce a large number of abuse stories and misuse case scenarios but requires extensive postprocessing to extract meaningful results. The resultant abuse stories and misuse case scenarios represent a diverse set of attacks and complement the ability of developers to manually create those security scenarios. As such, LLMs are a valuable tool to jumpstart the process of security testing for non-security experts.","","979-8-3315-3281-9","10.1109/ACSACW65225.2024.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10918277","large language models;abuse stories;security by design;misuse case","Fault tolerance;Software design;Heuristic algorithms;Large language models;Conferences;Software systems;Security;Particle swarm optimization;Testing;Software development management","","","","26","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Automated Domain Modeling with Large Language Models: A Comparative Study","K. Chen; Y. Yang; B. Chen; J. A. Hernández López; G. Mussbacher; D. Varró","Electrical and Computer Engineering, McGill University, Montreal, Canada; Electrical and Computer Engineering, McGill University, Montreal, Canada; Electrical and Computer Engineering, McGill University, Montreal, Canada; DIS, University of Murcia, Murcia, Spain; Electrical and Computer Engineering, McGill University, Montreal, Canada; Linköping University, Linköping, Sweden",2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS),"12 Dec 2023","2023","","","162","172","Domain modeling is an essential part of software engineering and serves as a way to represent and understand the concepts and relationships in a problem domain. Typically, software engineers interpret the problem description written in natural language and manually translate it into a domain model. Domain modeling can be time-consuming and highly depends on the expertise of software engineers. Recently, Large Language Models (LLMs) have exhibited remarkable ability in language understanding, generation, and reasoning. In this paper, we conduct a comprehensive, comparative study of using LLMs for fully automated domain modeling. We assess two powerful LLMs, GPT3.5 and GPT4, employing various prompt engineering techniques on a data set containing ten diverse domain modeling examples with reference solutions created by modeling experts. Our findings reveal that while LLMs demonstrate impressive domain understanding capabilities, they are still impractical for full automation, with the top-performing LLM achieving F1 scores of 0.76 for class generation, 0.61 for attribute generation, and 0.34 for relationship generation. Moreover, the F1 score is characterized by higher precision and lower recall; thus, domain elements retrieved by LLMs are often reliable, but there are many missing elements. Furthermore, modeling best practices are rarely followed in auto-generated domain models. Our data set and evaluation provide a valuable baseline for future research in automated LLM-based domain modeling.","","979-8-3503-2480-8","10.1109/MODELS58315.2023.00037","Universidad de Murcia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10344012","domain modeling;large language models;few-shot learning;chain-of-thought prompting;prompt engineering","Training;Knowledge engineering;Fault diagnosis;Automation;Natural languages;Data models;Software","","35","","33","IEEE","12 Dec 2023","","","IEEE","IEEE Conferences"
"AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs","L. Lv; W. Zhang; X. Tang; J. Wen; F. Liu; J. Han; S. Hu","Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods that extract malicious content from the model by carefully crafting prompts or suffixes, which has garnered significant attention from the research community. However, traditional attack methods, which primarily focus on the semantic level, are easily detected by the model. These methods overlook the difference in the model’s alignment protection capabilities at different output stages. To address this issue, we propose an adaptive position pre-fill jailbreak attack approach for executing jailbreak attacks on LLMs. Our method leverages the model’s instruction-following capabilities to first output pre-filled safe content, then exploits its narrative-shifting abilities to generate harmful content. Extensive black-box experiments demonstrate our method can improve the attack success rate by 47% on the widely recognized secure model (Llama2) compared to existing approaches. Our code can be found at: https://github.com/Yummy416/AdaPPA.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890715","Large Language Models;Model Security;Jailbreak Attacks;Pre-Fill Attacks","Adaptation models;Codes;Large language models;Semantics;Closed box;Signal processing;Acoustics;Security;Speech processing;Protection","","","","30","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"LLM-CoSen: Revisiting Collaborative Sensing with Large Language Models (LLMs)","X. Feng; Z. Sun; Z. Chen; C. Luo; Z. Zhou; V. C. M. Leung; W. Xu","School of Information Engineering, China University of Geosciences Beijing, China; Department of Computer Science, City University of Hong Kong, China; School of Information Engineering, China University of Geosciences Beijing, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science, City University of Hong Kong, China",IEEE Transactions on Mobile Computing,"","2025","PP","99","1","13","Collaborative sensing has emerged as a novel sensing paradigm, entailing multi-sensor data sharing and multimodal modeling to collaboratively understand sensing behaviors. However, current solutions, i.e., data-level and decision-level fusion methods, fall short of generality, expert knowledge, and holistic/chronic perspective. In this paper, we propose LLMCoSen to revisit collaborative sensing with Large Language Models (LLMs). Specifically, LLM-CoSen designs a semantic-level fusion approach for inference results for collaborative sensing. Such an approach is characterized by its generality, making it applicable to any heterogeneous devices, and its expert knowledge incorporation, which provides chronic, holistic, and insightful perspectives on the inference results. Regarding inference absence challenges, we propose a personalized model design method to constrain inference time, and a voting-based two-pass prompt engineering strategy for token completion. Regarding inference error challenges, we propose an accuracy restoration strategy for personalized models, and a two-level error estimator coupled with self-correction. Experimental results of human digital system use case on four corresponding benchmark datasets show LLM-CoSen can decrease inference absence by 72.83% and inference errors by 7.65% on average.","1558-0660","","10.1109/TMC.2025.3583345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11051039","Collaborative Sensing;Large Language Models","Sensors;Collaboration;Internet of Things;Accuracy;Servers;Semantics;Data integration;Computer architecture;Time factors;Prompt engineering","","","","","IEEE","25 Jun 2025","","","IEEE","IEEE Early Access Articles"
"ASPIRE: A Multi-Agent Framework for Execution-Free Code Analysis and Repair","B. A. Ramanan; M. A. Khan; A. Rao","Nokia Bell Labs, Murray Hill, NJ, USA; Nokia Bell Labs, Murray Hill, NJ, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","8811","8813","The proliferation of Large Language Models (LLMs) and agentic systems for code generation introduces new challenges in ensuring the reliability and correctness of generated programs. Errors in dynamically generated code can cascade through workflows, leading to inefficiencies or failures in critical systems. We present ASPIRE, a multi-agent framework designed to address these challenges by providing execution-free static analysis and laying the foundation for program repair. ASPIRE’s agents collaboratively simulate runtime traces, predict program states, and evaluate code correctness without requiring actual execution, making it particularly suited for LLM-generated code and agentic workflows. Experimental results on a curated dataset from Codeforces show ASPIRE achieving ~56% accuracy in verdict prediction, with a 33% improvement when employing multi-agent collaboration. While program repair remains an area of future work, ASPIRE demonstrates significant potential as a safeguard for dynamic, LLM-driven code generation systems.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825553","Large Language Models;Generative AI;OpenAI;GPT;Gemini;Multi-agent frameworks;Software Engineering;Code Generation;Code Repair;Code Evaluation;Static Analysis","Codes;Accuracy;Runtime;Large language models;Mission critical systems;Static analysis;Maintenance engineering;Robustness;Safety;Standards","","","","7","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Combining Logic and Large Language Models for Assisted Debugging and Repair of ASP Programs","R. Brancas; V. Manquinho; R. Martins","INESC-ID/IST - Universidade de Lisboa, Lisbon, Portugal; INESC-ID/IST - Universidade de Lisboa, Lisbon, Portugal; Carnegie Mellon University, Pittsburgh, USA","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","646","657","Logic programs are a powerful approach for solving NP-Hard problems. However, their declarative nature poses significant challenges in debugging. Unlike procedural paradigms, which allow for step-by-step inspection of program state, logic programs require reasoning about logical statements for fault localization. This complexity is especially significant in learning environments due to students' inexperience. We introduce FormHe, a novel tool that integrates logic-based techniques with Large Language Models (LLMs) to detect and correct issues in Answer Set Programming submissions. FormHe consists of two main components: a fault localization module and a program repair module. First, the fault localization module identifies specific faulty statements in need of modification. Next, FormHe applies program mutation techniques and leverages LLMs to repair the flawed code. The resulting repairs are then used to generate hints that guide students in correcting their programs. Our experiments with real buggy programs submitted by students show that FormHe accurately detects faults in 94% of cases and successfully repairs 58% of incorrect submissions.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988950","Fault Localization;Program Repair;Large Language Models;Answer Set Programming","Location awareness;Fault diagnosis;Software testing;NP-hard problem;Large language models;Answer set programming;Debugging;Machine learning;Maintenance engineering;Logic","","","","43","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Corpus Construction for Etiquette Entity and Relation Extraction in Ancient Chinese Texts Assisted by Large Language Models","Siriguleng","College of Computer Science and Technology, Inner Mongolia MINZU University, Tongliao, China",2025 IEEE 8th Information Technology and Mechatronics Engineering Conference (ITOEC),"23 Apr 2025","2025","8","","644","647","To address the issue of data scarcity in entity relationship extraction tasks within the domain of ancient texts and meet the needs of researchers in digital humanities, this paper proposes an automated data annotation method based on the large language model ChatGPT and prompt engineering. By treating the large language model as a “data annotator,” we developed an entity relationship extraction dataset tailored to ritual scenarios in ancient texts for the natural language processing field. This dataset includes five entity types and ten relationship types, characterized by precise labeling. To verify the robustness and versatility of the dataset, comprehensive experiments were conducted on three pre-trained language models. The results demonstrate that pre-trained models based on prompt learning achieve the best performance in entity relationship extraction tasks. This study shows that the data annotation method leveraging large language models provides a feasible solution for automated data annotation, and the corpus of etiquette entity relationship extraction lays an important foundation for the construction of etiquette question answering system and knowledge graph of ancient books.","2693-289X","979-8-3315-2948-2","10.1109/ITOEC63606.2025.10968300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10968300","etiquette entity and relation extraction;ancient chinese texts;LLMs","Mechatronics;Annotations;Large language models;Knowledge graphs;Chatbots;Robustness;Question answering (information retrieval);Data mining;Prompt engineering;Labeling","","","","11","IEEE","23 Apr 2025","","","IEEE","IEEE Conferences"
"Harnessing the potential of conversational AI: A Roadmap for Academic Applications","S. Venkatraman; S. Brindha; G. Lakshmi; V. M. Selvi; J. Sandhya","Department of Electronics, Dolcera ITES Pvt. Ltd., Telangana, India; Department of Electronics and Communication Engineering, Sri Sai Ram Engineering College, Chennai; Department of Electronics and Communication Engineering, Sri Sai Ram Engineering College, Chennai; Department of Electronics and Communication Engineering, Sri Sai Ram Engineering College, Chennai; Department of Electronics and Communication Engineering, Sri Sai Ram Engineering College, Chennai","2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)","12 Dec 2024","2024","","","1","5","Academic institutions rely on classroom timetables for smooth functioning of classes. Creating timetables, manually, is a time-intensive process. An attempt is made to automate, or semi-automate the process of timetable preparation in the proposed work using conversational AI tool prompts. The prompts can be adjusted to best fit the needs of timetable generation. Another use case discussed in the work is related to summarization of large reports using conversational AI tools. ChatGPT and Claude AI have been utilized to test the said use cases. GPT 4 1106 preview version was used for timetable generation while Claude AI 2.1 was used for report summarization in the work. Results indicate good accuracy and relevancy in the AI-generated outputs, suggesting that conversational AI could alleviate academicians' daily workload to certain extent.","","979-8-3315-0884-5","10.1109/ICPECTS62210.2024.10780187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10780187","AI;ChatGPT;Claude AI;Conversational AI tools;Prompt engineering;Timetable generation;Report summarization","Accuracy;Control systems;Chatbots","","","","13","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"Exploring the Potential of Offline LLMs in Data Science: A Study on Code Generation for Data Analysis","A. Nikolakopoulos; A. Litke; A. Psychas; E. Veroni; T. Varvarigou","School of Electrical and Computer Engineering, Institute of Communication and Computer Systems, National Technical University of Athens, Zografou, Greece; School of Electrical and Computer Engineering, Institute of Communication and Computer Systems, National Technical University of Athens, Zografou, Greece; School of Electrical and Computer Engineering, Institute of Communication and Computer Systems, National Technical University of Athens, Zografou, Greece; Research and Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; School of Electrical and Computer Engineering, Institute of Communication and Computer Systems, National Technical University of Athens, Zografou, Greece",IEEE Access,"17 Apr 2025","2025","13","","64087","64114","Large Language Models (LLMs) have recently attracted considerable attention from the scientific community, due to their advanced capabilities and potential to serve as vital tools across various industries and academic fields. An important implementation domain for LLMs is Data Science, in which they could enhance the efficiency of Data Analysis and Profiling tasks. With the utilization of LLMs in Data Analytics tools, end-users could directly issue data analysis queries in natural language, bypassing the need for specialized user interfaces. However, due to the sensitive nature of certain data in some organizations, it is unwise to consider using established, cloud-based LLMs. This article explores the feasibility and effectiveness of a standalone, offline LLM in generating code for performing data analytics, given a set of natural language queries. A methodology tailored to a code-specific LLM is presented, evaluating its performance in generating Python Spark code and successfully producing the desired result. The model is assessed on its efficiency and ability to handle natural language queries of varying complexity, exploring the potential for wider adoption of offline LLMs in future data analysis frameworks and software solutions.","2169-3536","","10.1109/ACCESS.2025.3556973","European Commission through the Horizon Europe Program through the Project “Reliable biomeTric tEchNologies to asSist Police authorities in cOmbating terrorism and oRganized crime (TENSOR)”(grant numbers:101073920); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10947006","Code generation;data analysis;data profiling;data science;large language models","Codes;Large language models;Natural languages;Data models;Data science;Software;Benchmark testing;Analytical models;Training;Technological innovation","","","","83","CCBY","1 Apr 2025","","","IEEE","IEEE Journals"
"Automated Functionality and Security Evaluation of Large Language Models","M. Ding; Y. Shen; M. Chen","Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China",2024 9th IEEE International Conference on Smart Cloud (SmartCloud),"24 Jun 2024","2024","","","37","41","Natural language processing (NLP) is rapidly developing. A series of Large Language Models (LLMs) have emerged, represented by ChatGPT, which have made significant breakthroughs in natural language understanding and generation, enabling fluent dialogue with humans, understanding human intentions, and completing complex tasks. However, in addition to the fairness and toxicity of traditional language models, some new problems, including hallucination, have also emerged in LLMs, making them hard to use. Evaluating LLMs manually is challenging due to subjectivity and inefficiency. In this paper, we focused on the fuzzy matching, toxicity detection, and hallucination detection in the evaluation of LLMs automatically, and fine-tune the Mixtral-8x7B Model, which can be deployed in private cloud environment, and prove the effectiveness of our method through experiments.","","979-8-3503-8950-0","10.1109/SmartCloud62736.2024.00014","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10566308","LLM;evaluation;fuzzy matching;toxicity;hallucination","Cloud computing;Toxicology;Accuracy;Graphics processing units;Chatbots;Security;Task analysis","","1","","23","IEEE","24 Jun 2024","","","IEEE","IEEE Conferences"
"Event-Triggered Optimal Control for Human in the Loop Multi-Agent Systems Under FDI Attacks","Y. Xu; H. Liang; T. Li; Y. Long; Q. Liu; X. Yang; Z. Huang","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Engineering, China University of Petroleum-Beijing at Karamay, Karamay, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China",2024 6th International Conference on Electronic Engineering and Informatics (EEI),"8 Oct 2024","2024","","","622","626","This paper investigates event-triggered optimal control for human-in-the-loop (HiTL) multi-agent systems (MASs) facing false data injection (FDI) attacks. An approach is introduced to enhance the security of sensor measurements against sparse attacks by employing a secure preselector, while also devising a state observer to estimate the system state, which is not directly measurable. Introducing an event-driven mechanism for solution transformation involves establishing a Hamilton-Jacobi-Bellman (HJB) equation, while employing a critic network along-side an experience replay technique to approximate it. Lyapunov stability theory is employed to derive an event-triggered condition ensuring the boundedness of consensus error and critic network weight error.","","979-8-3503-5359-4","10.1109/EEI63073.2024.10696302","National Natural Science Foundation of China(grant numbers:51939001,62322307,62273072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696302","Human in the Loop (HiTL);multi-agent systems (MASs);optimal control;event-triggered control;false data injection (FDI) attacks","Optimal control;Transforms;Observers;Human in the loop;Security;Informatics;Multi-agent systems;Lyapunov methods","","","","25","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"TSDCG: Tabular Synthetic Data with Code Generation LLMs","A. Mathur; K. Shah","AI Engineering, Autonomik AI, Mumbai, India; AI Engineering, Autonomik AI, Toronto, Canada",2025 3rd International Conference on Inventive Computing and Informatics (ICICI),"15 Jul 2025","2025","","","1","5","In the past years, there has been significant research on generating synthetic data, especially using Generative AI. Data engineering and analysis play crucial roles for any business in the study of past trends and forecasting future trends. However, real-world data often contains biases, missing values, and duplications, which limits its effectiveness in variety of tasks. Such issues can lead to model overfitting, underfitting, and poor generalization while training. To address these challenges, there has been extensive research on various GAN and LLM architectures and their ability to generate high quality synthetic data. This paper introduces and investigates a novel approach of using code-generating LLMs and advanced data profiling, in generating high quality synthetic data to enhance the speed and quality of the data generated.","","979-8-3315-3830-9","10.1109/ICICI65870.2025.11069449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11069449","Synthetic Data Generation;Generative Artificial Intelligence (AI);LLM (Large Language Model);Code Generation;Machine Learning","Training;Codes;Generative AI;Large language models;Machine learning;Market research;Generative adversarial networks;Informatics;Synthetic data;Overfitting","","","","23","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Benchmarking Large Language Models for Ethereum Smart Contract Development","E. Daspe; M. Durand; J. Hatin; S. Bradai","Orange Innovation, Caen, France; Orange Innovation, Caen, France; Orange Innovation, Caen, France; Sofrecom, Orange Innovation, Tunisia",2024 6th Conference on Blockchain Research & Applications for Innovative Networks and Services (BRAINS),"8 Nov 2024","2024","","","1","4","The integration of blockchain technology, particularly Ethereum and its smart contract, has revolutionized software programming. Solidity, Ethereum’s main language, is crucial due to its features for blockchain applications. However, the immutable nature of Smart Contract (SC) presents significant security issues, with vulnerabilities leading to financial risks. Meanwhile, Large language models (LLMS ) have transformed software development by enhancing coding efficiency and error detection. Despite their potential, current benchmarks often overlook niche languages like Solidity. This paper introduces the first benchmark to evaluate LLMs in Solidity smart contract generation, aiming to improve automated SC development and blockchain deployment reliability using a Test-Driven Development inspired methodology and pass@k metric. This work not only addresses a significant gap in LLM evaluation for blockchain applications but also extends the capabilities of LLMs in this specialized and critical area of software development.","2835-3021","979-8-3503-6784-3","10.1109/BRAINS63024.2024.10732686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10732686","Smart contracts;Generative AI;Dataset;Benchmark;Ethereum;Solidity;LLM","Measurement;Large language models;Smart contracts;Benchmark testing;Programming;Software;Filling;Software reliability;Security;Software development management","","","","23","IEEE","8 Nov 2024","","","IEEE","IEEE Conferences"
"Resilient Distributed Consensus in Multi-Agent Systems: Enhancing Transient Performance Under Communication Link Attacks","Y. Yin; Y. Xie; Q. Su","School of Automation Engineering, Northeast Electric Power University, Jilin, China; School of Automation Engineering, Northeast Electric Power University, Jilin, China; School of Automation Engineering, Northeast Electric Power University, Jilin, China","2024 7th International Conference on Robotics, Control and Automation Engineering (RCAE)","14 Jan 2025","2024","","","51","54","This article investigates the resilient cooperative output regulation (RCOR) challenge within a class of nonlinear multi-agent systems (MASs) featuring. The primary focus is on upholding predefined performance criteria, even in the face of potential communication link disruptions. This research addresses the limitations inherent in the tracking accuracy and the characteristics of the transient response of existing cooperative control methodologies for MASs, as well as the vulnerability of their communication frameworks to security threats. To this end, the paper introduces an innovative observer with prescribed performance and a robust control architecture that integrates backstepping and dynamic surface control (DSC) mechanisms, effectively mitigating the impact of communication link attacks. The theoretical robustness of the proposed control framework is substantiated through Lyapunov stability analysis, and the effectiveness is illustrated through an extensive simulation study.","","979-8-3503-5564-2","10.1109/RCAE62637.2024.10834252","Education Department of Jilin Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10834252","Multi-agent systems (MASs);prescribed performance;resilient control;communication link attack","Transient response;Backstepping;Accuracy;Observers;Robustness;Regulation;Security;Transient analysis;Multi-agent systems;Lyapunov methods","","","","10","IEEE","14 Jan 2025","","","IEEE","IEEE Conferences"
"SPELL: An End-to-End Tool Flow for LLM-Guided Secure SoC Design for Embedded Systems","S. Paria; A. Dasgupta; S. Bhunia","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",IEEE Embedded Systems Letters,"5 Dec 2024","2024","16","4","365","368","Modern embedded systems and Internet of Things (IoT) devices contain system-on-chips (SoCs) as their hardware backbone, which increasingly contain many critical assets (secure communication keys, configuration bits, firmware, sensitive data, etc.). These critical assets must be protected against wide array of potential vulnerabilities to uphold the system’s confidentiality, integrity, and availability. Today’s SoC designs contain diverse intellectual property (IP) blocks, often acquired from multiple 3rd-party IP vendors. Secure hardware design using them inevitably relies on the accrued domain knowledge of well-trained security experts. In this letter, we introduce SPELL, a novel end-to-end framework for the automated development of secure SoC designs. It leverages conversational large language models (LLMs) to automatically identify security vulnerabilities in a target SoC and map them to the evolving database of common weakness enumerations (CWEs); SPELL then filters the relevant CWEs, subsequently converting them to systemverilog assertions (SVAs) for verification; and finally, addresses the vulnerabilities via centralized security policy enforcement. We have implemented the SPELL framework using popular LLMs, such as ChatGPT and GEMINI, to analyze their efficacy in generating appropriate CWEs from user-defined SoC specifications and implement corresponding security policies for an open-source SoC benchmark. We have also explored the limitations of existing pretrained conversational LLMs in this context.","1943-0671","","10.1109/LES.2024.3447691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10779517","Assertion-based verification (ABV);common weakness enumerations (CWEs);large language models (LLMs);security policies;system-on-chip (SoC) security","Embedded systems;Prevention and mitigation;Large language models;Network-on-chip;Manuals;Intellectual property;Hardware;Security;Internet of Things;Microprogramming","","1","","13","IEEE","5 Dec 2024","","","IEEE","IEEE Journals"
"Reward Design Framework Based on Reward Components and Large Language Models","K. Jin; G. Tian; B. Huang; Y. Cui; X. Zheng","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)","4 Mar 2025","2024","","","278","282","The application of Large Language Models (LLMs) in the field of robotics has gained widespread attention. Due to their powerful code generation and contextual understanding capabilities, these models can generate reward functions from task commands, prompts, and environmental code, thus aiding robots in acquiring skills through reinforcement learning (RL). However, reward functions generated by existing models often suffer from issues of low executability and success rates, particularly when handling vague or complex task commands. These models usually require multiple iterations of optimization to achieve the desired outcomes. To address this challenge, we propose a reward function generation method based on reward components, which leverages human-level prior knowledge to improve generation efficiency and accuracy. Specifically, we have constructed a task→reward components dataset and fine-tuned a Reward Component Generator (RCG) using this dataset. The RCG then guides the automatic generation of reward functions for reinforcement learning tasks. Experimental results demonstrate that our method significantly improves reward executability, accuracy, and task success rate compared to state-of-the-art approaches. For more information, please visit our project website at: https://jkx-yy.github.io/RCG/","","979-8-3315-3122-5","10.1109/ICAIRC64177.2024.10899926","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10899926","robot manipulation skills;reinforcement learning;LLMs;reward function;reward components","Codes;Accuracy;Service robots;Large language models;Reinforcement learning;Learning (artificial intelligence);Generators;Planning;Optimization;Context modeling","","","","15","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning","S. Haldar; M. Pierce; L. Fernando Capretz","School of Information Technology, Fanshawe College, London, ON, Canada; Faculty of Business, Information Technology and Part-Time Studies, Fanshawe College, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada",IEEE Access,"19 Mar 2025","2025","13","","46070","46090","Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.","2169-3536","","10.1109/ACCESS.2025.3545882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904141","Capstone project;ChatGPT;generative AI;software testing education;Microsoft Copilot;sentiment analysis","Software testing;Education;Generative AI;Industries;Chatbots;Software engineering;Sentiment analysis;Large language models;Accuracy;Systematic literature review","","","","51","CCBY","26 Feb 2025","","","IEEE","IEEE Journals"
"Traj-LLM: A New Exploration for Empowering Trajectory Prediction With Pre-Trained Large Language Models","Z. Lan; L. Liu; B. Fan; Y. Lv; Y. Ren; Z. Cui","School of Transportation Science and Engineering, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China; Beijing Key Laboratory of Traffic Engineering, College of Metropolitan Transportation, Beijing University of Technology, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China",IEEE Transactions on Intelligent Vehicles,"30 Jul 2025","2025","10","2","794","807","Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving. Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of complex traffic semantics. This paper proposes Traj-LLM, the first to investigate the potential of using pre-trained Large Language Models (LLMs) without explicit prompt engineering to generate future motions from vehicular past trajectories and traffic scene semantics. Traj-LLM starts with sparse context joint encoding to dissect the agent and scene features into a form that LLMs understand. On this basis, we creatively explore LLMs' strong understanding capability to capture a spectrum of high-level scene knowledge and interactive information. To emulate the human-like lane focus cognitive function and enhance Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the Mamba module. Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant predictions. Extensive experiments manifest that Traj-LLM, fueled by prior knowledge and understanding prowess of LLMs, together with lane-aware probability learning, transcends the state-of-the-art methods across most evaluation metrics. Moreover, the few-shot analysis serves to substantiate Traj-LLM's performance, as even with merely 50% of the dataset, it surpasses the majority of benchmarks relying on complete data utilization. This study explores endowing the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent movements in a new way.","2379-8904","","10.1109/TIV.2024.3418522","National Key Research and Development Project of China(grant numbers:2022YFB4300400); National Natural Science Foundation of China(grant numbers:52202378); Open Research Project Program of the State Key Laboratory of Internet of Things for Smart City(grant numbers:SKL-IoTSC(UM)-2021-2023/ORP/GA08/2022); Chunhui Collaboration Project Program of the Ministry of Education of China(grant numbers:202200650); Youth Talent Support Program of Beihang University(grant numbers:YWF-22-L-1239); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574364","Autonomous vehicles;large language models (LLMs);mamba;trajectory prediction","Trajectory;Task analysis;Predictive models;Autonomous vehicles;Forecasting;Encoding;Cognition","","14","","79","IEEE","27 Jun 2024","","","IEEE","IEEE Journals"
"Addressing Data Poisoning and Model Manipulation Risks using LLM Models in Web Security","S. P. Shah; A. V. Deshpande","School of Computer and Information Sciences, University of the Cumberlands, Kentucky, USA; School of Computer and Information Sciences, University of the Cumberlands, Kentucky, USA","2024 International Conference on Distributed Systems, Computer Networks and Cybersecurity (ICDSCNC)","1 Apr 2025","2024","","","1","6","Data poisoning and model manipulation represent significant threats to cybersecurity, where adversaries intentionally inject malicious data into training sets, leading to compromised machine learning models. This study explores the efficacy of using Large Language Models (LLMs) to detect and mitigate such risks in cybersecurity. We propose a novel LLM-based framework that analyzes incoming data streams, identifies anomalies indicative of poisoning, and performs real-time model correction to maintain the integrity of security systems. Leveraging the generative and analytical capabilities of LLMs, our approach dynamically adapts to evolving attack vectors by detecting subtle changes in data distributions. We evaluated the framework on a cybersecurity dataset with known data poisoning incidents, achieving an accuracy of 97.5% in identifying poisoned data and reducing model manipulation attempts by 85% compared to conventional machine learning models. Additionally, our method exhibited a 30% improvement in response time to attacks, ensuring faster detection and mitigation. The study demonstrates that integrating LLMs into cybersecurity systems can significantly enhance resilience against sophisticated adversarial attacks.","","979-8-3503-7544-2","10.1109/ICDSCNC62492.2024.10941696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10941696","large language models (LLMs);data poisoning and model manipulation;data distributions","Adaptation models;Accuracy;Large language models;Prevention and mitigation;Machine learning;Data models;Real-time systems;Time factors;Computer security;Resilience","","","","19","IEEE","1 Apr 2025","","","IEEE","IEEE Conferences"
"A Novel Prompt Engineering Framework of Large Language Models for Detecting Gout Flares","L. Qian; Q. Hu; Q. Xie; E. Y. Chock; H. Xu; H. Na","Department of Biomedical Informatics and Data Science, School of Medicine, Yale University, New Haven, CT, USA; School of Public Health, Yale University, New Haven, CT, USA; Department of Biomedical Informatics and Data Science, School of Medicine, Yale University, New Haven, CT, USA; Section of Rheumatology, School of Medicine, Yale University, New Haven, CT, USA; Department of Biomedical Informatics and Data Science, School of Medicine, Yale University, New Haven, CT, USA; Department of Biomedical Informatics and Data Science, School of Medicine, Yale University, New Haven, CT, USA",2025 IEEE 13th International Conference on Healthcare Informatics (ICHI),"22 Jul 2025","2025","","","343","351","Gout is a prevalent condition impacting human health globally, frequently causing acute flares that necessitate emergency department (ED) visits. Despite the high incidence of ED encounters for gout, patients often fail to receive adequate continuity of care after discharge. Natural language processing (NLP) models have been employed to identify gout flares during ED visits to provide appropriate outpatient gout care. However, their effectiveness is constrained by the limited availability of annotated data. Traditional models, which rely on large, labeled datasets for training, face significant challenges in adapting to the resource-scarce environment of clinical settings. In this study, we leverage large language models (LLMs) to detect gout flares based on chief complaints, which are often the first available clinical information during ED visits. To enhance LLM performance, we propose a novel prompt engineering framework that systematically improves detection accuracy through the development of tailored prompts. Specifically, we introduce a generic prompt and refine it using gout flare detection guidelines to create a guideline-based prompt. Further, we analyze the errors from the guideline-based prompt and incorporate these insights to develop an error-case-refined prompt. Results from two gout datasets demonstrate that with well-designed prompts, LLMs outperform fine-tuned BERT models in detecting gout flares, even with a limited number of annotated cases. These findings highlight the immense potential of LLMs in gout flare and also other clinical applications, paving the way for improved performance through continued development and domain-specific adaptation.","2575-2634","979-8-3315-2094-6","10.1109/ICHI64645.2025.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081527","Large language model;gout detection;chief complaint;prompt engineering","Training;Adaptation models;Accuracy;Large language models;Reliability engineering;Natural language processing;Prompt engineering;Informatics;Faces;Guidelines","","","","30","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"AgoneTest: Automated creation and assessment of Unit tests leveraging Large Language Models","A. Lops; F. Narducci; A. Ragone; M. Trizio","Polytechnic University of Bari - Wideverse, Italy; Polytechnic University of Bari, Italy; University of Bari, Italy; Wideverse, Italy",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2440","2441","Software correctness is crucial, with unit testing playing an indispensable role in the software development lifecycle. However, creating unit tests is time-consuming and costly, underlining the need for automation. Leveraging Large Language Models (LLMs) for unit test generation is a promising solution, but existing studies focus on simple, small-scale scenarios, leaving a gap in understanding LLMs’ performance in real-world applications, particularly regarding integration and assessment efficacy at scale. Here, we present AgoneTest, a system focused on automatically generating and evaluating complex class-level test suites. Our contributions include a scalable automated system, a newly developed dataset for rigorous evaluation, and a detailed methodology for test quality assessment.CCSCONCEPTS• Software and its engineering → Automatic programming; Software testing and debugging.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764870","Software Testing;Large Language Model;Automatic Assessment","Software testing;Automation;Automatic programming;Large language models;Debugging;Software;Test pattern generators;Software engineering;Software development management","","","","6","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Broken Bags: Disrupting Service Through the Contamination of Large Language Models With Misinformation","Y. Mo; M. Tang; R. Lin; B. Zhou; X. Li","School of Information Engineering, Guilin Institute of Information Technology, Guilin, China; School of Information Engineering, Guilin Institute of Information Technology, Guilin, China; School of Information Engineering, Guilin Institute of Information Technology, Guilin, China; School of Information Engineering, Guilin Institute of Information Technology, Guilin, China; School of Electronic Engineering, Guilin Institute of Information Technology, Guilin, China",IEEE Access,"30 Jun 2025","2025","13","","109607","109623","Large language models (LLMs) have progressively become essential production tools in contemporary society, owing to their formidable natural language generation and contextual reasoning skills. To facilitate the development of current responses by LLMs, individuals have used retrieval-augmented generation (RAG) technology, which extracts material from the corpus to assist large language models in producing relevant replies. The extensive utilization of huge language models necessitates urgent RAG security research. Conventional RAG attack techniques exhibit inadequate hiding and a substantial volume of harmful messages. Consequently, we have introduced an innovative attack mechanism termed “Broken Bags,” which adeptly injects a minimal quantity of toxic text to mislead large language models. The attack is executed through a hybrid approach that incorporates artificial prompt templates, toxic content generated by LLMs, and filtering mechanisms. For instance, when the RAG system engages with publicly available knowledge bases, adversaries can take advantage of the accessibility of these RAG knowledge bases to introduce malicious texts into the retrieval database, so as to intentionally alter the model’s behavior. This work employs the linguistic similarity between toxic content and the geographical vector characteristics of the “query question” to influence the information returned by RAG, hence preventing the LLM from generating responses to the target questions. We developed and refined an artificial prompt template to render toxic language more akin to authentic human expressions and less detectable. Experimental data indicates that our attack success rate attains 94%. Ultimately, we systematically evaluate state-of-the-art defenses (including perplexity-based detection and knowledge extension, among others), and the findings indicate that these measures are unable to counter “Broken Bags,” hence significantly enhancing the success rate of assaults on RAG systems.","2169-3536","","10.1109/ACCESS.2025.3582519","Sangfor Technologies Company Ltd; 2023 Ministry of Education of China’s Second Phase of Supply and Demand Docking Employment Education Project through Guilin Institute of Information Technology Integration Collaborative Education Order Talent Training Exploration and Practice(grant numbers:20230106491); Innovation and Entrepreneurship Training Program for Chinese College Students in 2023: Application Research of Short-Term Weather Prediction Based on Artificial Intelligence LSTM Model through Guilin Institute of Information Technology(grant numbers:202313644001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11048579","Information security;information retrieval;large language models;RAG attack;prompt injection attacks","Security;Large language models;Internet;Knowledge based systems;Databases;Computer crime;Information technology;Generators;Fake news;Writing","","","","85","CCBY","23 Jun 2025","","","IEEE","IEEE Journals"
"Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation","S. Karpurapu; S. Myneni; U. Nettur; L. S. Gajja; D. Burke; T. Stiehm; J. Payne","Digital Innovation, SQAC, Ashburn, VA, USA; Department of Information Technology and Management, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, BML Munjal University, Gurugram, Haryana, India; Coveros Inc., Fairfax, VA, USA; Coveros Inc., Fairfax, VA, USA; Coveros Inc., Fairfax, VA, USA",IEEE Access,"30 Apr 2024","2024","12","","58715","58721","Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.","2169-3536","","10.1109/ACCESS.2024.3391815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506519","Agile software development;natural language processing;prompt engineering;software testing;test case automation;automated acceptance testing;zero-shot and few-shot;GPT-35 and GPT-4;PaLM-2;Llama-13B;cucumber;generative AI","Calculators;Testing;Best practices;Syntactics;Automation;Test pattern generators;Task analysis;Agile software development;Behavioral sciences;Large language models;Software testing","","10","","39","CCBY","22 Apr 2024","","","IEEE","IEEE Journals"
"Generative AI Enabled Actionable Decision Support in Cyber Security Operations for Enterprise Security","B. Saurabh; S. Utkrisht; S. Sandeep; D. Pankaj Kumar; U. Rajkumar","Centre for Development of Telematics, India; Centre for Development of Telematics, India; Centre for Development of Telematics, India; Centre for Development of Telematics, India; Centre for Development of Telematics, India",2024 ITU Kaleidoscope: Innovation and Digital Transformation for a Sustainable World (ITU K),"13 Dec 2024","2024","","","1","8","In the evolving cyber threat landscape, enterprises employ multiple security solutions such as Endpoint Detection and Response (EDR), Security Information and Event Management (SIEM), and Security Orchestration, Automation, and Response (SOAR). Security analysts are inundated with millions of security event logs from such security tools that makes it increasingly complex to manage and analyze these huge data effectively. Further, there is unavailability of dedicated as well as skilled manpower who can understand and analyse such security events. This paper proposes a novel approach based on generative AI using the state-of-the-art Mistral-7B language model to generate clear and actionable security response messages from these event logs. We demonstrate that this cutting-edge language model can translate complex logs into human-understandable security insights which can enhance analysts’ ability to prioritize and respond to threats.","","978-92-61-39091-4","10.23919/ITUK62727.2024.10772892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772892","Enterprise Security;Cyber Security Operations;Generative AI;Security Information and Event Management (SIEM)","Technological innovation;Automation;Generative AI;Large language models;Digital transformation;Natural language processing;ITU;Security;Floods;Computer crime","","","","18","","13 Dec 2024","","","IEEE","IEEE Conferences"
"Survey on Security Concepts to Adapt Flexible Manufacturing and Operations Management based upon Multi-Agent Systems","S. Braun; C. -T. Cheng; S. Dowey; J. Wollert","University of Applied Sciences Aachen, Goethestraße 1, Aachen, Germany; RMIT University, 124 La Trobe Street, Melbourne VIC, Australia; RMIT University, 124 La Trobe Street, Melbourne VIC, Australia; University of Applied Sciences Aachen, Goethestraße 1, Aachen, Germany",2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),"30 Jul 2020","2020","","","480","484","The increasing digitalization brings new opportunities but also puts new challenges to modern industrial systems. Software agents are one of the key technologies towards self-optimizing factories and are currently used to address the needs of cyber-physical production systems (CPPS). However their interplay in industrial settings needs to be understood better.This paper focusses on securing a cloud infrastructure for multi-agent systems for industrial sites. An industrial site contains multiple production processes that need to communicate with each other and each physical resource is abstracted with a software agent. This volatile architecture needs to be managed and protected from manipulation. The proposed infrastructure presents a security concept for TCP/IP communication between agents, machines, and external networks. It is based on open-source software and tested on a three-node edge cloud controlling a model-plant.","2163-5145","978-1-7281-5635-4","10.1109/ISIE45063.2020.9152210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152210","industrial agents;security;cloud technologies;digital factory;mulit-agent systems;cyber-physical production systems","Encryption;Time factors;Servers;Protocols;Cloud computing","","6","","23","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study","M. F. Qharabagh; Z. Dehghanian; H. R. Rabiee",Sharif University of Technology; Sharif University of Technology; Sharif University of Technology,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Grapheme-to-phoneme (G2P) conversion is critical in speech processing, particularly for applications like speech synthesis. G2P systems must possess linguistic understanding and contextual awareness of languages with homograph words and context-dependent phonemes. Large language models (LLMs) have recently demonstrated significant potential in various language tasks, suggesting that their phonetic knowledge could be leveraged for G2P. In this paper, we evaluate the performance of LLMs in G2P conversion and introduce prompting and post-processing methods that enhance LLM outputs without additional training or labeled data. We also present a benchmarking dataset designed to assess G2P performance on sentence-level phonetic challenges of the Persian language. Our results show that by applying the proposed methods, LLMs can outperform traditional G2P tools, even in an underrepresented language like Persian, highlighting the potential of developing LLM-aided G2P systems.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888370","Grapheme-to-Phoneme Conversion (G2P);Large Language Models (LLMs);Prompt Engineering;Bench-marking Dataset;Context-Sensitive Phonemes","Training;Measurement;Dictionaries;Large language models;Benchmark testing;Phonetics;Signal processing;Licenses;Speech synthesis;Context modeling","","","","37","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Fault-Tolerant Differential Privacy Routing of Human-Cyber-Physical Fusion Systems for Large Language Models Security","L. Lin; Y. Huang; X. Wang; S. Garg; S. Moussa; M. Alrashoud","Fujian Provincial Key Laboratory of Network Security and Cryptology, College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; School of Computer Science and Mathematics, Fujian University of Technology, Fuzhou, China; Fujian Provincial Key Laboratory of Network Security and Cryptology, College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China; Electrical Engineering Department, École de technologie supérieure, Montréal, QC, Canada; Applied Science and Technology, School of Engineering, Canadian University, Dubai; Department of Software Engineering (SWE), College of Computer and Information Sciences (CCIS), King Saud University, Riyadh, Saudi Arabia",IEEE Internet of Things Journal,"","2025","PP","99","1","1","The rapid proliferation of Internet of Things (IoT) systems has introduced complex networks of interconnected devices, computational resources, and web-based communication infrastructure. Privacy protection in IoT data routing is critical to enabling secure deployment of large language models (LLMs) for processing distributed sensor data, user queries, and device-generated content. However, IoT environments inherently involve heterogeneous devices, dynamic network topologies, and resource-constrained nodes, complicating the design of privacy-preserving routing mechanisms that simultaneously ensure reliability across diverse communication layers. To address these challenges, we propose an innovative FtPR (Fault-tolerant Privacy Routing) model based on secure multiparty computing mechanism, which enables secure and efficient data fusion and transmission in IoT networks. FtPR establishes a novel connection between IoT device clusters and data center network architecture AQDNn routers, leveraging the hierarchical architecture of AQDNn to construct completely independent spanning trees (CIST). By exploiting the non-overlapping paths between nodes in distinct CISTs, FtPR achieves fault-tolerant routing while maintaining privacy guarantees. Building on this framework, we introduce a secure multiparty computing mechanism to perturb link weights in the AQDNn. This ensures that link weights across different CISTs adhere to constrained ranges, preventing adversarial inference of routing paths. Each node operates with localized knowledge of its connected link weights, eliminating the need for global network visibility. Consequently, even if malicious actors compromise one or multiple nodes, they cannot reconstruct end-to-end communication paths, thereby preserving route anonymity. Experimental results demonstrate that FtPR improves IoT network performance and security, reducing misclassification rates and marginal release score compared to state-of-the-art methods.","2327-4662","","10.1109/JIOT.2025.3564766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10978008","Large Language Models Security;Privacy Routing;Secure Multiparty Computing;Completely Independent Spanning Trees","Internet of Things;Routing;Security;Fault tolerant systems;Fault tolerance;Privacy;Protection;Autonomous aerial vehicles;Routing protocols;Real-time systems","","","","","IEEE","28 Apr 2025","","","IEEE","IEEE Early Access Articles"
"The System Design Methodology of an Interactive Big Data Platform Based on Universal LLMs","Z. Zhang; L. Tang; Y. Bai; X. Yu; K. Yang; P. Jiang","Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China; Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China; Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China; Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China; Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China; Research and Development Department I, China Mobile Chengdu Institute of Research and Development, Chengdu, China",2024 5th International Conference on Information Science and Education (ICISE-IE),"12 Jun 2025","2024","","","40","44","This work elaborates on the design and implementation of an interactive big data platform system based on a universal LLM, including the main technical implementation plans and module designs of the system. The system employs a stepwise Text-To-SQL approach, utilizing few-shot techniques, RAG, and multi-source heterogeneous data access technologies, which can significantly enhance the direct executability of SQL. Based on this research, we have upgraded the original China Mobile Smart University Big Data Platform, empowered by AI to provide interactive data querying capabilities. This addresses the high professional barriers in data querying and analysis, and the weak front-end user perception of traditional big data platforms. It enables more people to harness the power of data.","","979-8-3315-0676-6","10.1109/ICISE-IE64355.2024.11025394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025394","Bigdata;Text to SQL;Large Language Models;database;SQL","Industries;Structured Query Language;Information science;Databases;Large language models;Education;Big Data;Data models;Usability;System analysis and design","","","","7","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Mitigating Insecure Outputs in Large Language Models(LLMs): A Practical Educational Module","M. A. Barek; M. M. Rahman; S. Akter; A. B. M. K. Islam Riad; M. A. Rahman; H. Shahriar; A. Rahman; F. Wu","Department of Intelligent Systems and Robotics, University of West Florida, USA; Dept. of Cybersecurity and Information Technology, University of West Florida, USA; Department of Intelligent Systems and Robotics, University of West Florida, USA; Department of Intelligent Systems and Robotics, University of West Florida, USA; Department of Intelligent Systems and Robotics, University of West Florida, USA; Center for Cybersecurity, University of West Florida, USA; Computer Science and Software Engineering, Auburn University, USA; Dept. of Computer Science, Tuskegee University, USA","2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)","26 Aug 2024","2024","","","2424","2429","Large Language Models (LLMs) have extensive ability to produce promising output. Nowadays, people are increasingly relying on them due to easy accessibility, rapid and outstanding outcomes. However, the use of these results without appropriate scrutiny poses serious security risks, particularly when they are integrated with other software, APIs, or plugins. This is because the LLM outputs are highly dependent on the prompts they receive. Therefore, it is essential to carefully clean these outputs before using them in additional software environments. This paper is designed to teach students about the potential dangers of contaminated LLM output within the context of web development through prelab, hands-on, and postlab experiences. Hands-on lab provides practical guidance on how to handle LLM vulnerabilities to make applications safe with some real-world examples in Python. This approach aims to provide students with a deeper understanding of the precautions necessary to ensure software against the vulnerabilities introduced by LLM output.","2836-3795","979-8-3503-7696-8","10.1109/COMPSAC61105.2024.00389","National Science Foundation(grant numbers:2100134,2433800,2310179,2209637,2421324,1946442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633444","Large Language Models;Cybersecurity;Insecure Output;Sanitization;Authentic Learning","Training;Navigation;Large language models;Computational modeling;Software;Security;Software development management","","1","","21","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"Toward a New Era of Rapid Development: Assessing GPT-4-Vision’s Capabilities in UML-Based Code Generation","G. Antal; R. Vozár; R. Ferenc","University of Szeged, Szeged, Hungary; University of Szeged, Szeged, Hungary; University of Szeged, Szeged, Hungary",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","84","87","The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model’s performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model’s potential completely.CCS CONCEPTS• Software and its engineering → Software prototyping.","","979-8-4007-0579-3","","Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734638","Large Language Models;Code Generation;OOP;UML;AI in Software Engineering","Software prototyping;Java;Codes;Source coding;Large language models;Unified modeling language;Neural networks;Software;Software engineering;Software development management","","","","10","","30 Oct 2024","","","IEEE","IEEE Conferences"
"An Evaluation of Reasoning Capabilities of Large Language Models in Financial Sentiment Analysis","K. Du; F. Xing; R. Mao; E. Cambria","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Information Systems and Analytics, National University of Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2024 IEEE Conference on Artificial Intelligence (CAI),"30 Jul 2024","2024","","","189","194","Large Language Models (LLMs) have garnered significant attention within the academic community due to their advanced capabilities in natural language understanding and generation. While empirical studies have shed light on LLMs’ proficiency in complex task reasoning, a lingering question persists in the field of Financial Sentiment Analysis (FSA): the extent to which LLMs can effectively reason about various financial attributes for FSA. This study employs a prompting framework to investigate this topic, assessing multiple financial attribute reasoning capabilities of LLMs in the context of FSA. By studying relevant literature, we first identified six key financial attributes related to semantic, numerical, temporal, comparative, causal, and risk factors. Our experimental results uncover a deficiency in the financial attribute reasoning capabilities of LLMs for FSA. For example, the examined LLMs such as PaLM-2 and GPT-3.5 display weaknesses in reasoning numerical and comparative attributes within financial texts. On the other hand, explicit prompts related to other financial attributes showcase varied utilities, contributing to LLMs’ proficiency in discerning financial sentiment.","","979-8-3503-5409-6","10.1109/CAI59869.2024.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605292","financial sentiment analysis;large language models;prompt engineering","Sentiment analysis;Analytical models;Large language models;Semantics;Cognition;Numerical models;Task analysis","","14","","52","IEEE","30 Jul 2024","","","IEEE","IEEE Conferences"
"Consensus of Fractional-Order Fuzzy Multi-Agent Systems: Exploring Impulsive Security Protocols","Y. Xu; Z. Zhang; X. Xie; R. M. Palhares; W. Li; Y. Wu","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, P.R. China; Department of Mathematics, Harbin Institute of Technology (Weihai), Weihai, P.R. China; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Electronics Engineering, Federal University of Minas Gerais, Belo, Horizonte, Brazil; Department of Mathematics, Harbin Institute of Technology (Weihai), Weihai, P.R. China; School of Automation, Southeast University, Nanjing, P.R. China",IEEE Transactions on Control of Network Systems,"","2025","PP","99","1","12","This paper investigates impulsive security protocols with the goal of achieving consensus of fractional-order fuzzy multi-agent systems (FOFMASs). In contrast to multi-agent systems characterized by ordinary differential equations, FOFMASs are created using T-S fuzzy rules and fractional-order differential equations, which allow for nonlinearities with parameter uncertainties and also manage memory effects. Due to the potential of denial-of-service (DoS) attacks, the topology among nodes may change, and it is necessary to gain insight into the security consensus under topology switching. This paper tackles the security protocol problem by designing a distributed impulsive control for each agent, relying on global delayed communications and topology switching resulting from DoS attacks. Analyzing the stability of impulse-free error systems is a salient problem. Supported by the Lyapunov method and average impulsive interval method, for two cases of self-stabilization or instability of the error system in the absence of impulse, some sufficient conditions are presented to ensure the exponential consensus of FOFMASs under distributed impulsive control with communication delays. Besides, several corollaries are provided to guarantee the consensus under distributed impulsive control without communication delays. Finally, two numerical examples are provided, and some numerical simulations are given to confirm the efficacy of the theoretical findings.","2325-5870","","10.1109/TCNS.2025.3543754","National Natural Science Foundation of China(grant numbers:62403250,62373196,62203114); Natural Science Foundation of Jiangsu(grant numbers:BK20240629,BK20231286,BK20220811); Key Laboratory of Industrial Internet of Things and Networked Control, Ministry of Education(grant numbers:2023FF05); China Postdoctoral Science Foundation(grant numbers:2024T170132); Jiangsu Funding Program for Excellent Postdoctoral Talent(grant numbers:2022ZB116); Natural Science Research Start-Up Foundation of Recruiting Talents of Nanjing University of Posts and Telecommunications(grant numbers:NY223167); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:24KJB120011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892296","Fractional-order fuzzy multi-agent systems;consensus;distributed impulsive control with communication delays;security control","Control systems;Delays;Switches;Security;Vehicle dynamics;Topology;Multi-agent systems;Denial-of-service attack;Electronic mail;Vectors","","1","","","IEEE","19 Feb 2025","","","IEEE","IEEE Early Access Articles"
"Enhancing Teaching Quality Through LLM: An Experimental Study on Prompt Engineering","E. Chen","Modern Educational Technology Center, Guangdong University of Foreign Studies, Guangzhou, China",2025 14th International Conference on Educational and Information Technology (ICEIT),"1 May 2025","2025","","","1","7","This study explores the application of Large Language Models Artificial Intelligence (LLM AI) in the assessment of course teaching quality, aiming to overcome the limitations of traditional teaching evaluation. Taking the experimental courses in School G as an example, we propose CORE, a framework to support generating feedback from student assessment during the course. Through the Solomon four-group design experiment, it validates the significant effectiveness of the teaching quality evaluation feedback generated by LLM in enhancing teachers' teaching quality. This feedback can help teachers improve teaching strategies, boost teaching effects, effectively make up for the limitations of traditional teaching assessment methods, and offer a new perspective and tool for teaching quality evaluation.","","979-8-3315-4088-3","10.1109/ICEIT64364.2025.10976127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976127","Prompt engineering;Large Language Models (LLM);Teaching evaluation;Prompt framework;Large Language Models","Large language models;Education;Prompt engineering;Information technology","","","","16","IEEE","1 May 2025","","","IEEE","IEEE Conferences"
"Meticulous Thought Defender: Fine-Grained Chain-of-Thought (CoT) for Detecting Prompt Injection Attacks of Large Language Models","L. Shi; Y. Kang; J. Hu; X. Li; M. Yang","China Telecom Research Institute, Beijing, China; China Telecom Research Institute, Beijing, China; China Telecom Research Institute, Beijing, China; China Telecom Research Institute, Beijing, China; China Telecom Research Institute, Beijing, China",IEEE Access,"4 Jul 2025","2025","13","","113194","113207","Large language models (LLMs) have exhibited exceptional capabilities across various natural language processing tasks, however, they remain susceptible to prompt injection attacks, which pose significant security challenges. Traditional detection methods often fail to effectively identify such attacks due to their reliance on static rules or surface-level analysis. In this study, we introduce a novel, fine-grained CoT based detection framework that enhances the interpretability and robustness of attack identification. By dissecting the step-by-step reasoning process of LLMs, our approach leverages multidimensional anomaly detection mechanisms-encompassing semantic analysis, consistency evaluations at both step and path levels, and confidence estimations to uncover subtle disruptions caused by malicious prompt manipulations. Experimental results demonstrate that our method achieves superior performance metrics, with significant improvements in accuracy and F1 score, outperforming traditional method by 1.16% in accuracy and 3.39% in F1 score, and surpassing LLMs’ intrinsic detection capabilities by 6.24% in accuracy and 7.65% in F1 score. This work not only fortifies the security of LLMs’ applications but also provides a foundational framework for future research on adaptive defenses against evolving attack strategies.","2169-3536","","10.1109/ACCESS.2025.3583759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11053836","Prompt injection detection;Chain-of-Thought (CoT);large language models (LLMs);natural language processing;model safety;cybersecurity","Cognition;Large language models;Security;Natural language processing;Accuracy;Anomaly detection;Semantics;Robustness;Logic;Systematics","","","","32","CCBY","27 Jun 2025","","","IEEE","IEEE Journals"
"Analyzing Automatic Code Generation for Learning Models in Generative AI","A. Jain; P. William; F. T. Ayasrah; G. P. Lakshmi; T. D. Diwan","10mind.ai (Advisor), California, USA; Deaprtment of Information Technology, Sanjivani College of Engineering, Amity University, Dubai, UAE; College of Education, Humanities and Science, Al Ain University, Al Ain, UAE; School of Computer Science and Engineering, Sandip University, Nashik, Maharashtra; Atal Bihari Vajpayee University, Bilaspur, India",2024 11th International Conference on Software Defined Systems (SDS),"18 Feb 2025","2024","","","50","58","Independent code generation models produced by generative AI provide a new way to software development. These models automatically generate code using machine learning based on input samples. This study examines the fundamentals, applications, problems, and future prospects of AI-related automated code generation technologies. Model-based software, domain-specific code, and testing procedures are examples of these research topics. Performance analysis and assessment are used to assess the efficacy, efficiency, and reliability of several automated code generating methods. The fact that these models have pros and cons and room for development is highlighted.","","979-8-3315-1832-5","10.1109/SDS64317.2024.10883930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10883930","Automatic code generation;Generative AI;machine learning;software development;testing methodologies;model-based development;domain-specific code generation","Analytical models;Codes;Generative AI;Training data;Software;Software reliability;Performance analysis;Research and development;Software development management;Testing","","3","","13","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"Can LLMs Generate Higher Quality Code Than Humans? An Empirical Study","M. T. Jamil; S. Abid; S. Shamail","Department of Computer Science, Lahore University of Management Sciences DHA, Lahore, Pakistan; Department of Software Engineering, National University of Computer and Emerging Sciences Chiniot-Faisalabad Campus, Pakistan; Department of Computer Science, Lahore University of Management Sciences DHA, Lahore, Pakistan",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","478","489","Large Language Models are being extensively used for AI-assisted programming and code generation. The challenge is to ensure that the generated code is not only functionally correct but also safe, reliable and trustworthy. In this direction, we conduct a comprehensive empirical analysis of AI-generated code to assess whether large language models (LLMs) can produce correct and higher-quality code than humans. We evaluate the code quality of 984 code samples generated by GPT-3.5-Turbo and GPT-4 using various prompt types (simple, instructional, and enhanced) against input queries from the HumanEval dataset. We also enhance the HumanEval benchmark by calculating code quality metrics for the human-written code it contains. Code quality metrics are calculated using established tools like Radon, Bandit, Pylint, and Complexipy, with human-written code serving as a baseline for comparison. To quantify performance, we employ the TOPSIS method to rank the models and human code by their proximity to ideal and anti-ideal code quality metrics. Our results demonstrate that GPT-4, when used with advanced prompts, produces code closest to the ideal solution, outperforming human-written code in several key metrics. Our work provides evidence that LLMs, when properly guided, can surpass human developers in generating high-quality code. Our code and datasets are available online.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025576","Large Language Models (LLMs);AI-assisted programming;code generation;code quality assessment;code quality metrics;trustworthy AI;GPT;HumanEval","Measurement;Codes;Large language models;Radon;Programming;Benchmark testing;Software;Quality assessment;Reliability;Data mining","","","","41","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Building Bridges, Not Walls: Fairness-Aware and Accurate Recommendation of Code Reviewers via LLm-Based Agents Collaboration","L. Wang; Q. Li; D. Cui; M. Wang; Y. Zhao; Y. Xu; H. Zhuang; Y. Zhou; L. Wang","Xidian University, Xi'an, China; Xidian University, Xi'an, China; Xidian University, Xi'an, China; Xidian University, Xi'an, China; University of Central Missouri, Warrensburg, United States; Xidian University, Xi'an, China; Xidian University, Xi'an, China; Xidian University, Xi'an, China; Xidian University, Xi'an, China",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","577","588","Code review is essential for maintenance of pull request-based software systems. Recommending suitable reviewers for code changes can enhance defect detection and knowledge dissemination. Despite extensive research, the inherent complexity of pull requests (PRs) and reviewer profiles continues to cause challenge for accurate matching them together. Furthermore, existing methods often amplify gender and racial/ethnic disparities due to the lack of attention to biases present in historical review records. To address these issues, we first collected a dataset from 4 large-scale open-source projects involving 50 -month revision history, reaching up to 30 attributes. This dataset includes gender and racial/ethnic information, which was inferred, validated, and incorporated to enable comprehensive data bias analysis in reviewer recommendation tasks. Additionally, we introduce a fairness-aware and accurate approach: CoReBM, which leverages the advanced semantic understanding capabilities of Large Language Models (LLMs) to comprehensively capture the nuanced textual context of both PRs and reviewers, utilizing the robust planning, collaborative, and decision-making abilities of multi-agent systems. CoReBM integrates diverse factors to improve recommendation performance while mitigating bias effects through the incorporation of candidates' gender and racial/ethnic attributes. We evaluate the effectiveness of our approach on this dataset, and the results demonstrate that CoReBM outperforms state-of-the-art methods in both accuracy and fairness in recommendation.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00067","National Natural Science Foundation of China(grant numbers:U21B2015,62372351); Xidian University Hangzhou Institute of Technology(grant numbers:XJ2023230039); Natural Science Foundation of Jiangsu Province(grant numbers:BK202302028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025855","code reviewer recommendation;large language models;multi-agent collaboration;data bias mitigation","Codes;Accuracy;Reviews;Large language models;Prevention and mitigation;Semantics;Collaboration;Software systems;Planning;Multi-agent systems","","","","83","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models","A. Nouri; B. Cabrero-Daniel; F. Törner; H. Sivencrona; C. Berger","Volvo Cars & Chalmers University of Technology, Gothenburg, Sweden; Department of Computer Science, University of Gothenburg, Gothenburg, Sweden; Volvo Cars, Gothenburg, Sweden; Zenseact, Gothenburg, Sweden; Department of Computer Science, University of Gothenburg, Gothenburg, Sweden",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","172","177","DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is ""Hazard Analysis & Risk Assessment"" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs). Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.CCS Concepts•Software and its engineering → Software verification and validation;•General and reference → Verification;•Computing methodologies → Natural language processing;•Computer systems organization → Dependable and fault-tolerant systems and networks.","","979-8-4007-0591-5","","Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556122","Hazard Analysis Risk Assessment;Autonomous Vehicles;DevOps;Safety;Large Language Model;Prompt Engineering;LLM;ChatGPT","Industries;Reviews;Natural languages;Fault tolerant systems;Organizations;Hazards;Software","","1","","17","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models","H. Tu; Z. Zhou; H. Jiang; I. N. B. Yusuf; Y. Li; L. Jiang","School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore",IEEE Transactions on Software Engineering,"17 Jul 2024","2024","50","7","1768","1788","Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in LLM4CBI can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies.","1939-3520","","10.1109/TSE.2024.3397822","National Natural Science Foundation of China(grant numbers:62032004,62302077); China Postdoctoral Science Foundation(grant numbers:2023M730472); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10521881","Software debugging;bug isolation;compilers;GCC;LLVM;reinforcement learning;large language models (LLMs)","Computer bugs;Program processors;Task analysis;Codes;Reinforcement learning;Production;Mathematical models","","4","","89","IEEE","7 May 2024","","","IEEE","IEEE Journals"
"LLMaaS: Serving Large-Language Models on Trusted Serverless Computing Platforms","Z. Cai; R. Ma; Y. Fu; W. Zhang; R. Ma; H. Guan","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Qingdao Institute of Software, College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Artificial Intelligence,"3 Mar 2025","2025","6","2","405","415","In recent years, the emergence of large-language models (LLMs) has profoundly transformed our production and lifestyle. These models have shown tremendous potential in fields, such as natural language processing, speech recognition, and recommendation systems, and are increasingly playing crucial roles in applications such as human–computer interaction and intelligent customer service. Efficient inference solutions for LLMs in data centers have been extensively researched, with a focus on meeting users’ quality of service requirements. In this article, we focus on two additional requirements that responsible LLM inference should meet under QoS conditions: security throughout the model execution process and low maintenance requirements for the inference system. Therefore, we propose LLMaaS, a trusted model inference platform based on a serverless computing platform aimed at providing inference as a service for LLMs. First, we design a trusted serverless computing platform based on software guard extension (SGX), which includes distributed identity verification and SGX device plugins to ensure the security and trustworthiness of the inference process. Additionally, to reduce the maintenance requirements of the system, we enhance the SGX-based deep learning computing framework, including replacing PyTorch and using a greedy algorithm for graph partitioning. We conduct tests on four typical large models, and the experimental results demonstrate that, with minimal overhead and user code modifications, we can ensure the security of model execution.","2691-4581","","10.1109/TAI.2024.3429480","Eighth Research Institute of China Aerospace Science and Technology Group Company, Ltd.(grant numbers:USCAST2023-17,USCAST2023-21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601537","Large-language models (LLMs);serverless computing;software guard extension (SGX)","Computational modeling;Transformers;Large language models;Decoding;Data models;Serverless computing;Security","","1","","39","IEEE","17 Jul 2024","","","IEEE","IEEE Journals"
"Towards Transparent Intrusion Detection: A Coherence-Based Framework in Explainable AI Integrating Large Language Models","A. Alnahdi; S. Narain","Miner School of Computer & Information Sciences, University of Massachusetts Lowell, Lowell, Massachusetts, USA; Miner School of Computer & Information Sciences, University of Massachusetts Lowell, Lowell, Massachusetts, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","87","96","Intrusion Detection Systems (IDS) are essential for maintaining cybersecurity by identifying potential threats within networks. However, the black-box nature of the Artificial Intelligence models used in many IDS deployments hinders their effectiveness. Explainable AI (XAI) methods have emerged to provide transparency, but their evaluation metrics—such as faithfulness, completeness, and stability—are generic and fail to measure coherence with domain-specific knowledge transparently. Domain-specific knowledge is typically expressed as natural language rules, yet manually applying these rules in real-time cybersecurity is labor-intensive, costly, and error-prone due to the high volume of incidents. This paper introduces a coherence evaluation metric designed to ensure XAI explanations align with IDS domain knowledge, aiding cybersecurity analysts in understanding and responding to incidents more effectively. We propose a framework that integrates Generative AI (GAI) with XAI to automate the coherence evaluation process, leveraging Large Language Models (LLMs) for optimal performance. Using the NSL-KDD and CICIDS2017 datasets, we demonstrate the effectiveness of our framework in improving the interpretability and trustworthiness of IDS predictions. Our findings revealed that the chain-of-thought (COT) prompting method achieved an 86% correctness rate in LLM responses for automating written rules towards the coherence metric. Additionally, our framework models written rules using colors, provides coherence or incoherence recommendations, improves the integration of XAI with LLMs, and ultimately advances human-computer interaction in security decision-making.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835438","Intrusion Detection Systems;Explainable AI;Coherence;Domain-specific Knowledge;Large Language Models;Automation;Prompt Engineering","Measurement;Human computer interaction;Explainable AI;Large language models;Decision making;Intrusion detection;Coherence;NSL-KDD;Real-time systems;Computer security","","","","35","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models and Prompt Settings for Context-Aware Financial Sentiment Analysis","R. Ahmed; S. A. Rauf; S. Latif","Department of Computer Science, Fatima Jinnah Women University, Rawalpindi, Pakistan; Department of Computer Science, Fatima Jinnah Women University, Rawalpindi, Pakistan; Department of Computer Science, SEECS, NUST, Islamabad, Pakistan",2024 5th International Conference on Advancements in Computational Sciences (ICACS),"21 Mar 2024","2024","","","1","9","Carefully crafted prompts can significantly enhance the accuracy and effectiveness of sentiment classification models. This paper explores the use of prompt engineering and large language models for financial sentiment analysis on financial reports of companies. Zero-shot and few-shot with prompts are designed to extract sentiment and contextual information. AI-generated synthetic examples were created for few-shot settings. Human-evaluated results are compared with four LLMs. Results show varying performance and output quality among LLMs, influenced by prompt design, report content, and task complexity. The LLMs’ responses varied in length, detail, and style, affecting their readability and usefulness. The paper discusses the implications and limitations of these findings, suggesting future research directions.","","979-8-3503-9478-8","10.1109/ICACS60934.2024.10473283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473283","Sentiment Analysis;Context Awareness;Large Language Models;Prompt Engineering","Sentiment analysis;Analytical models;Reviews;Computational modeling;Companies;Chatbots;Complexity theory","","9","","44","IEEE","21 Mar 2024","","","IEEE","IEEE Conferences"
"Sentiment and Response Priority Detection in Latvian E-mails Using Large Language Models: A Case Study for Low-Resource Languages","I. Birzniece; I. Andersone; J. Bicans; S. Balina","Institute of Applied Computer Systems Faculty of Computer Science Information Technology and Energy Riga, Technical University, Riga, Latvia; Institute of Applied Computer Systems Faculty of Computer Science Information Technology and Energy Riga, Technical University, Riga, Latvia; Institute of Applied Computer Systems Faculty of Computer Science Information Technology and Energy Riga, Technical University, Riga, Latvia; Datorzinibu centrs, Riga, Latvia",2025 11th International Conference on Computing and Artificial Intelligence (ICCAI),"11 Aug 2025","2025","","","333","341","This study investigates the capabilities of large language models (LLMs) in addressing sentiment analysis and response prioritization in Latvian texts, focusing on e-mails used in organizational workflows. In this work, we propose a response priority estimation schema based on detected severity, urgency, sentiment, and emotional level. To detect those attributes, we established a two-phase experimental approach for detecting urgency, severity, emotional intensity, and sentiment from e-mails using LLM with a zero-shot prompt technique. Considering the challenges associated with low-resource languages such as Latvian and the domain-specific requirement to maintain on-premise data processing, this study evaluates two LLMs: Meta’s Llama3 8B and Llama3.1 70B. Results highlight significant improvements in classification accuracy through refined prompt engineering, achieving up to 96-100% format accuracy, 74.6% for urgency, $\mathbf{7 4. 3 \%}$ for severity in two-class settings, and $\mathbf{9 3. 2 \%}$ and $\mathbf{9 5. 7 \%}$ for emotional intensity and sentiment detection, respectively. However, challenges persist, including discrepancies between human evaluations and model predictions. The findings emphasize the potential of LLMs for enhancing email management in lowresource language settings while outlining further improvement directions. This research advances the field of automated severity and urgency detection using LLMs, addressing a relatively underexplored area compared to the more extensively studied domains of sentiment analysis and affective computing, particularly in the context of low-resource languages.","","979-8-3315-2491-3","10.1109/ICCAI66501.2025.00058","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11106099","Large language models;Generative AI;Lowresource languages;Sentiment analysis;Emotion recognition;Context modeling;Machine learning","Sentiment analysis;Accuracy;Large language models;Employment;Organizations;Machine learning;Predictive models;Electronic mail;Prompt engineering;Information systems","","","","29","IEEE","11 Aug 2025","","","IEEE","IEEE Conferences"
"Türkçe Yazılmıs Tweet İletilerinin Afetle İlgili İki Asamalı Sınıflandırılmasında Büyük Dil Modellerinin Performans Karşılaştırması Performance Comparison of Large Language Models in Disaster Related Two-Stage Classification of Tweets Written in Turkish","E. Özcan; B. Beşer; E. Avcı; B. Kaya; A. K. Topallı","Elektrik ve Elektronik Müh. Böl., İzmir Ekonomi Üniversitesi, İstanbul, Türkiye; Elektrik ve Elektronik Müh. Böl., İzmir Ekonomi Üniversitesi, İstanbul, Türkiye; Elektrik ve Elektronik Müh. Böl., İzmir Ekonomi Üniversitesi, İstanbul, Türkiye; Elektrik ve Elektronik Müh. Böl., İzmir Ekonomi Üniversitesi, İstanbul, Türkiye; Elektrik ve Elektronik Müh. Böl., İzmir Ekonomi Üniversitesi, İstanbul, Türkiye",2024 Innovations in Intelligent Systems and Applications Conference (ASYU),"28 Nov 2024","2024","","","1","5","Natural disasters are very frequent in Turkiye, therefore it is quite vital to tackle the problems aroused after these disasters. This study proposes a system to reduce the losses caused by the natural disasters and provides a comparison method for the efficient selection of the system components. A database is formed from the tweet samples posted in the aftermath of the previous natural disasters and these tweets are classified in two stages using prompt engineering and large language models. In the first stage, the classification is done based on disaster type such as “earthquake”, “fire” or “flood”, then the tweets in these disaster types are classified for needs such as “search and rescue”, “equipment and food” in the second stage. In order to find the best model for aforementioned classifications, ChatGPT-3.5, fine-tuned ChatGPT-3.5 and ChatGPT-4 are selected and tested. Fine-tuned ChatGPT-3.5 with enhanced prompting is found to have the highest performance with 98.4% average success score for disaster classification. The success rate of the fine-tuned model for classification of needs is calculated as 95.6% in average. This study is expected not only to contribute to the Turkish language processing research area but also to support rescue organisations as well.","2770-7946","979-8-3503-7943-3","10.1109/ASYU62119.2024.10756988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756988","large language model;natural disaster;natural language processing;fine tuning;prompt engineering;Turkish;tweet;artificial intelligence","Technological innovation;Databases;Disasters;Large language models;Earthquakes;Prompt engineering;Floods;Intelligent systems","","","","0","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Large Language Models for Test-Free Fault Localization","A. Z. H. Yang; C. L. Goues; R. Martins; V. J. Hellendoorn","Carnegie Mellon University, Pittsburgh, United States; Carnegie Mellon University, Pittsburgh, United States; Carnegie Mellon University, Pittsburgh, United States; Carnegie Mellon University, Pittsburgh, United States",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","165","176","Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the $Defects4\mathcal{J}$ corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3623342","ANI(grant numbers:045917); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548193","Software and its engineering → Software functional properties;Computing methodologies → Neural networks","Location awareness;Deep learning;Training;Adaptation models;Codes;Computer bugs;Manuals","","31","","50","CCBY","14 Jun 2024","","","IEEE","IEEE Conferences"
"The Impact of Generative AI on Islamic Studies: Case Analysis of ""Digital Muhammad ibn Ismail Al-Bukhari""","A. El Ganadi; S. Aftar; L. Gagliardelli; S. Bergamaschi; F. Ruozzi","Department of Education and Humanities, University of Modena and Reggio Emilia, Reggio Emilia, Italy; Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Department of Education and Humanities, University of Modena and Reggio Emilia, Reggio Emilia, Italy",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","179","187","The emergence of large language models (LLMs) such as ChatGPT, LLaMA, Gemini, and Claude has transformed natural language processing (NLP) tasks by demonstrating remarkable capabilities in generating fluent and contextually appropriate responses. This paper examines the current state of LLMs, their applications, inherent challenges, and potential future directions necessitating multidisciplinary collaboration. A key focus is the application of generative AI in Islamic studies, particularly in managing sensitive content such as the Ahadith (corpus of sayings, actions, and approvals attributed to the Prophet Muḥammad). We detail the customization and refinement of the AI model, ""Digital Muḥammad ibn Ismail Al-Bukhari,"" designed to provide accurate responses based on the Sahih Al-Bukhari collection. Our methodology includes rigorous dataset curation, preprocessing, model customization, and evaluation to ensure the model’s reliability. Strategies to mitigate hallucinations involve implementing context-aware constraints, regular audits, and continuous feedback loops to maintain adherence to authoritative texts and correct biases. Findings indicate a significant reduction in hallucinations, though challenges such as residual biases and handling ambiguous queries persist. This research underscores the importance of recognizing LLMs’ limitations and highlights the need for collaborative efforts in fine-tuning these models with authoritative texts. It offers a framework for the cautious application of generative AI in Islamic studies, emphasizing continuous improvements to enhance AI reliability.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852480","Large Language Models;Hallucinations;Hadith Studies;AI in Islamic Studies;Context-aware Constraints;Bias Mitigation;Generative AI Applications;Digital Humanities;Prompt Engineering;Sahih Al-Bukhari;GPT builder;Religious Text Analysis","Analytical models;Accuracy;Text analysis;Large language models;Collaboration;Training data;Chatbots;Reliability engineering;Prompt engineering;Artificial intelligence","","","","35","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Benchmarking Large Language Models in Evidence-Based Medicine","J. Li; Y. Deng; Q. Sun; J. Zhu; Y. Tian; J. Li; T. Zhu","Institute for AI in Medicine, School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; Institute for AI in Medicine, School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; Department of Pathology, Nanjing Drum Tower Hospital, The Affiliated Hospital of Nanjing University Medical School, Nanjing, China; Institute for AI in Medicine, School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, U.K.",IEEE Journal of Biomedical and Health Informatics,"","2024","PP","99","1","14","Evidence-based medicine (EBM) represents a paradigm of providing patient care grounded in the most current and rigorously evaluated research. Recent advances in large language models (LLMs) offer a potential solution to transform EBM by automating labor-intensive tasks and thereby improving the efficiency of clinical decision-making. This study explores integrating LLMs into the key stages in EBM, evaluating their ability across evidence retrieval (PICO extraction, biomedical question answering), synthesis (summarizing randomized controlled trials), and dissemination (medical text simplification). We conducted a comparative analysis of seven LLMs, including both proprietary and open-source models, as well as those fine-tuned on medical corpora. Specifically, we benchmarked the performance of various LLMs on each EBM task under zero-shot settings as baselines, and employed prompting techniques, including in-context learning, chain-of-thought reasoning, and knowledge-guided prompting to enhance their capabilities. Our extensive experiments revealed the strengths of LLMs, such as remarkable understanding capabilities even in zero-shot settings, strong summarization skills, and effective knowledge transfer via prompting. Promoting strategies such as knowledge-guided prompting proved highly effective (e.g., improving the performance of GPT-4 by 13.10% over zero-shot in PICO extraction). However, the experiments also showed limitations, with LLM performance falling well below state-of-the-art baselines like PubMedBERT in handling named entity recognition tasks. Moreover, human evaluation revealed persisting challenges with factual inconsistencies and domain inaccuracies, underscoring the need for rigorous quality control before clinical application. This study provides insights into enhancing EBM using LLMs while highlighting critical areas for further research. The code is publicly available on Github.","2168-2208","","10.1109/JBHI.2024.3483816","National Natural Science Foundation of China (NSFC)(grant numbers:82302352,82172069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10723298","Large language models;evidence-based medicine;clinical NLP;Prompt engineering","Medical services;Sleep;Question answering (information retrieval);Prompt engineering;Large language models;Bioinformatics;Diseases;Cognition;Biological system modeling;Benchmark testing","","8","","","IEEE","21 Oct 2024","","","IEEE","IEEE Early Access Articles"
"Automatic Prompt Generation and Optimization by Leveraging Large Language Models to Enhance Few-Shot Learning in Biomedical Tasks","Y. Shi; X. Hu","College of Computing and Informatics, Drexel University, Philadelphia, PA, United States; College of Computing and Informatics, Drexel University, Philadelphia, PA, United States",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","1645","1654","Recent advancements in scaling large language models (LLMs) have enhanced various natural language processing (NLP) tasks. However, open-source moderately sized models, such as BERT, are still needed because of the high computational cost and concerns regarding data privacy from the LLMs, especially in the biomedical area. Prompt-based fine-tuning of BERT has demonstrated good performance in a few-shot setting. However, the prompt selection can result in substantial differences in final accuracy. This study introduces a simple yet effective approach that leverages LLMs, such as GPT-4 Turbo, to automatically generate and optimize task-specific prompts for BERT. Our approach includes two steps: automatic prompt generation and optimization. Initially, we design a framework to generate prompts for LLMs to infer a task-specific candidate prompt set. Subsequently, we employ a dialog with a chatbot to optimize the prompt iteratively. We conduct extensive evaluations and analyses on three different types of biomedical benchmarks. Our method demonstrates superior 5-shot learning performance, outperforming manual prompts by a substantial margin in low-resource settings, achieving up to a 7% absolute accuracy improvement. These results highlight that our method is a task-agnostic approach to utilizing LLMs and automatically enhancing performance on relatively small open-source models with limited resources and human effort.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825237","Prompt Engineering;Few-Shot Learning;GPT-4 Turbo;BERT;Large Language Models","Data privacy;Accuracy;Biological system modeling;Large language models;Computational modeling;Manuals;Data models;Computational efficiency;Few shot learning;Optimization","","1","","40","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Exploring the interplay between DataSpaces and Large Language Models","S. Distefano; Y. N. Yifru","Department of Mathematics and Computer Science, University of Messina, Messina, Italy; Department of Mathematics and Computer Science, University of Messina, Messina, Italy",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5506","5515","The confluence of Large Language Models (LLMs) and Dataspaces presents a captivating prospect for the future of data science and AI, fostering new avenues for data exploration, analysis, and knowledge extraction. This paper explores both patterns arising from the Dataspaces-LLMs convergence: Dataspaces applied to LLMs (DS4LLM) and LLMs applied to Dataspaces (LLM4DS), further investigating the latter. Dataspaces, a conceptual framework for data-centric systems, are poised to revolutionize data management and sharing, indeed. They provide a structured and secure environment for managing and integrating data from multiple sources. Traditional approaches to data management, nevertheless, are bottom-up, often hindering the timely use of data. This paper advocates for a top-down perspective, prioritizing data collection and provisioning to other data management tasks that have to be thus delivered on purpose and in a timely manner a-posteriori, after data (re)source discovery. To bridge the gap between these two approaches, LLMs emerge as a powerful tool able to unlock the value hidden within vast data repositories and Dataspaces. By LLMs, data management tasks such as filtering, cleaning, aggregation, integration, and augmentation can be automated into LLM4DS workflows. This research specifies some LLM4DS prompt templates that can be tailored to specific use cases for on demand and on purpose (a-posteriori) data management requiring post-scheme. A case study on a medical Dataspace demonstrates the practical application and suitability of the proposed LLM4DS approach to the problem at hand.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825298","Dataspace;Large Language Models;Prompt Engineering;Data Lifecycle;Post-scheme","Hands;Filtering;Large language models;Data science;Big Data;Data models;Cleaning;Data mining;Convergence","","1","","14","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Chain-of-Factors: A Zero-Shot Prompting Methodology Enabling Factor-Centric Reasoning in Large Language Models","M. Hussain; U. U. Rehman; T. D. T. Nguyen; S. Lee; S. T. Kim; S. -H. Bae; J. U. Kim","Department of Computer Science, UiT The Arctic University of Norway, Tromsø, Norway; Dept. of Computer Science & Engineering, Kyung Hee University, Yongin, South Korea; Dept. of Computer Science & Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea",2024 International Conference on Machine Learning and Applications (ICMLA),"4 Mar 2025","2024","","","1621","1627","Large language models (LLMs) have significantly improved numerous natural language processing tasks. However, their performance relies heavily on the provided instructions or prompts. Recently, several prompting methodologies have been developed to enhance the reasoning abilities of LLMs. Notably, the Chain-of-Thought (CoT) approach provides examples that help break down tasks into sub-steps, resulting in more accurate solutions. However, the process of generating detailed examples may not be user-friendly, as end users prefer providing task descriptions rather than a set of examples. In this study, we introduce Chain-of-Factors (CoF), an innovative zero-shot prompting methodology that incorporates task-specific instructions as a chain of factors into the prompt, aimed at enhancing the factor-centric reasoning abilities of LLMs. Experiments on three LLMs, including ChatGPT-3.5, Gemini, and GPT-4, show performance improvements ranging from 0.01% to 40.2% in accuracy on various symbolic reasoning and logical reasoning tasks compared with zero-shot and few-shot CoT. In summary, CoF enhances LLMs' reasoning abilities by including task-specific steps and instructions, while also decreasing the necessity for fine-tuning specific to each task.","1946-0759","979-8-3503-7488-9","10.1109/ICMLA61862.2024.00250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903447","Factor-Centric Reasoning;Prompt Engineering;Large Language Models","Accuracy;Large language models;Machine learning;Cognition;Natural language processing;Distance measurement","","","","15","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Toward CPS Security Monitoring: Employing Multi-Agent Systems and Digital Twins","Z. Lagache; A. Mercier; O. -E. -K. Aktouf; A. Baudet","Grenoble INP, LCIS, Univ. Grenoble Alpes, Valence, France; Grenoble INP, LCIS, Univ. Grenoble Alpes, Valence, France; Grenoble INP, LCIS, Univ. Grenoble Alpes, Valence, France; Grenoble INP, LCIS, Univ. Grenoble Alpes, Valence, France",2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW),"3 Dec 2024","2024","","","303","306","This paper proposes a study on designing architecture models based on the multi-agent system (MAS) and digital twin (DT) concepts to take care of cyber-physical systems security. CPS are used in a network to address a complex problem and allow to support physical process by sensing. CPS could be vulnerable to attack, from hardware and physical attacks to software attacks and even including network attacks. To meet these challenges, we explored several approaches to ally MAS and DT with the aim to benefit from the scalability and adaptability of MASs and the enhanced modelling of DTs. As a result of this exploration, we present a innovative approach to tackle networking attacks of CPSs by proposing an model architecture and apply it in an experiment to detect blackhole attacks.","2994-810X","979-8-3503-6704-1","10.1109/ISSREW63542.2024.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771338","Digital Twins;Decentralized Systems;Security;Cyber Physical Systems","Wireless sensor networks;Adaptation models;Scalability;Computer architecture;Software;Digital twins;Software reliability;Sensors;Security;Multi-agent systems","","","","16","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"SHR: Enhancing Event Argument Extraction Ability of Large language Models with Simple-Hard Refining","J. Wu; C. Zhang; Z. Hu; J. Yu","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5468","5475","Event Argument Extraction (EAE) aims to identify and extract key information such as entities, times, and locations related to specific events from text and serves as a fundamental task for many NLP applications. Recent researches have utilized large language models (LLMs) for EAE, effectively addressing the resource-intensive nature of annotating training datasets for this task. However, when performing EAE on longer texts (document-level EAE), the presence of descriptions unrelated to the events within document-level EAE can lead LLMs to identify incorrect arguments. To address this issue, we propose Simple-Hard Refining: a novel prompt framework that segments EAE into straightforward and complex extraction tasks. Based on the complexity of inference, we divide EAE task into simple-argument extraction and hard-argument extraction. By utilizing a chain of prompt to perform simple and hard argument extraction sequentially, noise introduced by irrelevant description for simple-argument extraction can be effectively alleviated. Furthermore, we explore the potential of LLMs to furnish dependable explanations for their extraction outcomes. We design an explanation-based prompting method that involves a three-step explanation process: relevant sentence extraction, argument role semantic analysis, and argument role entity localization. This method further enhances the extraction accuracy at each stage of the framework. Our experiments demonstrate that our method achieves state-of-the-art performance, surpassing various baselines that utilize LLMs for the EAE task. Ablation studies further verify the effectiveness of each stage of our framework and show the ability of our proposed approach to effectively mitigate noise. Our work contributes to the structured extraction of event argument information using LLMs.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825685","prompt engineering;event argument extraction;large language models","Training;Location awareness;Accuracy;Large language models;Refining;Noise;Semantics;Interference;Data models;Data mining","","","","30","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework","S. García-Méndez; F. De Arriba-Pérez","Information Technologies Group - atlanTTic, University of Vigo, Vigo, Spain; Information Technologies Group - atlanTTic, University of Vigo, Vigo, Spain","2024 11th International Conference on Social Networks Analysis, Management and Security (SNAMS)","20 Feb 2025","2024","","","25","32","Social media platforms enable instant and ubiquitous connectivity and are essential to social interaction and communication in our technological society. Apart from its advantages, these platforms have given rise to negative behaviors in the online community, the so-called cyberbullying. Despite the many works involving generative Artificial Intelligence (AI) in the literature lately, there remain opportunities to study its performance apart from zero/few-shot learning strategies. Accordingly, we propose an innovative and real-time solution for cyberbullying detection that leverages stream-based Machine Learning (ML) models able to process the incoming samples incrementally and Large Language Models (LLMS) for feature engineering to address the evolving nature of abusive and hate speech online. An explainability dashboard is provided to promote the system's trustworthiness, reliability, and accountability. Results on experimental data report promising performance close to 90 % in all evaluation metrics and surpassing those obtained by competing works in the literature. Ultimately, our proposal contributes to the safety of online communities by timely detecting abusive behavior to prevent long-lasting harassment and reduce the negative consequences in society.","2831-7343","979-8-3315-1834-9","10.1109/SNAMS64316.2024.10883785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10883785","Cyberbullying and misbehavior;explainability;Large Language Models;Machine Learning;security and trust;social networks;streaming/real-time analytics","Measurement;Large language models;Cyberbullying;Machine learning;Learning (artificial intelligence);Real-time systems;Safety;Security;Reliability;Proposals","","","","33","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Integrating Generative AI in Software Engineering Education: Practical Strategies","Y. Li; J. Keung; X. Ma","Dept of Computer Science, City University of Hong Kong, Hong Kong, China; Dept of Computer Science, City University of Hong Kong, Hong Kong, China; Dept of Computer Science, City University of Hong Kong, Hong Kong, China",2024 International Symposium on Educational Technology (ISET),"26 Sep 2024","2024","","","49","53","The transformative influence of generative artificial intelligence (AI), notably large language models (LLMs), has significantly reshaped the software engineering (SE) landscape, impacting various aspects of software development within industry and academia. The imperative to integrate generative AI into educational programs arises from the necessity to furnish graduates with contemporary methodologies that enhance software quality and streamline development processes. Nevertheless, a research gap exists concerning the systematic integration of established SE education guidelines with specific course contexts to strengthen SE education through incorporating generative AI. In response to this gap, our study presents a vision for integrating generative AI into SE education, with a particular emphasis on practical integration strategies aimed at endowing students with essential competencies tailored for contemporary software development. Aligning our vision with the knowledge domains within SE education, we delineate its application across specific areas such as code generation, auto test case completion, and others. The overall objective of these proposed initiatives is to furnish students in SE with an updated and immersive learning experience, thereby addressing the evolving demands of the field.","2766-2144","979-8-3503-6141-4","10.1109/ISET61814.2024.00019","University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685663","software engineering;education;generative AI;large language models;code generation;auto test case completion","Industries;Systematics;Generative AI;Large language models;Software quality;Educational technology;Software engineering","","1","","20","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Chat or Trap? Detecting Scams in Messaging Applications with Large Language Models","Y. -C. Chang; E. Aïmeur","Department of Computer Science and Operations Research, University of Montreal, Montreal, Canada; Department of Computer Science and Operations Research, University of Montreal, Montreal, Canada",2024 8th Cyber Security in Networking Conference (CSNet),"28 Jan 2025","2024","","","92","99","Messaging applications have become integral to everyday communication, but their widespread use has also made them a hotbed of various scams. Cybercriminals exploit these platforms, using sophisticated social engineering techniques to deceive individuals, build trust and achieve financial gain. The advent of Generative Artificial Intelligence (GenAI) has further exacerbated the problem of scams, enabling the creation of more sophisticated and convincing fraudulent schemes. Much research has focused on detecting phishing emails and spam messages, overlooking scenarios where malicious actors initiate conversations in a way that appears harmless. This paper proposes leveraging Large Language Models (LLMs) to detect scams in chats on messaging applications. A comprehensive dataset comprising real-world scam and non-scam chat segments is constructed, followed by a thorough performance comparison of various LLMs in identifying scam indicators within chat segments. Additionally, a comparative analysis is performed between LLMs and human participants in recognizing these deceptive interactions through a detailed survey. The findings highlight the potential of LLMs to mitigate the growing threat of scams in messaging applications, thereby enhancing the security of digital communications.","2768-0029","979-8-3315-3410-3","10.1109/CSNet64211.2024.10851753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851753","Scam Detection;Large Language Models (LLMs);Messaging Applications;Social Engineering;Cyberse-curity","Surveys;Large language models;Unsolicited e-mail;Prevention and mitigation;Phishing;Oral communication;Digital communication;Solids;Security;Protection","","","","31","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair","A. Silva; S. Fang; M. Monperrus","KTH Royal Institute of Technology, Sweden; NC State University, USA; KTH Royal Institute of Technology, Sweden",IEEE Transactions on Software Engineering,"","2025","PP","99","1","16","Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tune LLMs with naive code representations and does not scale to frontier models. To address this problem, we propose RepairLLaMA, a novel program repair approach that 1) identifies optimal code representations for APR with fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient fine-tuning technique (PEFT) for program repair. This results in RepairLLaMA producing a highly effective ‘program repair adapter’ for fixing bugs with AI. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals and produce better patches. Second, parameter-efficient fine-tuning helps fine-tuning to converge and clearly contributes to the effectiveness of RepairLLaMA in fixing bugs outside the fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144 Defects4J v2, 109 HumanEval-Java, and 20 GitBug-Java bugs, outperforming all baselines.","1939-3520","","10.1109/TSE.2025.3581062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11039501","Automated Program Repair;Large Language Models;Code Representations;Parameter-Efficient Fine-Tuning","Maintenance engineering;Codes;Computer bugs;Location awareness;Adaptation models;Tuning;Pipelines;Source coding;Prompt engineering;Large language models","","2","","","CCBY","18 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Consolidating TinyML Lifecycle with Large Language Models: Reality, Illusion, or Opportunity?","G. Wu; S. Tarkoma; R. Morabito","Department of Computer Science, University of Helsinki, Finland; Department of Computer Science, University of Helsinki, Finland; Department of Computer Science, University of Helsinki, Finland",IEEE Internet of Things Magazine,"","2025","PP","99","1","16","The evolving requirements of Internet of Things (IoT) applications are driving an increasing shift toward bringing intelligence to the edge, enabling real-time insights and decision-making within resource-constrained environments. Tiny Machine Learning (TinyML) has emerged as a key enabler of this evolution, facilitating the deployment of ML models on devices such as microcontrollers and embedded systems. However, the complexity of managing the TinyML lifecycle, including stages such as data processing, model optimization and conversion, and device deployment, presents significant challenges and often requires substantial human intervention. Motivated by these challenges, we began exploring whether Large Language Models (LLMs) could help automate and streamline the TinyML lifecycle. We developed a framework that leverages the natural language processing (NLP) and code generation capabilities of LLMs to reduce development time and lower the barriers to entry for TinyML deployment. Through a case study involving a computer vision classification model, we demonstrate the framework’s ability to automate key stages of the TinyML lifecycle. Our findings suggest that LLM-powered automation holds potential for improving the lifecycle development process and adapting to diverse requirements. However, while this approach shows promise, there remain obstacles and limitations, particularly in achieving fully automated solutions. This paper sheds light on both the challenges and opportunities of integrating LLMs into TinyML workflows, providing insights into the path forward for efficient, AI-assisted embedded system development.","2576-3199","","10.1109/MIOT.2025.3575927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11027717","TinyML;Large Language Models (LLMs);Lifecycle Automation;Embedded IoT Systems;MLOps for TinyML;Edge AI","Tiny machine learning;Internet of Things;Codes;Adaptation models;Data models;Computational modeling;Training;Hardware;Prompt engineering;Optimization","","","","","IEEE","9 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Large Language Models for Networking: Applications, Enabling Techniques, and Challenges","Y. Huang; H. Du; X. Zhang; D. Niyato; J. Kang; Z. Xiong; S. Wang; T. Huang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Automation, Guangdong University of Technology, Guangzhou, China; Information Systems Technology and Design (ISTD) Pillar, Singapore University of Technology and Design, Tampines, Singapore; Purple Mountain Laboratories, Nanjing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Network,"14 Jan 2025","2025","39","1","235","242","The rapid evolution of network technologies and the growing complexity of network tasks necessitate a paradigm shift in how networks are designed, configured, and managed. With a wealth of knowledge and expertise, large language models (LLMs) are one of the most promising candidates. This paper aims to pave the way for constructing domain-adapted LLMs for networking. Firstly, we present potential LLM applications for vertical network fields and showcase the mapping from natural language to network language. Then, several enabling technologies are investigated, including parameter-efficient finetuning and prompt engineering. The insight is that language understanding and tool usage are both required for network LLMs. Driven by the idea of embodied intelligence, we propose the ChatNet, a domain-adapted network LLM framework with access to various external network tools. ChatNet can reduce the time required for burdensome network planning tasks significantly, leading to a substantial improvement in processing efficiency. Finally, key challenges and future research directions are highlighted.","1558-156X","","10.1109/MNET.2024.3435752","the BUPT Excellent Ph.D. Students Foundation(grant numbers:CX2022113); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614634","Large Language Models;Generative AI;Intentdriven Networking;Network Intelligence","Natural languages;Task analysis;Manuals;Protocols;Knowledge engineering;Artificial intelligence;Planning","","22","","15","IEEE","30 Jul 2024","","","IEEE","IEEE Magazines"
"ECHO: An Approach to Enhance Use Case Quality Exploiting Large Language Models","G. De Vito; F. Palomba; C. Gravino; S. Di Martino; F. Ferrucci","Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Italy; Department of Electrical, Engineering and Information Technologies, University of Naples ""Federico II"", Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Italy",2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),"1 Jan 2024","2023","","","53","60","UML use cases are commonly used in software engineering to specify the functional requirements of a system since they are an effective tool for interacting with stakeholders thanks to the use of natural languages. However, producing high-quality use cases can be challenging due to the lack of precise guidelines and suitable tools. This can lead to problems, e.g. inaccuracy and incompleteness, in the derived software artifacts and the final product. Recent advancements in Natural Language Processing and Large Language Models (LLMs) can provide the premises for developing tools supporting activities based on natural languages. In this paper, we propose ECHO, a novel approach for supporting software engineers in enhancing the quality of UML use cases using LLMs. Our approach consists of a co-prompt engineering approach and an iterative and interactive process with the LLM to improve the quality of use cases, based on practitioners’ feedback. To prove the feasibility of the proposal, we instantiated the approach using ChatGPT and performed a controlled experiment to assess its effectiveness by involving seven software engineering professionals. Three were part of the experimental group and used ECHO to improve the quality of the use cases. Three others were the control group and enhanced the quality of use cases manually. Finally, the last participant acted as an oracle, blind w.r.t. the groups, and evaluated the quality of the enhanced use cases, both qualitatively by means of a questionnaire, and quantitatively, by means of the Use Case Points metric. Results show that ECHO can effectively support software engineers to improve use cases’ quality thanks to the prompts suitably designed to interact with ChatGPT.","2376-9521","979-8-3503-4235-2","10.1109/SEAA60479.2023.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371553","UML Use Cases;Large Language Models;Prompt Engineering;Size/Effort estimation","Measurement;Unified modeling language;Estimation;Chatbots;Software;Stakeholders;Proposals","","4","","26","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"Text-based Personality Prediction Using Large Language Models","M. Molchanova; D. Olshevskaya","Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","82","90","This paper addresses the task of text-based personality detection using Large Language Models (LLMs). The study focuses primarily on exploring the text-based detection of extraversion, a trait represented in both the Big Five personality traits and MBTI models. The study utilized three datasets that included annotations for different personality models and encompassed data from various sources. We examined both the performance metrics evaluating the LLMs and the models’ decision-making processes while applying various prompt engineering methods.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852483","Institute of Physics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852483","Large Language Models;Prompt Engineering;Personality;Big Five personality traits;Five factor model;MBTI;ChatGPT","Measurement;Annotations;Large language models;Decision making;Predictive models;Data models;Prompt engineering","","","","28","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"LLMs for Hardware Security: Boon or Bane?","R. Kande; V. Gohil; M. DeLorenzo; C. Chen; J. Rajendran","Texas A&M University, USA; Texas A&M University, USA; Texas A&M University, USA; Texas A&M University, USA; Texas A&M University, USA",2024 IEEE 42nd VLSI Test Symposium (VTS),"29 May 2024","2024","","","1","4","Large language models (LLMs) have emerged as transformative tools within the hardware design and verification lifecycle, offering numerous capabilities in accelerating design processes. Recent research has showcased the efficacy of LLMs in translating design specifications into source code through hardware description languages. Researchers are also using LLMs to generate test cases and write assertion rules to bolster the detection of hardware vulnerabilities. Thus, the semiconductor industry is swiftly integrating LLMs into its design workflows. However, this adoption is not without its challenges.While LLMs offer remarkable benefits, they concurrently introduce security concerns that demand a thorough examination. These concerns manifest as potential vulnerabilities indirectly introduced into the designs while generating the design code, or by directly equipping the attackers with novel avenues for exploitation. In this paper, we discuss the emerging security implications due to the capabilities introduced by LLMs in the context of hardware design verification, evaluate the capabilities of existing security detection and mitigation techniques, and highlight the possible future security attacks that use LLMs.","2375-1053","979-8-3503-6378-4","10.1109/VTS60656.2024.10538871","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538871","LLM;hardware;verification;security","Industries;Codes;Hardware security;Source coding;Design methodology;Electronics industry;Very large scale integration","","2","","48","IEEE","29 May 2024","","","IEEE","IEEE Conferences"
"From Theory to Practice: Code Generation Using LLMs for CAPEC and CWE Frameworks","M. Shahzad; J. Wilson; I. Al Azher; H. Alhoori; M. Rahimi","Department of Computer Science, Northern Illinois University, DeKalb, Illinois, USA; Department of Computer Science, Northern Illinois University, DeKalb, Illinois, USA; Department of Computer Science, Northern Illinois University, DeKalb, Illinois, USA; Department of Computer Science, Northern Illinois University, DeKalb, Illinois, USA; Department of Computer Science, Northern Illinois University, DeKalb, Illinois, USA",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","137","144","The increasing complexity and volume of software systems have heightened the importance of identifying and mitigating security vulnerabilities. The existing software vulnerability datasets frequently fall short in providing comprehensive, detailed code snippets explicitly linked to specific vulnerability descriptions, reducing their utility for advanced research and hindering efforts to develop a deeper understanding of security vulnerabilities. To address this challenge, we present a novel dataset that provides examples of vulnerable code snippets corresponding to Common Attack Pattern Enumerations and Classifications (CAPEC) and Common Weakness Enumeration (CWE) descriptions. By employing the capabilities of Generative Pre-trained Transformer (GPT) models, we have developed a robust methodology for generating these examples. Our approach utilizes GPT-4o, Llama and Claude models to generate code snippets that exhibit specific vulnerabilities as described in CAPEC and CWE documentation. This dataset not only enhances the understanding of security vulnerabilities in code but also serves as a valuable resource for training machine learning models focused on automatic vulnerability detection and remediation. Preliminary evaluations suggest that the dataset generated by Large Language Models demonstrates high accuracy and can serve as a reliable reference for vulnerability identification systems. We found consistent results across the three models, with 0.98 cosine similarity among codes. The final dataset comprises 615 CAPEC code snippets in three programming languages: Java, Python, and JavaScript, making it one of the most extensive and diverse resources in this domain. This research contributes to the field of cybersecurity by introducing an innovative dataset that supports advanced studies on software vulnerabilities and facilitates the development of tools for their prevention and mitigation.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00022","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028365","Large Language Models;Code Generation;Software Vulnerability;Cybersecurity","Training;Java;Codes;Accuracy;Prevention and mitigation;Transformers;Software systems;Software reliability;Computer security;Python","","","","37","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Large Language Model-Based Time-Series Data Summarization Using a Novel Multi-Tiered Retrieval Pipeline","S. Shrimali","University of Illinois Urbana-Champaign, Urbana, Illinois, USA",2025 IEEE 11th International Conference on High Performance and Smart Computing (HPSC),"20 Jun 2025","2025","","","7","12","Financial time-series data summarization is a critical task for analysts and decision-makers; yet current approaches rely on labor-intensive manual interpretation or bespoke reporting scripts. In this research, a novel multi-tier retrieval pipeline that leverages large language models (LLMs) to automatically generate concise and accurate summaries of financial data is proposed. By integrating traditional templated methods with retrieval-augmented prompts (using domain-specific examples to guide the LLM) the challenge of balancing linguistic fluency with numerical precision is addressed. The pipeline is evaluated on a knowledge base constructed from daily OHLCV data of 20 large-cap companies, with additional testing on a mid-cap ticker not present in the original dataset (COF). Over 15 independent runs were conducted; these experimental results demonstrate that while direct summarization captures general trends, this templated and refined multi-tier retrieval pipeline ensures factual correctness and achieves the best overall performance in terms of ROUGE, BERTScore, and numeric fidelity. This pipeline has the potential to transform time-series analysis by reducing manual workload and improving the reliability of automated financial data summarization.","","979-8-3315-9663-7","10.1109/HPSC66065.2025.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11038769","Large Language Models (LLMs);Time-Series Data Summarization;Financial Analytics;Retrieval-Augmented Generation;Prompt Engineering;Stock Time-Series Analysis;Time Series Data Retrieval","Accuracy;Large language models;Pipelines;Time series analysis;Manuals;Transforms;Market research;Data models;Numerical models;Prompt engineering","","","","8","IEEE","20 Jun 2025","","","IEEE","IEEE Conferences"
"Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware","B. J. Walton; M. E. Khatun; J. M. Ghawaly; A. Ali-Gombe","Dept. of Computer Science & Engineering, Louisiana State University, Baton Rouge, United States; Dept. of Computer Science & Engineering, Louisiana State University, Baton Rouge, United States; Dept. of Computer Science & Engineering, Louisiana State University, Baton Rouge, United States; Dept. of Computer Science & Engineering, Louisiana State University, Baton Rouge, United States",2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops),"18 Mar 2025","2024","","","248","254","Malware analysis is a complex process of examining and evaluating malicious software’s functionality, origin, and potential impact. This arduous process typically involves dissecting the software to understand its components, infection vector, propagation mechanism, and payload. Over the years, deep reverse engineering of malware has become increasingly tedious, mainly due to modern malicious codebases’ fast evolution and sophistication. Essentially, analysts are tasked with identifying the elusive needle in the haystack within the complexities of zero-day malware, all while under tight time constraints. Thus, in this paper, we explore leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples. Built on GPT-4o-mini model, MalParse is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering. Additionally, MalParse performs malware categorization, distinguishing potential malware from benign applications, thereby saving time during the malware reverse engineering process. Despite not being fine-tuned for Android malware analysis, we demonstrate that through optimized and advanced prompt engineering MalParse can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels. In addition, leveraging the backward tracing of the summaries from package to function levels allowed us to pinpoint the precise code snippets responsible for malicious behavior.","","979-8-3315-3281-9","10.1109/ACSACW65225.2024.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917287","android;malware analysis;reverse engineering;summarization;large language models (LLMs)","Analytical models;Codes;Accuracy;Large language models;Conferences;Semantics;Reverse engineering;Malware;Vectors;Prompt engineering","","2","","17","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Lateral Phishing With Large Language Models: A Large Organization Comparative Study","M. Bethany; A. Galiopoulos; E. Bethany; M. Bahrami Karkevandi; N. Beebe; N. Vishwamitra; P. Najafirad","Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Secure AI and Autonomy Laboratory, San Antonio, TX, USA; Secure AI and Autonomy Laboratory, San Antonio, TX, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA",IEEE Access,"10 Apr 2025","2025","13","","60684","60701","The emergence of Large Language Models (LLMs) has heightened the threat of phishing emails by enabling the generation of highly targeted, personalized, and automated attacks. Traditionally, many phishing emails have been characterized by typos, errors, and poor language. These errors can be mitigated by LLMs, potentially lowering the barrier for attackers. Despite this, there is a lack of large-scale studies comparing the effectiveness of LLM-generated lateral phishing emails to those crafted by humans. Current literature does not adequately address the comparative effectiveness of LLM and human-generated lateral phishing emails in a real-world, large-scale organizational setting, especially considering the potential for LLMs to generate more convincing and error-free phishing content. To address this gap, we conducted a pioneering study within a large university, targeting its workforce of approximately 9,000 individuals including faculty, staff, administrators, and student workers. Our results indicate that LLM-generated lateral phishing emails are as effective as those written by communications professionals, emphasizing the critical threat posed by LLMs in leading phishing campaigns. We break down the results of the overall phishing experiment, comparing vulnerability between departments and job roles. Furthermore, to gather qualitative data, we administered a detailed questionnaire, revealing insights into the reasons and motivations behind vulnerable employee’s actions. This study contributes to the understanding of cyber security threats in educational institutions and provides a comprehensive comparison of LLM and human-generated phishing emails’ effectiveness, considering the potential for LLMs to generate more convincing content. The findings highlight the need for enhanced user education and system defenses to mitigate the growing threat of AI-powered phishing attacks.","2169-3536","","10.1109/ACCESS.2025.3555500","Department of Homeland Security (DHS), United States Secret Service, National Computer Forensics Institute (NCFI)(grant numbers:70US0920D70090004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943116","Artificial intelligence;cybersecurity;disinformation;generative AI;large language models;phishing;text generation","Phishing;Electronic mail;Organizations;Large language models;Computer crime;Training;Social networking (online);Prevention and mitigation;Market research;Internet","","1","","62","CCBY","27 Mar 2025","","","IEEE","IEEE Journals"
"Assessing LLMs for High Stakes Applications","S. K. Gallagher; J. Ratchford; T. Brooks; B. Brown; E. Heim; S. McMillan; W. R. Nichols; S. Rallapalli; C. Smith; N. VanHoudnos; N. Winski; A. O. Mellinger","Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA; Software Engineering Institute, Carnegie Mellon University, Pennsylvania, USA",2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),"18 Jun 2024","2024","","","103","105","Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in re-producibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.","2832-7659","979-8-4007-0501-4","10.1145/3639477.3639720","Department of Defense(grant numbers:FA8702-15-D-0002); Carnegie Mellon University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554692","Large language models;TEVV;metrics;scaling;HCI;trust","Measurement;Security;Reliability;Tuning;Software engineering","","","","18","CCBY","18 Jun 2024","","","IEEE","IEEE Conferences"
"A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture","Q. Lu; L. Zhu; X. Xu; Y. Liu; Z. Xing; J. Whittle","Data61, CSIRO, Sydney, Australia; Data61, CSIRO, Sydney, Australia; Data61, CSIRO, Sydney, Australia; Data61, CSIRO, Sydney, Australia; Data61, CSIRO, Sydney, Australia; Data61, CSIRO, Sydney, Australia",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","1","6","Large language model (LLM) based chatbots, such as ChatGPT, have attracted huge interest in foundation models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. However, the architecture design of foundation model based systems has not yet been systematically explored. There is limited understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and system design options. Our taxonomy comprises three categories: the pretraining and adaptation of foundation models, the architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy can serve as concrete guidance for designing foundation model based systems.CCS CONCEPTS•Software and its engineering → Software design engineering;•Computer systems organization → Architectures;•Computing methodologies → Artificial intelligence.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556054","Software architecture;foundation model;responsible AI;large language model;LLM;taxonomy;generative AI","Adaptation models;Software design;Software architecture;Taxonomy;Computer architecture;Organizations;Chatbots","","4","","20","CCBY","18 Jun 2024","","","IEEE","IEEE Conferences"
"Memory Efficient with Parameter Efficient Fine-Tuning for Code Generation Using Quantization","Purnawansyah; Z. Ali; H. Darwis; L. B. Ilmawan; S. R. Jabir; A. R. Manga","Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia; Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia; Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia; Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia; Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia; Department of Computer Science, Universitas Muslim Indonesia, Makassar, Indonesia",2024 18th International Conference on Ubiquitous Information Management and Communication (IMCOM),"12 Feb 2024","2024","","","1","6","Code Large Language Models (Code LLMs) such as Code LLaMa and StarCoder have exhibited outstanding proficiency in tasks required for specific tasks like code generation. Several conducted research to similar task by utilizing fine-tuning techniques from state-of-the-art base models for more specific related task. However, due to the cost limitations and limited computing resources, performing fine-tuning from large language models is excessively high. In this study, we utilized Low-Rank Adaptation (LoRA) for base large language models such as LLaMA-2 and Phi-1.5, which uses trainable rank decomposition matrices. Furthermore, we injected Quantized LoRA (QLoRA) to help reduce memory usage while training the model and analyzed the contribution to GPU usage. Notably, our findings reveal that employing these techniques for fine-tuning on small datasets yields cost-effective and viable alternatives for language-related tasks, showcasing competitive performance compared to state-of-the-art models like CodeLLaMa 7B substantiated by lower train loss achieved in our experiments.","","979-8-3503-3101-1","10.1109/IMCOM60618.2024.10418267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418267","fine tuning;large language models;quantization;text generation","Training;Adaptation models;Codes;Quantization (signal);Computational modeling;Memory management;Task analysis","","1","","26","IEEE","12 Feb 2024","","","IEEE","IEEE Conferences"
"Watermarking Large Language Models and the Generated Content: Opportunities and Challenges","R. Zhang; F. Koushanfar","University of California, San Diego; University of California, San Diego","2024 58th Asilomar Conference on Signals, Systems, and Computers","4 Apr 2025","2024","","","1779","1786","The widely adopted and powerful generative large language models (LLMs) have raised concerns about intellectual property rights violations and the spread of machine-generated misinformation. Watermarking serves as a promising approch to establish ownership, prevent unauthorized use, and trace the origins of LLM -generated content. This paper summarizes and shares the challenges and opportunities we found when watermarking LLMs. We begin by introducing techniques for wa-termarking LLMs themselves under different threat models and scenarios. Next, we investigate watermarking methods designed for the content generated by LLMs, assessing their effectiveness and resilience against various attacks. We also highlight the importance of watermarking domain-specific models and data, such as those used in code generation, chip design, and medical applications. Furthermore, we explore methods like hardware acceleration to improve the efficiency of the watermarking process. Finally, we discuss the limitations of current approaches and outline future research directions for the responsible use and protection of these generative AI tools.","2576-2303","979-8-3503-5405-8","10.1109/IEEECONF60004.2024.10942607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10942607","Watermarking;Large Language Models;Hard-ware Security","Threat modeling;Generative AI;Shape;Large language models;Watermarking;Intellectual property;Medical services;Protection;Fake news;Resilience","","","","105","IEEE","4 Apr 2025","","","IEEE","IEEE Conferences"
"Effective Intended Sarcasm Detection Using Fine-tuned Llama 2 Large Language Models","F. D. Heraldi; Z. Ruskanda","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia","2024 11th International Conference on Advanced Informatics: Concept, Theory and Application (ICAICTA)","28 Nov 2024","2024","","","1","6","Detecting sarcasm in English text is a significant challenge in sentiment analysis due to the discrepancy between implied and explicit meanings. Previous studies using Transformer-based models for intended sarcasm detection show room for improvement, and the development of large language models (LLMs) presents a substantial opportunity to enhance this area. This research leverages the open-source Llama 2 LLM, released by Meta, fine-tuned to develop an effective sarcasm detection model. Our proposed system design generalizes the use of Llama 2 for text classification but is specifically designed for sarcasm detection, sarcasm category classification, and pairwise sarcasm identification. Data from the iSarcasmEval dataset and additional sources, totaling 21,599 samples for sarcasm detection, 3,457 for sarcasm category classification, and 868 for pairwise sarcasm identification, were used. Methods include prompt development, fine-tuning using Parameter Efficient Fine-tuning (PEFT) with Quantized Low Rank Adaptation (QLoRA), and zero-shot approach. Our model demonstrates significant improvements, sarcasm detection model and pairwise sarcasm identification model are surpassing top models on previous study: F1-score of 0.6867 for sarcasm detection, Macro-F1 of 0.1388 for sarcasm category classification, and accuracy of 0.9 for pairwise sarcasm identification. Results demonstrate that Llama 2, combined with external datasets and effective prompt engineering, enhances intended sarcasm detection. The PEFT technique with QLoRA reduces memory requirements without compromising performance, enabling model development on devices with limited computational resources. This research underscores the importance of context and intention in intended sarcasm detection, with dataset labeling discrepancies remaining a significant challenge.","2996-2552","979-8-3315-2031-1","10.1109/ICAICTA63815.2024.10763281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763281","sarcasm detection;large language models;Llama 2;fine-tune;prompt engineering","Performance evaluation;Adaptation models;Accuracy;Large language models;Computational modeling;Text categorization;Memory management;Transformers;Prompt engineering;Context modeling","","1","","30","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Foundation Models for Automatic Issue Labeling","G. Colavito","University of Bari, Bari, Italy",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","127","131","Foundation models are transforming software engineering practices through their ability to understand and generate code, process natural language, and automate various development tasks. Despite their potential, effectively applying these models to specialized software engineering tasks remains challenging due to the need for domain-specific understanding and accurate labeling of data. This research project investigates how foundation models can be leveraged to automate labeling tasks in software engineering, with a specific focus on issue classification as a representative case study. Issue tracking systems, while essential for collaborative software development, often suffer from misclassification problems that require significant manual effort to correct. We explore how foundation models can be adapted to automatically label issues accurately, reducing the need for manual intervention while maintaining high-quality classification. The project examines several key aspects: the capabilities of different foundation models in understanding software engineering artifacts, methods for adapting these models to specific labeling tasks through techniques like prompt engineering and few-shot learning, and approaches for integrating automated labeling into real-world scenarios. This research contributes to the broader understanding of how foundation models can be effectively applied to reduce manual labeling efforts across various software engineering contexts, using issue classification as a concrete demonstration of their potential.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024389","Generative AI;Large Language Models;Few-shot Learning;Issue Tracking;Software Maintenance and Evolution","Software maintenance;Foundation models;Natural languages;Focusing;Manuals;Data models;Labeling;Prompt engineering;Few shot learning;Software engineering","","","","51","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Zero and Few Short Learning Using Large Language Models for De-Identification of Medical Records","Y. S. Yashwanth; R. Shettar","Department of Computer Science and Engineering, RV College of Engineering, Bengaluru, Karnataka, India; Department of Computer Science and Engineering, RV College of Engineering, Bengaluru, Karnataka, India",IEEE Access,"14 Aug 2024","2024","12","","110385","110393","The paper aims to evaluate and provide a comparative analysis of the performance and fine-tuning cost of various Large Language Models (LLMs) such as GPT-3.5, GPT-4, PaLM, Bard, and Llama in automating the de-identification of Protected Health Information (PHI) from medical records, ensuring patient and healthcare professional privacy. Zero-shot learning was utilized initially to assess the capabilities of these LLMs in de-identifying medical data. Subsequently, each model was fine-tuned with varying training set sizes to observe changes in performance. The study also investigates the impact of the specificity of prompts on the accuracy of de-identification tasks. Fine-tuning LLMs with specific examples significantly enhanced the accuracy of the de-identification process, surpassing the zero-shot learning accuracy of pre-trained counterparts. Notably, a fine-tuned GPT-3.5 model with a few-shot learning technique was able to exceed the performance of a zero-shot learning GPT-4 model, with 99% accuracy. Detailed prompts resulted in higher task accuracy across all models, yet fine-tuned models with brief instructions still outperformed pre-trained models given detailed prompts. Also, the fine-tuned models were more resilient to medical record format change than the zero-shot models. Code, calculations, and comparisons are available at https://github.com/YashwanthYS/De-Identification-of-medical-Records. The findings underscore the potential of LLMs, particularly when fine-tuned, to effectively automate the de-identification of PHI in medical records. The study highlights the importance of model training and prompt specificity in achieving high accuracy in de-identification tasks.","2169-3536","","10.1109/ACCESS.2024.3439680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630540","De-identification;Bard;fine-tuning;GPT-3.5;GPT-4;PaLM;privacy;prompt engineering;large language models;Llama","Task analysis;Training;Medical services;Large language models;Zero-shot learning;Prompt engineering;Few shot learning","","3","","50","CCBYNCND","7 Aug 2024","","","IEEE","IEEE Journals"
"Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels","V. Nguyen; C. Pham","Northeastern University, Boston, MA, USA; Boston University, Boston, MA, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","8550","8559","The increasing frequency of suicidal thoughts highlights the importance of early detection and intervention. Social media platforms, where users often share personal experiences and seek help, could be utilized to identify individuals at risk. However, the large volume of daily posts makes manual review impractical. This paper explores the use of Large Language Models (LLMs) to automatically detect suicidal content in text-based social media posts. We propose a novel method for generating pseudo-labels for unlabeled data by prompting LLMs, along with traditional classification fine-tuning techniques to enhance label accuracy. To create a strong suicide detection model, we develop an ensemble approach involving prompting with Qwen2-72B-Instruct, and using fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate our approach on the dataset of the Suicide Ideation Detection on Social Media Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we conduct a comprehensive analysis to assess the impact of different models and fine-tuning strategies on detection performance. Experimental results show that the ensemble model significantly improves the detection accuracy, by 5% points compared with the individual models. It achieves a weight F1 score of 0.770 on the public test set, and 0.731 on the private test set, providing a promising solution for identifying suicidal content in social media. Our analysis shows that the choice of LLMs affects the prompting performance, with larger models providing better accuracy. Our code and checkpoints are publicly available at https://github.com/khanhvynguyen/Suicide_Detection_LLMs.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825313","large language models;text classification;limited labels;semi-supervised learning;prompt engineering;suicide detection;social media analysis","Analytical models;Visualization;Accuracy;Social networking (online);Reviews;Large language models;Big Data;Robustness;Ensemble learning;Videos","","1","","44","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Enforcement of Web3 Security by Use Blockchain and LLMs","V. Oleshchuk","University of Agder, Kristiansand, Norway",2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),"21 Dec 2023","2023","1","","391","394","In this paper we analyze how and why new emerging technologies originally developed outside security fields such as Blockchain and Large Language Models (LLMs) can and may impact the development of cybersecurity. It proposes a security framework to deal with security challenges in Web3.","2770-4254","979-8-3503-5805-6","10.1109/IDAACS58523.2023.10348890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10348890","cybersecurity threats;blockchain;LLMs;Web3","Analytical models;Smart contracts;Data acquisition;Blockchains;Computer security","","","","13","IEEE","21 Dec 2023","","","IEEE","IEEE Conferences"
"Applications and Implications of Large Language Models in Qualitative Analysis: A New Frontier for Empirical Software Engineering","M. De Morais Leça; L. Valença; R. Santos; R. De Souza Santos","University of Calgary, Calgary, AB, Canada; University of Calgary, Calgary, AB, Canada; Universidade Federal de Pernambuco, Recife, PE, Brazil; University of Calgary, Calgary, AB, Canada",2025 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE),"10 Jul 2025","2025","","","36","43","Context. The use of large language models for qualitative analysis is gaining attention in various fields, including software engineering, where qualitative methods are essential to understanding human and social factors. Goal. This study aimed to investigate how LLMs are currently used in qualitative analysis and how they can be used in software engineering research, focusing on identifying the benefits, limitations, and practices associated with their application. Method. We conducted a systematic mapping study and analyzed 20 relevant studies to explore reports of using LLM for qualitative analysis reported in the literature. Findings. Our findings indicate that LLMs are primarily used for tasks such as coding, thematic analysis, and data categorization, with benefits including increased efficiency and support for new researchers. However, limitations such as output variability, challenges capturing nuanced perspectives, and ethical concerns regarding privacy and transparency were also evident. Discussions. The study highlights the need for structured strategies and guidelines to optimize LLM use in qualitative research within software engineering. Such strategies could enhance the effectiveness of LLMs while addressing ethical considerations. Conclusion. While LLMs show promise for supporting qualitative analysis, human expertise remains essential for data interpretation, and continued exploration of best practices will be crucial for their effective integration into empirical software engineering research.","","979-8-3315-0225-6","10.1109/WSESE66602.2025.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11071418","Large Language Models;Prompt Engineering;Qualitative Analysis;Qualitative Research","Ethics;Privacy;Systematics;Large language models;Stochastic processes;Encoding;Social factors;Prompt engineering;Software engineering;Guidelines","","","","39","IEEE","10 Jul 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles","Y. Abe; S. Matsuo; R. Kondo; R. Hisano","Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo The Canon Institute for Global Studies, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo The Canon Institute for Global Studies, Tokyo, Japan",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","4799","4808","Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially when combined with the mode ensemble, outperform the buy-and-hold strategy in terms of Sharpe ratio during periods of rising consumer price index (CPI). However, traditional strategies are more effective during declining CPI trends or sharp market downturns. These results suggest that while LLMs can enhance portfolio management, they may require complementary strategies to optimize performance across varying market conditions.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825362","Large language models;Finance;Prompt engineering;Persona;Ensemble method;Portfolio management","Measurement;COVID-19;Fluctuations;Economic indicators;Large language models;Market research;Data models;Ensemble learning;Portfolios;Investment","","","","30","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Evaluating the Universality of “Do Anything Now” Jailbreak Prompts on Large Language Models","S. Nabavirazavi; S. Zad; S. S. Iyengar",Florida International University; Florida International University; Florida International University,2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00691","00696","The advent of large language models (LLMs) and their capabilities in diverse downstream tasks have attracted considerable adversarial attention, leading to various attack strategies. Among notable adversarial techniques targeting LLMs, jailbreak attacks have recently emerged as a means to bypass safety mechanisms and generate harmful content. Jailbreaks mislead LLMs to instruct users on tasks that have a negative societal impact, including illegal activities, malware distribution, and fraud. To systematically evaluate and robustify the corresponding refusal mechanisms and safety alignment measures against jailbreak strategies in subsequent iterations, a scalable approach is essential. The widespread presence of jailbreak scenarios, the inconsistency between different LLM usage policies, and the frequency of LLM updates make it challenging to have a unified evaluation pipeline. This paper introduces LLM Jailbreak Evaluator (LLM-JBE), an innovative framework that efficiently evaluates jailbreak prompts across a wide range of LLMs. We demonstrate the capabilities of LLM-JBE by measuring the effectiveness of jailbreak prompts across various OpenAI models. In particular, we demonstrate that basic jailbreak prompts remain highly effective and achieve attack success rates above 50% on GPT-4, GPT-3.5, and o1. The code is available at github.com/sina-nabavi/LLM-JBE.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903781","Army Research Office(grant numbers:W911NF-21-1-0264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903781","Large Language Models;LLM;Transformer;Jailbreak Prompt;Prompt Engineering;Adversarial Machine Learning;ChatGPT;Artificial Intelligence;Natural Language Processing","Large language models;Conferences;Computational modeling;Pipelines;Transformers;Particle measurements;Malware;Safety;Fraud;Prompt engineering","","","","31","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Privacy-Preserving Prompt Injection Detection for Smart Cloud-Deployed Large Language Models","G. Hao; J. Wu","Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan",2025 IEEE 10th International Conference on Smart Cloud (SmartCloud),"20 May 2025","2025","","","26","31","Large Language Models (LLMs) have recently gained popularity for tasks like text generation and automated reasoning, often hosted on cloud servers to handle large-scale user interactions. Yet this setup faces a critical challenge, prompt injection attacks, where adversaries craft malicious text to bypass an LLM’s safety constraints and policy filters. Attackers constantly evolve new injection tactics, rendering static defenses ineffective. While service providers may wish to store user inputs for security audits, this practice poses serious privacy and data-protection risks. User prompts can contain personal or confidential information subject to strict regulatory requirements. To address these conflicting demands of security monitoring and user privacy, we propose a privacy-preserving prompt injection detection scheme via hash and embedding-based auditing for smart cloud-deployed LLMs. Our approach segments each user prompt into smaller chunks and stores only non-invertible representations of them—cryptographic hashes for exact matches and vector embeddings for near-match detection. This enables robust identification of both verbatim and paraphrased threats without retaining any plaintext data. Furthermore, newly discovered injection patterns can trigger retrospective audits on previously stored chunks, greatly enhancing attack traceability. We also introduce an optional blockchain-based logging mechanism for tamper-proof event records. Empirical results show that our method achieves high detection accuracy against diverse injection techniques while safeguarding sensitive user information.","","979-8-3315-9665-1","10.1109/SmartCloud66068.2025.00009","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006851","large language models (LLMs);prompt injection;privacy-preserving;blockchain;smart cloud","Privacy;Large language models;Rendering (computer graphics);Vectors;Real-time systems;Blockchains;Safety;Security;Servers;Tuning","","","","15","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models","L. Chen; Y. Lei; S. Jin; Y. Zhang; L. Zhang","Robotics and Autonomous Driving Lab, Baidu Research, Sunnyvale, CA, USA; Robotics and Autonomous Driving Lab, Baidu Research, Sunnyvale, CA, USA; Robotics and Autonomous Driving Lab, Baidu Research, Sunnyvale, CA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Robotics and Autonomous Driving Lab, Baidu Research, Sunnyvale, CA, USA",IEEE Robotics and Automation Letters,"20 May 2024","2024","9","7","6075","6082","Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present a method for extracting the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated in a user-friendly manner. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ TD3, the widely-used RL baseline method, and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua can significantly reduce the sample complexity of TD3 in four robot tasks of panda_gym and achieve high success rates in 12 sparsely rewarded robot tasks in RLBench, where the standard TD3 fails. Additionally, we validated RLingua's effectiveness in real-world robot experiments through Sim2Real, demonstrating that the learned policies are effectively transferable to real robot tasks.","2377-3766","","10.1109/LRA.2024.3400189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529514","Reinforcement learning (RL);large language models (LLMs);robotic manipulations;sample complexity","Complexity theory;Robot motion;Codes;Training;Standards;Reinforcement learning;Large language models;Manipulators","","8","","37","IEEE","13 May 2024","","","IEEE","IEEE Journals"
"Assessing the Code Clone Detection Capability of Large Language Models","Z. Zhang; T. Saber","School of Computer Science, University of Galway Ireland z.zhang; Lero, the Science Foundation Ireland Research Centre for Software",2024 4th International Conference on Code Quality (ICCQ),"3 Jul 2024","2024","","","75","83","This study aims to assess the performance of two advanced Large Language Models (LLMs), GPT-3.S and GPT-4, in the task of code clone detection. The evaluation involves testing the models on a variety of code pairs of different clone types and levels of similarity, sourced from two datasets: BigCloneBench (human-made) and GPTCloneBench (LLM-generated). Findings from the study indicate that GPT-4 consistently sur-passes GPT-3.5 across all clone types. A correlation was observed between the GPTs' accuracy at identifying code clones and code similarity, with both GPT models exhibiting low effectiveness in detecting the most complex Type-4 code clones. Additionally, GPT models demonstrate a higher performance identifying code clones in LLM-generated code compared to humans-generated code. However, they do not reach impressive accuracy. These results emphasize the imperative for ongoing enhancements in LLM capabilities, particularly in the recognition of code clones and in mitigating their predisposition towards self-generated code clones-which is likely to become an issue as software engineers are more numerous to leverage LLM-enabled code generation and code refactoring tools.","","979-8-3503-6646-4","10.1109/ICCQ60895.2024.10576803","Science Foundation Ireland(grant numbers:18/CRT/6223); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576803","Code Clone Detection;Large Language Models (LLMs);GPT-3.5;GPT-4;Semantic Analysis","Analytical models;Codes;Accuracy;Correlation;Cloning;Software;Task analysis","","3","","42","IEEE","3 Jul 2024","","","IEEE","IEEE Conferences"
"CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation","J. Peng; L. Cui; K. Huang; J. Yang; B. Ray","Department of Computer Science, Columbia University, New York, NY, U.S.A.; Department of Computer Science, Columbia University, New York, NY, U.S.A.; Department of Computer Science, Columbia University, New York, NY, U.S.A.; Department of Computer Science, Columbia University, New York, NY, U.S.A.; Department of Computer Science, Columbia University, New York, NY, U.S.A.",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","33","40","Large Language Models (LLMs) have significantly aided developers by generating or assisting in code writing, enhancing productivity across various tasks. While identifying incorrect code is often straightforward, detecting vulnerabilities in code is more challenging, especially for developers with limited security knowledge, which poses considerable security risks of using LLM-generated code and underscores the need for robust evaluation benchmarks that assess both functional correctness and security. Current security evaluation benchmarks like CyberSecEval and SecurityEval are hindered by unclear and impractical specifications, failing to assess both functionality and security accurately. To tackle these deficiencies, we introduce CWEval, a novel outcome-driven evaluation framework designed to enhance the evaluation of secure code generation by LLMs. This framework not only assesses code functionality but also its security simultaneously with high-quality task specifications and outcome-driven test oracles which provides high accuracy. Coupled with CWEval-bench, a multilingual, security-critical coding benchmark, CWEval provides a rigorous empirical security evaluation on LLM-generated code, overcoming the shortcomings of previous benchmarks. Through our evaluations, CWEval reveals a notable portion of functional but insecure code generated by LLMs, and shows a serious inaccuracy of previous evaluations, ultimately contributing significantly to the field of secure code generation. We open-source our artifact at: https://github.com/Co1lin/CWEval.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028476","secure code generation;LLM code generation;benchmark;vulnerability","Productivity;Codes;Accuracy;Large language models;Conferences;Benchmark testing;Writing;Encoding;Multilingual;Security","","1","","25","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"THINK: Tackling API Hallucinations in LLMs via Injecting Knowledge","J. Liu; Y. Zhang; D. Wang; Y. Li; W. Dong","National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","229","240","Large language models (LLMs) have made significant strides in code generation but often struggle with API hallucination issues, especially for the third-party library. Existing approaches attempt to enhance LLMs by incorporating documentation. However, they face three main challenges: the introduction of irrelevant information that distracts the model; reliance solely on documentation that results in discrepancies between API descriptions and practical usage; and the absence of comprehensive error post-processing mechanisms. To address these challenges, we propose THINK11THINK's benchmark and code is available at https://github.com/Leah-Ljx/think., a knowledge injection method that leverages a custom API knowledge database with two phases: pre-execution enhancement and post-execution optimization. The former reduces irrelevant information and integrates multiple knowledge sources, while the latter identifies seven API error types and suggests three heuristic correction strategies. We manually construct a benchmark by collecting and filtering complex API-related tasks from GitHub to evaluate the effectiveness of our method. The experimental results demonstrate that our method can significantly improve the correctness of API usage in the context of LLMs. We reduce the error rate of programs from 61.18% to 16.64% for GPT-3.5 and from 41.49% to 5.58% for GPT-4o across tasks involving different libraries.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00029","National Natural Science Foundation of China(grant numbers:U2341212); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992555","Large Language Models;API Hallucination;Code Generation;Retrieval-Augmented Generation;Prompt Engineering","Codes;Error analysis;Large language models;Documentation;Benchmark testing;Libraries;Software;Software reliability;Optimization;Software development management","","","","57","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Do Code LLMs Understand Design Patterns?","Z. Pan; X. Song; Y. Wang; R. Cao; B. Li; Y. Li; H. Liu",Northwestern University; Northwestern University; Zhejiang University; Alibaba Group; Alibaba Group; Alibaba Group; Northwestern University,2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","209","212","Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project’s design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models’ understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028452","large language models;code llm;software engineering;empirical analysis","Adaptation models;Codes;Large language models;Conferences;Computer bugs;Encoding;Software reliability;Pattern recognition;Standards;Software development management","","","","17","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Impact of Large Language Models of Code on Fault Localization","S. Ji; S. Lee; C. Lee; Y. -S. Han; H. Im","Yonsei University, Seoul, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","302","313","Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each method or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest. In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune 13 representative encoder, encoder-decoder, and decoder-based LLMCs (across 7 different architectures) for FL tasks. Unlike previous approaches, LLM Cs can analyze code sequences that do not compile. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6%, 64.2%, and 72.3% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. We also conduct an in-depth investigation of key factors that may affect the FL performance of LLMCs. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989036","Fault Localization;Vulnerability Detection;Large Language Model of Code;Fine-Tuning;Deep Learning","Location awareness;Software testing;Codes;Large language models;Source coding;Computer architecture;Benchmark testing;Feature extraction;Software debugging;Software engineering","","1","","69","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"HPC-Coder: Modeling Parallel Programs using Large Language Models","D. Nichols; A. Marathe; H. Menon; T. Gamblin; A. Bhatele","Department of Computer Science, University of Maryland, College Park, MD, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Livermore Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Computer Science, University of Maryland, College Park, MD, USA",ISC High Performance 2024 Research Paper Proceedings (39th International Conference),"10 May 2024","2024","","","1","12","Parallel programs in high performance computing (HPC) continue to grow in complexity and scale in the exascale era. The diversity in hardware and parallel programming models make developing, optimizing, and maintaining parallel software even more burdensome for developers. One way to alleviate some of these burdens is with automated development and analysis tools. Such tools can perform complex and/or remedial tasks for developers that increase their productivity and decrease the chance for error. Until recently, such tools for code development and performance analysis have been limited in the complexity of tasks they can perform, especially for parallel programs. However, with recent advancements in language modeling, and the availability of large amounts of open-source code related data, these tools have started to utilize predictive language models to automate more complex tasks. In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes. We introduce a new dataset of HPC and scientific codes and use it to fine-tune several pre-trained models. We compare several pre-trained LLMs on HPC-related tasks and introduce a new model, HPC-Coder, fine-tuned on parallel codes. In our experiments, we show that this model can auto-complete HPC functions where generic models cannot, decorate for loops with OpenMP pragmas, and model performance changes in scientific application repositories as well as programming competition solutions.","","978-3-9826336-0-2","10.23919/ISC.2024.10528929","National Science Foundation(grant numbers:2047120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528929","large language models;parallel code generation;performance modeling","Productivity;Codes;Parallel programming;High performance computing;Predictive models;Data models;Software","","17","","43","CCBY","10 May 2024","","","Prometeus GmbH","Prometeus GmbH Conferences"
"Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering","S. Huang; F. Fu; K. Yang; K. Zhang; F. Yang","School of Media Engineering, Communication University of Zhejiang, Hangzhou, China; School of Media Engineering, Communication University of Zhejiang, Hangzhou, China; School of Media Engineering, Communication University of Zhejiang, Hangzhou, China; School of Media Engineering, Communication University of Zhejiang, Hangzhou, China; School of Media Engineering, Communication University of Zhejiang, Hangzhou, China",2024 IEEE 4th International Conference on Software Engineering and Artificial Intelligence (SEAI),"20 Sep 2024","2024","","","220","225","The rapid advancement of artificial intelligence (AI) has led to significant strides in the development of large language models, which are increasingly adept at comprehending and processing natural language. These models possess substantial memory capacity and can exhibit logical reasoning. However, their efficacy in addressing the emotional and empathetic aspects of human interaction remains a challenge. Current models may provide superficial advice that fails to penetrate the depth of an individual's emotional state.We introduce a novel approach to enhance the emotional intelligence of large language models, aiming to fostering a more empathetic and emotionally attuned interaction with users. We conducted experiments with several prominent models, including ChatGLM and ERNIE Bot, em-ploying a variety of promptings. These ranged from presenting examples without explicit directives to providing a limited set of examples and extending narratives from a given context. We simulated therapeutic conversations to evaluate the models' performance in emotionally charged scenarios. Our findings indicate that by refining the guidance mechanisms for these models, it is possible substantially to improve their capacity for emotional engagement and understanding.","","979-8-3503-7434-6","10.1109/SEAI62072.2024.10674052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10674052","Large language model;Prompt Engineering;artificial intelligence;psychological counseling","Employee welfare;Emotion recognition;Large language models;Refining;Natural languages;Psychology;Chatbots","","2","","12","IEEE","20 Sep 2024","","","IEEE","IEEE Conferences"
"Resilient, Federated Large Language Models over Wireless Networks: Why the PHY Matters","V. C. Andrei; A. Djuhera; X. Li; U. J. Mönich; W. Saad; H. Boche","Chair of Theoretical Information Technology, Technical University of Munich, Munich, Germany; Chair of Theoretical Information Technology, Technical University of Munich, Munich, Germany; Chair of Theoretical Information Technology, Technical University of Munich, Munich, Germany; Chair of Theoretical Information Technology, Technical University of Munich, Munich, Germany; Electrical and Computer Engineering Department, Virginia Tech, Arlington, VA, USA; Chair of Theoretical Information Technology, Technical University of Munich, Munich, Germany",GLOBECOM 2024 - 2024 IEEE Global Communications Conference,"11 Mar 2025","2024","","","5211","5216","In this paper, the problem of training large language models (LLMs) in split federated learning over real-world wireless networks is investigated. In the considered system, the embedding layers of an LLM are first computed at a client and then trans-mitted over a wireless MIMO-OFDM link to a server instance for further processing, continuing the forward- and initiating the backpropagation of the training to the originating client. Due to channel impairments and adversarial attacks, the server needs to compute the model losses and gradients using corrupted parameters such as embeddings in LLMs. The computation of the corresponding model losses is rigorously characterized using such perturbed embeddings and a direct connection to the communication mean-squared error (MSE) for models beyond simple neural networks is established. Subsequently, the communication errors are modeled as part of the training process, and a method to design beamforming, scheduling and power allocation is proposed, ensuring high task performance and model convergence even in the case of worst-case jamming. Results on two natural language processing tasks using different LLM architectures confirm the validity of the theoretical analysis and prove the effectiveness of the proposed wireless system design in terms of accuracy and F1 score.","2576-6813","979-8-3503-5125-5","10.1109/GLOBECOM52923.2024.10901454","Ministry of Education; Ministry of Economic Affairs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10901454","Federated Learning;Beamforming;scheduling and resource allocation;Multiple-Input-Multiple-Output (MIMO);Large language models (LLMs)","Training;Wireless sensor networks;Upper bound;Computational modeling;Large language models;Wireless networks;MIMO;Servers;Resource management;Jamming","","1","","21","IEEE","11 Mar 2025","","","IEEE","IEEE Conferences"
"Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code","J. Shin; C. Tang; T. Mohati; M. Nayebi; S. Wang; H. Hemmati","Lassonde School of Engineering, York University, Toronto, Canada; NA; Schulich School of Engineering, University of Calgary, Calgary, Canada; Lassonde School of Engineering, York University, Toronto, Canada; Lassonde School of Engineering, York University, Toronto, Canada; Lassonde School of Engineering, York University, Toronto, Canada",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","490","502","The rapid advancements in large language models (LLMs) have greatly expanded the potential for automated code-related tasks. Two primary methodologies are used in this domain: prompt engineering and fine-tuning. Prompt engineering involves applying different strategies to query LLMs, like Chat-GPT, while fine-tuning further adapts pre-trained models, such as CodeBERT, by training them on task-specific data. Despite the growth in the area, there remains a lack of comprehensive comparative analysis between the approaches for code models. In this paper, we evaluate GPT-4 using three prompt engineering strategies-basic prompting, in-context learning, and taskspecific prompting-and compare it against 17 fine-tuned models across three code-related tasks: code summarization, generation, and translation. Our results indicate that GPT-4 with prompt engineering does not consistently outperform fine-tuned models. For instance, in code generation, GPT-4 is outperformed by finetuned models by 28.3% points on the MBPP dataset. It also shows mixed results for code translation tasks. Additionally, a user study was conducted involving 27 graduate students and 10 industry practitioners. The study revealed that GPT-4 with conversational prompts, incorporating human feedback during interaction, significantly improved performance compared to automated prompting. Participants often provided explicit instructions or added context during these interactions. These findings suggest that GPT-4 with conversational prompting holds significant promise for automated code-related tasks, whereas fully automated prompt engineering without human involvement still requires further investigation.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025620","Prompt engineering;Fine-tuning;LLM4SE;Empirical study;Survey","Industries;Training;Surveys;Adaptation models;Codes;Translation;Large language models;Software;Data models;Prompt engineering","","","","90","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"AAINA: Autonomous and Autogenerative Integrable Narrative All-Purpose Chatbot Using Regenerative AI","A. Singhal; A. Singh; A. Sripesh; N. K. Pandey; A. K. Mishra; A. Dumka","Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Department of CSE, Graphic Era Hill University, Dehradun, India; Department of CSE, Women Institute of Technology, Dehradun, India","2024 International Conference on Computer, Electronics, Electrical Engineering & their Applications (IC2E3)","10 Jan 2025","2024","","","1","6","This document is about evolution of Generative AI and Large Language Models to make multipurpose chatbots. It revolves around how the appropriate models are selected, trained on the required data, then further fine-tuned for customization, optimization and deployment, and final augmentation with the help of certain frameworks to get better results and model performance. This paper offers comprehensive instructions for developing AAINA, an LLM-powered chatbot because AAINA can process many input formats, like text, voice, images, and video. It can be highly customized to work with different communication channels. The paper also explores how Natural Language Processing (NLP) methods can be combined with LLMs to create a chatbot that can hold individual and meaningful conversations. The paper describes the potential uses of AAINA in different industries, such as IT, education, law, automotive, and medical industry, showcasing its capability to provide 24/7 assistance and effectively resolve user issues. Through the process of fine-tuning AAINA, users can fine-tune the chatbot for specific requirements, enabling a personalized and optimal user experience.","","979-8-3503-8853-4","10.1109/IC2E362166.2024.10827618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827618","Large Language Models;Chatbot;fine-tuning;GPT;Claude;LLMs;Generative AI;Language Models for Massive Applications;Retrival Augmented Generation;Outcome Reward Models;Transformer Model;Prompt Engineering;Re-inforcement learning","Industries;Generative AI;Large language models;Education;Oral communication;Communication channels;Chatbots;Data models;User experience;Optimization","","","","15","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"VeCoGen: Automating Generation of Formally Verified C Code With Large Language Models","M. Sevenhuijsen; K. Etemadi; M. Nyberg","Scania & KTH Royal Institute of Technology, Södertälje, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Scania & KTH Royal Institute of Technology, Södertälje, Sweden",2025 IEEE/ACM 13th International Conference on Formal Methods in Software Engineering (FormaliSE),"12 Jun 2025","2025","","","101","112","Large language models have demonstrated impressive capabilities in generating code, yet they often produce programs with flaws or deviations from intended behavior, limiting their suitability for safety-critical applications. To address this limitation, this paper introduces VeCoGen, a novel tool that combines large language models with formal verification to automate the generation of formally verified C programs. VeCoGen takes a formal specification in ANSI/ISO C Specification Language, a natural language specification, and a set of test cases to attempt to generate a verified program. This program-generation process consists of two steps. First, VeCoGen generates an initial set of candidate programs. Secondly, the tool iteratively improves on previously generated candidates. If a candidate program meets the formal specification, then we are sure the program is correct. We evaluate VeCoGen on 15 problems presented in Codeforces competitions. On these problems, VeCoGen solves 13 problems. This work shows the potential of combining large language models with formal verification to automate program generation.","2575-5099","979-8-3315-3794-4","10.1109/FormaliSE66629.2025.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024330","Code Generation;Large Language Models;Formal Verification;Iterative Code Improvement","Codes;Program processors;Limiting;Large language models;Natural languages;Programming;Specification languages;Formal specifications;Formal verification;Software development management","","","","61","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Deterministic Automatic Refactoring at Scale","J. Gehring",Moderne,2023 IEEE International Conference on Software Maintenance and Evolution (ICSME),"11 Dec 2023","2023","","","541","546","In the context of continually growing large code repositories where code refactoring is an ongoing requirement, we highlight the effectiveness of OpenRewrite as a tool for conducting large-scale code refactoring. OpenRewrite leverages Lossless Semantic Trees (LST) to represent code and applies recipes to search and implement changes. These recipes are openly available and can be executed locally or accessed through the Moderne platform for public repositories. We provide a concise overview of the underlying technology, instructions for utilizing the tool, and we compare its performance against a manual approach and two prominent large language models (LLM): ChatGPT and StarChat-β. Our comparison is based on the execution time of the tool and the accuracy of the implemented changes. Additionally, we present three distinct use cases that demonstrate the versatile applications of the tool. A demonstration of OpenRewrite’s recipe which detects vulnerabilities and automatically fixes them is available at the following link: https://www.youtube.com/watch?v=L1-_cQUX-JA.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336295","Automatic code refactoring;AST;LST;JUnit;Vulnerabilities;Code Migration;Static Code Analysis;Generative AI","Software maintenance;Codes;Costs;Semantics;Manuals;Writing;Chatbots","","","","20","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Analyzing the Potency of Pretrained Transformer Models for Automated Program Repair","M. Leiwig; B. Swierzy; C. Bungartz; M. Meier","University of Bonn, Lamarr Institute, Bonn, Germany; University of Bonn, Bonn, Germany; University of Bonn, Lamarr Institute, Bonn, Germany; Fraunhofer FKIE, University of Bonn, Lamarr Institute, Bonn, Germany",2024 50th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),"27 Dec 2024","2024","","","72","79","Manually finding and fixing bugs is cumbersome work, which consumes valuable resources in the software development cycle. In this work, we examine the capability of pretrained transformer models to tackle the task of automated program repair. Previous research has been focused on inherently different machine learning architectures for solving this use case. Our contributions include a novel dataset for fine-tuning the models, the introduction of a windowing technique augmenting the pretrained model and the evaluation on the commonly used Defects4J benchmark along with an ablation study. The findings demonstrate that leveraging our dataset leads to enhanced model performance surpassing Bugs2Fix. Our model enhancements significantly boost overall performance, enabling resulting models to achieve parity with the current state of the art by fixing 30 bugs in 27 minutes on Defects4J. This shows that pretrained transformers are promising for the task of automated bug fixing and should be considered by future research. However, similar to the existing state-of-the-art solutions, the performance still needs be improved to provide practical benefits to end users.","2376-9521","979-8-3503-8026-2","10.1109/SEAA64295.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803362","Software debugging;Computer aided software engineering;Artificial intelligence","Location awareness;Analytical models;Adaptation models;Computer bugs;Machine learning;Maintenance engineering;Transformers;Software;Security;Software development management","","","","19","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"An Approach for Rapid Source Code Development Based on ChatGPT and Prompt Engineering","Y. Li; J. Shi; Z. Zhang","Purple Mountain Laboratories, Nanjing, Jiangsu, China; Purple Mountain Laboratories, Nanjing, Jiangsu, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Information Engineering University, Zhengzhou, Henan, China",IEEE Access,"18 Apr 2024","2024","12","","53074","53087","Code generation stands as a powerful technique in modern software development, improving development efficiency, reducing errors, and fostering standardization and consistency. Recently, ChatGPT has exhibited immense potential in automatic code generation. However, existing researches on code generation lack guidance for practical software development process. In this study, we utilized ChatGPT to develop a web-based code generation platform consisting of key components: User Interface, Prompt Builder, and Backend Service. Specifically, Prompt Builder dynamically generated comprehensive prompts to enhance model generation performance. We conducted experiments on 2 datasets to evaluate the performance of code generation in our approach. through 8 widely used metrics. The results demonstrate that (1) our Prompt Builder is effective, resulting in a 65.06% improvement in the exact match (EM), a 38.45% improvement in Bilingual Evaluation Understudy (BLEU), a 15.70% improvement in CodeBLEU, and a 50.64% improvement in Pass@1. (2) In real development scenarios, 98.5% of test cases can be validated through manual validation, highlighting the genuine assistance provided by the ChatGPT-based code generation approach.","2169-3536","","10.1109/ACCESS.2024.3385682","Science and Technology Project of the State Grid Corporation of China(grant numbers:No.5108-202324069A-1-1-ZN); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493021","ChatGPT;prompt engineering;code generation;software development;large language models","Codes;Chatbots;Natural languages;Source coding;Software;Task analysis;Computational modeling;Software development management;Large language models","","10","","40","CCBYNCND","8 Apr 2024","","","IEEE","IEEE Journals"
"Semantics Preserving Emoji Recommendation with Large Language Models","Z. Qiu; K. Qiu; H. Lyu; W. Xiong; J. Luo","School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","7131","7140","Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent. Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text. However, they ignore the essence of users’ behavior on social media in that each text can correspond to multiple reasonable emojis. To better assess a model’s ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model’s ability to recommend emojis that maintain the semantic consistency with the user’s text. To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged. If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics. The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation. To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task. Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%. Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis (https://github.com/VIStA-H/SemanticsPreservingEmojiRec).","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825696","semantics preserving;emoji recommendation;large language models;prompt engineering","Social networking (online);Large language models;Semantics;Predictive models;Big Data;Benchmark testing;Digital communication;Data models;Complexity theory;Emojis","","2","","42","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Toward Hardware Security Benchmarking of LLMs","R. Afsharmazayejani; M. M. Shahmiri; P. Link; H. Pearce; B. Tan","University of Calgary, Canada; University of Calgary, Canada; University of Calgary, Canada; University of New South Wales, Sydney, Australia; University of Calgary, Canada",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","7","With the rapid advancement and proliferation of large language models (LLMs), there is a pressing need to explore and, crucially, evaluate their utility. Recently, LLMs have shown promise in digital design, with evidence of some ability to produce functional HDL code. However, to better understand LLM capabilities and guide the ongoing development of LLMs, we need approaches to evaluate the quality of generated artifacts across myriad dimensions. Thus, this work proposes an approach for evaluating the security of LLM-generated designs, which is especially important as security is an ongoing concern. We provide new insights into the challenges and desiderata for benchmarking LLMs for hardware security risks. This paper outlines our initial work developing a security-focused evaluation suite for LLM-aided HDL generation. We present an illustrative preliminary use of our evaluation suite to show the insights we can gain from security evaluation.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691745","Natural Sciences and Engineering Research Council of Canada; Intel Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691745","Hardware Security;CAD;LLM;Benchmarking","Codes;Hardware security;Large language models;Conferences;Pressing;Benchmark testing;Hardware design languages","","2","","29","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Transitive Inference in Large Language Models and Prompting Intervention","W. Wu; W. Deng","Mashang Consumer Finance Co., Ltd, Chongqing, China; Mashang Consumer Finance Co., Ltd, Chongqing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Transitive inference (TI) is a critical form of deductive reasoning, essential to both human and animal cognition. This study explores whether state-of-the-art large language models (LLMs) possess TI capabilities and examines the impact of two emerging prompting methods on model performance. Four LLMs—GPT-3.5-Turbo, GPT-4, Llama3-8B, and Qwen—are evaluated using a TI task involving a 10-item hierarchy. Results indicate that these models demonstrate solid performance, along with human-like behavioral effects such as the symbolic distance effect, terminal item effect, and context effect. The sequence of input premises significantly affects model accuracy, with all models showing a preference for the chain condition over the jump condition. Two prompting methods — Model Confidence prompts (likelihood tests) and Chain-of-Thought prompts—are applied in order to further enhance TI performance. While GPT-4 benefits the most from these prompts, other models experience a decline in performance. Additionally, behavioral biases like the terminal item effect persist and are even amplified following prompt adjustments. Therefore, specific prompting methods may not be universally effective across different models, and in some cases, may cause adverse effects. Further research is needed to better understand the nature of TI in LLMs.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889585","large language models;transitive inference;content effect;symbolic distance;prompt engineering","Solid modeling;Animals;Large language models;Psychology;Signal processing;Solids;Cognition;Acoustics;Speech processing;Context modeling","","","","29","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"WebFuzzAuto: An Automated Fuzz Testing Tool Integrating Reinforcement Learning and Large Language Models for Web Security","X. Chen; J. Liu; Y. Zhang; Q. Hu; Y. Han; R. Zhang; J. Ran; L. Yan; B. Huang; S. Ma; J. Wang","Department of Information, Beijing City University, Beijing, China; Key Laboratory of Network Assessment Technology CAS, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Security Technology Department, Suzhou Prism Seven Color Technology Information Co., Ltd., Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China",2024 12th International Conference on Information Systems and Computing Technology (ISCTech),"22 Jan 2025","2024","","","1","10","This paper proposes a novel, fully automated fuzz testing tool for web applications that integrates reinforcement learning and large model techniques to enhance the intelligence of vulnerability detection and code coverage. By designing a reinforcement learning environment, the testing agent can intelligently explore complex web applications and continuously adjust strategies to discover more vulnerabilities. The tool employs dynamic adjusters and curriculum learning strategies to gradually optimize key parameters, enhancing the stability and convergence speed of the model. Additionally, it integrates a vulnerability detection module and a feedback manager. After each testing step, it calculates rewards and adjusts expert feedback based on the detection results, further optimizing the model’s learning path. Furthermore, it leverages natural language processing techniques from large language models for deep analysis, formulating testing strategies and guiding the behavior of the testing agent. Experimental results demonstrate that this tool outperforms the traditional OWASP ZAP tool in terms of code coverage and vulnerability detection rates on three major web applications: OWASP Juice Shop, phpMyAdmin, and WordPress, verifying its effectiveness and advantages in complex web applications.","","979-8-3503-7986-0","10.1109/ISCTech63666.2024.10845318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845318","web security;Reinforcement Learning;Large Language Models;vulnerability detection","Codes;Large language models;Computational modeling;Reinforcement learning;Fuzzing;Stability analysis;Natural language processing;Security;Testing;Information systems","","","","19","IEEE","22 Jan 2025","","","IEEE","IEEE Conferences"
"The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats","W. Brach; K. Košťál; M. Ries","Faculty of Informatics and Information Technologies, Slovak University of Technology, Bratislava, Slovakia; Faculty of Informatics and Information Technologies, Slovak University of Technology, Bratislava, Slovakia; Faculty of Informatics and Information Technologies, Slovak University of Technology, Bratislava, Slovakia",IEEE Access,"30 May 2025","2025","13","","91808","91825","The exponential growth of unstructured text data presents a fundamental challenge in modern data management and information retrieval. While Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, their potential to transform unstructured text into standardized, structured formats remains largely unexplored - a capability that could revolutionize data processing workflows across industries. This study breaks new ground by systematically evaluating LLMs’ ability to convert unstructured recipe text into the structured Cooklang format. Through comprehensive testing of four models (GPT-4o, GPT-4o-mini, Llama3.1:70bLlama3.3:70b, and Llama3.1:8b), an innovative evaluation approach is introduced that combines traditional metrics (WER, ROUGE-L, TER) with specialized metrics for semantic element identification. Our experiments reveal that GPT-4o with few-shot prompting achieves breakthrough performance (ROUGE-L: 0.8209, WER: 0.3509), demonstrating for the first time that LLMs can reliably transform domain-specific unstructured text into structured formats without extensive training. Although model performance generally scales with size, we uncover surprising potential in smaller models like Llama3.1:8b for optimization through targeted fine-tuning. These findings open new possibilities for automated structured data generation across various domains, from medical records to technical documentation, potentially transforming the way organizations process and utilize unstructured information. For further information, source code, and associated resources, please refer to the source code repository (https://github.com/fiit-ba/text-to-cooklang).","2169-3536","","10.1109/ACCESS.2025.3573030","EU NextGenerationEU through the Recovery and Resilience Plan for Slovakia(grant numbers:09I05-03-V02-00057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11014059","Large language models;text generation;natural language processing;prompt engineering;structured text;few-shot learning","Measurement;Accuracy;Natural language processing;Documentation;Transforms;Medical services;Cognition;Large language models;Analytical models;Training","","","","48","CCBY","23 May 2025","","","IEEE","IEEE Journals"
"Advances and Open Challenges in Federated Foundation Models","C. Ren; H. Yu; H. Peng; X. Tang; B. Zhao; L. Yi; A. Z. Tan; Y. Gao; A. Li; X. Li; Z. Li; Q. Yang","Wallenberg-NTU Presidential Postdoctoral Fellow with the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computer, Nankai University, China; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Sweden; School of Medicine, Yale University, USA; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Executive Vice President of the Digital Research Institute of ENN Group, Langfang, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and the Chief AI Officer of WeBank, Shenzhen, China",IEEE Communications Surveys & Tutorials,"","2025","PP","99","1","1","The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI). This integration offers enhanced capabilities, while addressing concerns of privacy, data decentralization and computational efficiency. This paper provides a comprehensive survey of the emerging field of Federated Foundation Models (FedFM), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of FMs. A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization. Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed. Moreover, this paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL. It highlights the potential of quantum computing to revolutionize the processes of training, inference, optimization and security. This survey also introduces the implementation requirement of FedFM and some practical FedFM applications. It highlights lessons learned with a clear understanding of our findings for FedFM. Finally, this survey not only provides insights into the current state and challenges of FedFM, but also offers a blueprint for future research directions, emphasizing the need for developing trustworthy solutions. It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field.","1553-877X","","10.1109/COMST.2025.3552524","Internal talent award (TRACS) with Wallenberg-NTU Presidential Postdoctoral Fellowship; National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); RIE 2020 Advanced Manufacturing and Engineering (AME) Programmatic Fund(grant numbers:A20G8b0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930890","Federated learning;foundation models;federated foundation models;large language models;efficient training/aggregation;trustworthiness;incentivization;evaluation;quantum computing","Frequency modulation;Surveys;Training;Artificial intelligence;Adaptation models;Computational modeling;Foundation models;Data privacy;Reviews;Security","","5","","","IEEE","18 Mar 2025","","","IEEE","IEEE Early Access Articles"
"VRank: Enhancing Verilog Code Generation from Large Language Models via Self-Consistency","Z. Zhao; R. Qiu; I. -C. Lin; G. L. Zhang; B. Li; U. Schlichtmann","Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany; Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany; Computer Architecture and IC Design, National Cheng Kung University; Hardware for Artificial Intelligence Group, Technical University of Darmstadt; Research Group of Digital Integrated Systems, University of Siegen; Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany",2025 26th International Symposium on Quality Electronic Design (ISQED),"30 May 2025","2025","","","1","7","Large Language Models (LLMs) have demonstrated promising capabilities in generating Verilog code from module specifications. To improve the quality of such generated Verilog codes, previous methods require either time-consuming manual inspection or generation of multiple Verilog codes, from which the one with the highest quality is selected with manually designed testbenches. To enhance the generation efficiency while maintaining the quality of the generated codes, we propose VRank, an automatic framework that generates Verilog codes with LLMs. In our framework, multiple code candidates are generated with LLMs by leveraging their probabilistic nature. Afterwards, we group Verilog code candidates into clusters based on identical outputs when tested against the same testbench, which is also generated by LLMs. Clusters are ranked based on the consistency they show on testbench. To determine the best candidate, Chain-of-Thought is further applied to select the best candidate from the top-ranked clusters. By systematically analyzing diverse outputs of generated codes, VRank reduces errors and enhances the overall quality of the generated Verilog code. Experimental results on the VerilogEval-Human benchmark demonstrate a significant 10.5% average increase in functional correctness (pass@1) across multiple LLMs, demonstrating VRank's effectiveness in improving the accuracy of automated hardware description language generation for complex design tasks.","1948-3295","979-8-3315-0942-2","10.1109/ISQED65160.2025.11014398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11014398","Large Language Model;Verilog code generation","Codes;Accuracy;Large language models;Manuals;Inspection;Probabilistic logic;Hardware;Cognition;Reliability;Hardware design languages","","","","35","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"Ten Challenging Problems in Federated Foundation Models","T. Fan; H. Gu; X. Cao; C. S. Chan; Q. Chen; Y. Chen; Y. Feng; Y. Gu; J. Geng; B. Luo; S. Liu; W. K. Ong; C. Ren; J. Shao; C. Sun; X. Tang; H. X. Tae; Y. Tong; S. Wei; F. Wu; W. Xi; M. Xu; H. Yang; X. Yang; J. Yan; H. Yu; H. Yu; T. Zhang; Y. Zhang; X. Zhang; Z. Zheng; L. Fan; Q. Yang","WeBank, Shenzhen, China; WeBank, Shenzhen, China; Southwestern University of Finance and Economic, Chengdu, China; Universiti Malaya, Kuala Lumpur, Malaysia; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Southwestern University of Finance and Economic, Chengdu, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Duke Kunshan University, Kunshan, China; Duke Kunshan University, Kunshan, China; Institute of Innovation, E Fund Management Company Ltd., Guangzhou, China; Universiti Malaya, Kuala Lumpur, Malaysia; KTH Royal Institute of Technology, Stockholm, Sweden; Duke Kunshan University, Kunshan, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Universiti Malaya, Kuala Lumpur, Malaysia; Beihang University, Beijing, China; Beihang University, Beijing, China; Shanghai Jiao Tong University, Shanghai, China; Xi’an Jiaotong University, Xi’an, China; Huazhong University of Science and Technology, Wuhan, China; Xi’an Jiaotong University, Xi’an, China; Southwestern University of Finance and Economic, Chengdu, China; Institute of Innovation, E Fund Management Company Ltd., Guangzhou, China; Southwestern University of Finance and Economic, Chengdu, China; Nanyang Technological University, Singapore; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Nanyang Technological University, Singapore; Huazhong University of Science and Technology, Wuhan, China; Shanghai Jiao Tong University, Shanghai, China; WeBank, Shenzhen, China; Academy for Artificial Intelligence, Hong Kong Polytechnic University, Kowloon, Hong Kong",IEEE Transactions on Knowledge and Data Engineering,"5 Jun 2025","2025","37","7","4314","4337","Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: “Foundational Theory,” which aims to establish a coherent and unifying theoretical framework for FedFMs. “Data,” addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; “Heterogeneity,” examining variations in data, model, and computational resources across clients; “Security and Privacy,” focusing on defenses against malicious attacks and model theft; and “Efficiency,” highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.","1558-2191","","10.1109/TKDE.2025.3555328","National Natural Science Foundation of China(grant numbers:62476228,62425202); Suzhou Frontier Science and Technology Program(grant numbers:SYG202310); Ministry of Education, Singapore, under its Academic Research Fund Tier 1, the Ministry of Higher Education, Malaysia(grant numbers:FRGS/1/2024/ICT02/UM/01/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10944288","Federated foundation models (FedFMs);federated learning;foundation models;large language models;privacy-preserving Ai","Frequency modulation;Foundation models;Privacy;Optimization;Adaptation models;Watermarking;Knowledge transfer;Training;Fans;Data privacy","","","","232","IEEE","28 Mar 2025","","","IEEE","IEEE Journals"
"Research on Program Automatic Repair Method Combining Context Optimization Strategy and Large Language Models","Y. Li; J. Guo","Beijing University of Chemical Technology, Beijing, China; Beijing University of Chemical Technology, Beijing, China",2024 4th International Symposium on Computer Technology and Information Science (ISCTIS),"4 Oct 2024","2024","","","26","34","Automated program repair techniques address software errors, vulnerabilities, and defects through automation. With the rapid development of deep learning, deep learning-based automated repair techniques have improved repair performance but still face challenges of high data demands and low accuracy. The emergence of large language models offers new solutions. This paper proposes a program automated repair method called CodeFixer, which combines contextual optimization strategies and large language models. This method utilizes a Tree-LSTM model to learn the relationship between erroneous statements and their contexts, providing relevant contextual information to the large language model. By employing prompt engineering, the program repair process is divided into error analysis and patch generation stages. During patch generation, the quality and reliability of patches are enhanced by integrating tree-of-thoughts and model fine-tuning, and candidate patches are optimized through an automated evaluation consistency strategy. To evaluate the performance of CodeFixer, experiments were conducted on 395 errors in the Defects4J V1.2 dataset, successfully repairing 92 errors. Validation results on the QuixBugs dataset in both Java and Python programming languages also outperformed existing automated repair techniques.","","979-8-3503-5456-0","10.1109/ISCTIS63324.2024.10698980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698980","Automatic Program Repair;Large Language Models;Patch Generation;Tree of Thoughts","Accuracy;Large language models;Maintenance engineering;Reliability theory;Software;Prompt engineering;Optimization;Long short term memory;Context modeling;Python","","","","18","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Exploiting LLMs for E-Learning: A Cybersecurity Perspective on AI-Generated Tools in Education","D. Greco; L. Chianese","DIG - Department of Management, Economics and Industrial Engineering Politecnico di Milano, Milan, Italy; DIBRIS - Department of Informatics, Bioengineering, Robotics and Systems Engineering, Università degli Studi di Genova, Genoa, Italy",2024 IEEE International Workshop on Technologies for Defense and Security (TechDefense),"7 Feb 2025","2024","","","237","242","AI and LLM technologies have emerged rapidly, creating more opportunities to develop and enhance e-learning. This paper provides a detailed account of the tools built by the current and future generations of LLMs and AI technologies for creating complex e-learning applications. It introduces the application of these tools, including innovative learning, context-awareness and intelligent learning systems. Moreover, it delves systematically into the security issues of these AI-integrated e-learning environments. The work applies strict research approaches to determine and describe possible risks in system design, data processing, and AI model deployment. We also undertake deeper penetration tests to assess the level of preparedness of these platforms against different types of attacks from the attackers such as attacks targeting the AI components, injection of unverified data and privacy violations. Our study findings reveal the opportunity to apply AI in e-learning, thus highlighting the importance of addressing these concerns. It is a complete guide for the developers and the educational institutions to integrate the LLMs and AI and keep the security strong. This paper adds to the existing literature on the role of AI in education and cybersecurity, providing essential information to the future advancements of safe and innovative e-learning systems.","","979-8-3315-0558-5","10.1109/TechDefense63521.2024.10863662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10863662","Technology-enhanced learning;large language models (LLMs);vulnerability testing;cybersecurity in e-learning;AI-generated code;security assessment;vulnerability analysis;GPT-4o;web application security","Data privacy;Analytical models;Electronic learning;Conferences;Data processing;Data models;Application security;Artificial intelligence;System analysis and design","","2","","19","IEEE","7 Feb 2025","","","IEEE","IEEE Conferences"
"Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration","X. Cheng; B. Liu; X. Liu; E. Liu; Z. Huang","State Key Laboratory of Photonics and Communications, School of Electronics, Peking University, Beijing, China; State Key Laboratory of Photonics and Communications, School of Electronics, Peking University, Beijing, China; State Key Laboratory of Photonics and Communications, School of Electronics, Peking University, Beijing, China; State Key Laboratory of Photonics and Communications, School of Electronics, Peking University, Beijing, China; State Key Laboratory of Photonics and Communications, School of Electronics, Peking University, Beijing, China",IEEE Transactions on Network Science and Engineering,"","2025","PP","99","1","21","To support future intelligent multifunctional sixthgeneration (6G) wireless communication networks, Synesthesia of Machines (SoM) is proposed as a novel paradigm for artificial intelligence (AI)-native intelligent multi-modal sensingcommunication integration. However, existing SoM system designs rely on task-specific AI models and face challenges such as scarcity of massive high-quality datasets, constrained modeling capability, poor generalization, and limited universality. Recently, foundation models (FMs) have emerged as a new deep learning paradigm and have been preliminarily applied to SoM-related tasks, but a systematic design framework is still lacking. In this paper, we for the first time present a systematic categorization of FMs for SoM system design, dividing them into generalpurpose FMs, specifically large language models (LLMs), and SoM domain-specific FMs, referred to as wireless foundation models. Furthermore, we derive key characteristics of FMs in addressing existing challenges in SoM systems and propose two corresponding roadmaps, i.e., LLM-based and wireless foundation model-based design. For each roadmap, we provide a framework containing key design steps as a guiding pipeline and several representative case studies of FM-empowered SoM system design. Specifically, we propose LLM-based path loss generation (LLM4PG) and scatterer generation (LLM4SG) schemes, and wireless channel foundation model (WiCo) for SoM mechanism exploration, LLM-based wireless multi-task SoM transceiver (LLM4WM) and wireless foundation model (WiFo) for SoMenhanced transceiver design, and wireless cooperative perception foundation model (WiPo) for SoM-enhanced cooperative perception, demonstrating the significant superiority of FMs over taskspecific models. Finally, we summarize and highlight potential directions for future research.","2327-4697","","10.1109/TNSE.2025.3587238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11074348","Intelligent multi-modal sensing-communication integration;Synesthesia of Machines (SoM);foundation models (FMs);large language models (LLMs);wireless foundation models","Sensors;Frequency modulation;Wireless communication;Wireless sensor networks;Foundation models;Transceivers;Radio frequency;Training;System analysis and design;Point cloud compression","","","","","IEEE","8 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation","S. Tomic; E. Alégroth; M. Isaac","Blekinge Institute of Technology, Karlskrona, Sweden; Blekinge Institute of Technology, Karlskrona, Sweden; Synteda, Gothenburg, Sweden","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","487","497","Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present PathFinder, a Multi-Agent LLM (MALLM) framework that incorporates four agents responsible for (a) perception and summarization, (b) decision-making, (c) input handling and extraction, and (d) validation, which work collaboratively to automate exploratory web-based GUI testing. The goal of this study is to assess how different LLMs, applied to different agents, affect the efficacy of automated exploratory GUI testing. We evaluate PathFinder with three models, Mistral-Nemo, Gemma2, and Llama3.1, on four e-commerce websites. Thus, 27 permutations of the LLMs, across three agents (excluding the validation agent), to test the hypothesis that a solution with multiple agents, each using different LLMs, is more efficacious (efficient and effective) than a multi-agent solution where all agents use the same LLM. The results indicate that the choice of LLM constellation (combination of LLMs) significantly impacts efficacy, suggesting that a single LLM across agents may yield the best balance of efficacy (measured by F1-score). Hypothesis to explain this result include, but are not limited to: improved decision-making consistency and reduced task coordination discrepancies. The contributions of this study are an architecture for MALLM-based GUI testing, empirical results on its performance, and novel insights into how LLM selection impacts the efficacy of automated testing.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989038","Vinnova(grant numbers:2024-00242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989038","Multi-Agent Systems;Large Language Models (LLMs);Automated Testing;MALLM;AI-Assisted Software Testing","Software testing;Adaptation models;Large language models;Decision making;Electronic commerce;Test pattern generators;Testing;Graphical user interfaces;Periodic structures;Multi-agent systems","","","","33","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Evaluating the Efficacy of Large Language Models in Identifying Phishing Attempts","H. Patel; U. Rehman; F. Iqbal","Department of Computer Science, Western University, London, Ontario; Department of Computer Science, Western University, London, Ontario; College of Technological Innovation, Zayed University, Zubai, United Arab Emirates",2024 16th International Conference on Human System Interaction (HSI),"9 Aug 2024","2024","","","1","7","Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world. By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security. These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources. By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information. Building on this pervasive issue within modern technology, this paper will aim to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of “419 Scam” emails. The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria. The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails.","2158-2254","979-8-3503-6291-6","10.1109/HSI61632.2024.10613528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10613528","Phishing Email Detection;Large Language Models (LLMs);General Pretrained Transformer (GPT);Bidirectional Encoder Representations from Transformers (BERT);Natural Processing Language (NPL);Social Engineering","Adaptation models;Phishing;Large language models;Computational modeling;Focusing;Predictive models;Chatbots","","1","","36","IEEE","9 Aug 2024","","","IEEE","IEEE Conferences"
"Leveraging LLMs for Automated Analysis of Biomedical Data","R. Ji; K. Gong; L. Huang; W. Yang; R. Yu","School of Public Health, University of Michigan, Ann Arbor, United States; The First Affiliated Hospital of XMU, School of Medicine, Xiamen University, Xiamen, China; The First Affiliated Hospital of XMU, School of Medicine, Xiamen University, Xiamen, China; Aginome Scientific, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China","2024 9th International Conference on Communication, Image and Signal Processing (CCISP)","28 Nov 2024","2024","","","67","71","Large Language Models (LLMs), specifically trained to understand and process natural language queries, represent a transformative approach in biomedical data analysis, particularly for querying and interpreting complex datasets such as those available on cBioPortal. In this paper we implemented an automated workflow to analyze biomedical datasets and return relevant results, where LLMs are utilized for query analysis and code generation tasks. The significance of this work lies in its potential to facilitate access to complex biomedical data, reducing the barrier for researchers with limited computational skills, and to streamline the process of data analysis and interpretation. The use of LLMs ensures that queries can be formulated in intuitive natural language, making this workflow accessible to a broader audience. By automating the data analysis process, our workflow can be applied to a batch of data analysis tasks across a large number of datasets simultaneously, facilitating faster hypothesis generation and validation in biomedical research for accelerating discoveries in cancer genomics and personalized medicine.","","979-8-3503-5665-6","10.1109/CCISP63826.2024.10765518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765518","automated;biomedical data analysis;cBioPortal;large language models","Data analysis;Precision medicine;Large language models;Natural languages;Genomics;Signal processing;Programming;Data models;Reliability;Bioinformatics","","","","13","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Leveraging the Power of LLMs to Transform Robot Programs into Low-Code","B. Schenkenfelder; C. Salomon; M. Schwandtner; R. Zefferer; M. Derfler; M. Wimmer","Software Competence Center Hagenberg GmbH, Hagenberg, Austria; Software Competence Center Hagenberg GmbH, Hagenberg, Austria; Software Competence Center Hagenberg GmbH, Hagenberg, Austria; Software Competence Center Hagenberg GmbH, Hagenberg, Austria; ENGEL Austria GmbH, Schwertberg, Austria; Johannes Kepler University, Linz, Austria",2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA),"16 Oct 2024","2024","","","1","4","Low-Code is a paradigm for domain experts to deal with their software requirements themselves, potentially without training in software engineering. In industrial automation, and in particular in robot programming, there are many layers of abstraction from atomic instructions up to compound skills. Given the recent advances in Artificial Intelligence (AI) for software engineering, we present an initial exploratory study that demonstrates the transformation of robot programs into low-code with Large Language Models (LLMs) using examples from an industry-academia collaboration. In particular, we investigate the capabilities of the LLMs GPT-40 and Mistral 7B in terms of robot program analysis and abstraction. The results show the potential of LLMs for low-code generation, but also raise questions and directions for future research.","1946-0759","979-8-3503-6123-0","10.1109/ETFA61755.2024.10711043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711043","Low-Code;Robot Programming;Large Language Models","Training;Service robots;Large language models;Collaboration;Transforms;Software;Compounds;Manufacturing automation;Software engineering;Robot programming","","","","13","IEEE","16 Oct 2024","","","IEEE","IEEE Conferences"
"A Multi-Expert Large Language Model Architecture for Verilog Code Generation","B. Nadimi; H. Zheng","Dept. of Computer Science and Engineering, University of South Florida, Tampa, Florida, United States; Dept. of Computer Science and Engineering, University of South Florida, Tampa, Florida, United States",2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","5","Recently, there has been a surging interest in using large language models (LLMs) for Verilog code generation. However, the existing approaches are limited in terms of the quality of the generated Verilog code. To address such limitations, this paper introduces an innovative multi-expert LLM architecture for Verilog code generation (MEV-LLM). Our architecture uniquely integrates multiple LLMs, each specifically fine-tuned with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog code for each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691683","Large Language Models;Multiple Expert architectures;fine-tuning;Verilog;dataset;transformers","Codes;Large language models;Conferences;Machine learning;Transformers;Hardware;Complexity theory;Hardware design languages","","8","","22","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"DomainLynx: Leveraging Large Language Models for Enhanced Domain Squatting Detection","D. Chiba; H. Nakano; T. Koide","NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan",2025 IEEE 22nd Consumer Communications & Networking Conference (CCNC),"5 May 2025","2025","","","1","9","Domain squatting poses a significant threat to Internet security, with attackers employing increasingly sophisticated techniques. This study introduces DomainLynx, an innovative compound AI system leveraging Large Language Models (LLMs) for enhanced domain squatting detection. Unlike existing methods focusing on predefined patterns for top-ranked domains, DomainLynx excels in identifying novel squatting techniques and protecting less prominent brands. The system's architecture integrates advanced data processing, intelligent domain pairing, and LLM-powered threat assessment. Crucially, DomainLynx incorporates specialized components that mitigate LLM hallucinations, ensuring reliable and context-aware detection. This approach enables efficient analysis of vast security data from diverse sources, including Certificate Transparency logs, Passive DNS records, and zone files. Evaluated on a curated dataset of 1,649 squatting domains, DomainLynx achieved 94.7% accuracy using Llama-3-70B. In a month-long real-world test, it detected 34,359 squatting domains from 2.09 million new domains, outperforming baseline methods by 2.5 times. This research advances Internet security by providing a versatile, accurate, and adaptable tool for combating evolving domain squatting threats. DomainLynx's approach paves the way for more robust, AI-driven cybersecurity solutions, enhancing protection for a broader range of online entities and contributing to a safer digital ecosystem.","2331-9860","979-8-3315-0805-0","10.1109/CCNC54725.2025.10976114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976114","Domain Squatting;Large Language Models (LLMs);Cybersecurity;Compound AI System","Accuracy;Large language models;Internet security;Ecosystems;Focusing;Threat assessment;Compounds;Reliability;Protection;Testing","","","","17","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Comprehensive Study on Integrating AI-Powered Threat Intelligence Using Large Language Models","N. P; S. A. Ratnam; S. Bhaskaran","Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India","2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)","14 Jul 2025","2025","3","","2141","2146","This paper presents a comprehensive analysis of algorithmic efficiency within automated testing tools, with a particular emphasis on integrating AI-powered threat intelligence through Large Language Models (LLMs) for enhanced penetration testing. The investigation focuses on measuring the time complexity, resource utilization, and various performance metrics of these tools. We extend our analysis to explore how LLMs and multi-agent systems can augment dynamic and static analysis during Automated Penetration Testing (APT). We rigorously quantify the performance of both in-house developed tools and existing automation frameworks, especially those that leverage LLMs. The primary goal is to evaluate the strengths and weaknesses of the algorithm by comparing the performance of the tool, analyzing the underlying algorithms, and evaluating critical aspects such as scalability, resource management, and optimization strategies to improve real-world applications, particularly in cybersecurity.","","979-8-3315-3607-7","10.1109/ICCSAI64074.2025.11063731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11063731","Large Language Models;Automated PenTesting;Docker;Cyber Security;Optimized Routing","Large language models;Static analysis;Software quality;Algorithmic efficiency;Time measurement;Resource management;Time complexity;Optimization;Testing;Penetration testing","","","","18","IEEE","14 Jul 2025","","","IEEE","IEEE Conferences"
"Enhancing Classification and Reasoning Abilities of Large Language Models for Predicting Affect from Ambulatory Data","S. Kalyanasundaram; C. S. Phanord; L. L. Ruzic; R. H. Kaiser; T. Chaspari","Institute of Cognitive Science & Department of Computer Science, University of Colorado, Boulder, USA; Department of Psychology and Neuroscience, University of Colorado, Boulder, USA; Department of Psychology and Neuroscience, University of Colorado, Boulder, USA; Department of Psychology and Neuroscience, University of Colorado, Boulder, USA; Institute of Cognitive Science & Department of Computer Science, University of Colorado, Boulder, USA",2025 IEEE 13th International Conference on Healthcare Informatics (ICHI),"22 Jul 2025","2025","","","336","342","This study investigates the potential of large language models (LLMs) to leverage their extensive pre-trained knowledge for conducting affect classification using ambulatory data collected from smartphone sensors in support of precise mental health (MH) monitoring. We propose several prompting strategies to conduct a three-way classification of happiness and sadness levels: baseline, temporal method, personalization method, and two variations of a combination method to guide the learning process. These designs incorporate in-context learning, prompt engineering, and additional MH-related context to enhance model performance. Results indicate that the combination strategy outperforms the others in terms of balanced accuracy and macro F1 score for both the sadness and happiness tasks using longitudinal GPS, phone log, and accelerometer data. Additionally, we conduct a linguistic analysis of the LLM’s reasoning that led to its decision across these strategies. The findings suggest that prompting methods where the LLM reasoning incorporates language related to affect, analytical thinking and authenticity yield improved affect classification performance.","2575-2634","979-8-3315-2094-6","10.1109/ICHI64645.2025.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081617","large language models;prompt engineering;reasoning;affect;mental health;ambulatory monitoring","Large language models;Mental health;Medical services;Linguistics;Cognition;Sensors;Prompt engineering;Informatics;Monitoring;Global Positioning System","","","","38","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"Visualizing Large Language Models: A Brief Survey","A. M. P. Brasoveanu; A. Scharl; L. J. B. Nixon; R. Andonie","Modul University Vienna GmbH, Vienna, Austria; Modul University Vienna GmbH, Vienna, Austria; Modul University Vienna GmbH, Vienna, Austria; Central Washington University, 400 E University Way, Ellensburg, WA, USA",2024 28th International Conference Information Visualisation (IV),"17 Oct 2024","2024","","","236","245","This paper explores the current landscape of visualizing large language models (LLMs). The main objective was threefold. Firstly, we investigate how we can visualize LLM-specific techniques such as prompt engineering, instruction tuning, or guidance. Secondly, LLM causality, interpretability, and explainability are examined through visualization. And finally, we showcase the role of visualization in illuminating the integration of multiple modalities. We are interested in discovering the papers that present visualization systems instead of those that use visualization to showcase a part of their work. Our survey aims to synthesize the state-of-the-art in LLM visualization, offering a compact resource for exploring future research avenues.","2375-0138","979-8-3503-8016-3","10.1109/IV64223.2024.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10714298","Large Language Models;Prompt Engineering;Visualization of Neural Networks;Natural Language Processing (NLP);Explainable AI (XAI)","Surveys;Visualization;Annotations;Large language models;Cause effect analysis;Prompt engineering;Tuning;Business","","","","88","IEEE","17 Oct 2024","","","IEEE","IEEE Conferences"
"Entity Extraction from High-Level Corruption Schemes via Large Language Models","P. Koletsis; P. -K. Gemos; C. Chronis; I. Varlamis; V. Efthymiou; G. T. Papadopoulos","Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece; Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece; Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece; Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece; Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece; Dept. of Informatics and Telematics, Harokopio University of Athens, Athens, Greece",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2753","2761","The rise of financial crime that has been observed in recent years has created an increasing concern around the topic and many people, organizations and governments are more and more frequently trying to combat it. Despite the increase of interest in this area, there is a lack of specialized datasets that can be used to train and evaluate works that try to tackle those problems. This article proposes a new micro-benchmark dataset for algorithms and models that identify individuals and organizations, and their multiple writings, in news articles, and presents an approach that assists in its creation. Experimental efforts are also reported, using this dataset, to identify individuals and organizations in financial-crime-related articles using various low-billion parameter Large Language Models (LLMs). For these experiments, standard metrics (Accuracy, Precision, Recall, F1 Score) are reported and various prompt variants comprising the best practices of prompt engineering are tested. In addition, to address the problem of ambiguous entity mentions, a simple, yet effective LLM-based disambiguation method is proposed, ensuring that the evaluation aligns with reality. Finally, the proposed approach is compared against a widely used stateof-the-art open-source baseline, showing the superiority of the proposed method.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10824994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824994","Financial Crime;Large Language Models;Named-Entity Recognition;Text Mining;Prompt Engineering","Measurement;Large language models;Standards organizations;Government;Writing;Big Data;Data models;Prompt engineering;Data mining;Best practices","","","","27","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Leveraging LLMs for Interaction Point Detection and Payload Alignment in CTF challenges","J. Chen; R. Li; F. Qin; C. Tang","College of Electronic Science and Technology, National University of Defense Technology, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, China",2025 2nd International Conference on Electronic Engineering and Information Systems (EEISS),"24 Jul 2025","2025","","","1","5","Capture The Flag (CTF) competitions have become a vital platform for cybersecurity skill development, offering participants hands-on opportunities to analyze and exploit real-world vulnerabilities. Among various CTF challenge categories, PWN tasks—centered on binary vulnerability analysis and exploitation—present unique technical hurdles, particularly in the detection of user interaction points and the alignment of exploit payloads. While recent advances in Large Language Models (LLMs) have revolutionized the automated exploitation of web vulnerabilities, their application to binary exploitation remains nascent. This paper investigates the potential of LLMs, to automate two critical stages of binary CTF challenge solving: interaction point detection and payload alignment. We demonstrate how LLMs, leveraging their understanding of code semantics and security concepts, can reduce the manual workload for participants, and lower barriers to entry. Our findings highlight the promise of integrating LLMs into CTF workflows, paving the way for more efficient and accessible binary vulnerability research and exploitation.","","979-8-3315-2329-9","10.1109/EEISS65394.2025.11085643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11085643","CTF challenges;large language models;automated exploit generation","Electric potential;Codes;Large language models;Semantics;Manuals;Computer security;Payloads;Information systems","","","","12","IEEE","24 Jul 2025","","","IEEE","IEEE Conferences"
"An Overview of Security Threats, Attack Detection and Defense for Large-Scale Multi-Agent Systems (LSMAS) in Internet of Things (IoT)","J. Cai; L. Wen; H. Feng; K. Fang; J. Chen; W. Wei; W. Wang","College of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, China; School of Innovation Engineering, Macau University of Science and Technology, Macau, China; College of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, China; College of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, China; School of Software, Dalian University of Technology, Dalian, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; School of Medical Technology, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Industrial Cyber-Physical Systems,"27 Dec 2024","2025","3","","70","81","With the global advancement of the Internet of Things (IoT), large-scale multi-agent systems (LSMAS) technology has been increasingly adopted across various industries. Despite the widespread use of IoT, vulnerabilities in its software and hardware components pose significant challenges to ensuring security. To address the security concerns faced by LSMAS, this article explores its application in IoT domains such as smart grids, smart manufacturing, and smart healthcare. By examining the structural layers of IoT-the perception layer, network layer, and application layer-this paper analyzes the specific security threats encountered by each layer, along with the defense strategies designed to counter these attacks. Furthermore, we categorize and compare both traditional and AI-based attack detection methods, dividing the latter into machine learning-based, deep learning-based, and transfer learning-based approaches. This analysis highlights the strengths and weaknesses of the various attack detection methods currently deployed in LSMAS, identifying the existing challenges they face. Finally, we outline potential trends for the future development of LSMAS in IoT, offering research directions for experts in related fields.","2832-7004","","10.1109/TICPS.2024.3514552","National Natural Science Foundation of China(grant numbers:62403433); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LQ23F020001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10787271","Attack defense;attack detection;IoT application;large-scale multi-agent systems;security threat","Internet of Things;Security;Smart grids;Hardware;Software;Collaboration;Smart manufacturing;Real-time systems;Power system stability;Power system reliability","","","","72","IEEE","9 Dec 2024","","","IEEE","IEEE Journals"
"Model Generation with LLMs: From Requirements to UML Sequence Diagrams","A. Ferrari; S. Abualhaija; C. Arora","Consiglio Nazionale delle Ricerche (CNR); SnT University of Luxembourg, Luxembourg; Monash University",2024 IEEE 32nd International Requirements Engineering Conference Workshops (REW),"21 Aug 2024","2024","","","291","300","Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.","2770-6834","979-8-3503-9551-8","10.1109/REW61692.2024.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628665","Natural Language Processing (NLP);Large Language Models (LLMs);Prompt Engineering;ChatGPT;Model Generation;Sequence Diagrams","Large language models;Unified modeling language;Manuals;Chatbots;Stakeholders;Requirements engineering;Reliability;Standards;System analysis and design;Context modeling","","11","","39","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Model-Driven Prompt Engineering","R. Clarisó; J. Cabot","Universitat Oberta de Catalunya (UOC), Barcelona, Spain; Universitat Oberta de Catalunya (UOC), Barcelona, Spain",2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS),"12 Dec 2023","2023","","","47","54","Generative artificial intelligence (AI) systems are capable of synthesizing complex content such as text, source code or images according to the instructions described in a natural language prompt. The quality of the output depends on crafting a suitable prompt. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems.Through experimentation, the creative and research communities have created guidelines and strategies for creating good prompts. However, even for the same task, these best practices vary depending on the particular system receiving the prompt. Moreover, some systems offer additional features using a custom platform-specific syntax, e.g., assigning a degree of relevance to specific concepts within the prompt.In this paper, we propose applying model-driven engineering to support the prompt engineering process. Using a domain-specific language (DSL), we define platform-independent prompts that can later be adapted to provide good quality outputs in a target AI system. The DSL also facilitates managing prompts by providing mechanisms for prompt versioning and prompt chaining. Tool support is available thanks to a Langium-based Visual Studio Code plugin.","","979-8-3503-2480-8","10.1109/MODELS58315.2023.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10343974","prompt engineering;model-driven engineering;domain-specific language;generative AI;large language models","Visualization;Source coding;Natural languages;Syntactics;Model driven engineering;DSL;Artificial intelligence","","11","","52","IEEE","12 Dec 2023","","","IEEE","IEEE Conferences"
"LLM-Based Edge Intelligence: A Comprehensive Survey on Architectures, Applications, Security and Trustworthiness","O. Friha; M. Amine Ferrag; B. Kantarci; B. Cakmak; A. Ozgun; N. Ghoualmi-Zine","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Artificial Intelligence and Digital Science Research Center, Technology Innovation Institute, Abu Dhabi, UAE; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Headquarters, Edge Signal, Ottawa, ON, Canada; Headquarters, Edge Signal, Ottawa, ON, Canada; Department of Computer Science, Badji Mokhtar-Annaba University, Annaba, Algeria",IEEE Open Journal of the Communications Society,"18 Sep 2024","2024","5","","5799","5856","The integration of Large Language Models (LLMs) and Edge Intelligence (EI) introduces a groundbreaking paradigm for intelligent edge devices. With their capacity for human-like language processing and generation, LLMs empower edge computing with a powerful set of tools, paving the way for a new era of decentralized intelligence. Yet, a notable research gap exists in obtaining a thorough comprehension of LLM-based EI architectures, which should incorporate crucial elements such as security, optimization, and responsible development. This survey aims to bridge this gap by providing a comprehensive resource for both researchers and practitioners. We explore LLM-based EI architectures in-depth, carefully analyzing state-of-the-art paradigms and design decisions. To facilitate efficient and scalable edge deployments, we perform a comparative analysis of recent optimization and autonomy techniques specifically designed for resource-constrained edge environments. Additionally, we shed light on the extensive potential of LLM-based EI by demonstrating its varied practical applications across a wide range of domains. Acknowledging the utmost importance of security, our survey thoroughly investigates potential vulnerabilities inherent in LLM-based EI deployments. We explore corresponding defense mechanisms to protect the integrity and confidentiality of data processed at the edge. In conclusion, highlighting the essential aspect of trustworthiness, we outline best practices and guiding principles for the responsible development and deployment of these systems. By conducting a comprehensive review of these key components, our survey aims to support the ethical development and strategic implementation of LLM-driven EI, paving the way for its transformative impact on diverse applications.","2644-125X","","10.1109/OJCOMS.2024.3456549","Natural Sciences and Engineering Research Council (NSERC) Discovery Program; NSERC CREATE TRAVERSAL Program; Innovation for Defence Excellence and Security (IDEaS) Program from the Department of National Defence (DND); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669603","Edge intelligence (EI);generative AI;large language models (LLMs);security;privacy;trustworthiness;responsible AI","Artificial intelligence;Computational modeling;Security;Robot sensing systems;Technological innovation;Surveys;Real-time systems","","36","","458","CCBYNCND","9 Sep 2024","","","IEEE","IEEE Journals"
"Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair","X. Cai; L. Jiang","Singapore Management University, Singapore; Singapore Management University, Singapore","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","360","371","Automated Program Repair (APR) aims to enhance software reliability by automatically generating bug-fixing patches. Recent work has improved the state-of-the-art of APR by fine-tuning pre-trained large language models (LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning be-comes weakened in data scarcity scenarios, and data scarcity can be a common issue in practice, limiting fine-tuning performance. To alleviate this limitation, this paper adapts prompt tuning for enhanced APR and conducts a comprehensive study to evaluate its effectiveness in data scarcity scenarios, using three LLMs of different sizes and six diverse datasets across four programming languages. Prompt tuning rewrites the input to a model by adding extra prompt tokens and tunes both the model and the prompts on a small dataset. These tokens provide task-specific knowledge that can improve the model for APR, which is especially critical in data scarcity scenarios. Moreover, domain knowledge has proven crucial in many code intelligence tasks, but existing studies fail to leverage domain knowledge during the prompt tuning for APR. To close this gap, we introduce knowledge prompt tuning, an approach that adapts prompt tuning with six distinct types of code- or bug-related domain knowledge for APR. Our work, to the best of our knowledge, is the first to adapt and evaluate prompt tuning and the effectiveness of code- or bug-related domain knowledge for APR, particularly under data scarcity settings. Our evaluation results demonstrate that prompt tuning with knowledge generally outperforms fine-tuning under various experimental settings, achieving an average improvement of 87.33% over fine-tuning in data scarcity scenarios.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992581","automatic program repair;prompt tuning;large language model;bug knowledge","Adaptation models;Codes;Large language models;Source coding;Computer bugs;Maintenance engineering;Data models;Software;Software reliability;Tuning","","","","47","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Evaluating OpenAI Large Language Models for Generating Logical Abstractions of Technical Requirements Documents","A. Perko; F. Wotawa","CD Lab QAMCAS, Graz University of Technology, Institute of Software Technology, Graz, Austria; CD Lab QAMCAS, Graz University of Technology, Institute of Software Technology, Graz, Austria","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","26 Sep 2024","2024","","","238","249","Since the advent of Large Language Models (LLM[s]) a few years ago, they have not only reached the mainstream but have become a commodity. Their application areas steadily expand because of sophisticated model architectures and enormous training corpora. However, accessible chatbot user interfaces and human-like responses may cause a tendency to overestimate their abilities. This study contributes to demonstrating the strengths and weaknesses of LLMs. In this work, we bridge methods from sub-symbolic and symbolic AI. In particular, we evaluate the capabilities of LLMs to convert textual requirements documents into their logical representation, enabling analysis and reasoning. This task demonstrates a use case close to industry, as requirements analysis is key in requirements and system engineering. Our experiments evaluate the popular model family used in OpenAI’s ChatGPT, GPT-3.5, and GPT-4. The underlying goal of testing for the correct abstraction of meaning is not trivial, as the relationship between input and output semantics is not directly measurable. Thus, it is necessary to approximate translation correctness through quantifiable criteria. Most notably, we defined consistency-based metrics for the plausibility and stability of translations. Our experiments give insights into syntactical validity, semantic plausibility, stability of translations, and parameter configurations for LLM translations. We use real-world requirements and test the LLMs’ performance out of the box and after pre-training. Experimentally, we demonstrated the strong relation between ChatGPT parameters and the stability of translations. Finally, we showed that even the best model configurations produced syntactically faulty (5%) or semantically implausible (7%) output and are not stable in their results.","2693-9177","979-8-3503-6563-4","10.1109/QRS62785.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684632","NLP;requirements engineering;large language models;ChatGPT;logical abstraction;symbolic AI","Measurement;Training;Codes;Large language models;Stability criteria;Semantics;Syntactics","","","","27","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Using AI and Big Data in the HealthCare Sector to help build a Smarter and more Intelligent HealthCare System","S. K. Marimekala; J. Lamb; R. Epstein; V. Bhupathi","STSM and Thought Leader IBM; Adjunct Faculty, Mathematics, Pace University, Pleasantville, NY, USA; Hybrid Cloud Distributed Automation IBM; DT Portfolio Manager-Applications DXC Technology",2024 IEEE World AI IoT Congress (AIIoT),"10 Jul 2024","2024","","","356","362","The purpose of this paper is to demonstrate the use of AI and Big Data in HealthCare Sector to help build a smarter and more intelligent HealthCare System. Researchers in HealthCare Sectors are relying heavily on big data and compute power to build correlations by using statistical methods and artificial intelligence (AI) models. These models enable Healthcare Sector participants to manage HealthCare for a core set of the population. They also help providers to analyze the impact of decisions on their most vulnerable patients. There are many factors that are considered in performing big data analysis, some of them are: the patient’s medical history, genetic information, eating habits and fitness regimen. The data that is analyzed includes several key decision-making processes. Some of the challenges with the data used include data quality, data validation, data knowledge, domain expertise, and data integration challenges with various end points. While performing data analysis, the HealthCare Sectors must take security and data governance (HIPPA regulations etc.) into consideration. Big data analysis follows the (4P) approach [1], preference, prediction, personalization, and promotion. The question that arises most often is the type of data that is the most reliable for analysis in the HealthCare Sector. Most HealthCare organizations use demographic information, diagnosis, treatment, prescription drugs, laboratory tests, physiologic monitoring data, hospitalization, and patient insurance for their analysis. Since the data comes from multiple sources [2], there is a big challenge to perform data integration, extraction, and transformation as it consumes large amounts of resources and compute power, coupled with the additional challenges of data aggregation, data enrichment and format inconsistencies. To address this challenge and to analyze the process completely requires data scientists who have domain knowledge and expertise to extract, enrich and transform data. and group them into a meaningful format for proper data analysis and research. The HealthCare Sector faces this as a major challenge. Another key component in big data analysis is the lack of data visualization tools that can take structured and unstructured data and build customized dashboards for data correlation. There are also challenges with big data management in the HealthCare Sector. This data needs to be highly secured, with proper guardrails with tightened security measures and controls, and with proper data governance in place. There is also a need for a robust infrastructure that can handle large amounts of medical data that can allow researchers to analyze, build data correlation models and generate meaningful insights by using AI models through prompt engineering techniques [3] to build data correlation. In our paper, our focus is to understand, through several use cases the key challenges in gathering and grouping HealthCare data (both structured and unstructured data) from various sources. We also focus on understanding the impact of technical advancements in emerging AI technologies and how it plays a vital role in defining and deriving meaningful data insights for research and for learning HealthCare data patterns with a primary focus on data authenticity, ethics, privacy, governance, integrity, and security.","","979-8-3503-8780-3","10.1109/AIIoT61789.2024.10578989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578989","Generative AI;ChatGPT;Guardrails;Big Data;AI Models;Large Language Models;ML;Prompt Engineering and Prompt Tuning","Correlation;Medical services;Big Data;Chatbots;Data models;Regulation;Security","","7","","11","IEEE","10 Jul 2024","","","IEEE","IEEE Conferences"
"Marching Forward: Redefining Human-Machine Interactions in Conversational AI Through Hybrid Intelligence, Blockchain Security, and Autonomous Agents","P. Charanya; S. R. B S; N. K. T; P. A","Department of Artificial Intelligence and Machine Learning, Sri Eshwar College of Engineering, Coimbatore, India; Sri Eshwar College of Engineering, Coimbatore, India; Sri Eshwar College of Engineering, Coimbatore, India; Sri Eshwar College of Engineering, Coimbatore, India",2024 9th International Conference on Communication and Electronics Systems (ICCES),"6 Feb 2025","2024","","","792","800","Conversational AI has emerged as an essential instrument in enhancing human-computer interaction, with applications spanning customer service to personal assistants. This paper offers a comprehensive examination of recent developments in conversational AI, including novel techniques in natural language generation (NLG), dialogue systems, and dynamic response optimization. We analyze advancements including transformer-based architectures, reinforcement learning for dialogue generation, and retrieval-augmented models, emphasizing their roles in enhancing the quality and contextual accuracy of conversations. Additionally, we examine the feasibility of blockchain technology as the foundation for decentralized AI, promoting secure, transparent, and distributed frameworks for AI applications, hence improving privacy and control in data exchanges. Hybrid methodologies that combine symbolic reasoning with deep learning are analyzed, highlighting their effectiveness in addressing complex dialogues. These advancements, supported by empirical research and benchmark performance, signify the evolution towards more personalized, intelligent, and decentralized conversational systems.","","979-8-3503-7797-2","10.1109/ICCES63552.2024.10859659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859659","Conversational AI;Chatbot;Generative Model;Human-machine interaction","Human computer interaction;Deep learning;Conversational artificial intelligence;Scalability;Natural language generation;Reinforcement learning;Transformers;Real-time systems;Blockchains;Security","","","","16","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"EnStack: An Ensemble Stacking Framework of Large Language Models for Enhanced Vulnerability Detection in Source Code","S. Z. Ridoy; M. Shazzad Hossain Shaon; A. Cuzzocrea; M. S. Akter","Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh; Department of Computer Science and Engineering, Oakland University, Rochester, MI, USA; University of Calabria, Rende, Italy; Department of Computer Science, Oakland University, MI, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","6356","6364","Automated detection of software vulnerabilities is critical for enhancing security, yet existing methods often struggle with the complexity and diversity of modern codebases. In this paper, we propose a novel ensemble stacking approach that synergizes multiple pre-trained large language models (LLMs)—CodeBERT, GraphCodeBERT, and UniXcoder—to improve vulnerability detection in source code. Our method uniquely combines the semantic understanding of CodeBERT, the structural code representations of GraphCodeBERT, and the cross-modal capabilities of UniXcoder. By fine-tuning these models on the Draper VDISC dataset and integrating their predictions using meta-classifiers such as Logistic Regression, Support Vector Machines (SVM), Random Forest, and XGBoost, we effectively capture complex code patterns that individual models may miss. The meta-classifiers aggregate the strengths of each model, enhancing overall predictive performance. Our ensemble demonstrates significant performance gains over existing methods, with notable improvements in accuracy, precision, recall, F1-score, and AUC-score. This advancement addresses the challenge of detecting subtle and complex vulnerabilities in diverse programming contexts. The results suggest that our ensemble stacking approach offers a more robust and comprehensive solution for automated vulnerability detection, potentially influencing future AI-driven security practices.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825609","Large Language Models (LLMs);Vulnerability Detection;Source Code Analysis;Ensemble Stacking;CodeBERT;GraphCodeBERT;UniXcoder","Support vector machines;Logistic regression;Codes;Accuracy;Source coding;Large language models;Stacking;Semantics;Predictive models;Security","","1","","31","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Quest-RE QUestion Generation and Exploration STrategy for Requirements Engineering","H. Hasso; B. Fischer-Starcke; H. Geppert","Fraunhofer FKIE, Wachtberg, Germany; Fraunhofer FKIE, Wachtberg, Germany; Fraunhofer FKIE, Wachtberg, Germany",2024 IEEE 32nd International Requirements Engineering Conference Workshops (REW),"21 Aug 2024","2024","","","1","9","The quest for achieving completeness in requirements engineering (RE) is a complex challenge that requires innovative approaches to uncover and address hidden or incomplete requirements. This study introduces Quest- RE, a novel methodology leveraging Large Language Models (LLMs), specifically ChatGPT-4, to enhance the RE process through dynamic question generation and exploration strategies. By generating targeted questions referring to requirements, Quest- RE aims to improve communication between stakeholders and the RE team, thereby enhancing precision and completeness of requirements specifications (RS). The approach not only helps to identify gaps and missing elements in the requirements but also facilitates a deeper understanding and critical examination of the documented needs. For a given requirement, Quest- RE uses an algorithmic approach to generate related questions and question objectives. This ensures thorough exploration of each requirement beyond its initial scope. Two illustrative examples, one of it using requirements from the Fault Tolerant System Services (FTSS) and the Scheduling Services of NASA's X-38 Crew Return Vehicle, highlight the practical applicability and effectiveness of the approach in a complex engineering project. The study proposes that the incorporation of LLMs into RE can significantly influence communication and identification of requirements, thereby improving their overall quality. This research contributes to this area by offering an LLM - based approach to improve requirements elicitation and analysis, while also improving the readability and completeness of RS.","2770-6834","979-8-3503-9551-8","10.1109/REW61692.2024.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628592","Requirements Engineering (RE);Large Language Models (LLMs);ChatGPT-4;Question Generation;Read-ability of Requirements Specifications (RS);Completeness of Requirements Specifications (RS)","Scalability;Large language models;NASA;Fault tolerant systems;Question generation;Requirements engineering;Stakeholders","","","","26","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models","H. Hajipour; K. Hassler; T. Holz; L. Schönherr; M. Fritz",CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security,2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),"10 May 2024","2024","","","684","709","Large language models (LLMs) for automatic code generation have recently achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. Training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively evaluated for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. This involves proposing a novel few-shot prompting approach. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we use our method to create a collection of diverse non-secure prompts for various vulnerability scenarios. This dataset serves as a benchmark to evaluate and compare the security weaknesses of code language models.","","979-8-3503-4950-4","10.1109/SaTML59370.2024.00040","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516658","Code language models;Large language models;Few-shot prompting;Security vulnerabilities benchmark","Codes;Training data;Closed box;Static analysis;Benchmark testing;Programming;Data models","","10","","63","IEEE","10 May 2024","","","IEEE","IEEE Conferences"
"Memorization in LLM-Based Program Repair","J. Kong; M. Cheng; X. Xie","Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore",2025 IEEE/ACM International Workshop on Automated Program Repair (APR),"13 Jun 2025","2025","","","40","42","Automated Program Repair (APR) is a powerful technique for mitigating the impact of software bugs in software development. The recent remarkable success of Large Language Models (LLMs) has set new state-of-the-art performance in APR. However, the extensive use of large training corpora raises concerns about whether these impressive capabilities genuinely generalize to unseen tasks or primarily rely on memorizing vast amounts of pretraining data. To address this issue, this paper introduces a memorization-inducing prompting strategy, MemInducer, to investigate the extent of memorization in LLMs for APR. Specifically, MemInducer is designed to prompt LLMs to recall responses from their training corpus. Subsequently, we assess memorization by measuring the similarity between the responses generated by the LLM and the corresponding ground truth. Experimental results reveal that memorization is indeed present in existing APR benchmarks, with over 78% of corrected bugs producing results that are entirely identical to the ground truth.","","979-8-3315-2585-9","10.1109/APR66717.2025.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029650","Code Memorization;Automated Program Repair;Large Language Model","Training;Codes;Large language models;Conferences;Computer bugs;Maintenance engineering;Benchmark testing;Software;Software development management","","","","20","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"LitAI: Enhancing Multimodal Literature Understanding and Mining with Generative AI","G. Medisetti; Z. Compson; H. Fan; H. Yang; Y. Feng","University of North Texas, Denton, TX, USA; University of North Texas, Denton, TX, USA; University of North Texas, Denton, TX, USA; University of North Texas, Denton, TX, USA; University of North Texas, Denton, TX, USA",2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),"15 Oct 2024","2024","","","471","476","Information processing and retrieval in literature are critical for advancing scientific research and knowledge discovery. The inherent multimodality and diverse literature formats, including text, tables, and figures, present significant challenges in literature information retrieval. This paper introduces LitAI, a novel approach that employs readily available generative AI tools to enhance multimodal information retrieval from literature documents. By integrating tools such as optical character recognition (OCR) with generative AI services, LitAI facilitates the retrieval of text, tables, and figures from PDF documents. We have developed specific prompts that leverage in-context learning and prompt engineering within Generative AI to achieve precise information extraction. Our empirical evaluations, conducted on datasets from the ecological and biological sciences, demonstrate the superiority of our approach over several established baselines including Tesseract-OCR and GPT-4. The implementation of LitAI is accessible at https://github.com/ResponsibleAILab/LitAI.","2770-4319","979-8-3503-5142-2","10.1109/MIPR62202.2024.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707959","Literature Mining;OCR;Generative AI;Prompt Engineering;ChatGPT;GPT-4","Accuracy;Generative AI;Optical character recognition;Information processing;Information retrieval;Portable document format;Knowledge discovery;Biology;Data mining;Prompt engineering","","","","16","IEEE","15 Oct 2024","","","IEEE","IEEE Conferences"
"Relationalizing Tables with Large Language Models: The Promise and Challenges","Z. Huang; E. Wu","Columbia University, New York, United States; DSI, Columbia University, New York, United States",2024 IEEE 40th International Conference on Data Engineering Workshops (ICDEW),"17 Jun 2024","2024","","","305","309","Tables in the wild are usually not relationalized, making querying them difficult. To relationalize tables, recent works designed seven transformation operators, and deep neural networks were adopted to automatically find the sequence of operators, achieving an accuracy of 57.0%. In comparison, earlier versions of large language models like GPT-3.5 only reached 13.1%. However, these results were obtained using naive prompts. Furthermore, GPT-4 is recently available, which is substantially larger and more performant. This study examines how the selection of models, specifically GPT-3.5 and GPT-4, and various prompting strategies, such as Chain-of-Thought and task decomposition, affect accuracy. The main finding is that GPT4, combined with Task Decomposition and Chain-of-Thought, attains a remarkable accuracy of 74.6%. Further analysis of errors made by GPT-4 shows the challenges that about half of the errors are not due to the model’s shortcomings, but rather to ambiguities in the benchmarks. When these benchmarks are disambiguated, GPT-4’s accuracy improves to 86.9%.","2473-3490","979-8-3503-8403-1","10.1109/ICDEW61823.2024.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555085","Large Language Model;Data Transformation;Prompt Engineering;Data Management","Analytical models;Accuracy;Conferences;Artificial neural networks;Benchmark testing;Metadata;Data engineering","","1","","14","IEEE","17 Jun 2024","","","IEEE","IEEE Conferences"
"Genetic Algorithm for Prompt Engineering with Novel Genetic Operators","H. Tanaka; N. Mori; M. Okada","Graduate School of Informatics, Osaka Metropolitan University, Osaka, Japan; Graduate School of Informatics, Osaka Metropolitan University, Osaka, Japan; Graduate School of Informatics, Osaka Metropolitan University, Osaka, Japan",2023 15th International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter),"9 Apr 2024","2023","","","209","214","In recent years, the advancement of Large Language Models (LLMs) has garnered significant attention in the field of Artificial Intelligence (AI), exhibiting exceptional performance across a wide variety of natural language processing (NLP) tasks. However, despite the high generality of LLMs, there exists a problem in controlling them to produce the desired output for each task. Fine-tuning is a conventional approach to improve performance for specific tasks, albeit at the expense of substantial time and computational resources. Prompt engineering serves as an effective alternative, steering models towards desired outputs for particular tasks, and has been validated to enhance the performance of LLMs. However, manual design of prompts is labor-intensive, which has increased interest in the automation of prompt engineering. In this study, we propose a method to automate prompt engineering optimization utilizing a genetic algorithm with novel genetic operators. Through experiments conducted to explore instructional prompts for solving Japanese multiple-choice questions, the efficacy of the proposed method was affirmed. The findings of this study underscore the feasibility of genetic algorithm-based automatic prompt engineering and genetic operators for prompts, and show their efficacy for Japanese, which has distinct linguistic characteristics compared to English and other languages.","","979-8-3503-8382-9","10.1109/IIAI-AAI-Winter61682.2023.00047","JSPS(grant numbers:23K11252); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488291","Prompt engineering;Genetic algorithm;Large language model","Automation;Computational modeling;Manuals;Linguistics;Natural language processing;Task analysis;Informatics","","2","","20","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Understandable Test Generation Through Capture/Replay and LLMs","A. Deljouyi","Delft University of Technology, Delft, Netherlands",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","261","263","Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques. Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles. In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3639789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554970","Automatic Test Generation;Carving and Replaying;Large Language Models;Readability;Understandability;Unit Testing","Software testing;Statistical analysis;Medical services;Writing;Generators;Test pattern generators;Software engineering","","2","","34","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization","P. Vijayaraghavan; A. Nitsure; C. Mackin; L. Shi; S. Ambrogio; A. Haran; V. Paruthi; A. Elzein; D. Coops; D. Beymer; T. Baldwin; E. Degan","IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM, Austin, TX, USA; IBM, Austin, TX, USA; IBM, Austin, TX, USA; IBM, Austin, TX, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA; IBM Research, San Jose, CA, USA",2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),"6 Nov 2024","2024","","","1","10","Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there’s a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets - VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs’ understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL. CCS Concepts •Computing methodologies $\rightarrow$ Natural language generation; •Hardware $\rightarrow$ Hardware description languages and compilation.","","979-8-3503-6356-2","10.1109/MLCAD62225.2024.10740226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740226","VHDL Code Generation;VHDL Code Summarization;Chain-of-Descriptions;LLM;VHDL;VHDL-Eval;VHDL-Xform","Measurement;Solid modeling;Codes;VHDL;Design automation;Systematics;Large language models;Refining;Hardware;Standards","","1","","23","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Towards Security Awareness Enhancement using Dynamic and Adaptive Behavior Learning Models","P. Basnet; I. Nepal; R. Khan","School of Computing and Analytics, College of Informatics, Northern Kentucky University, KY, USA; School of Computing and Analytics, College of Informatics, Northern Kentucky University, KY, USA; School of Computing and Analytics, College of Informatics, Northern Kentucky University, KY, USA",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0636","0639","As cyber threats continue to evolve alongside various forms of technological advancements, human behavior remains a critical vulnerability in modern security infrastructures. Understanding and addressing users’ personalized behavioral traits is vital to strengthening an organization’s overall security posture, as even well-designed technical defenses can be compromised by poor decision-making or lack of awareness. This paper proposes a dynamic, AI-driven cybersecurity framework that leverages User and Entity Behavior Analytics (UEBA), Machine Learning (ML), and Large Language Models (LLMs) to deliver personalized, real-time security awareness and training modules. Built on a feedback- and feedforward-enhanced Input-Process-Output (IPO) model, the system analyzes multi-modal behavioral data to detect anomalies, optimize policies, and adapt training content dynamically. The framework enhances engagement, improves long-term security behavior, and strengthens organizational resilience against emerging threats. Empirical evaluation demonstrates the model’s scalability and effectiveness, while addressing challenges related to data privacy, integration complexity, and contextual adaptability.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105229","Cybersecurity Awareness;User and Entity Behavior Analytics;Machine Learning;Large Language Models;Adaptive Training;Human-Centric Security","Training;Adaptation models;Analytical models;Large language models;Scalability;Machine learning;Data models;Real-time systems;Computer security;Resilience","","","","12","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Generative AI Foundations in Python: Discover key techniques and navigate modern challenges in LLMs","C. Rodriguez; S. Shaikh",NA; NA,Generative AI Foundations in Python: Discover key techniques and navigate modern challenges in LLMs,"","2024","","","","","Begin your generative AI journey with Python as you explore large language models, understand responsible generative AI practices, and apply your knowledge to real-world applications through guided tutorialsKey FeaturesGain expertise in prompt engineering, LLM fine-tuning, and domain adaptationUse transformers-based LLMs and diffusion models to implement AI applicationsDiscover strategies to optimize model performance, address ethical considerations, and build trust in AI systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe intricacies and breadth of generative AI (GenAI) and large language models can sometimes eclipse their practical application. It is pivotal to understand the foundational concepts needed to implement generative AI. This guide explains the core concepts behind -of-the-art generative models by combining theory and hands-on application. Generative AI Foundations in Python begins by laying a foundational understanding, presenting the fundamentals of generative LLMs and their historical evolution, while also setting the stage for deeper exploration. You’ll also understand how to apply generative LLMs in real-world applications. The book cuts through the complexity and offers actionable guidance on deploying and fine-tuning pre-trained language models with Python. Later, you’ll delve into topics such as task-specific fine-tuning, domain adaptation, prompt engineering, quantitative evaluation, and responsible AI, focusing on how to effectively and responsibly use generative LLMs. By the end of this book, you’ll be well-versed in applying generative AI capabilities to real-world problems, confidently navigating its enormous potential ethically and responsibly.What you will learnDiscover the fundamentals of GenAI and its foundations in NLPDissect foundational generative architectures including GANs, transformers, and diffusion modelsFind out how to fine-tune LLMs for specific NLP tasksUnderstand transfer learning and fine-tuning to facilitate domain adaptation, including fields such as financeExplore prompt engineering, including in-context learning, templatization, and rationalization through chain-of-thought and RAGImplement responsible practices with generative LLMs to minimize bias, toxicity, and other harmful outputsWho this book is forThis book is for developers, data scientists, and machine learning engineers embarking on projects driven by generative AI. A general understanding of machine learning and deep learning, as well as some proficiency with Python, is expected.","","9781835464915","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769393.pdf&bkn=10769392&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Franc: A Lightweight Framework for High-Quality Code Generation","M. L. Siddiq; B. Casey; J. C. S. Santos","University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA",2024 IEEE International Conference on Source Code Analysis and Manipulation (SCAM),"19 Dec 2024","2024","","","106","117","In recent years, the use of automated source code generation utilizing transformer-based generative models has grown in popularity. These models can generate code according to the developers' requirements. However, recent research showed that these automatically generated source codes can contain vulnerabilities and other quality issues. Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is not only time-consuming but also resource-intensive and costly. Thus, in this paper, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models. FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score. Moreover, the framework uses prompt engineering to fix persistent quality issues. We evaluated FRANC with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (FRANC). The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability. The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts. FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.","2470-6892","979-8-3315-2850-8","10.1109/SCAM63643.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795381","code generation;code quality;code security;large language models","Java;Analytical models;Codes;Source coding;Large language models;Maintenance engineering;Transformers;Prompt engineering","","4","","74","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Enhancing Pull Request Reviews: Leveraging Large Language Models to Detect Inconsistencies Between Issues and Pull Requests","A. T. Işık; H. Kübra Çağlar; E. Tüzün","Computer Science, Bilkent University, Ankara, Turkey; Computer Science, Bilkent University, Ankara, Turkey; Computer Science, Bilkent University, Ankara, Turkey",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","168","178","Context: Efficient Pull Request (PR) review process is critical in software development. This process includes checking the alignment between PRs and their corresponding issues. The traditional manual PR review often struggles with identifying inconsistencies between the intended improvements or fixes outlined in issues and the actual changes proposed in PRs. This difference can lead to overlooked inconsistencies in the PR acceptance process.Objective: We aim to enhance the PR review process by leveraging modern LLMs to detect inconsistencies between issue descriptions and code changes in submitted PRs.Method: We manually labeled a statistically significant sample of PRs from the Transformers repository to assess their alignment with corresponding issue descriptions. Each PR was categorized into one of four groups: exact, missing, tangling, or missing and tangling. This labeled dataset served as the benchmark for evaluating the performance of four widely used models: Llama3.1-70B-Instruct, Llama-3.1-405B-Instruct, GPT-4o, and GPT-4o mini. The models were tested using three distinct prompts designed to capture different aspects of issues and PRs. Each model was tasked with identifying tangled and missing elements, and their outputs were compared against the manually labeled data to assess their accuracy and reliability.Results: The manual labeling process in the stratified-sampled Transformers repository revealed the following distribution of PR-issue pair alignments: 68.04% were exact, 16.5% were missing, 13.40% were tangling, and 2.06% exhibited both missing and tangling characteristics. A strong correlation was observed between PR merge status and exact alignment, with 75.46% of merged PRs classified as exact, compared to only 29.03% of unmerged PRs. These findings highlight opportunities for improving the current code review process. For automated classification, the most effective prompt configuration combined issue text, PR text, and PR diff, enabling better detection of alignment inconsistencies. Among the models tested, GPT-4o and Llama-3.1-405B-Instruct delivered the highest performance, achieving the best F1 weighted scores of 0.5948 and 0.6190, respectively.Conclusion: Despite a notable correlation between PR merge status and exact alignment, our analysis revealed that merged PRs can still contain inconsistencies, such as missing or tangling changes. While the tested LLMs showed potential in automating PR-issue alignment, their current performance is limited. This underscores the need for further refinement to enhance their accuracy and reliability. Improved LLM-based tools could streamline the PR review process, reducing manual effort and enhancing code quality.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052818","Code review;issue;GPT-4o;GPT-4o mini;Llama;Llama-3.1-70B-Instruct;Llama-3.1-405B-Instruct;Large Language Models;PR issue match;pull-request review","Codes;Correlation;Accuracy;Reviews;Large language models;Manuals;Transformers;Reliability;Labeling;Software development management","","","","31","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"RTL Agent: An Agent-Based Approach for Functionally Correct HDL Generation via LLMs","S. Ranga; R. Mao; D. Bhattacharjee; E. Cambria; A. Chattopadhyay","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; CSA, Imec, Leuven, Belgium; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",2024 IEEE 33rd Asian Test Symposium (ATS),"14 Mar 2025","2024","","","1","6","LLMs as code generators have undergone rapid progress over the past couple of years. However, the models on their own provide no guarantees for the functional correctness of the generated code. Functional tests can not only be used by designers to assess the functional correctness of code, but also to guide them towards the solution of the problem. The same can be applied to LLMs performing automatic code generation through the use of the Reflexion technique. Reflexion is an agent-based workflow where the model generating code iterates over a loop of code generation, getting feedback from the test bench, reflecting on the feedback, and making appropriate changes to the code. The technique is known to drastically improve the performance of LLMs on software code generation. In this work, we adopt the technique for hardware description language (HDL) code generation as RTL Agent - an implementation of the workflow for Verilog generation with Reflexion. We compare multiple LLMs with standard inference vs with Reflexion on the VerilogEval benchmark. Within 5 iterations of the feedback loop, we observe a relative improvement of 33.2% for the low-performance model Llama3 and an average of 17.7% for the high-performance models GPT-4o and GPT-4o-mini in their Pass@1 performance. We present a cost analysis of the technique to enable cost-performance trade-offs.","2377-5386","979-8-3315-2916-1","10.1109/ATS64447.2024.10915277","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915277","HDL Generation;Large Language Models;Reflexion","Codes;Large language models;Software;Reflection;Hardware;Generators;Cost benefit analysis;Iterative methods;Hardware design languages;Standards","","","","23","IEEE","14 Mar 2025","","","IEEE","IEEE Conferences"
"JARVIS: Disjoint Large Language Models on Radio VLANs for Intelligent Services","M. S. Perelló; J. Groen; W. Liu; S. Ioannidis; K. Chowdhury","Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA; Institute for the Wireless Internet of Things, Northeastern University, Boston, MA, USA",MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM),"6 Dec 2024","2024","","","869","874","Large Language Models (LLMs) have changed the way we access and interpret information, communicate with each other and even operate computer systems through autonomous code generation. Typically, these billion-parameter models rely on cloud storage and execution due to their computational demands. In this paper, we challenge this status quo by proposing JARVIS, a distributed LLM framework that splits model layers across edge devices with limited compute resources, trading off computation for increased peer-level communication. JARVIS is robust to individual node failures, including recovery methods for lost layers via peer-level duplication. We evaluate JARVIS using Google’s open-source Gemma LLM (2B parameters) deployed over 18 software-defined radios in the NSF Colosseum RF emulator. Our evaluation explores LLM performance degradation from node losses, providing insights into node prioritization in tactical environments. The JARVIS software code is released for community exploration and adoption.","2155-7586","979-8-3503-7423-0","10.1109/MILCOM61039.2024.10773726","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773726","Artificial Intelligence;Generative AI;Large Language Models;Network Slicing;Distributed Computing","Radio frequency;Cloud computing;Codes;Large language models;Computational modeling;Redundancy;Software;Peer-to-peer computing;Software radio;Resilience","","","","23","IEEE","6 Dec 2024","","","IEEE","IEEE Conferences"
"Ethical and Legal Considerations of Large Language Models: A Systematic Review of the Literature","S. Alkamli; M. Al-Yahya; K. Alyahya","Information Technology Department, College of Computer and Information Sciences, King Saud Univerity, Riyadh, Saudi Arabia; Information Technology Department, College of Computer and Information Sciences, King Saud Univerity, Riyadh, Saudi Arabia; Information Technology Department, College of Computer and Information Sciences, King Saud Univerity, Riyadh, Saudi Arabia",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","576","586","Large Language Models (LLMs) such as OpenAI’s GPT-4 and Google’s Gemini have rapidly emerged as influential tools across various sectors, including healthcare, education, research, and law. Despite their benefits, LLMs present significant ethical and legal challenges that necessitate thorough examination. This systematic review aims to synthesize existing literature on these considerations, focusing on issues such as bias, privacy, transparency, misinformation, plagiarism, accountability, and security. By analyzing 43 peer-reviewed studies published over the last five years, we identify the primary ethical and legal concerns associated with LLMs, categorize these concerns, and report the research methods and recommendations proposed to mitigate potential harms. Our findings reveal that while significant progress has been made in understanding these issues, there remains a need for more cohesive and comprehensive approaches to address the ethical and legal implications of LLMs. This review provides valuable insights for researchers, policymakers, and developers, emphasizing the critical importance of addressing ethical and legal considerations to maintain public trust and ensure the beneficial impact of LLM technologies.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852451","Large Language Models;Ethics;Bias;Privacy;Transparency;Misinformation;Plagiarism;Accountability;Fairness;Security","Ethics;Privacy;Law;Large language models;Plagiarism;Focusing;Medical services;Security;Fake news;Systematic literature review","","1","","70","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Multi-Agent Systems: A Survey","A. Dorri; S. S. Kanhere; R. Jurdak","University of New South Wales, Sydney, NSW, AU; University of New South Wales, Sydney, NSW, AU; Commonwealth Scientific and Industrial Research Organisation, Canberra, ACT, AU",IEEE Access,"13 Jun 2018","2018","6","","28573","28593","Multi-agent systems (MASs) have received tremendous attention from scholars in different disciplines, including computer science and civil engineering, as a means to solve complex problems by subdividing them into smaller tasks. The individual tasks are allocated to autonomous entities, known as agents. Each agent decides on a proper action to solve the task using multiple inputs, e.g., history of actions, interactions with its neighboring agents, and its goal. The MAS has found multiple applications, including modeling complex systems, smart grids, and computer networks. Despite their wide applicability, there are still a number of challenges faced by MAS, including coordination between agents, security, and task allocation. This survey provides a comprehensive discussion of all aspects of MAS, starting from definitions, features, applications, challenges, and communications to evaluation. A classification on MAS applications and challenges is provided along with references for further studies. We expect this paper to serve as an insightful and comprehensive resource on the MAS for researchers and practitioners in the area.","2169-3536","","10.1109/ACCESS.2018.2831228","Data61 Ph.D. Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8352646","Multi-agent systems;survey;MAS applications;challenges","Task analysis;Multi-agent systems;Computer science;Security;Australia;Computational modeling;Decision making","","661","","167","OAPA","30 Apr 2018","","","IEEE","IEEE Journals"
"Studying How Configurations Impact Code Generation in LLMs: The Case of ChatGPT","B. Donato; L. Mariani; D. Micucci; O. Riganelli","University of Milano-Bicocca, Milano, Italy; University of Milano-Bicocca, Milano, Italy; University of Milano-Bicocca, Milano, Italy; University of Milano-Bicocca, Milano, Italy",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","442","453","Leveraging LLMs for code generation is becoming increasingly common, as tools like ChatGPT can suggest method implementations with minimal input, such as a method signature and brief description. Empirical studies further highlight the effectiveness of LLMs in handling such tasks, demonstrating notable performance in code generation scenarios. However, LLMs are inherently non-deterministic, with their output influenced by parameters such as temperature, which regulates the model's level of creativity, and top-p, which controls the choice of the tokens that shall appear in the output. Despite their significance, the role of these parameters is often overlooked. This paper systematically studies the impact of these parameters, as well as the number of prompt repetitions required to account for non-determinism, in the context of 548 Java methods. We observe significantly different performances across different configurations of ChatGPT, with temperature having a marginal impact compared to the more prominent influence of the top-p parameter. Additionally, we show how creativity can enhance code generation tasks. Finally, we provide concrete recommendations for addressing the non-determinism of the model.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025952","LLMs;code generation;ChatGPT;temperature;top-p;repetitions","Java;Codes;Collaboration;Chatbots;Temperature control;Artificial intelligence;Creativity;Software development management;Context modeling","","1","","26","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification","N. Tihanyi; Y. Charalambous; R. Jain; M. A. Ferrag; L. C. Cordeiro","Technology Innovation Institute (TII), Abu Dhabi, UAE; The University of Manchester, Manchester, UK; Technology Innovation Institute (TII), Abu Dhabi, UAE; Guelma University, Algeria; University of Manchester, UK Federal University of Amazonas, Brazil",2025 IEEE/ACM International Conference on Automation of Software Test (AST),"21 Jul 2025","2025","","","136","147","This paper presents a novel approach integrating Large Language Models (LLMs) with Formal Verification for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. Mathematical proofs and the stack trace of the vulnerabilities support these counterexamples. Using a specially designed prompt, we combine the source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50, 000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI’s capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle.","2833-9061","979-8-3315-0179-2","10.1109/AST66626.2025.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081727","Large Language Models;Formal Verification;Automatic Program Repair","Codes;Large language models;Source coding;Maintenance engineering;Model checking;Transformers;Software;Security;Formal verification;Context modeling","","","","122","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Boosting Large Language Models for System Software Retargeting: A Preliminary Study","M. Zhong; F. Lv; L. Wang; L. Qiu; H. Geng; H. Cui; X. Feng","SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","321","326","System software bridges hardware platforms and high-level applications. As new hardware platforms emerge, developers must customize code to support various system software, a process known as “retargeting”. This process is time-consuming and poorly automated. While large language models (LLMs) are proficient in general code generation tasks, their effectiveness in retargeting is limited by code complexity and abstract function descriptions. This paper presents TeSyn, a novel framework to enhance the code generation capabilities for system software retargeting. TeSyn comprises three steps: target-specific value extraction, common code clustering, and template synthesis. To evaluate TeSyn's effectiveness, we intro-duce SysRetar, the first dataset for system software retargeting, covering four types of system software and 195 hardware platforms. In our experiments, we select five LLMs and fine-tune CodeLLaMA-7B-Instruct on SysRetar to create SysRetar-LLM. Results show that TeSyn significantly enhances retargeting performance across five LLMs. Furthermore, code generated by SysRetar- LLM requires substantially less modification than the manual retargeting approach (Fork-Flow), suggesting potential improvements in efficiency. Given these promising results, we outline future research directions for advancing retargeting through LLMs. The dataset and code are publicly available at https://huggingface.co/doczll05/SysRetar-LLM.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992492","Software Retargeting;LLMs;Code Generation","Bridges;Codes;Large language models;Manuals;Boosting;Hardware;System software;Complexity theory","","","","33","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Leveraging Generative AI for Architecture Knowledge Management","R. Dhar; K. Vaidhyanathan; V. Varma","SERC, IIIT Hyderabad, India; SERC, IIIT Hyderabad, India; IREL, IIIT Hyderabad, India",2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C),"21 Aug 2024","2024","","","163","166","While documenting Architectural Knowledge (AK) is crucial, it is frequently neglected in many projects, and existing manual tools are underutilized. Although undocumented, Archi-tecture Knowledge (AK) is dispersed across various sources such as source code, documentation, and runtime logs. To address this, automated tools for efficient AK extraction and documentation are essential. Even after generating AK, navigating through vast the Architectural Records can be overwhelming. Building on that, we propose an automated Architectural Knowledge Management (AKM) System using Information Extraction and Generative AI, which generates AK from various source for a given system and answers architectural queries with respect to the given system. The development of an efficient Architectural Knowledge Management (AKM) system, which is both effective and user-friendly, entails the resolution of numerous challenges. It requires consolidating diverse AK data sources scattered across code, dia-grams, repository commits, and online platforms. The integration of Multimodal AI for AK extraction, incorporation of global AK, and leveraging Generative AI for AK documentation further compounds the problem. Moreover, generating contextually appropriate query responses adds another layer of complexity. To this end, we performed an initial exploratory study on generating Architectural Design Decisions using generative Large Language Models (LLM) in the context of Architecture Decision Records (ADR). Our initial results have been promising indicating the potential impact of GenAI for architectural knowledge management.","2768-4288","979-8-3503-6625-9","10.1109/ICSA-C63560.2024.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628192","Software architecture;AKM;Machine learning;Generative AI;Information Extraction;LLM","Runtime;Generative AI;Software architecture;Navigation;Source coding;Soft sensors;Documentation","","3","","11","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"AI-Powered Code Review Assistant for Streamlining Pull Request Merging","C. Adapa; S. S. Avulamanda; A. R. K. Anjana; A. Victor","India Systems Development Labs, IBM India Private Limited, Bangalore, India; India Systems Development Labs, IBM India Private Limited, Bangalore, India; India Systems Development Labs, IBM India Private Limited, Bangalore, India; India Systems Development Labs, IBM India Private Limited, Bangalore, India","2024 IEEE International Conference for Women in Innovation, Technology & Entrepreneurship (ICWITE)","23 Apr 2024","2024","","","323","327","WatsonX, a comprehensive data and AI platform, adeptly addresses contemporary challenges by meticulously training, validating, tuning, and deploying data to drive impactful business outcomes. The intricate task of timely merging Pull Requests (PRs) poses a significant challenge for software development teams, directly influencing business operations. This paper introduces an innovative solution leveraging AI, particularly harnessing generative AI techniques with the Falcon40-B model through the platform. The AI bot facilitates an initial PR review, offering insightful feedback on code formatting, best practices, and minor issues and streamlines collaboration by automatically assigning and notifying PR reviewers. The overarching goal is the continuous evolution of this AI bot into an intelligent reviewer, capable of assessing code from a functional standpoint. The implementation of this solution holds the promise of significantly enhancing PR management and expediting the entire development workflow.","","979-8-3503-8328-7","10.1109/ICWITE59797.2024.10503540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503540","WatsonX;Generative AI;Pull request;Falcon40B;Code review;GitHub webhooks;Intelligent bot","Training;Codes;Reviews;Merging;Chatbots;Software;Task analysis","","1","","17","IEEE","23 Apr 2024","","","IEEE","IEEE Conferences"
"Combining Large Language Models with Static Analyzers for Code Review Generation","I. Jaoua; O. B. Sghaier; H. Sahraoui","Université de Montréal, Montreal, Canada; Université de Montréal, Montreal, Canada; Université de Montréal, Montreal, Canada",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","174","186","Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (DataAugmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a realworld dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025622","Code Review;Knowledge-Based Systems;Language Models;Retrieval-Augmented Generation","Training;Codes;Automation;Reviews;Large language models;Retrieval augmented generation;Knowledge based systems;Pipelines;Static analysis;Hybrid power systems","","","","62","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Prompt Engineering for Curriculum Design","J. Leung; Z. Shen","College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore",2024 4th International Conference on Educational Technology (ICET),"6 Feb 2025","2024","","","97","101","Large Language Models (LLMs) have recently demonstrated successes in a broad range of areas that require language understanding and generation. The design of course curricula is a key part of the educational process, as it helps students understand expectations and learning goals, and teachers to maintain consistency when conducting the course. Thus, there would be benefits if educators could leverage the capabilities of LLMs. The first benefit is the potential for improving course content, which would lead to better learning outcomes for students. Secondly, educators can save time by having an LLM assist in creating course material or assessments. However, many methods of using LLMs in specific applications fine-tune LLMs, which requires task-specific data as well as technical knowledge to perform the fine-tuning. In this work we use Prompt Engineering, a set of methods that aims to improve generated responses from LLMs by altering the input prompt to the LLM. Often, however, advice for designing prompts is very broad, such as ""be concise"". We utilize prompt patterns to create and propose three reusable, specific, and customizable prompts that can be used to assist in the design of a course syllabus, lesson material, and assessment questions. The use of prompt patterns additionally aims to produce reliable and consistent results from LLMs.","","979-8-3503-7694-4","10.1109/ICET62460.2024.10869091","Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869091","prompt engineering;large language model;curriculum design;prompt pattern","Knowledge engineering;Visualization;Image synthesis;Large language models;Educational technology;Chatbots;Prompt engineering;Reliability","","","","16","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"Using Assignment Incentives to Reduce Student Procrastination and Encourage Code Review Interactions","K. Wang; R. Lawrence","Department of Computer Science, University of British Columbia, Kelowna, BC, Canada; Department of Computer Science, University of British Columbia, Kelowna, BC, Canada",2023 International Conference on Computational Science and Computational Intelligence (CSCI),"19 Jul 2024","2023","","","1628","1633","Procrastination causes student stress, reduced learning and performance, and results in very busy help sessions immediately before deadlines. A key challenge is encouraging students to complete assignments earlier rather than waiting until right before the deadline, so the focus becomes on the learning objectives rather than just meeting deadlines. This work presents an incentive system encouraging students to complete assignments many days before deadlines. Completed assignments are code reviewed by staff for correctness and providing feedback, which results in more student-instructor interactions and may help reduce student use of generative AI. The incentives result in a change in student behavior with 45% of assignments completed early and 30% up to 4 days before the deadline. Students receive real-time feedback with no increase in marking time.","2769-5654","979-8-3503-6151-3","10.1109/CSCI62032.2023.00270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590517","incentives;procrastination;code review;generative AI;time management","Codes;Scientific computing;Reviews;Generative AI;Real-time systems;Stress;Computational intelligence","","","","26","IEEE","19 Jul 2024","","","IEEE","IEEE Conferences"
"MDRE-LLM: A Tool for Analyzing and Applying LLMs in Software Reverse Engineering","A. Boronat; J. Mustafa","School of Computing and Mathematical Sciences, University of Leicester, Leicester, UK; School of Computing and Mathematical Sciences, University of Leicester, Leicester, UK","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","850","854","Understanding and maintaining software systems often requires extracting high-level abstractions, such as domain models, from source code. MDRE-LLM addresses this challenge by integrating Large Language Models (LLMs) with traditional Model-Driven Reverse Engineering (MDRE) techniques, offering an innovative approach to automate and enhance domain model recovery. The tool supports flexible granularity strategies and validates LLM -generated models against deterministic baselines. MDRE-LLM addresses diverse use cases, including analyzing legacy systems with minimal documentation, rapidly compre-hending large-scale codebases, and validating LLM performance in reverse engineering tasks. These capabilities have the potential to improve software analysis and refactoring while advance AI-driven research and education by fostering systematic experimentation and collaboration. The tool and a webcast are available at https://zenodo.org/uploads/14072106.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992421","Reverse engineering;domain model recovery;large language models;RAG","Analytical models;Systematics;Large language models;Source coding;Reverse engineering;Collaboration;Documentation;Aging;Software systems","","1","","12","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Long-Term Memory for Large Language Models Through Topic-Based Vector Database","Y. Zhang; Z. Yu; W. Jiang; Y. Shen; J. Li","Koiverse.AI, Hangzhou, China; Koiverse.AI, Hangzhou, China; Koiverse.AI, Hangzhou, China; Koiverse.AI, Hangzhou, China; Koiverse.AI, Hangzhou, China",2023 International Conference on Asian Language Processing (IALP),"12 Dec 2023","2023","","","258","264","Large language models (LLMs) have garnered sub-stantial attention and significantly transformed the landscape of artificial intelligence, due to their human-like understanding and generation capabilities. However, despite their excellent capabilities, LLMs lack the latest information and are constrained by limited context memory, which limits their effectiveness in many real-time applications that require up-to-date information, such as personal AI assistants. Inspired by the recent study on enhancing LLMs with infinite external memory using vector database, this paper proposes a topic-based vector database to enable LLMs to achieve long-term personalized memory. By leveraging prompt engineering to fully utilize the semantic understanding capabilities of LLMs, an efficient topic-based per-sonalized memory management system is designed to store and update user's preferences and characteristics. This system can be applied in various AI assistant domains, such as companion robots, to efficiently store personal memories of users through conversations, ultimately fulfilling their needs in a personalized manner.","2159-1970","979-8-3503-3078-6","10.1109/IALP61005.2023.10337079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10337079","LLMs;vectorDB;prompt engineering;long-term memory","Databases;Memory management;Semantics;Oral communication;Real-time systems;Artificial intelligence;Robots","","2","","35","IEEE","12 Dec 2023","","","IEEE","IEEE Conferences"
"Coffee Masterclass: An Experience of Co-Creation with Prompt Engineering and Generative AI for Immersive Environments Development","A. Rozo-Torres; W. J. Sarmiento","Multimedia Research Group, Universidad Militar Nueva Granada, Bogotá, Colombia; Multimedia Research Group, Universidad Militar Nueva Granada, Bogotá, Colombia",2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),"29 May 2024","2024","","","1170","1171","This work presents the design and development process of an immersive experience applying a co-creation approach between humans and generative artificial intelligence tools. From the point of view of any user, Coffee Masterclass is an immersive experience that brings anyone to the art and pleasure of preparing specialty coffees. However, the Coffee Masterclass is the result of the inclusion of prompt engineering outputs in each stage of the building process. The co-creation approach is included in all development processes, i.e., from the narrative to the visual content generated through code writing, which has been co-created between the creative team and GenAI. This work tells details of this approach, including how the generative artificial intelligence tools were used in each stage of immersive experience development. This work shows the advantage of involvement in a development team of people with skills in prompt engineering and interaction with Large Language Models. Also, it includes recommendations to other development teams, including generative artificial intelligence tools by future developments.","","979-8-3503-7449-0","10.1109/VRW62533.2024.00379","Universidad Militar Nueva Granada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536243","Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed/augmented reality","Visualization;Three-dimensional displays;Codes;Art;Conferences;Buildings;Immersive experience","","1","","3","IEEE","29 May 2024","","","IEEE","IEEE Conferences"
"Improving the Readability of Automatically Generated Tests Using Large Language Models","M. Biagiola; G. Ghislotti; P. Tonella","Software Institute - Università della Svizzera italiana, Lugano, Switzerland; Software Institute - Università della Svizzera italiana, Lugano, Switzerland; Software Institute - Università della Svizzera italiana, Lugano, Switzerland","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","162","173","Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989020","Large Language Models;Software Testing;Readability","Software testing;Hands;Codes;Large language models;Semantics;Generators","","","","59","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Generating Test Scenarios from NL Requirements Using Retrieval-Augmented LLMs: An Industrial Study","C. Arora; T. Herda; V. Homm","Monash University, Melbourne, Australia; Austrian Post Group IT, Vienna, Austria; Austrian Post Group IT, Vienna, Austria",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","240","251","Test scenarios are specific instances of test cases that describe a sequence of actions to validate a particular software functionality. By outlining the conditions under which the software operates and the expected outcomes, test scenarios ensure that the software functionality is tested in an integrated manner. Test scenarios are crucial for systematically testing an application under various conditions, including edge cases, to identify potential issues and guarantee overall performance and reliability. Manually specifying test scenarios is tedious and requires a deep understanding of software functionality and the underlying domain. It further demands substantial effort and investment from already time- and budget-constrained requirements engineers and testing teams. This paper presents an automated approach (RAGTAG) for test scenario generation using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG allows the integration of specific domain knowledge with LLMs' generation capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post with bilingual requirements in German and English. Our results from an interview survey conducted with four experts on five dimensions – relevance, coverage, correctness, coherence and feasibility, affirm the potential of RAGTAG in automating test scenario generation. Specifically, our results indicate that, despite the difficult task of analyzing bilingual requirements, RAGTAG is able to produce scenarios that are well-aligned with the underlying requirements and provide coverage of different aspects of the intended functionality. The generated scenarios are easily understandable to experts and feasible for testing in the project environment. The overall correctness is deemed satisfactory; however, gaps in capturing exact action sequences and domain nuances remain, underscoring the need for domain expertise when applying LLMs.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628480","Requirements Engineering;Requirements-driven Testing;Test Scenarios;Large Language Models (LLMs);Industry Study","Surveys;Large language models;Coherence;Software systems;Software reliability;Scenario generation;Requirements engineering","","8","","38","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Benchmarking Causal Study to Interpret Large Language Models for Source Code","D. Rodriguez-Cardenas; D. N. Palacio; D. Khati; H. Burke; D. Poshyvanyk","Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA",2023 IEEE International Conference on Software Maintenance and Evolution (ICSME),"11 Dec 2023","2023","","","329","334","One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. LLMs are rooted in the concept of emergent capabilities in which machines statistically learn complex patterns from code data. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs’ performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, number of tokens, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model’s performance. In an effort to bring statistical rigor to the evaluation of LLMs, this paper introduces a benchmarking strategy named Galeras comprised of curated testbeds for three SE tasks (i.e., code completion, code summarization, and commit generation) to help aid the interpretation of LLMs’ performance.We illustrate the insights of our benchmarking strategy by conducting a case study on the performance of ChatGPT under distinct prompt engineering methods. The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT’s generative performance by an average treatment effect of ≈ 3%. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics (≈ 0.412). The end result of our case study is to showcase causal inference evaluations, in practice, to reduce confounding bias. By reducing the bias, we offer an interpretable solution for the accuracy metric under analysis.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336302","Software Engineering;Testbeds;Large Language Models;dl4se;Interpretability","Measurement;Training;Software maintenance;Codes;Correlation;Source coding;Semantics","","8","","49","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"LLM - Based Code Generation for Querying Temporal Tabular Financial Data","M. Lashuel; G. Kurdistan; A. Green; J. S. Erickson; O. Seneviratne; K. P. Bennett","Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Management, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY, USA; The Future of Computing Institute at Rensselaer Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY, USA",2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr),"10 Dec 2024","2024","","","1","8","We examine the question of “how well large language models (LLMs) can answer questions using temporal tabular financial data by generating code?”. Leveraging advanced language models, specifically GPT-4 and Llama 3, we aim to scrutinize and compare their abilities to generate coherent and effective code for Python, R, and SQL based on natural language prompts. We design an experiment to assess the performance of LLMs on natural language prompts on a large temporal financial dataset. We created a set of queries with hand-crafted R code answers. To investigate the strengths and weaknesses of LLMs, each query was created with different factors that characterize the financial meaning of the queries and their complexity. We demonstrate how to create specific zero-shot prompts to generate code to answer natural language queries about temporal financial tabular data. We develop specific system prompts for each language to ensure they correctly answer time-oriented questions. We execute this experiment on two LLMs (GPT-4 and Llama 3), assess if the outputs produced are executable and correct, and assess the efficiency of the produced code for Python, SQL, and R. We find that while LLMs have promising performance, their performance varies greatly across the languages, models, and experimental factors. GPT-4 performs best on Python (95.2% correctness) but has significantly weaker performance on SQL (87.6% correctness) and R (79.0% correctness). Llama 3 is less successful at generating code overall, but it achieves its best results in R (71.4% correctness). A multi-factor statistical analysis of the results with respect to the defined experimental factors provides further insights into the specific areas of challenge in code generation for each LLM. Our preliminary results on this modest benchmark demonstrate a framework for developing larger, comprehensive, unique benchmarks for both temporal financial tabular data and R code generation. While Python and SQL already have benchmarks, we are filling in the gaps for R. Powerful AI agents for text-to-code generation, as demonstrated in this work, provide a critical capability required for the next-generation AI-based natural language financial intelligence systems and chatbots, directly addressing the complex challenges presented by querying temporal tabular financial data.","2640-7701","979-8-3503-5483-6","10.1109/CIFEr62890.2024.10772804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772804","large language models;code generation;gpt-4;llama3;financial data;tabular data;benchmarking;financial industry;automation","Economics;Structured Query Language;Codes;Statistical analysis;Large language models;Natural languages;Benchmark testing;Filling;Data models;Next generation networking","","","","28","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Security Time-Varying Formation Control for Multi-Agent Systems Under Denial-of-Service Attacks via Unknown Input Observer","T. Shi; F. Zhu","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China",IEEE Transactions on Network Science and Engineering,"22 Jun 2023","2023","10","4","2372","2385","The article investigates the security time-varying formation control problem when the communication network of multi-agent systems (MASs) suffers from Denial-of-Service (DoS) attacks based on switched controller, state observers and unknown input reconstruction (UIR). To begin with, for the follower agents with unknown inputs, through an interval observer, the estimations of unknown input (UI) and system state can be obtained asymptotically and simultaneously by a Luenberger-like state observer (LLSO) combining with a UIR. After this, a distributed control protocol (DCP) on formation tracking is proposed based on the UIR and the state estimation, which can compensate the UI successfully. Moreover, the time space is divided into three parts utilizing the transmission interruption caused by DoS attacks, and the gain of the controller is designed corresponding to three different DoS attack duration, respectively. With the help of some concept in switched system, it is demonstrated that the given formation pattern can be reached under the proposed DCP. Finally, to verify the effectiveness of the proposed methods, a vehicle cruise control system used as the simulation example is given.","2327-4697","","10.1109/TNSE.2023.3246594","National Natural Science Foundation of China(grant numbers:61973236,61573256); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049128","DoS attacks;formation control;multi-agent systems;security control;unknown input","Observers;Formation control;Security;Denial-of-service attack;Laplace equations;Cyberattack;Aerospace electronics","","24","","51","IEEE","20 Feb 2023","","","IEEE","IEEE Journals"
"Look Before You Leap: An Exploratory Study of Uncertainty Analysis for Large Language Models","Y. Huang; J. Song; Z. Wang; S. Zhao; H. Chen; F. Juefei-Xu; L. Ma","The University of Tokyo, Tokyo, Japan; University of Alberta, Edmonton, AB, Canada; University of Alberta, Edmonton, AB, Canada; University of Alberta, Edmonton, AB, Canada; The University of Sydney, Sydney, NSW, Australia; New York University, New York, NY, USA; The University of Tokyo, Tokyo, Japan",IEEE Transactions on Software Engineering,"13 Feb 2025","2025","51","2","413","429","The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, the potential erroneous behavior (e.g., the generation of misinformation and hallucination) has also raised severe concerns for the trustworthiness of LLMs, especially in safety-, security- and reliability-sensitive industrial scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by classic machine learning (ML) models, the unique characteristics of recent LLMs (e.g., adopting self-attention mechanism as its core, very large-scale model size, often used in generative contexts) pose new challenges for the behavior analysis of LLMs. Up to the present, little progress has been made to better understand whether and to what extent uncertainty estimation can help characterize the capability boundary of an LLM, to counteract its undesired behavior, which is considered to be of great importance with the potential wide-range applications of LLMs across industry domains. To bridge the gap, in this paper, we initiate an early exploratory study of the risk assessment of LLMs from the lens of uncertainty. In particular, we conduct a large-scale study with as many as twelve uncertainty estimation methods and eight general LLMs on four NLP tasks and seven programming-capable LLMs on two code generation tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings confirm the potential of uncertainty estimation for revealing LLMs’ uncertain/non-factual predictions. The insights derived from our study can pave the way for more advanced analysis and research on LLMs, ultimately aiming at enhancing their trustworthiness.","1939-3520","","10.1109/TSE.2024.3519464","JST CRONOS(grant numbers:JPMJCS24K8); JST-Mirai Program(grant numbers:JPMJMI20B8); JSPS KAKENHI(grant numbers:JP21H04877,JP23H03372,JP24K02920); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10820047","Large language models;deep neural networks;uncertainty estimation;software reliability","Uncertainty;Estimation;Codes;Hidden Markov models;Adaptation models;Artificial intelligence;Training;Risk management;Electronic mail;Transformers","","8","","121","IEEE","1 Jan 2025","","","IEEE","IEEE Journals"
"Generative AI for Self-Healing Systems","P. Khlaisamniang; P. Khomduean; K. Saetan; S. Wonglapsuwan","Artificial Intelligence Association of Thailand, Bangkok, Thailand; Artificial Intelligence Association of Thailand, Bangkok, Thailand; Artificial Intelligence Association of Thailand, Bangkok, Thailand; Artificial Intelligence Association of Thailand, Bangkok, Thailand",2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),"19 Dec 2023","2023","","","1","6","In large-scale system production, the risk of component failures like hardware issues, software bugs, network disruptions, and memory errors is a concern. To mitigate this, human experts such as IT analysts, system engineers, and infrastructure architects use system monitoring to detect and respond to failures. This study aims to integrate generative AI technology into self-healing systems to enhance the operations of large-scale systems and facilitate automatic repairs. The focus is on optimizing system functionality and efficiency at scale while reducing reactive tasks that require human intervention. Our proposed solutions involve leveraging generative AI for anomaly detection, code generation, debugging and auto generative report within self-healing systems. Furthermore, the automated response ansible scripts, generated by generative AI such as GPT-4 to create a comprehensive and efficient python code completion solution that enhances backend system functionality and repairs failing components.","2831-4565","979-8-3503-7121-5","10.1109/iSAI-NLP60301.2023.10354608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354608","Self-Healing Systems;Generative AI;Anomaly Detection;Code Generation","Codes;Production;Maintenance engineering;Software;Natural language processing;Large-scale systems;Artificial intelligence","","5","","22","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models","Z. Xu; Y. Liu; G. Deng; K. Wang; Y. Li; L. Shi; S. Picek","University of New South Wales, Australia; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Huazhong University of Science and Technology, China; University of New South Wales, Australia; Nanyang Technological University, Singapore; Radboud University, The Netherlands",2025 IEEE Security and Privacy Workshops (SPW),"3 Jul 2025","2025","","","270","277","Security concerns for large language models (LLMs) have intensified, particularly regarding jailbreaking attempts via malicious inputs. Studying new jailbreak attacks can help with red teaming to secure the LLMs. For open-source LLMs, embedding-based attacks can achieve high effectiveness. However, existing embedding-based attacks only optimize the suffix of the prompt, leading to unnecessary complexity and rendering them easier to detect. We propose a novel attack method that directly manipulates entire LLM inputs without separating them into bodies and suffixes. However, manipulating entire LLM inputs faces the challenges of random or nonsensical repetitive responses. To address these challenges, we propose Clip, whose main strategy is to clip each input dimension based on the mean and standard deviation of the model vocabulary during model inference. Experiments show that Clip improves the attack success rate (ASR) of continuous embedding attacks with full LLM inputs from 62% to 83% for LLaMa and from 38% to 83 % for Vicuna.","2770-8411","979-8-3315-6643-2","10.1109/SPW67851.2025.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050782","Large Language Models;Safety;Ethics;Robust-ness;Security","Vocabulary;Privacy;Ethics;Large language models;Rendering (computer graphics);Complexity theory;Safety;Security;Standards;Faces","","","","25","IEEE","3 Jul 2025","","","IEEE","IEEE Conferences"
"Actionable Cyber Threat Intelligence Using Knowledge Graphs and Large Language Models","R. Fieblinger; M. T. Alam; N. Rastogi","Mittweida University of Applied Sciences, Mittweida, Germany; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA",2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW),"20 Aug 2024","2024","","","100","111","Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdS trike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.","2768-0657","979-8-3503-6729-4","10.1109/EuroSPW61312.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628558","Cyber Threat Intelligence;Large Language Models;Knowledge Graphs;Threat Prediction","Large language models;Refining;Knowledge graphs;Organizations;Predictive models;Ontologies;Cyber threat intelligence","","14","","64","IEEE","20 Aug 2024","","","IEEE","IEEE Conferences"
"Insight generation and complex datasets analysis with LLMs in AdTech","A. Moskovets; M. Serras","Data Science Department, Seedtag, Madrid, Spain; Data Science Department, Seedtag, Madrid, Spain",Global Congress on Emerging Technologies (GCET-2024),"26 Mar 2025","2024","","","1","8","This study pioneers the application of large language models (LLMs) to the practical task of automating insight generation in the advertising technology (AdTech) sector, a traditionally labor-intensive area demanding substantial human effort. We critically assess the capabilities of GPT-4 and GPT-4o through a replicated analysis of real-world advertising campaign data, containing contextual signals and key performance indicators (KPIs) like impressions, clicks, and engagement rates. Our findings reveal that LLMs have the potential to significantly increase the efficiency of data analysis, succeed at processing complex datasets, and generate actionable insights aligned with the business context. While GPT-4 excels in criteria such as alignment, relevance, and fluency, deploying these models presents challenges, including stochastic behavior and occasional errors in factuality. By describing the original methodology and pinpointing areas for future improvement, this research not only confirms the transformative potential of LLMs in AdTech but also serves as a foundational blueprint for automating business-related data analysis processes.","","979-8-3315-4260-3","10.1109/GCET64327.2024.10934405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934405","Large Language Models;Data Analysis Automation;Agent-Based Systems;Advertising Technology;AI in Business Applications","Data analysis;Automation;Large language models;Key performance indicator;Stochastic processes;Advertising;Business","","","","17","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Prompting Large Language Models for Topic Modeling","H. Wang; N. Prakash; N. K. Hoang; M. S. Hee; U. Naseem; R. K. -W. Lee","Singapore University of Design and Technology, Singapore; Singapore University of Design and Technology, Singapore; VinUniversity, Vietnam; Singapore University of Design and Technology, Singapore; James Cook University of North Queensland, Australia; Singapore University of Design and Technology, Singapore",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","1236","1241","Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopic’s ability to uncover relevant topics in multiple datasets.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386113","topic modeling;large language models;prompt engineering","Measurement;Semantics;Merging;Focusing;Manuals;Big Data;Benchmark testing","","17","","36","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models using Arabic Prompts to Generate Python Codes","N. J. Al-Khafaji; B. K. Majeed","Technical Insititute - Nassiriyah, Southern Technical University, Dhi-Qar, Iraq; Technical Insititute - Nassiriyah, Southern Technical University, Dhi-Qar, Iraq",2024 4th International Conference on Emerging Smart Technologies and Applications (eSmarTA),"26 Aug 2024","2024","","","1","5","Currently, the popularity of large language models (LLMs) for instance, ChatGPT from OpenAI and Gemini from Google is increasing greatly in our lives, due to their unparalleled performance in various applications. These models play a vital role in both academic and industrial fields. With this popularity, evaluating these models has become extremely important, especially when using the Arabic language. Despite the increasing popularity and performance of AI, there have been no empirical studies evaluating the use of LLMs for Arabic prompts in the field of code generation. However, the code generation in LLM can be strongly influenced by the choice of prompt. Evaluating the LLMs by Arabic prompts helps us better understand the strengths and weaknesses of these models. Therefore, we highlighted the evaluation of the most popular LLM programs (Chatgpt-3.5, ChatGPT-4 and Gemini) when generating Python codes based on Arabic prompts. In this study we employed CodeBLUE score and Flake8 as a metric to evaluate the LLMs capabilities for code generation via Arabic prompts. Our results indicate significant differences in performance across different LLMs and prompts levels. This study lays the foundation for further research into LLM capabilities based on Arabic prompts and suggests practical implications for using LLM in automated code generation and test-driven development tasks.","","979-8-3503-5413-3","10.1109/eSmarTA62850.2024.10638877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638877","Large language model;Programming language;ChatGPT;Gemini;Flake8;CodeBLUE","Measurement;Codes;Large language models;Chatbots;Internet;Task analysis;Python","","1","","18","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"The Fact Selection Problem in LLM-Based Program Repair","N. Parasaram; H. Yan; B. Yang; Z. Flahy; A. Qudsi; D. Ziaber; E. T. Barr; S. Mechtaev",University College London; University College London; University College London; University College London; University College London; University College London; University College London; Peking University,2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2574","2586","Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open -source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029841","automated program repair;large language models;prompt engineering","Systematics;Large language models;Computer bugs;Semantics;Maintenance engineering;Benchmark testing;Syntactics;Software engineering;Software development management;Python","","2","","55","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Improving Patch Correctness Analysis via Random Testing and Large Language Models","F. Molina; J. M. Copia; A. Gorla","IMDEA Software Institute, Madrid, Spain; IMDEA Software Institute and Universidad Politécnica de Madrid, Madrid, Spain; IMDEA Software Institute, Madrid, Spain","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","27 Aug 2024","2024","","","317","328","Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 % of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50% of the incorrect patches identified by state-of-the-art techniques.","2159-4848","979-8-3503-0818-1","10.1109/ICST60714.2024.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638611","Patch Correctness Assessment;Random Testing;Large Language Models","Software testing;Fault diagnosis;Analytical models;Costs;Large language models;Maintenance engineering;Benchmark testing","","","","52","IEEE","27 Aug 2024","","","IEEE","IEEE Conferences"
"GAT-SQL: An Advanced Prompt Engineering Approach for Effective Text-to-SQL Interactions","S. Almohaimeed; S. Almohaimeed; L. Wang","University of Central Florida, Orlando, Florida, USA; University of Central Florida, Orlando, Florida, USA; University of Central Florida, Orlando, Florida, USA",2024 IEEE Congress on Evolutionary Computation (CEC),"8 Aug 2024","2024","","","1","10","In natural language processing, recent advancements in large language models (LLMs) have significantly impacted the text-to-SQL task, particularly in single-question interactions. However, multi-turn question interactions present unique challenges not fully addressed by current LLMs like GPT-3.5-turbo and GPT-4.5-turbo. In this paper, we perform a comprehensive and systematic analysis on a multi-turn interaction dataset known as SParC to compare various existing prompt engineering methods, including prompt representations and in-context learning methods. Following this, we present GAT-SQL, a novel prompt engineering approach based on three techniques: GAT representation, GAT reviser, and GAT verifier. Comparing our GAT representation and GAT verifier techniques to the previous methods of prompt engineering, they were very successful for the zero-shot experiments. GAT representations improve performance by an average of 2.9% EX and 3.8% IX across all existing question representation methods. While GAT verifier results in a greater improvement in accuracy by an average of 3.6% EX and 4% IX. Furthermore, with regard to in-context learning experiments, the GAT reviser achieved 77.4% EX and 59.9% IX, outperforming the best state-of-the-art model by 3.4% EX and 6.4% IX. As a further demonstration of the effectiveness of GAT-SQL, we tested it on another dataset of multi-turn interactions named CoSQL. GAT reviser achieved a new benchmark in the CoSQL competition, achieving 74.5% EX and 50.2% IX, higher than the closest baseline by 6.3% EX and 9.7% IX.","","979-8-3503-0836-5","10.1109/CEC60901.2024.10611969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611969","Semantic Parsing;Text-To-SQL;Prompt Engineering;Large Language Models","Analytical models;Structured Query Language;Systematics;Accuracy;Semantics;Benchmark testing;Predictive models","","","","22","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?","Y. Chen; J. Wu; X. Ling; C. Li; Z. Rui; T. Luo; Y. Wu","Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China; Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China; Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China; Stony Brook University, USA; Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China; Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China; Institute of Software, Chinese Academy of Sciences, China University of Chinese Academy of Sciences, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","459","471","In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3647633","Chinese Academy of Sciences(grant numbers:XDA0320401); National Natural Science Foundation of China(grant numbers:62202457); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554921","Large language models;automatic program repair;repository-level bugs;context;static analysis","Analytical models;Codes;Accuracy;Computer bugs;Maintenance engineering;Benchmark testing;Task analysis","","3","","37","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data","Y. Feng; S. Vanam; M. Cherukupally; W. Zheng; M. Qiu; H. Chen",University of North Texas; University of North Texas; University of North Texas; Argonne National Laboratory; Dakota State University; University of North Texas,"2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","2 Aug 2023","2023","","","876","885","The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users’ attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT’s code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT’s code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screen-shots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.","0730-3157","979-8-3503-2697-0","10.1109/COMPSAC57700.2023.00117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196869","ChatGPT;Coding Generation;Software Engineering;Large Language Models (LLMs);Generative Models;Social Media","Codes;Social networking (online);Debugging;Chatbots;Software;Task analysis;Interviews","","70","","31","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"AdversaFlow: Visual Red Teaming for Large Language Models with Multi-Level Adversarial Flow","D. Deng; C. Zhang; H. Zheng; Y. Pu; S. Ji; Y. Wu","Zhejiang University, China; Zhejiang University, China; Zhejiang University, China; Zhejiang University, China; Zhejiang University, China; Zhejiang University, China",IEEE Transactions on Visualization and Computer Graphics,"25 Nov 2024","2025","31","1","492","502","Large Language Models (LLMs) are powerful but also raise significant security concerns, particularly regarding the harm they can cause, such as generating fake news that manipulates public opinion on social media and providing responses to unethical activities. Traditional red teaming approaches for identifying AI vulnerabilities rely on manual prompt construction and expertise. This paper introduces AdversaFlow, a novel visual analytics system designed to enhance LLM security against adversarial attacks through human-AI collaboration. AdversaFlow involves adversarial training between a target model and a red model, featuring unique multi-level adversarial flow and fluctuation path visualizations. These features provide insights into adversarial dynamics and LLM robustness, enabling experts to identify and mitigate vulnerabilities effectively. We present quantitative evaluations and case studies validating our system's utility and offering insights for future AI security solutions. Our method can enhance LLM security, supporting downstream scenarios like social media regulation by enabling more effective detection, monitoring, and mitigation of harmful content and behaviors.","1941-0506","","10.1109/TVCG.2024.3456150","National Key Research and Development Program of China(grant numbers:2023YFB3107100); Key “Pioneer” R&D Projects of Zhejiang Province(grant numbers:2023C01120); National Natural Science Foundation of China(grant numbers:U22A2032,62072400); Collaborative Innovation Center of Artificial Intelligence by MOE and Zhejiang Provincial Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681029","Visual Analytics for Machine Learning;Artificial Intelligence Security;Large Language Models;Text Visualization","Security;Analytical models;Training;Visual analytics;Artificial intelligence;Toxicology;Safety","Computer Graphics;Humans;Social Media;Computer Security;Artificial Intelligence;Algorithms;Large Language Models","1","","78","IEEE","16 Sep 2024","","","IEEE","IEEE Journals"
"Black-box Steganography for Large Language Models","X. Li; Z. Wang; X. Zhang","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China",IEEE Transactions on Circuits and Systems for Video Technology,"","2025","PP","99","1","1","In recent years, the rapid development of deep learning has brought new opportunities for steganography. However, the current advanced white-box model steganography methods are not suitable for large language models. Since the parameter scale and complexity of large language models are far beyond that of ordinary models, retraining them to hide secret data is extremely challenging. Moreover, the cover parameters or structures of the embedded data are vulnerable to detection by attackers. To enhance practicality, we propose a black-box steganographic scheme for large language models, which embeds secret data into the third-party pre-trained large language models using backdoor techniques without knowing the internal complex structure and parameters of the large language models. Specifically, the sender first encodes the secret data into trigger labels and then uses a certain proportion of trigger samples and clean samples to fine-tune the third-party large language model to embed the secret data without significantly reducing the model performance. The receiver uses trigger samples to extract the secret data by interacting with the large language model, thereby achieving covert communication of the secret data. Experiments demonstrate the effectiveness of the proposed scheme in terms of embedding capacity, robustness, and security.","1558-2205","","10.1109/TCSVT.2025.3574808","Shanghai Municipal Education Commission(grant numbers:22CGA46); National Natural Science Foundation of China(grant numbers:62376148); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11017758","Large Language Models;Steganography;Backdoor","Steganography;Data models;Watermarking;Data mining;Large language models;Glass box;Transformers;Neural networks;Closed box;Training","","","","","IEEE","29 May 2025","","","IEEE","IEEE Early Access Articles"
"Exploration and Practice of Code Commenting Based on Prompt Engineering","Y. Ding; B. Wang; H. Zhang; S. Li","School of Software, Northwestern Polytechnical University, Xi'an, China; School of Software, Northwestern Polytechnical University, Xi'an, China; School of Software, Northwestern Polytechnical University, Xi'an, China; School of Information Science and Technology, Beijing University of Technology, Beijing, China",2025 7th International Conference on Computer Science and Technologies in Education (CSTE),"29 Jul 2025","2025","","","98","102","This paper investigates the application of code comment generation technology based on Prompt Engineering in software development. With the increasing scale and complexity of software projects, traditional manual code commenting methods face issues of low efficiency and unstable quality. To address these challenges, we employ Prompt Engineering to design precise prompts that guide Large Language Models (LLMs) to automatically generate high-quality code comments. In this paper, we primarily utilized ChatGPT-4 and designed a code comment generation process for C language code based on Prompt Engineering. The experimental validation shows that this method is highly efficient in generating accurate and consistent comments. Additionally, this proposed approach has been applied to parse comments for erroneous code, assisting developers in quickly locating and fixing problems. The experimental results demonstrate that the method based on Prompt Engineering can significantly enhance the quality of code comments and development efficiency. It also provides new ideas and tools for software development and programming education.","","979-8-3315-1166-1","10.1109/CSTE64638.2025.11092128","Northwestern Polytechnical University; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11092128","Prompt Engineering;Code Comment Generation;Error Code Comment Parsing;LLM;ChatGPT","Codes;Accuracy;Education;Static analysis;Manuals;Writing;Software;Prompt engineering;Programming profession;Software development management","","","","20","IEEE","29 Jul 2025","","","IEEE","IEEE Conferences"
"Cloud SecNavigator: RAG Approach to Bridge Gaps and Strengthen Cloud Security Practices with RAGAS Assessment","R. Watanabe; S. Okada; K. Watarai; T. Mitsunaga","INIAD, Toyo University, Tokyo, Japan; INIAD, Toyo University, Tokyo, Japan; INIAD, Toyo University, Tokyo, Japan; INIAD, Toyo University The Tokyo Foundation for Policy Research, Tokyo, Japan",2024 International Conference on Engineering and Emerging Technologies (ICEET),"12 Mar 2025","2024","","","1","6","In recent years, many cyber incidents have resulted from misconfigurations of AWS. Although AWS provides exten-sive security guidelines, the sheer volume of documentation makes it difficult for developers to read and apply them completely. To address this, we propose a development support tool, Cloud SecNavigator. This tool uses Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to extract relevant content and accurately respond to user queries based on AWS documentation. Our evaluation measures Cloud SecNavigator's output accuracy using two approaches: (1) Retrieval-Augmented Generation Assessment (RAGAS) and (2) a comparative accuracy assessment between Cloud SecNavigator-generated responses and a non-RAG LLM (GPT-40). The results indicate that Cloud Sec-Navigator achieves superior accuracy, highlighting its potential as an effective development support tool.","2831-3682","979-8-3315-3289-5","10.1109/ICEET65156.2024.10913535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10913535","Cloud Security;Retrieval-Augmented Generation (RAG);LLM (Large Language Models);RAGAS","Accuracy;Large language models;Cloud computing security;Retrieval augmented generation;Documentation;Medical services;User interfaces;Internet;Security;Usability","","","","24","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models","P. Maddigan; T. Susnjak","School of Mathematical and Computational Sciences, Massey University, Auckland, New Zealand; School of Mathematical and Computational Sciences, Massey University, Auckland, New Zealand",IEEE Access,"12 May 2023","2023","11","","45181","45193","The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.","2169-3536","","10.1109/ACCESS.2023.3274199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121440","ChatGPT;codex;end-to-end visualisations from natural language;GPT-3;large language models;natural language interfaces;text-to-visualisation","Data visualization;Task analysis;Data models;Codes;Chatbots;Transformers;Market research;Natural language processing;Text recognition","","137","","23","CCBYNCND","8 May 2023","","","IEEE","IEEE Journals"
"On Codex Prompt Engineering for OCL Generation: An Empirical Study","S. Abukhalaf; M. Hamdaqa; F. Khomh","Department of Computer and Software Engineering, Software and Emerging Technologies Lab (SAET), Polytechnique Montréal, Montréal, Canada; Department of Computer and Software Engineering, Software and Emerging Technologies Lab (SAET), Polytechnique Montréal, Montréal, Canada; Department of Computer and Software Engineering, SoftWare Analytics and Technologies Lab (SWAT), Polytechnique Montréal, Montréal, Canada",2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR),"12 Jul 2023","2023","","","148","157","The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00033","Canadian Institute for Advanced Research; Science and Engineering Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173990","Codex;Prompt Engineering;Object Constraint Language (OCL);Code Generation;Large Language Models","Productivity;Codes;Unified modeling language;Semantics;Syntactics;Writing;Reliability engineering","","19","","29","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models","D. W. Yip; A. Esmradi; C. F. Chan","Logistic and Supply Chain MultiTech, R&D Centre (LSCM), Hong Kong; Logistic and Supply Chain MultiTech, R&D Centre (LSCM), Hong Kong; Logistic and Supply Chain MultiTech, R&D Centre (LSCM), Hong Kong",2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),"5 Apr 2024","2023","","","1","5","Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content. As LLM-integrated applications gain wider adoption, they face growing susceptibility to such attacks. This study introduces a novel evaluation framework for quantifying the resilience of applications. The framework incorporates innovative techniques designed to ensure representativeness, interpretability, and robustness. To ensure the representativeness of simulated attacks on the application, a meticulous selection process was employed, resulting in 115 carefully chosen attacks based on coverage and relevance. For enhanced interpretability, a second LLM was utilized to evaluate the responses generated from these simulated attacks. Unlike conventional malicious content classifiers that provide only a confidence score, the LLM-based evaluation produces a score accompanied by an explanation, thereby enhancing interpretability. Subsequently, a resilience score is computed by assigning higher weights to attacks with greater impact, thus providing a robust measurement of the application's resilience. To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that Llama2, the newer model exhibited higher resilience compared to ChatGLM. This finding substantiates the effectiveness of the framework, aligning with the prevailing notion that newer models tend to possess greater resilience. Moreover, the framework exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution. Overall, the framework offers valuable insights that empower organizations to make well-informed decisions to fortify their applications against potential threats from prompt injection.","","979-8-3503-4107-2","10.1109/CSDE59766.2023.10487667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487667","Large Language Model;Prompt Injection;Cyber Security","Weight measurement;Software architecture;Computational modeling;Organizations;Data engineering;Robustness;Data models","","4","","46","IEEE","5 Apr 2024","","","IEEE","IEEE Conferences"
"Large Language Models on Small Resource-Constrained Systems: Performance Analysis and Trade-Offs","L. Seymour; B. Kutukcu; S. Baidya","Electrical Engineering, Computer Science, Western Kentucky University, KY, USA; Electrical and Computer Engineering, University of California San Diego, CA, USA; Computer Science and Engineering, University of Louisville, KY, USA",SoutheastCon 2025,"25 Apr 2025","2025","","","1362","1369","Generative AI like the Large Language Models (LLMs) has become more available for the general consumer in recent years. Publicly available services, e.g., ChatGPT, perform token generation on networked cloud server hardware, effectively removing the hardware entry cost for end users. However, the reliance on network access for these services, privacy and security risks involved, and sometimes the needs of the application make it necessary to run LLMs locally on edge devices. A significant amount of research has been done on optimization of LLMs and other transformer-based models on non-networked, resource-constrained devices, but they typically target older hardware. Our research intends to provide a ‘baseline’ characterization of more recent commercially available embedded hardware for LLMs, and to provide a simple utility to facilitate batch testing LLMs on recent Jetson hardware. We focus on the latest line of NVIDIA Jetson devices (Jetson Orin), and a set of publicly available LLMs (Pythia) ranging between 70 million and 1.4 billion parameters. Through detailed experimental evaluation with varying software and hardware parameters, we showcase tradeoff spaces and optimization choices. Additionally, we design our testing structure to facilitate further research that involves performing batch LLM testing on NVIDIA Jetson hardware.","1558-058X","979-8-3315-0484-7","10.1109/SoutheastCon56624.2025.10971638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971638","Embedded Systems;Large Language Models;Evaluation;Machine Learning;Resource Constraints;Charac-terization;Performance Benchmark","Performance evaluation;Privacy;Large language models;System performance;Transformers;Hardware;Software;Servers;Security;Optimization","","1","","31","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Detecting Code Vulnerabilities using LLMs","L. Huynh; Y. Zhang; D. Jayasundera; W. Jeon; H. Kim; T. Bi; J. B. Hong","The University of Western Australia, Perth, Australia; The University of Western Australia, Perth, Australia; The University of Western Australia, Perth, Australia; Sungkyunkwan University, Seoul, South Korea; Sungkyunkwan University, Seoul, South Korea; The University of Western Australia, Perth, Australia; The University of Western Australia, Perth, Australia",2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),"11 Jul 2025","2025","","","401","414","Large language models (LLMs) have emerged as a promising tool for detecting code vulnerabilities, potentially offering advantages over traditional rule-based methods. This paper proposes an enhanced framework for vulnerability detection using LLMs, incorporating various prompt engineering strategies to improve performance. We evaluate several techniques, including role-based prompting, zero-shot chain-of-thought, and structured prompting approaches, on the DiverseVul dataset of C/C++ vulnerabilities. Our experiments assess the framework’s performance across different code structures, contextual information levels, and LLM capabilities. Our results show that using our dynamic prompt engineering technique, you can improve the F1 score by up to 100% with GPT-3.5, a widely used LLM model. We also observe that GPT-4o, Gemini 2.0 Flash, and Meta Llama 3.1 generally outperform GPT-3.5, and all models are very poor when it comes to correctly identifying the type of vulnerability in the code, with the best F1 score of 0.16 observed. However, our follow-up experiments on LLM-based vulnerability correction (i.e., patching) show a 45.77% success rate using GPT-4o, demonstrating promising results in leveraging LLMs for enhancing software security and providing insights into optimizing prompt engineering for vulnerability detection tasks.","2158-3927","979-8-3315-1201-9","10.1109/DSN64029.2025.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11068842","Code Vulnerability;CWE;Large Language Models;Prompt Engineering","Analytical models;Codes;Accuracy;Large language models;Software;Human in the loop;Prompt engineering;Security;Faces;Context modeling","","","","54","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"An Open-source Cross-Industry and Cloud-agnostic Generative AI Platform","A. R. Ali; K. Kumar; M. A. Siddiqui; M. Zahid","Citi Innovation Lab, London, United Kingdom; Capgemini, Tokyo, Japan; Inception Institute of AI, Abu Dhabi, UAE; Nvidia Reading, United Kingdom",2024 International Joint Conference on Neural Networks (IJCNN),"9 Sep 2024","2024","","","1","10","Generative AI (GenAI) has recently gained immense popularity within the Machine Learning (ML) field due to its potential for enabling a wide array of applications. Businesses globally are eager to leverage this transformative technology; however, they encounter several challenges in utilizing the technology, including initial and running costs, data protection, infrastructure, adaptability across use-cases, accuracy, ethical implications, and the burgeoning carbon impact of GenAI, which act as barriers to entry. A low-cost, low-carbon impact GenAI platform that exclusively uses open-source components and smaller task-specific Large Language Models (LLMs) is necessary.This paper presents a GenAI platform that employs open-source components and models orchestrated for multiple cross-industry use-cases. The platform is designed to be cloud-provider agnostic, highly secure, scalable, and requires low infrastructure to deploy, thereby using comparatively less energy and having a lower carbon impact. These features enable several advantages, including domain transference, localization in different languages using prompt tuning, and easy deployment of complex multimodal, cross-industry, cross-domain use-cases. We have tested our platform on numerous use-cases across several key patterns, including text-to-text, text-to-code, text-to-analytics and text-to-image. All these configurations provide a semantic interface to unstructured text, tabular data, images, videos, and speech. The platform, the methodology, and ideas presented in this paper will provide enterprises with a blueprint of how to orchestrate GenAI using open-source components.Our experience building a multi-modal open GenAI platform and our investigation and analyses of primary patterns in enterprise use-cases for GenAI suggest that the proposed low-cost, low-carbon, model-to-the-data platform has the potential to revolutionize the adoption of GenAI technologies within business enterprises. We believe that the proposed platform can be transformative in this space, and we anticipate its future applications with excitement.","2161-4407","979-8-3503-5931-2","10.1109/IJCNN60899.2024.10650688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10650688","Generative AI;Large Language Models;Transformers;Natural Language Processing;Text Generation;Code Generation;Image Generation;Low Carbon Emission","Adaptation models;Accuracy;Generative AI;Data security;Computational modeling;Data models;Computational efficiency","","2","","42","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs","H. Gelman; J. D. Hastings","The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA",2025 13th International Symposium on Digital Forensics and Security (ISDFS),"2 Jun 2025","2025","","","1","6","Insider threats wield an outsized influence on orga-nizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.","2768-1831","979-8-3315-0993-4","10.1109/ISDFS65363.2025.11012066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11012066","Insider Threats;Sentiment Analysis;Large Language Models (LLMs);Synthetic Data Generation;Organizational Security Policy;Job Reviews","Ethics;Sentiment analysis;Reviews;Large language models;Employment;Organizations;Intellectual property;Threat assessment;Protection;Synthetic data","","","","47","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"Performance Analysis of Prompt-Engineering Techniques for Large Language Model","M. Son; S. Lee","Department of Metabiohealth, Autonomous Driving Lab, MODULABS, Sungkyunkwan University, Republic of Korea; Department of Smart Automobile, Autonomous Driving Lab, MODULABS, Soonchunhyang University, Republic of Korea",2025 IEEE International Conference on Consumer Electronics (ICCE),"26 Mar 2025","2025","","","1","5","In this paper, we analyze the performance of three large language models (LLMs) - Llama3, Gemma2, and Mistral - using various combinations of prompt engineering techniques, focusing on the TruthfulQA and Winogrande datasets. In TruthfulQA, which emphasizes evaluating the truthfulness and cognitive errors of language models, the Chain of Thought (CoT) and Retrieval-Augmented Generation (RAG) techniques demonstrated superior performance. On the other hand, in Wino-grande, which assesses contextual reasoning abilities based on common-sense knowledge, CoT and In-Context Learning (ICL) were found to be effective. The performance analysis by model revealed that Llama3 exhibited the most significant improvement when prompt engineering techniques were applied. Meanwhile, Gemma2 achieved the highest overall performance across all evaluation metrics. This study highlights the importance of selecting appropriate prompt engineering techniques tailored to each model to maximize LLM performance, demonstrating that effective combinations of techniques can substantially enhance the models' reasoning abilities and accuracy.","2158-4001","979-8-3315-2116-5","10.1109/ICCE63647.2025.10930066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930066","Large Language Model;Prompt Engineering;In-context learning;Chain of Thought;Retrieval-Augmented Generation","Measurement;Hands;Analytical models;Data analysis;Large language models;Retrieval augmented generation;Focusing;Real-time systems;Performance analysis;Prompt engineering","","","","25","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Large Language Models for Wireless Networks: An Overview from the Prompt Engineering Perspective","H. Zhou; C. Hu; D. Yuan; Y. Yuan; D. Wu; X. Chen; H. Tabassum; X. Liu","McGill University, Canada; McGill University, Canada; McGill University, Canada; McGill University, Canada; McGill University, Canada; McGill University, Canada; York University, Canada; McGill University, Canada",IEEE Wireless Communications,"23 Jul 2025","2025","32","4","98","106","Recently, large language models (LLMs) have been successfully applied to many fields, showing outstanding comprehension and reasoning capabilities. Despite their great potential, LLMs usually require dedicated pretraining and fine-tuning for domain-specific applications such as wireless networks. These adaptations can be extremely demanding for computational resources and datasets, while most network devices have limited computation power, and there are a limited number of high-quality networking datasets. To this end, this work explores LLM-enabled wireless networks from the prompt engineering perspective, that is, designing prompts to guide LLMs to generate desired output without updating LLM parameters. Compared with other LLM-driven methods, prompt engineering can better align with the demands of wireless network devices, for example, higher deployment flexibility, rapid response time, and lower requirements on computation power. In particular, this work first introduces LLM fundamentals and compares different prompting techniques such as in-context learning, chain-of-thought, and self-refinement. Then we propose two novel prompting schemes for network applications: iterative prompting for network optimization, and self-refined prompting for network prediction. The case studies show that the proposed schemes can achieve comparable performance as conventional machine learning techniques, and our proposed prompting-based methods avoid the complexity of dedicated model training and fine-tuning, which is one of the key bottlenecks of existing machine learning techniques.","1558-0687","","10.1109/MWC.001.2400384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930397","","Prompt engineering;Wireless networks;Retrieval augmented generation;Costs;Training;Adaptation models;Computational modeling;Cognition;Planning;Knowledge based systems;Large language models","","1","","15","IEEE","17 Mar 2025","","","IEEE","IEEE Magazines"
"Large Language Models in Modern Forensic Investigations: Harnessing the Power of Generative Artificial Intelligence in Crime Resolution and Suspect Identification","A. Nikolakopoulos; S. Evangelatos; E. Veroni; K. Chasapas; N. Gousetis; A. Apostolaras; C. D. Nikolopoulos; T. Korakis","Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; Department of Electronics Engineering, Hellenic Mediterranean University, Crete, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece","2024 5th International Conference in Electronic Engineering, Information Technology & Education (EEITE)","5 Sep 2024","2024","","","1","5","Large Language Models (LLMs) have recently captured the attention of the scientific c ommunity. S ince t he global launch of LLM-based chatbots in late 2022, the field h as witnessed a rapid increase in interest from researchers, technology providers and citizens alike. With its wide-ranging applicability, Generative Artificial I ntelligence (GenAI) h as t he p otential to impact various aspects of society, from improving communication and accessibility to transforming industries such as healthcare, education and security. More specifically, in the field of Forensic Science, LLMs could offer significant b enefits as sisting Law Enforcement Agencies (LEAs) and Forensic Practitioners in crime investigations. This paper proposes the implementation of a Retrieval Augmented Generation (RAG) LLM, trained with criminology data, to provide swift and actionable insights into specific incidents, thereby enhancing Forensic Data Analysis and facilitating the daily operations of LEAs.","","979-8-3503-7287-8","10.1109/EEITE61750.2024.10654427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654427","Generative Artificial Intelligence;Large Language Models;Security;Forensics;Law Enforcement Agencies;Forensic Data Analysis;Biometric Data","Law enforcement;Forensics;Large language models;Education;Natural languages;Medical services;Performance analysis","","4","","23","EU","5 Sep 2024","","","IEEE","IEEE Conferences"
"State of Hardware Fuzzing: Current Methods and the Potential of Machine Learning and Large Language Models","K. I. Gubbi; M. Tarighat; A. Sudarshan; I. Kaur; P. D. Kota; A. Sasan; H. Homayoun","Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA",2025 26th International Symposium on Quality Electronic Design (ISQED),"30 May 2025","2025","","","1","7","Hardware fuzzing has emerged as a powerful technique for detecting security vulnerabilities and functional bugs in modern hardware systems. Unlike traditional verification approaches that rely on predefined testbenches and formal proofs, hardware fuzzing generates and mutates inputs dynamically to uncover unexpected behaviors. Despite its effectiveness, hardware fuzzing faces challenges such as test case explosion, coverage limitations, and debugging complexity. Recent advancements in Machine Learning (ML) and Large Language Models (LLMs) offer new opportunities to enhance hardware fuzzing by improving test case generation, optimizing coverage feedback, and automating debugging processes. This paper provides a comprehensive survey of the current state of hardware fuzzing, highlighting its methodologies, applications, and limitations. Furthermore, we explore the potential of ML and LLMs in augmenting fuzzing workflows and discuss key challenges that must be addressed for broader adoption. By synthesizing insights from existing research and industry practices, we outline future research directions that can bridge the gap between automated hardware fuzzing and intelligent, adaptive testing frameworks.","1948-3295","979-8-3315-0942-2","10.1109/ISQED65160.2025.11014308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11014308","Hardware Fuzzing;Machine Learning;Large Language Models;Hardware Security Verification","Training;Large language models;Hardware security;Computational modeling;Computer bugs;Machine learning;Debugging;Fuzzing;Hardware;Test pattern generators","","","","8","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"A Novel Framework for Malware Analysis: Integrating Large Language Models with Heuristic Techniques","B. Lee; C. Easttom","Computer Science Department, Vanderbilt University, Nashville, TN, United States of America; Computer Science Department, Vanderbilt University",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00516","00524","The current study examines the use of large language models, LLMs, for heuristic malware analysis. The novelty of the current approach lies in a two-phase approach that involves the segmentation and labeling of source code using LLMs, followed by the threat analysis of individual functions and methods. The methodology is demonstrated by injecting malicious code into an open-source Android application and analyzing the results using the proposed LLM-based detection system. Findings indicate that LLM-powered detection systems can effectively identify suspicious code patterns and provide threat assessments without specific malware detection training, successfully detecting overtly malicious code and subtle, potentially compromised functions. The solution highlights and addresses LLM limitations and explores their ability to generate malware variants, raising questions about their dual-use nature in software security.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903801","Large Language Models;Malware;Heuristic Analysis;Software Security;Generative Pretrained Transformers","Training;Adaptation models;Large language models;Computational modeling;Source coding;Organizations;Transformers;Malware;Threat assessment;Computer security","","","","20","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Secure Software Architecture for Enterprise Generative Artificial Intelligence","H. Joshi","Enterprise Architect, San Francisco, California, USA",2024 IEEE 19th International Conference on Computer Science and Information Technologies (CSIT),"8 May 2025","2024","","","1","5","This paper examines the transformative impact of Generative Artificial Intelligence (GAI) on Enterprise Business Applications. It explores GAI's potential to revolutionize core business functions while addressing adoption challenges in enterprise environments. The research proposes a novel architecture design pattern tailored for enterprise use cases, aiming to balance GAI's capabilities with stringent security and privacy requirements for sensitive data. The study illuminates GAI's capacity to enhance decisionmaking, elevate operational efficiency, and redefine stakeholder engagement across marketing, sales, and finance operations. It further illustrates a technical design prototype of a practical use case for business operations emphasizing increased productivity and reduced manual tasks in product marketing business processes. By offering a nuanced understanding of GAI's transformative potential in enterprise applications, this research aims to guide organizations in leveraging this technology responsibly and effectively. The paper concludes by emphasizing the importance of maintaining high standards of data security and ethical practice while remaining at the forefront of technological innovation in the enterprise sector.","2766-3639","979-8-3315-4263-4","10.1109/CSIT65290.2024.10982676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10982676","Artificial Intelligence;Security;Enterprise Architecture;Design Patterns;Software Engineering;Large Language Models;Integration;Business Process","Ethics;Technological innovation;Protocols;Generative AI;Software architecture;Navigation;Decision making;Organizations;Computer architecture;Stakeholders","","","","8","IEEE","8 May 2025","","","IEEE","IEEE Conferences"
"Are Large Language Models Memorizing Bug Benchmarks?","D. Ramos; C. Mamede; K. Jain; P. Canelas; C. Gamboa; C. Le Goues","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","1","8","Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage.In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and 5-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028362","Automated Program Repair;Large Language Model;Data Leakage","Training;Measurement;Codes;Accuracy;Large language models;Computer bugs;Benchmark testing;Maintenance engineering;Data models;Software engineering","","1","","40","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Knowledge Graph Based Repository-Level Code Generation","M. Athale; V. Vaddina","Computer Science, Northeastern University, Boston, MA, USA; Applied Research, Quantiphi Inc., Toronto, ON, Canada",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","169","176","Recent advancements in Large Language Models (LLMs) have transformed code generation from natural language queries. However, despite their extensive knowledge and ability to produce high-quality code, LLMs often struggle with contextual accuracy, particularly in evolving codebases. Current code search and retrieval methods frequently lack robustness in both the quality and contextual relevance of retrieved results, leading to suboptimal code generation. This paper introduces a novel knowledge graph-based approach to improve code search and retrieval leading to better quality of code generation in the context of repository-level tasks. The proposed approach represents code repositories as graphs, capturing structural and relational information for enhanced context-aware code generation. Our framework employs a hybrid approach for code retrieval to improve contextual relevance, track inter-file modular dependencies, generate more robust code and ensure consistency with the existing codebase. We benchmark the proposed approach on the Evolutionary Code Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark, and demonstrate that our method significantly outperforms the baseline approach. These findings suggest that knowledge graph based code generation could advance robust, context-sensitive coding assistance tools.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028191","Code Retrieval;Code Generation;Knowledge Graphs;Large Language Models","Codes;Accuracy;Large language models;Conferences;Natural languages;Knowledge graphs;Benchmark testing;Robustness;Hybrid power systems;Encoding","","","","27","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"CHEMFUZZ: Large Language Models-Assisted Fuzzing for Quantum Chemistry Software Bug Detection","F. Qiu; P. Ji; B. Hua; Y. Wang","School of Software Engineering, University of Science and Technology of China, China; Computer Science Department, The Johns Hopkins University, USA; School of Software Engineering, University of Science and Technology of China, China; School of Software Engineering, University of Science and Technology of China, China","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","19 Feb 2024","2023","","","103","112","Quantum chemistry software implements the first principle quantum computation and is indispensable in both scientific research and chemical industries. Any bugs in such software will lead to serious consequences, thus defeating its trustworthiness and reliability. However, bug detection techniques for such software have not been fully investigated. In this paper, to fill this gap, we propose a novel approach to fuzz quantum chemistry software with the aid of Large Language Models (LLMs). Our basic idea is utilize LLMs to mutate and generate syntactic and semantic valid input files from seed inputs, by proving valuable domain-specific knowledge of chemistry. With this basic idea, we have designed and implemented CHEMFuzz, a fully automatic fuzzing framework to fuzz quantum chemistry software for bugs. Our evaluation of CHEMFUZZ leverages popular LLMs including GPT3.5, Claude-2, and Bart as test oracles to generate parameters to mutate inputs and analyze computation results. CHEMFUZZ detected 40 unique bugs, which have been classified and reported to developers, with a code coverage of 17.4%.","2693-9371","979-8-3503-5939-8","10.1109/QRS-C60940.2023.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430007","Quantum Chemistry Software;Fuzzing;Large Language Models;Security Test","Quantum chemistry;Computational modeling;Computer bugs;Fuzzing;Syntactics;Software;Software reliability","","1","","51","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"Benchmarking and Evaluating Large Language Models in Phishing Detection for Small and Midsize Enterprises: A Comprehensive Analysis","J. Zhang; P. Wu; J. London; D. Tenney","Department of Technology Management, School of Engineering, University of Bridgeport, Bridgeport, CT, USA; Department of Biomedical Engineering, School of Engineering, University of Bridgeport, Bridgeport, CT, USA; Department of Technology Management, School of Engineering, University of Bridgeport, Bridgeport, CT, USA; Department of Technology Management, School of Engineering, University of Bridgeport, Bridgeport, CT, USA",IEEE Access,"18 Feb 2025","2025","13","","28335","28352","The proliferation of Generative Artificial Intelligence (GenAI) has driven significant innovation but also introduced new security risks, particularly in social engineering attacks such as phishing. Despite the potential for misuse of GenAI in such attacks, research on its use for defense against these threats is limited. To address this issue, this study addresses the unique cybersecurity needs of small and midsize enterprises (SMEs) by utilizing high-quality email datasets, categorized into human and AI-generated phishing and legitimate emails, to evaluate the effectiveness of different GenAI models. The results demonstrated that the open-source Llama-3-8b-instruct model outperformed other alternatives, achieving the highest accuracy and F1-score, while offering a cost-effective and flexible solution for SMEs. This approach lies in its focus on base models with default parameters, emphasizing ease of implementation without the need for additional training or fine-tuning. Compared to existing methods, this approach simplifies adoption while maintaining robust detection capabilities. The proposed prompt template enables LLMs to provide explanations and suggestions, assisting users in making informed decisions. Practical recommendations for SMEs include deployment strategies and cost-effective management of human factors in cybersecurity. However, the study acknowledges limitations in its reliance on base models and emphasizes the need for further research on fine-tuning and parameter optimization. These findings suggest the feasibility of using LLMs for real-world cybersecurity applications against phishing attacks in the context of SMEs, although certain risks and challenges remain.","2169-3536","","10.1109/ACCESS.2025.3540075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10878987","Large language models (LLMs);generative artificial intelligence (AI);small-to-medium enterprise;cybersecurity;technology management;social engineering;phishing detection","Phishing;Electronic mail;Data models;Large language models;Computer security;Security;Accuracy;Unsolicited e-mail;Benchmark testing;Analytical models","","1","","156","CCBY","10 Feb 2025","","","IEEE","IEEE Journals"
"Large Language Models to Enhance Malware Detection in Edge Computing","C. Rondanini; B. Carminati; E. Ferrari; A. Kundu; A. Jajoo","DiSTA, University of Insubria, Italy; DiSTA, University of Insubria, Italy; DiSTA, University of Insubria, Italy; Cisco Research, USA; Cisco Research, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","1","10","The increasing sophistication of malware in recent years has underscored the need for more advanced detection approaches, particularly in edge computing environments. Traditional methods and machine learning models, have shown promise but face significant limitations in handling evolving malware types. To address these challenges, we explore the application of Large Language Models (LLMs) for malware detection in edge computing. LLMs offer enhanced pattern recognition, contextual understanding, and the ability to analyze complex data, making them well-suited for dynamic, resource-constrained edge environments. However, their use introduces challenges such as model size and computational demands. In this paper, we propose a vision architecture that leverages LLMs’ strengths while addressing their limitations, providing a framework for continuous learning and adaptability. We discuss the architecture’s potential impact on improving malware detection as well as future research directions in the field.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835694","Malware;Security;Edge;Large Language Models","Data privacy;Image edge detection;Computational modeling;Large language models;Training data;Computer architecture;Pressing;Malware;Security;Edge computing","","","","52","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"A Systematic Literature Review on Large Language Models Applications in Computer Programming Teaching Evaluation Process","A. F. Pereira; R. Ferreira Mello","Centro de Estudos e Sistemas Avançados do Recife, Recife, Pernambuco, Brazil; Centro de Estudos e Sistemas Avançados do Recife, Recife, Pernambuco, Brazil",IEEE Access,"4 Jul 2025","2025","13","","113449","113460","Tools based on the use of Large Language Models (LLMs) have improved the computer programming teaching process, automated feedback processes, facilitated program repair, and enabled personalized learning experiences. This research examines which and how LLM-based opportunities are applied in the computer programming teaching assessment process and how LLMs are applied to improve evaluation accuracy, their impact on student learning outcomes, and the challenges in scaling these technologies. Key opportunities arise from prompt engineering, which optimizes precision and LLM-generated feedback relevance, and feedback propagation techniques, which offer scalable solutions for large-scale programming courses. LLMs are also applied effectively in debugging assistance to detect and repair syntactic and semantic errors in student code. This review identifies several research directions, including prompt engineering refinement, improved feedback system scalability, and deeper exploration of the long-term educational impacts of LLM. The study concludes that LLMs are effective in enhancing the assessment process, but a balanced approach combining human oversight with automated feedback is crucial to fostering critical thinking and ensuring long-term learning success in programming education.","2169-3536","","10.1109/ACCESS.2025.3584060","Centro de Estudos e Sistemas Avançados do Recife; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11058691","Programming education;assessment processes;large language models;adaptive learning","Education;Programming profession;Maintenance engineering;Debugging;Databases;Codes;Scalability;Real-time systems;Large language models;Accuracy","","","","33","CCBY","30 Jun 2025","","","IEEE","IEEE Journals"
"Challenges and Opportunities for Survey Research in the Age of Generative AI: An Experience Report","F. N. Meem; J. Smith; B. Johnson","Department of Computer Science, George Mason University, Virginia, USA; Department of Computer Science, Lafayette College, Pennsylvania, USA; Department of Computer Science, George Mason University, Virginia, USA",2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC),"16 Oct 2024","2024","","","423","428","Survey research, while common, has known challenges and limitations. In this paper, we discuss the challenges and concerns of conducting research using online survey in the age of widespread development and use of generative AI technologies. We base our discussion on our recent experiences conducting survey research to better understand software practitioners’ knowledge of and experience with automated program repair (APR). In our efforts, we encountered both anticipated and unexpected challenges, much of which stemmed from advancements in AI-assisted automation (e.g., generative AI). For example, we found that many of the open ended responses provided were likely generated by AI. Based on our experiences, we outline how we mitigated or handled these challenges and discuss opportunities for improving survey design and administration to ensure high quality research and outcomes in the age of advanced and widely available AI technologies.","1943-6106","979-8-3503-6613-6","10.1109/VL/HCC60511.2024.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10714567","human survey;online survey;generative AI;survey validation;automated program repair;AI-generated data;AI-generated responses","Surveys;Visualization;Automation;Generative AI;Maintenance engineering;Software","","1","","15","IEEE","16 Oct 2024","","","IEEE","IEEE Conferences"
"Optimizing Human-AI Interaction: Innovations in Prompt Engineering","R. Deshmukh; R. Raut; M. Bhavsar; S. Gurav; Y. Patil","Computer Department, JSPM's Rajarshi Shahu College of Engineering, Pune, India; Computer Department, JSPM's Rajarshi Shahu College of Engineering, Pune, India; Computer Department, JSPM's Rajarshi Shahu College of Engineering, Pune, India; Computer Department, JSPM's Rajarshi Shahu College of Engineering, Pune, India; Computer Department, JSPM's Rajarshi Shahu College of Engineering, Pune, India",2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),"13 Mar 2025","2025","","","1240","1246","In the realm of artificial intelligence., effective communication between humans and large language models (LLMs) such as ChatGPT is vital. This research paper proposes a structured methodology for prompt generation., developed through an in-depth analysis of existing research studies., to optimize interactions with LLMs. The exploration in the crucial role of prompt engineering in enhancing the quality and relevance of AI-generated outputs is also made. By conducting a comprehensive review of the literature., the aim is to identify best practices for prompt design., emphasizing the importance of clarity., specificity., and contextualization. Our findings indicate that tailored prompts can significantly improve accuracy across various applications., including health care., education., and customer service. Furthermore., the challenges faced are address the challenges of maintaining consistency and reliability in AI responses., highlighting the necessity for standardized guidelines in prompt engineering. This work underscores the transformative potential of prompt engineering., advocating for its integration in high-stakes environments to promote effective and ethical communication with AI systems.","","979-8-3315-2754-9","10.1109/IDCIOT64235.2025.10914815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10914815","prompt engineering;natural language processing (NLP);ChatGPT;prompt design;prompt optimization;prompt engineering best practices;ethical prompt engineering","Ethics;Technological innovation;Reviews;Large language models;Education;Medical services;Chatbots;Reliability engineering;Prompt engineering;Best practices","","1","","21","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Automated Testing of the GUI of a Real-Life Engineering Software using Large Language Models","T. Rosenbach; D. Heidrich; A. Weinert","German Aerospace Center (DLR), Institute of Software Technology, Cologne, Germany; German Aerospace Center (DLR), Institute of Software Technology, Germany; German Aerospace Center (DLR), Institute of Software Technology, Cologne, Germany","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","103","110","One important step in software development is testing the finished product with actual users. These tests aim, among other goals, at determining unintuitive behavior of the software as it is presented to the end-user. Moreover, they aim to determine inconsistencies in the user-facing interface. They provide valuable feedback for the development of the software, but are time-intensive to conduct.In this work, we present GERALLT, a system that uses Large Language Models (LLMs) to perform exploratory tests of the Graphical User Interface (GUI) of a real-life engineering software. GERALLT automatically generates a list of potential unintuitive and inconsistent parts of the interface. We present the architecture of GERALLT and evaluate it on a real-world use case of the engineering software, which has been extensively tested by developers and users. Our results show that GERALLT is able to determine issues with the interface that support the software development team in future development of the software.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962502","Software Testing;Graphical User Interface;Acceptance Testing;Large Language Models","Software testing;Large language models;Conferences;Computer architecture;Software;Graphical user interfaces;Software development management","","","","32","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements","S. M. Abtahi; A. Azim","Faculty of Engineering and Applied Science, Ontario Tech University; Faculty of Engineering and Applied Science, Ontario Tech University",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","82","92","This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI’s GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations—where the model generates plausible but incorrect outputs—is addressed by a custom-built ""Code Comparison App,"" which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052822","LLMs;Static Code Analysis;Issue Detection;GPT-3.5 Turbo;GPT-4o;Prompt Engineering;RAG;Code Comparison;Software Development Automation","Codes;Costs;Automation;Large language models;Retrieval augmented generation;Software quality;Static analysis;Real-time systems;Prompt engineering;Software development management","","","","36","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"CoDocBench: A Dataset for Code-Documentation Alignment in Software Maintenance","K. Pai; P. Devanbu; T. Ahmed","University of California, Davis, USA; University of California, Davis, USA; University of California, Davis, USA",2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","451","455","One of the central tasks in software maintenance is being able to understand and develop code changes. Thus, given a natural language description of the desired new operation of a function, an agent (human or AI) might be asked to generate the set of edits to that function to implement the desired new operation; likewise, given a set of edits to a function, an agent might be asked to generate a changed description, of that function’s new workings. Thus, there is an incentive to train a neural model for change-related tasks. Motivated by this, we offer a new, “natural”, large dataset of coupled changes to code and documentation mined from actual high-quality GitHub projects, where each sample represents a single commit where the code and the associated docstring were changed together. We present the methodology for gathering the dataset, and some sample, challenging (but realistic) tasks where our dataset provides opportunities for both learning and evaluation. We find that current models (specifically Llama-3.1 405B, Mixtral $8 \times 22 \mathrm{~B}$) do find these maintenance-related tasks challenging.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00077","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025763","llms;code generation;docstring generation","Software maintenance;Codes;Source coding;Natural languages;Documentation;Data mining;Artificial intelligence;Software development management","","","","18","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Kubernetes for Generative AI Solutions: A complete guide to designing, optimizing, and deploying Generative AI workloads on Kubernetes","A. Srirama; S. Gupta; R. Saha",NA; NA; NA,"Kubernetes for Generative AI Solutions: A complete guide to designing, optimizing, and deploying Generative AI workloads on Kubernetes","","2025","","","","","Master the complete Generative AI project lifecycle on Kubernetes (K8s) from design and optimization to deployment using best practices, cost-effective strategies, and real-world examples.Key FeaturesBuild and deploy your first Generative AI workload on Kubernetes with confidenceLearn to optimize costly resources such as GPUs using fractional allocation, Spot Instances, and automationGain hands-on insights into observability, infrastructure automation, and scaling Generative AI workloadsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI (GenAI) is revolutionizing industries, from chatbots to recommendation engines to content creation, but deploying these systems at scale poses significant challenges in infrastructure, scalability, security, and cost management. This book is your practical guide to designing, optimizing, and deploying GenAI workloads with Kubernetes (K8s) the leading container orchestration platform trusted by AI pioneers. Whether you're working with large language models, transformer systems, or other GenAI applications, this book helps you confidently take projects from concept to production. You’ll get to grips with foundational concepts in machine learning and GenAI, understanding how to align projects with business goals and KPIs. From there, you'll set up Kubernetes clusters in the cloud, deploy your first workload, and build a solid infrastructure. But your learning doesn't stop at deployment. The chapters highlight essential strategies for scaling GenAI workloads in production, covering model optimization, workflow automation, scaling, GPU efficiency, observability, security, and resilience. By the end of this book, you’ll be fully equipped to confidently design and deploy scalable, secure, resilient, and cost-effective GenAI solutions on Kubernetes.What you will learnExplore GenAI deployment stack, agents, RAG, and model fine-tuningImplement HPA, VPA, and Karpenter for efficient autoscalingOptimize GPU usage with fractional allocation, MIG, and MPS setupsReduce cloud costs and monitor spending with Kubecost toolsSecure GenAI workloads with RBAC, encryption, and service meshesMonitor system health and performance using Prometheus and GrafanaEnsure high availability and disaster recovery for GenAI systemsAutomate GenAI pipelines for continuous integration and deliveryWho this book is forThis book is for solutions architects, product managers, engineering leads, DevOps teams, GenAI developers, and AI engineers. It's also suitable for students and academics learning about GenAI, Kubernetes, and cloud-native technologies. A basic understanding of cloud computing and AI concepts is needed, but no prior knowledge of Kubernetes is required.","","9781836209928","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11099034.pdf&bkn=11099033&pdfType=book","","","","","","","","29 Jul 2025","","","Packt Publishing","Packt Publishing eBooks"
"Automated Consistency Analysis of LLMs","A. Patwardhan; V. Vaidya; A. Kundu","Department of Computer Science, Stony Brook University, Stony Brook, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Cisco Research, San Jose, USA","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","16 Jan 2025","2024","","","118","127","Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses?In this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity.","","979-8-3503-8674-5","10.1109/TPS-ISA62245.2024.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835426","Cybersecurity;Generative AI;Large Language Models;Agents;Consistency;Trustworthiness;Validity;Reliability;Hallucination","Industries;Privacy;Analytical models;Generative AI;Large language models;Government;Benchmark testing;Reliability;Intelligent systems;Computer crime","","1","","39","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"A Comprehensive Overview of Backdoor Attacks in Large Language Models Within Communication Networks","H. Yang; K. Xiang; M. Ge; H. Li; R. Lu; S. Yu","School of Computer Science and Engineering and the School of Cyber Security, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering and the School of Cyber Security, University of Electronic Science and Technology of China, Chengdu, China; RAN and Computing Power Systems Department, ZTE Corporation, Shenzhen, Guangdong, China; School of Computer Science and Engineering and the School of Cyber Security, University of Electronic Science and Technology of China, Chengdu, China; Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia",IEEE Network,"20 Nov 2024","2024","38","6","211","218","The Large Language Models (LLMs) are poised to offer efficient and intelligent services for future mobile communication networks, owing to their exceptional capabilities in language comprehension and generation. However, the extremely high data and computational resource requirements for the performance of LLMs compel developers to resort to outsourcing training or utilizing third-party data and computing resources. These strategies may expose the model within the network to maliciously manipulated training data and processing, providing an opportunity for attackers to embed a hidden backdoor into the model, termed a backdoor attack. Backdoor attack in LLMs refers to embedding a hidden backdoor in LLMs that causes the model to perform normally on benign samples but exhibit degraded performance on poisoned ones. This issue is particularly concerning within communication networks where reliability and security are paramount. Despite the extensive research on backdoor attacks, there remains a lack of in-depth exploration specifically within the context of LLMs employed in communication networks, and a systematic review of such attacks is currently absent. In this survey, we systematically propose a taxonomy of backdoor attacks in LLMs as used in communication networks, dividing them into four major categories: input-triggered, prompt-triggered, instruction-triggered, and demonstration-triggered attacks. Furthermore, we conduct a comprehensive analysis of the benchmark datasets. Finally, we identify potential problems and open challenges, offering valuable insights into future research directions for enhancing the security and integrity of LLMs in communication networks.","1558-156X","","10.1109/MNET.2024.3367788","National Natural Science Foundation of China(grant numbers:62072078,62072081,U2023212); National Key Research and Development Program of China(grant numbers:2022YFB4501200); Sichuan Science and Technology Program(grant numbers:2023ZYD0142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440367","Backdoor attacks;Large Language Models","Training;Computational modeling;Data models;Predictive models;Training data;Security;Solid modeling;Large language models;Mobile communication;Communication networks","","25","","15","IEEE","20 Feb 2024","","","IEEE","IEEE Magazines"
"How Far Have We Gone in Binary Code Understanding Using Large Language Models","X. Shang; S. Cheng; G. Chen; Y. Zhang; L. Hu; X. Yu; G. Li; W. Zhang; N. Yu","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","1","12","Binary code analysis plays a pivotal role in various software security applications, such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, understanding binary code is challenging for reverse engineers due to the absence of semantic information. Therefore, automated tools are needed to assist human players in interpreting binary code. In recent years, two groups of technologies have shown promising prospects: (1) Deep learning-based technologies have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This makes participants wonder about the ability of LLMs in binary code understanding. In this work, we propose a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios. The benchmark covers two key binary code understanding tasks, including function name recovery and binary code summarization. We gain valuable insights into their capabilities and limitations through extensive evaluations of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00012","Natural Science Foundation of China(grant numbers:U20B2047,62072421,62002334,62102386,62121002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795058","Reverse Engineering;Binary Code Understanding;Program Comprehension;Large Language Models","Software maintenance;Large language models;Source coding;Semantics;Reverse engineering;Binary codes;Benchmark testing;Malware;Security;Software engineering","","3","","67","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Improving BPMN XOR Gateway Labels through Dynamic Prompt Engineering","S. Ayad; F. Alsayoud","Computer Science department, Arab Open university, KSA; Computer Science department, Arab Open university, KSA",2024 International Conference on Microelectronics (ICM),"31 Dec 2024","2024","","","1","6","Business Process Modeling plays a critical role in improving operational efficiency, consistency, and transparency within organizations. Despite the significant focus on syntactic and semantic aspects, pragmatic quality, the clarity and comprehensibility of BP models, remains underexplored. This paper addresses this gap by proposing a Large Language Models approach to enhance the naming of XOR split outgoing edges in BPMN models. The proposed method automates the generation of clear, meaningful, and contextually appropriate edge labels. These labels are evaluated using the all-MiniLM-L6-v2 model, a pre-trained sentence transformer that maps sentences into a dense vector space, enabling semantic search and clustering. Our results show that this LLM-based approach significantly improves the pragmatic quality of BPMN models, enhancing both model clarity and decision-making efficiency.","2159-1679","979-8-3503-7939-6","10.1109/ICM63406.2024.10815778","Arab Open University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10815778","Large language models;Business process models;Pragmatic quality;Exclusive Split gateways;Natural Language Inference","Semantic search;Process modeling;Decision making;Organizations;Logic gates;Syntactics;Transformers;Vectors;Prompt engineering;Pragmatics","","","","16","IEEE","31 Dec 2024","","","IEEE","IEEE Conferences"
"Problematic Tokens: Tokenizer Bias in Large Language Models","J. Yang; Z. Wang; Y. Lin; Z. Zhao","Syracuse University, Syracuse, USA; Florida Atlantic University, Boca Raton, USA; Florida Atlantic University, Boca Raton, USA; New York University, NYC, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","6387","6393","Recent advancements in large language models (LLMs), such as GPT-4 and GPT-4o, have shown exceptional performance, especially in languages with abundant resources like English, thanks to extensive datasets that ensure robust training. Conversely, these models exhibit limitations when processing under-resourced languages such as Chinese and Korean, where issues including hallucinatory responses remain prevalent. This paper traces the roots of these disparities to the tokenization process inherent to these models. Specifically, it explores how the tokenizer’s vocabulary, often used to speed up the tokenization process and reduce tokens but constructed independently of the actual model training data, inadequately represents non-English languages. This misrepresentation results in the propagation of ‘under-trained’ or ‘untrained’ tokens, which perpetuate biases and pose serious concerns related to data security and ethical standards. We aim to dissect the tokenization mechanics of GPT-4o, illustrating how its simplified token-handling methods amplify these risks and offer strategic solutions to mitigate associated security and ethical issues. Through this study, we emphasize the critical need to rethink tokenization frameworks to foster more equitable and secure AI technologies. The code and data are available at: https://github.com/yeyimilk/LLMGPT4o","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825615","Large language models;GPT-4;GPT-4o;tokenizer bias;bias mitigation;data security;privacy","Training;Ethics;Vocabulary;Translation;Large language models;Training data;Linguistics;Tokenization;Data models;Standards","","2","","13","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Are Large Language Models Table-based Fact-Checkers?","H. Zhang; Q. Si; P. Fu; Z. Lin; W. Wang","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD),"10 Jul 2024","2024","","","3086","3091","Table-based Fact Verification (TFV) aims to extract the entailment relationship between statements and structured tables. Existing TFV methods based on small-scale models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown strong zero-shot and in-context learning capabilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study on whether LLMs are table-based fact-checkers. In detail, we design various prompts to explore how in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to investigate the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which will benefit further research on table reasoning.","2768-1904","979-8-3503-4918-4","10.1109/CSCWD61410.2024.10580146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10580146","Table-based Fact Verification;Large Language Models;In-context Learning;Instruction Tuning","Federated learning;Large language models;Computational modeling;Performance gain;Chatbots;Data models;Cognition","","1","","27","IEEE","10 Jul 2024","","","IEEE","IEEE Conferences"
"Advancing Secure and Standard Source Code Generation Techniques","M. L. Siddiq","University of Notre Dame, Notre Dame, IN, USA",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","53","57","The rise of ChatGPT and GitHub Copilot has sparked a surge in developers leveraging large language models (LLMs) for code generation, aiming to automate software development processes. However, these tools can generate substandard and vulnerable code. Notably, a significant portion of developers in the US embrace LLMs due to productivity boost. However, research indicates that LLM-generated code may compromise security, with users often overestimating its reliability. To address these challenges, this proposal aims to enhance the quality and security of generated code in outputs. The proposal includes an empirical study of code generation models' training sets and benchmarks for code and security smells. It also consists of a framework, SALLM, to automatically benchmark code generation models from the security perspective. This proposal is a work in progress in creating quality datasets to reinforce the code generation model and generate standard and secure code. By establishing trust in LLM-based tools and generating secure and standard code, developers can confidently integrate them into their workflows and rely on them.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024325","code generation;quality;reinforced learning;secure code","Training;Codes;Biological system modeling;Source coding;Benchmark testing;Security;Proposals;Surges;Standards;Software development management","","","","40","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Generation of Robot Manipulation Plans Using Generative Large Language Models","J. -P. Töberg; P. Cimiano","Center for Cognitive Interaction Technology (CITEC) and Joint Research Center on Cooperative and Cognition-enabled AI (CoAI JRC), Bielefeld University, Bielefeld, Germany; Center for Cognitive Interaction Technology (CITEC) and Joint Research Center on Cooperative and Cognition-enabled AI (CoAI JRC), Bielefeld University, Bielefeld, Germany",2023 Seventh IEEE International Conference on Robotic Computing (IRC),"25 Mar 2024","2023","","","190","197","Designing plans that allow robots to carry out actions such as grasping an object or cutting a fruit is a timeconsuming activity requiring specific skills and knowledge. The recent success of Generative Large Language Models (LLMs) has opened new avenues for code generation. In order to evaluate the ability of LLMs to generate code representing manipulation plans, we carry out experiments with different LLMs in the CRAM framework. In our experimental framework, we ask an LLM such as ChatGPT or GPT-4 to generate a plan for a specific target action given the plan (called designator within CRAM) for a given reference action as an example. We evaluate the generated designators against a ground truth designator using machine translation and code generation metrics, as well as assessing whether they compile. We find that GPT-4 slightly outperforms ChatGPT, but both models achieve a solid performance above all evaluated metrics. However, only ~36 % of the generated designators compile successfully. In addition, we assess how the chosen reference action influences the code generation quality as well as the compilation success. Unexpectedly, the action similarity negatively correlates with compilation success. With respect to the metrics, we obtain either a positive or negative correlation depending on the used model. Finally, we describe our attempt to use ChatGPT in an interactive fashion to incrementally refine the initially generated designator. On the basis of our observations we conclude that the behaviour of ChatGPT is not reliable and robust enough to support the incremental refinement of a designator.","","979-8-3503-9574-7","10.1109/IRC59093.2023.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473591","Robot Plan Generation;Large Language Models;Action Similarity;CRAM;GPT","Measurement;Solid modeling;Codes;Correlation;Grasping;Chatbots;Solids","","2","","25","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"LLM-IMC: Automating Analog In-Memory Computing Architecture Generation with Large Language Models","D. Vungarala; M. H. Amin; P. Mercati; A. Roohi; R. Zand; S. Angizi","New Jersey Institute of Technology, Newark, NJ, USA; University of South Carolina, Columbia, SC, USA; Intel Labs, Hillsboro, OR, USA; University of Illinois Chicago, Chicago, IL, USA; University of South Carolina, Columbia, SC, USA; New Jersey Institute of Technology, Newark, NJ, USA",2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),"28 May 2025","2025","","","292","292","Resistive crossbars enabling analog In-Memory Computing (IMC) have garnered significant attention from academia and industry as a promising architecture for Deep Neural Network (DNN) acceleration, thanks to their high memory access bandwidth and in-situ computing capabilities. However, the knowledge-intensive hardware design process and the lack of high-quality circuit netlists have constrained design space exploration and optimization of analog IMC to behavioral system-level tools. In this one-page abstract, we introduce LLM-IMC, a novel fine-tune-free Large Language Model (LLM) framework, supported by a Python-based tool, designed for analog IMC SPICE code generation. LLM-IMC systematically addresses these limitations by automating the creation of diverse IMC simulation scripts, enabling efficient design space exploration through LLM-driven performance, and outlining an integration roadmap for hardware-oriented neuromorphic crossbar design flows.","2576-2621","979-8-3315-0281-2","10.1109/FCCM62733.2025.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008939","in-memory computing;resistive crossbars;large language model;spice code generation","Industries;Codes;Neuromorphics;Large language models;Computer architecture;Artificial neural networks;In-memory computing;SPICE;Space exploration;Optimization","","","","4","IEEE","28 May 2025","","","IEEE","IEEE Conferences"
"Model Selection for HERITAGE-AI: Evaluating LLMs for Contextual Data Analysis of Maryland’s Domestic Traffic Ads (1824–1864)","R. K. Gnanasekaran; L. Perine; M. Conrad; R. Marciano","College of Information, University of Maryland, College Park, USA; College of Information, University of Maryland, College Park, USA; College of Information, University of Maryland, College Park, USA; College of Information, University of Maryland, College Park, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2419","2430","The HERITAGE-AI (Harnessing Enhanced Research and Instructional Technologies for Archival Generative Exploration using AI), as part of the IMLS grant initiative, GenAI-4-Archive, aims to analyze sensitive historical datasets ethically using advanced AI technologies. One of the key tasks of this project focuses on selecting the most suitable Large Language Model (LLM) for analyzing the Domestic Traffic Ads (DTA) published in Maryland between 1824 and 1864 by slave traders—a dataset rich in historical significance yet fraught with ethical considerations. Analyzing sensitive historical datasets presents unique ethical and technical challenges. This paper presents a comparative evaluation of leading LLMs to identify the optimal model to meet HERITAGE-AI’s objectives. We survey contemporary models, including OpenAI’s GPT-4o, Anthropic’s Claude Sonnet, Meta’s Llama 3.2, and Google’s Gemini, to identify the most suitable model for Generative AI-based analysis of the DTA dataset. The objective is to select an LLM that can handle the sensitive nature of the data responsibly while providing accurate and insightful analysis. Three critical evaluation criteria, among others, are established for this reason: Sensitivity to Historical Context, Privacy and Security, and Customizability. Our analysis follows a three-step approach: evaluating free versions, paid versions, and enterprise-grade cloud-based implementations of these LLMs. Our findings reveal that while free and paid versions offer varying degrees of accessibility, they fall short in providing the necessary privacy, security, multi-user access, and customization required for analyzing sensitive historical data like the DTA dataset. In the third step, by comparing the cloud-based implementations of Azure OpenAI’s GPT-4o, AWS Bedrock’s Claude, and AWS Bedrock’s Llama3.2 LLMs, Azure openAI GPT-4o emerges as the most suitable option for this project. Although GPT-4o and Claude were close contenders, Gpt-4o demonstrated robust mechanisms due to its high accuracy, ethical sensitivity, robust privacy controls, and scalability in a cloud-based environment. It also offers extensive customizability, allowing for effective integration of the DTA dataset and alignment with the project’s ethical standards. Future work will involve domain experts and community members in implementing Azure OpenAI GPT-4o for the DTA dataset analysis.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825591","Institute of Museum and Library Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825591","Large Language Models (LLMs);HERITAGE-AI;Sensitive Historical Data Analysis;Ethical AI;Privacy and Security","Ethics;Privacy;Analytical models;Data privacy;Accuracy;Sensitivity;Data analysis;Scalability;Large language models;Security","","","","25","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Enhancing Automated Grading with Capabilities of LLMs: Using Prompt Engineering and RAG Techniques","T. Abeywardana; N. Nandadewa; V. Wickramasinghe; S. Rohanadheera; T. Nadungodage; P. Hewagamage","School of Computing, University of Colombo, Colombo, Sri Lanka; School of Computing, University of Colombo, Colombo, Sri Lanka; School of Computing, University of Colombo, Colombo, Sri Lanka; School of Computing, University of Colombo, Colombo, Sri Lanka; School of Computing, University of Colombo, Colombo, Sri Lanka; School of Computing, University of Colombo, Colombo, Sri Lanka",2025 5th International Conference on Advanced Research in Computing (ICARC),"16 Apr 2025","2025","","","1","6","This research explores the potential of Large Language Models (LLMs) to automate the grading process in education by harnessing LLMs’ sophisticated understanding of language and instructions following nature. We explore the effectiveness of providing subject knowledge and utilizing prompt engineering techniques to grade the students’ written answers for different question types and various theoretical subjects. A grading rubric was employed to ensure consistency and fairness in the assessment process. The study results highlighted the importance of providing external knowledge within the prompt to enhance the automated student answer grading utilizing LLMs. Including grading rubrics, model answers, and course content significantly enhanced the accuracy of scores assigned by the LLM, reducing deviations from human evaluator scores. Providing course content or model answers also helped define the expected answer scope and guide the LLM in determining other possible correct answers. Using prompt engineering techniques within the prompt failed to outperform the basic prompt, suggesting the need for further exploration and refinement in prompt design strategies.","","979-8-3315-3098-3","10.1109/ICARC64760.2025.10962827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962827","LLM;RAG;AES;Grading;Prompt engineering","Knowledge engineering;Analytical models;Accuracy;Large language models;Computational modeling;Education;Cognition;Prompt engineering;Context modeling","","","","17","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"SQLify Me: An Automated Text-To-SQL Query Generator","A. Y. Mohamed; M. H. Zain-elabdeen; S. A. Sayed; S. S. Abdelwahab; Z. E. Mohamed; Z. M. Ali; Y. A. Kawashti; H. Hindy","Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt; Computer Science Department, Faculty of Computer & Information Sciences, Ain Shams University, Cairo, Egypt",2024 34th International Conference on Computer Theory and Applications (ICCTA),"25 Apr 2025","2024","","","297","302","Data plays a crucial role in computer science, serving as its backbone in various applications. Understanding and interacting with data stored in databases often requires technical knowledge of query languages, posing a challenge for non-technical users. This paper aims to bridge this gap by developing a Text-to-SQL generation system, facilitating natural language queries to database operations. In SQLIFYME, we adopted a dual-path approach to implement the Text-to-SQL generation model. The Seq2Seq model, utilizing transformers, processes natural language queries to generate corresponding Structured Query Language (SQL) queries. Simultaneously, generative AI techniques are employed, applying prompt engineering to the Llama2 LLM to manage complex and nuanced queries and further refine and enhance query generation capabilities. After extensive experiments with Seq2Seq models, LLMs, and different prompt engineering techniques, we reached the optimal configuration for our proposed model. The experimental results demonstrate the effectiveness of both approaches in accurately translating natural language queries into SQL commands. The Seq2Seq model achieves high accuracy in handling structured queries, while the generative AI approach excels in handling complex and nuanced queries, highlighting the versatility of our system. Notably, the system achieved an impressive execution score of 0.8 using the T-5 base model.","2770-6575","979-8-3315-2968-0","10.1109/ICCTA64612.2024.10974783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974783","Text-to-SQL;Natural Language Processing (NLP);Transformers;Prompt Engineering;Generative AI;Large Language Models (LLMs)","Training;Structured Query Language;Translation;Accuracy;Databases;Speech recognition;Syntactics;Transformers;Natural language processing;Prompt engineering","","","","19","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Large language models for automated sleep staging","A. Honnavalli; H. P. G L; A. H. Badyal; A. S. C A; G. Srinivasa","Department of CSE, PES Center for Pattern Recognition, PES University, Bangalore, India; Department of CSE, PES Center for Pattern Recognition, PES University, Bangalore, India; Department of CSE, PES Center for Pattern Recognition, PES University, Bangalore, India; Department of CSE, PES Center for Pattern Recognition, PES University, Bangalore, India; Department of CSE, PES Center for Pattern Recognition, PES University, Bangalore, India",2025 15th International Conference on Electrical Engineering (ICEENG),"17 Jun 2025","2025","","","1","6","Sleep holds a lot of information about the human body. Tracking your sleep can provide valuable insights into your sleep patterns, including duration, quality, and stages of sleep. This information can help one understand if they are getting enough restorative sleep or if there are disruptions that need addressing. Sleep is closely linked to overall health. By monitoring your sleep patterns, you can identify potential health issues such as sleep disorders (e.g., sleep apnea, insomnia) or underlying medical conditions that may affect your sleep quality. If these sleep disorders are detected and treated early, many major cardiovascular diseases, neurological diseases and mental health disorders can be prevented. We evaluated four LLMs—Llama8b-8192, Llama-70b, Mixtral, and Gemma2-9b-it—on sleep stage classification. Llama-8b-8192 achieved the highest accuracy at 44.4%, highlighting the limitations of general-purpose LLMs for medical time-series classification. We have tried to use its intelligence in identifying patterns between various PSG data signal channels and sleep stages.","","979-8-3315-1901-8","10.1109/ICEENG64546.2025.11031373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11031373","Large language models;Polysomnography. Prompt Engineering;Sleep-Staging;Hallucination","Neurological diseases;Technological innovation;Accuracy;Large language models;Sleep apnea;Data models;Numerical models;Prompt engineering;Biomedical monitoring;Medical diagnostic imaging","","","","15","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Prompts Matter: Insights and Strategies for Prompt Engineering in Automated Software Traceability","A. D. Rodriguez; K. R. Dearstyne; J. Cleland-Huang","College of Engineering, University of Notre Dame, Notre Dame, Indiana; College of Engineering, University of Notre Dame, Notre Dame, Indiana; College of Engineering, University of Notre Dame, Notre Dame, Indiana",2023 IEEE 31st International Requirements Engineering Conference Workshops (REW),"28 Sep 2023","2023","","","455","464","Large Language Models (LLMs) have the potential to revolutionize automated traceability by overcoming the challenges faced by previous methods and introducing new possibilities. However, the optimal utilization of LLMs for automated traceability remains unclear. This paper explores the process of prompt engineering to extract link predictions from an LLM. We provide detailed insights into our approach for constructing effective prompts, offering our lessons learned. Additionally, we propose multiple strategies for leveraging LLMs to generate traceability links, improving upon previous zero-shot methods on the ranking of candidate links after prompt refinement. The primary objective of this paper is to inspire and assist future researchers and engineers by highlighting the process of constructing traceability prompts to effectively harness LLMs for advancing automatic traceability.","2770-6834","979-8-3503-2691-8","10.1109/REW57809.2023.00087","USA National Science Foundation(grant numbers:SHF-1901059,SHF-1909007,PFI-TT-2122689); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260721","automated software traceability;large language models;prompt engineering","Conferences;Software;Requirements engineering","","30","","29","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Evaluation of code clone detection using large language models","X. Wang","Xidian University, No. 266, Xinglong Section, Xifeng Road, Chang'an District, Xi'an City, Shaanxi Province, People's Republic of China",5th International Conference on Signal Processing and Machine Learning (CONF SPML 2025),"18 Apr 2025","2025","2025","","212","217","Code clone detection has extensive and critical applications in software metrics, plagiarism detection, aspect mining, code compression, and software supply chain vulnerability detection. Traditional token-based and tree-based clone detection methods are effective for syntactically similar clones. However, they often struggle to detect semantic clones—code segments that are semantically and functionally equivalent but syntactically distinct—since functionality similarity is not immediately apparent from syntax alone. This study explores the potential of large language models (LLMs) for code clone detection. By inputting code pairs into LLMs alongside prompt engineering techniques, we evaluated the capacity of LLMs to identify different types of code clones. Through comparative analyses of accuracy and recall rates between LLM-based methods and traditional token-based and tree-based approaches across various code clone types, we aimed to assess the advantages, limitations, and unique capabilities of LLMs in handling complex code similarity tasks. Experimental results demonstrate that LLMs outperform traditional methods in detecting semantic clones, achieving an accuracy of up to 100% and a recall rate of up to 92%, underscoring their value in modern software engineering practices. However, LLMs still face limitations, particularly in detecting semantic clones with significant structural differences, necessitating further improvements in future research.","","978-1-83724-314-3","10.1049/icp.2025.1037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10969749","","","","","","","","18 Apr 2025","","","IET","IET Conferences"
"The Efficient Development of Conflict Structure Datasets for Evaluating Sentiment Recognition Bias in Large Language Models","K. Inoshita","Faculty of Data Science, Shiga University, Hikone, Japan",2024 International Conference on Electrical Engineering and Informatics (ICELTICs),"10 Dec 2024","2024","","","7","12","The rapid advancement of AI technology has led to the development of large language models (LLMs) that acquire extensive general knowledge from vast amounts of text data and are utilized in various tasks. However, LLMs also inherit biases from their training data, resulting in discriminatory behavior related to gender, race, and political ideologies. In the field of national security, sentiment recognition bias towards specific countries poses significant problems. Previous studies have developed datasets to evaluate these biases, but several issues remain in their methods. This study proposes a new method for developing datasets to evaluate sentiment recognition bias in LLMs, using tweet data related to the Ukraine-Russia war. The method involves automated sentiment labeling and anonymization using LLMs to create efficient and high-precision datasets. GPT -3.5-turbo was used for initial sentiment labeling, with human corrections applied to select tweets positive towards Ukraine and negative towards Russia. Anonymization processing with LLMs ensured specific names did not influence bias evaluation. Experimental results confirmed the proposed method effectively evaluates sentiment recognition bias in LLMs across various conflict structures. Metrics such as Emotion Inversion Consistency Rate (EICR), Positive Odds, and Negative Odds quantitatively assessed biases. The results clearly demonstrated LLMs‘ emotional biases towards specific countries and their recognition of conflict structures. In conclusion, this study presents a new method for evaluating biases in LLMs and demonstrates its effectiveness. Future research will focus on expanding dataset size and improving anonymization to further enhance fairness and reliability. This study lays the groundwork for more robust evaluations of LLM biases.‘’","","979-8-3503-6682-2","10.1109/ICELTICs62730.2024.10776050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776050","Large Language Models;Sentiment Recognition Bias;Dataset Development;Conflict Structure;Automated Processing","Measurement;Data privacy;Sentiment analysis;Large language models;Training data;Information filtering;Reliability;Labeling;National security;Information integrity","","","","26","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"ChatGPT vs. Gemini: Comparative Evaluation in Cybersecurity Education with Prompt Engineering Impact","T. Nguyen; H. Sayadi","Department of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, CA, USA; Department of Computer Engineering and Computer Science, California State University, Long Beach, Long Beach, CA, USA",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","9","The advent of Large Language Models (LLMs) has revolutionized numerous domains, notably education, by offering powerful tools for personalized learning and automated assistance. These models have the potential to significantly enhance the educational experience, particularly in the field of Computer Science (CS), where the complexity and rapidly evolving nature of topics present unique challenges and opportunities. In this study, we present a comparative evaluation into the transformative potential of LLMs in CS education, with a specific focus on cybersecurity. Our study centers on two leading LLMs: Ope-nAI's ChatGPT and Google's Gemini Pro, employing a three-fold assessment methodology. Firstly, we analyze the subject matter within cybersecurity education to identify key topics and challenges for examination. Secondly, we meticulously assess and compare the efficacy of ChatGPT and Gemini across various factors in producing satisfactory responses. Lastly, we explore the impact of leveraging prompt engineering on enhancing the quality of responses generated by these AI tools. Through this holistic approach, our research aims to provide insights into the strengths, limitations, and potential avenues for enhancement of these models, thereby enriching the ongoing discourse on LLMs integration in higher education.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893499","Artificial Intelligence;ChatGPT;Education;Gemini;Large Language Models;Prompt Engineering","Computer science;Large language models;Computational modeling;Education;Chatbots;Internet;Complexity theory;Prompt engineering;Computer security","","","","46","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Innovating Translation: Lessons Learned from BWX Generative Language Engine","V. Burégio; I. Pereira; H. Cabral","Software Engineering Bureau Works, Lafayette, CA, USA; Software Engineering Bureau Works, Lafayette, CA, USA; Software Engineering Bureau Works, Lafayette, CA, USA",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","98","99","The integration of Translation Management Systems (TMS) and Large Language Models (LLMs) has revolutionized the translation landscape, offering nuanced and culturally sensitive translations. This paper explores the lessons learned from developing the BWX Generative Language Engine, an award-winning Generative AI tool for translation, which exemplifies the application of generative AI in translation management. Lessons include the transformative impact of AI, the accelerated delivery of beta features with LLMs, and the strategic integration of enabling technologies. Additionally, insights are drawn from strategic testing for optimal model routing, caching mechanisms, fallbacks, and the importance of security and data policy awareness.CCS CONCEPTS• Information systems • Information systems applications • Computing platforms","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556084","Generative AI;Translation Management Systems;Large Language Models;Software Engineering","Technological innovation;Generative AI;Computational modeling;Routing;Software reliability;Security;Engines","","","","0","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Multi-Agent hierarchical workflow for autonomous code generation with Large Language Models","A. S; R. Sekar; O. K. C U; P. D; S. M","School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India","2025 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)","2 Apr 2025","2025","","","1","5","This paper presents a multi-agent hierarchical workflow tailored for automating data analysis, code generation, and visualization, focusing specifically on user-provided CSV datasets. The workflow integrates AlphaCodium with LangChain, LangGraph, and GPT, enabling autonomous code generation, unit test development, and debugging. The system operates by analyzing the uploaded dataset, assigning agents that work sequentially: a programmer agent generates code using the Skeleton-Of-Code approach, a unit test agent verifies the code, and an executor agent runs the code. A debugging agent is also included to identify and resolve any issues. The AI agents involved are capable of dynamically accessing online resources, including documentation and references to enhance their decision-making processes. This work attempts to exemplify the application of AI in automating not only code execution but also the planning and validation stages by writing unit tests in the software development process to reflect the increasing role of AI in advancing automation for rapid code generation within the software industry.","2688-0288","979-8-3315-2983-3","10.1109/SCEECS64059.2025.10940635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940635","Autonomous Agents;OpenAI;GPTs;Retrieval-Augmented Generation;Software Engineering","Productivity;Industries;Technological innovation;Codes;Employment;Decision making;Software;Encoding;Artificial intelligence;Software development management","","","","21","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"ChatPhishDetector: Detecting Phishing Sites Using Large Language Models","T. Koide; H. Nakano; D. Chiba","NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan; NTT Security Holdings Corporation & NTT Corporation, Tokyo, Japan",IEEE Access,"28 Oct 2024","2024","12","","154381","154400","Large Language Models (LLMs), such as ChatGPT, are significantly impacting various fields. While LLMs have been extensively studied for code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, remains largely unexplored. To counter the increasing cyber-attacks that leverage LLMs for creating more sophisticated and convincing phishing content, it is crucial to automate detection by harnessing LLMs’ advanced capabilities. This paper introduces ChatPhishDetector, a novel system that employs LLMs to identify phishing sites. Our approach involves using a web crawler to collect website information, generating prompts for LLMs based on the gathered data, and extracting detection results from LLM responses. This system enables accurate detection of multilingual phishing sites by identifying impersonated brands and social engineering techniques within the entire website context, without requiring machine learning model training. We evaluated our system’s performance using our own dataset and compared it with baseline systems and several LLMs. Experiments using GPT-4V showed exceptional results, achieving 98.7% precision and 99.6% recall, surpassing the detection performance of other LLMs and existing systems. These findings highlight the potential of LLMs for protecting users from online fraudulent activities and provide crucial insights for strengthening defenses against phishing attacks.","2169-3536","","10.1109/ACCESS.2024.3483905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10723311","Large language models;phishing sites;social engineering","Phishing;Uniform resource locators;Large language models;Crawlers;Codes;Web pages;Security;Accuracy;Visualization;Cognition","","8","","61","CCBY","21 Oct 2024","","","IEEE","IEEE Journals"
"Code Clone Detection Techniques Based on Large Language Models","A. A. Almatrafi; F. A. Eassa; S. A. Sharaf","Computer Science Department, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Computer Science Department, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Computer Science Department, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia",IEEE Access,"19 Mar 2025","2025","13","","46136","46146","Code duplication, commonly known as code cloning, is a persistent challenge in software development. While reusing code fragments boosts productivity, excessive cloning poses challenges to maintenance and elevates the risk of bugs. Therefore, integrating code clone detection into the development process is crucial. The extensive code-related knowledge inherent in Large Language Models (LLMs) renders them high-potential candidates for addressing diverse software engineering challenges. However, the effectiveness of LLMs in the specific task of code clone detection requires precise evaluation. This paper proposes an innovative methodology leveraging few-shot instruction-tuned GPT-3.5 Turbo and GPT-4 to detect code clones across all types, focusing on complex clones (Type-3 and Type-4). Unlike conventional approaches confined to specific language pairs or tasks, our method employs versatile language models, showcases generalization strengths for semantic understanding, and leverages instruction tuning with few-shot inference for task-specific adaptability in code clone detection. A conversational dataset was crafted from BigCloneBench for instruction tuning, enhancing task alignment and performance. This study evaluates the proficiency of LLMs in identifying code clones, analyzing the impact of instruction tuning, and assessing the efficiency across various clone types. Experimental results demonstrate these models achieving competitive performance against existing tools for overall and complex clone detection. Integration into an Integrated Development Environment (IDE) enables real-time detection and automated refactoring, bridging the gap between theoretical advancements and practical usability. This work highlights the potential of generalized LLMs setting a new standard in a field traditionally dominated by specialized tools and demonstrates their adaptability for complex challenges in code analysis and maintainability.","2169-3536","","10.1109/ACCESS.2025.3549780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10918947","Clone detection;code duplication;fine-tuning;instruction tuning;large language models;natural language processing;transformers","Cloning;Codes;Transformers;Adaptation models;Software engineering;Tuning;Natural language processing;Training;Syntactics;Real-time systems","","1","","40","CCBY","10 Mar 2025","","","IEEE","IEEE Journals"
"Temporal Context Awareness: A Defense Framework Against Multi-Turn Manipulation Attacks on Large Language Models","P. Kulkarni; A. Namer",NA; NA,2025 IEEE Conference on Artificial Intelligence (CAI),"7 Jul 2025","2025","","","930","935","Many Large Language Models (LLMs) today are vulnerable to multi-turn manipulation attacks,where adversaries gradually build context through seemingly benign conversational turns to elicit harmful or unauthorized responses. These attacks exploit the temporal nature of dialogue to evade single-turn detection methods, posing a significant risk to the safe deployment of LLMs. This paper introduces the Temporal Context Awareness (TCA)framework, a novel defense mechanism designed to address this challenge by continuously analyzing semantic drift, cross-turn intention consistency, and evolving conversational patterns. The TCA framework integrates dynamic context embedding analysis, cross-turn consistency verification, and progressive risk scoring to detect and mitigate manipulation attempts effectively. Preliminary evaluations on simulated adversarial scenarios demonstrate the framework's potential to identify subtle manipulation patterns often missed by traditional detection techniques, offering a much-needed layer of security for conversational AI systems. In addition to outlining the design of TCA, we analyze diverse attack vectors and their progression across multi-turn conversations, providing valuable insights into adversarial tactics and their impact on LLM vulnerabilities. Our findings underscore the pressing need for robust, context-aware defenses in conversational AI systems and highlight the TCA framework as a promising direction for securing LLMs while preserving their utility in legitimate applications","","979-8-3315-2400-5","10.1109/CAI64502.2025.00164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050551","LLM Security;Multi-turn attacks;prompt security;obfuscation;prompt injection;security;trustworthy AI;jailbreak","Conversational artificial intelligence;Large language models;Semantics;Context awareness;Pressing;Oral communication;Vectors;Security;Manipulator dynamics","","","","15","IEEE","7 Jul 2025","","","IEEE","IEEE Conferences"
"Enhancing Communication and Data Transmission Security in RAG Using Large Language Models","V. Gummadi; P. Udayaraju; V. R. Sarabu; C. Ravulu; D. R. Seelam; S. Venkataramana","Expedia Group, Austin, United States; Department of CSE, School of Engineering and Sciences, SRM University, AP, India; Wallmart Global Tech, Texas, USA; Rocket Companies, Michigan, United States; Wallmart Global Tech, Bentonville, Arkansas; Department of IT, SRKR Engineering College, Bhimavaram",2024 4th International Conference on Sustainable Expert Systems (ICSES),"3 Dec 2024","2024","","","612","617","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources, enabling more useful information and generating accurate responses. This paper explores RAG's architecture and applications, combining generator and retriever models to access and utilize vast external data repositories. While RAG holds significant promise for various Natural Language Processing (NLP) processes like dialogue generation, summarization, and question answering, it also presents unique security challenges that must be addressed to ensure system integrity and reliability. RAG systems face several security threats, including data poisoning, model manipulation, privacy leakage, biased information retrieval, and harmful outputs generation. Generally, in the traditional RAG application, security threat is one of the major concerns. To tighten the security system and enhance the efficiency of the model on processing more complex data this paper outlines key strategies for securing RAG-based applications to mitigate these risks paper outlines key strategies for securing RAG-based applications to mitigate these risks. Ensuring data security through filtering, sanitization, and provenance tracking can prevent data poisoning and enhance the quality of external knowledge sources. Strengthening model security via adversarial training, input validation, and anomaly detection improves resilience against manipulative attacks. Implementing output monitoring and filtering techniques, such as factual verification, language moderation, and bias detection, ensures the accuracy and safety of generated responses. Additionally, robust infrastructure and access control measures, including secure data storage, secure APIs, and regulated model access, protect against unauthorized access and manipulation. Moreover, this study analyzes various use cases for LLMs enhanced by RAG, including personalized recommendations, customer support automation, content creation, and advanced search functionalities. The role of vector databases in optimizing RAG-driven generative AI is also discussed, highlighting their ability to efficiently manage and retrieve large-scale data for improved response generation. By adhering to these security measures and leveraging best practices from leading industry sources such as Databricks, AWS, and Milvus, developers can ensure the robustness and trustworthiness of RAG-based systems across diverse applications.","","979-8-3315-4036-4","10.1109/ICSES63445.2024.10763024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763024","RAG;LLM;Query Analysis;Security Enhancement;Data Privacy","Training;Accuracy;Filtering;Large language models;Data models;Vectors;Robustness;Safety;Security;Resilience","","","","15","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Harnessing Generative Llms to Detect and Explain Suicidal Ideation in Brazilian Portuguese Texts","J. P. C. Azevedo; A. C. de Oliveira; F. Silva; L. Coutinho; A. S. Teles","Federal University of Maranhão, São Luís, Brazil; Parnaíba Delta Federal University, Parnaíba, Brazil; Federal University of Maranhão, São Luís, Brazil; Federal University of Maranhão, São Luís, Brazil; Federal Institute of Maranhão, Araioses, Brazil",2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS),"4 Jul 2025","2025","","","15","20","Suicide remains a critical public health problem, with increasing cases of Suicide Ideation (SI) necessitating improved identification and intervention strategies. Although generative Large Language Models (LLMs) have demonstrated potential in text analysis for mental health applications, their effectiveness in accurately detecting SI and generating reliable explanations remains underexplored, specially in Brazilian Portuguese (PT-BR) language. This study proposes a new architecture for the Boamente, an AI-based system for suicide prevention. The proposal aims to incorporate into Boamente architecture an explanation generation stage through prompt engineering. The aim is to improve text detection through the classifier model and exploit prompt engineering through an explainer model to generate explanations of why a text in PT-BR does or does not contain SI, increasing the model's interpretability and providing more transparent justifications for its predictions. The proposed structure expands the Boamente architecture by exploring generative LLMs as classifier models (i.e., SI identification) and integrating an explainer model, which generates justifications for predictions. A quantitative evaluation was conducted with different LLMs using classification performance metrics, in which Qwen 2.5 (14B) achieved the highest AUC (0.9898), while the 3B and 7B versions of the same model achieved the best Recall (0.9545). In addition, a qualitative evaluation was conducted with the participation of three professionals (a computer scientist, a linguist, and a psychologist) to analyze the explanations generated by LLMs. The participants considered that LLaMA 3.1 (8B) produced the highest quality justifications. The findings highlight the potential of combining classification and explanation LLMs to enhance explainability and trust in an AI-driven system for suicide prevention.","2372-9198","979-8-3315-2610-8","10.1109/CBMS65348.2025.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11058904","Generative AI;GenAI;Large Language Models;LLM;Suicide Prevention;Mental Health;Digital Phenotyping","Text analysis;Prevention and mitigation;Large language models;Text detection;Computer architecture;Mental health;Predictive models;Prompt engineering;Reliability;Public healthcare","","","","26","IEEE","4 Jul 2025","","","IEEE","IEEE Conferences"
"Tales From the Trenches: Expectations and Challenges From Practice for Code Review in the Generative AI Era","N. Davila; J. Melegati; I. Wiese","Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Free University of Bozen-Bolzano, Bolzano, Italy; Department of Computing, Federal University of Technology - Paraná, Campo Mourao, Brazil",IEEE Software,"4 Oct 2024","2024","41","6","38","45","In this study, we investigate what has been discussed about generative AI in the code review context by performing a gray literature review. We analyzed 42 documents and found insights from practice and proposals of solutions using generative AI models.","1937-4194","","10.1109/MS.2024.3428439","CAPES(grant numbers:88887.480572/2020-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10604721","","Codes;Reviews;Generative AI;Encoding;Chatbots;Software development management;Software engineering;Security;Internet","","2","","15","IEEE","18 Jul 2024","","","IEEE","IEEE Magazines"
"Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks","A. Diaf; A. A. Korba; N. Elislem Karabadji; Y. Ghamri-Doudane","LRS, Badji Mokhtar Annaba University, Algeria; LRS, Badji Mokhtar Annaba University, Algeria; National Higher School of Technology and Engineering, Annaba, Algeria; L3I, University of La Rochelle, France",2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT),"12 Aug 2024","2024","","","117","123","In recent years, numerous large-scale cyberattacks have exploited Internet of Things (IoT) devices, a phenomenon that is expected to escalate with the continuing proliferation of IoT technology. Despite considerable efforts in attack detection, intrusion detection systems remain mostly reactive, responding to specific patterns or observed anomalies. This work proposes a proactive approach to anticipate and mitigate malicious activities before they cause damage. This paper proposes a novel network intrusion prediction framework that combines Large Language Models (LLMs) with Long Short Term Memory (LSTM) networks. The framework incorporates two LLMs in a feedback loop: a fine-tuned Generative Pre-trained Transformer (GPT) model for predicting network traffic and a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) for evaluating the predicted traffic. The LSTM classifier model then identifies malicious packets among these predictions. Our framework, evaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant improvement in predictive capabilities, achieving an overall accuracy of 98%, offering a robust solution to IoT cybersecurity challenges.","2325-2944","979-8-3503-6944-1","10.1109/DCOSS-IoT61029.2024.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10621562","Security;Intrusion Prediction;GPT;BERT;Large Language Models;LSTM;Internet of Things (IoT)","Accuracy;Prevention and mitigation;Large language models;Bidirectional control;Telecommunication traffic;Predictive models;Transformers","","13","","21","IEEE","12 Aug 2024","","","IEEE","IEEE Conferences"
"Transforming the field of Vulnerability Prediction: Are Large Language Models the key?","M. Siavvas; I. Kalouptsoglou; E. Gelenbe; D. Kehagias; D. Tzovaras","Centre for Research and Technology Hellas/Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/Information Technologies Institute, Thessaloniki, Greece; Polish Academy of Sciences (PAN), Institute of Theoretical and Applied Informatics, Gliwice, Poland; Centre for Research and Technology Hellas/Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/Information Technologies Institute, Thessaloniki, Greece","2024 32nd International Conference on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)","13 Dec 2024","2024","","","1","6","Vulnerability prediction is an important mechanism for secure software development, as it enables the early identification and mitigation of software vulnerabilities. Vulnerability Prediction Models (VPMs) are Machine Learning (ML) models able to detect potentially vulnerable software components based on information retrieved from their source code. Despite the notable advancements in the field of vulnerability prediction, especially with the utilization of Deep Learning (DL) and text mining techniques, current literature still lacks a highly accurate, reliable, and practical VPM. Recently, the Large Language Models (LLMs), which have demonstrated remarkable capabilities in text understanding and processing, have started being utilized for vulnerability prediction, demonstrating highly promising results. The purpose of the present paper is to explore the utilization of LLMs in the field of vulnerability detection, identify challenges and open issues that still need to be addressed, and potentially propose directions for future research. Our analysis suggests that while LLM-based VPMs have outperformed traditional DL approaches in vulnerability prediction, significant challenges still need to be addressed to be considered sufficiently accurate, reliable, and practical.","2375-0227","979-8-3315-3130-0","10.1109/MASCOTS64422.2024.10786575","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786575","Software Security;Vulnerability Prediction;Large Language Models;Transformers;Review;Survey","Text mining;Analytical models;Accuracy;Large language models;Computational modeling;Predictive models;Transformers;Software;Software reliability;Software engineering","","1","","48","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"Prompt Engineering Based Generative AI as a Service (GAIaaS) for Intent-Based Networking","H. Kukkalli; A. Dandekar; T. Bauschert","Chair of Communication Networks, Faculty of Electrical Engineering and Information Technology, Technische Universität Chemnitz, Germany; School of Electrical Engineering and Computer Science, TU Berlin, Germany; Chair of Communication Networks, Faculty of Electrical Engineering and Information Technology, Technische Universität Chemnitz, Germany",NOMS 2025-2025 IEEE Network Operations and Management Symposium,"15 Jul 2025","2025","","","1","7","This paper presents a framework that integrates Generative AI as a Service (GAIaaS) into Service Management and Orchestration (SMO) systems to enable intent-based automation. By leveraging ChatGPT-4o's advanced natural language capabilities, the system interprets user intents and generates policy-driven service chain configurations. Prompt engineering techniques are employed to evaluate the model's performance across key areas, including response time for intent processing, token usage efficiency, repeatability of outputs, infrastructure cost, and multilingual support. The framework consists of various components such as orchestration engine, cloud network function controller, and SDN controller and automates service chain design and resource management. The obtained results demonstrate its reliable and scalable performance across different scenarios. However, challenges related to handling large prompts and sustaining performance under high loads have been identified. This work highlights the potential of GAIaaS to provide scalable, adaptive, and intelligent network automation.","2374-9709","979-8-3315-3163-8","10.1109/NOMS57970.2025.11073676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11073676","Generative AI;LLM;Prompt Engineering;ChatGPT;Service Management and Orchestration (SMO);5G Mobile Networks;Cloud Network Function Controller;CNFC;SDNC","Intelligent networks;Automation;Costs;Natural languages;Multilingual;Prompt engineering;Time factors;Resource management;Reliability;Engines","","","","18","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"A GPT-Based Code Review System With Accurate Feedback for Programming Education","D. -K. Lee; I. Joe","Department of Computer Science, Hanyang University, Seoul, Republic of Korea; Department of Computer Science, Hanyang University, Seoul, Republic of Korea",IEEE Access,"24 Jun 2025","2025","13","","105724","105737","The increasing demand for programming education and growing class sizes require immediate and personalized feedback. However, integrating Large Language Models (LLMs) like ChatGPT in introductory programming courses raises concerns about AI-assisted cheating. In large-scale settings, faulty code submissions may lead LLMs to overanalyze, causing unnecessary token consumption. This paper proposes a GPT-4o-based code review system that provides accurate feedback while reducing token usage and preventing AI-assisted cheating. Unlike general-purpose LLM tools for professionals, the system is pedagogically designed for primary and secondary students by focusing on review necessity and learner-friendly feedback. The system features a Code Review Module (CRM) that reduces token usage via a Review Necessity Chain (RNC), and Code Correctness Check Module (CCM) combining test case validation with LLM-based assessment. To prevent AI-assisted cheating, the system provides automated feedback on submitted code without prompting and revealing correct answers, which are accessed only through the “Ask Code Tutor” button. In usability test, the system detected up to 42.86% more errors than a conventional online judge. BERTScore analysis showed that over 80% of the system-generated reviews were semantically aligned with human feedback. A performance comparison with state-of-the-art systems demonstrated a blocking success rate of 86%, with a comparable review omission rate. These results indicate that the system provides more accurate feedback than conventional automated code reviews, while achieving token efficiency and supporting self-directed learning through educational feedback. Thus, it can serve as a practical solution for scalable programming education in primary and secondary classes.","2169-3536","","10.1109/ACCESS.2025.3581139","DLAB in RECRUITING PRIMARY AND SECONDARY students for the usability testing of the Code Review System and by C3 Coding in providing an online judge system dataset for experimental evaluation.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11039773","Large language models (LLMs);GPT-4o;programming education;learner-friendly code reviews;LangChain","Codes;Reviews;Education;Programming profession;Chatbots;Proposals;Automation;Accuracy;Usability;Training","","","","29","CCBY","18 Jun 2025","","","IEEE","IEEE Journals"
"APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent Threats Using Large Language Models","S. Benabderrahmane; P. Valtchev; J. Cheney; T. Rahwan","Division of Science., New York University; Computer Science Dpt, The University of Montreal UQAM, Montreal, Canada; The University of Edinburgh, School of informatics, Edinburgh, UK; New York University in Abu Dhabi, UAE. Division of Science",2025 13th International Symposium on Digital Forensics and Security (ISDFS),"2 Jun 2025","2025","","","1","6","Advanced Persistent Threats (APTs) pose a ma-jor cybersecurity challenge due to their stealth and ability to mimic normal system behavior, making detection particularly difficult in highly imbalanced datasets. Traditional anomaly detection methods struggle to effectively differentiate APT-related activities from benign processes, limiting their applicability in real-world scenarios. This paper introduces APT-LLM, a novel embedding-based anomaly detection framework that integrates large language models (LLMs)-BERT, ALBERT, DistiIBERT, and RoBERTa-with autoencoder architectures to detect APTs. Unlike prior approaches, which rely on manually engineered features or conventional anomaly detection models, APT-LLM leverages LLMs to encode process-action provenance traces into semantically rich embeddings, capturing nuanced behavioral patterns. These embeddings are analyzed using three autoencoder architectures-Baseline Autoencoder (AE), Variational Autoen-coder (VAE), and Denoising Autoencoder (DAE)-to model normal process behavior and identify anomalies. The best-performing model is selected for comparison against traditional methods. The framework is evaluated on real-world, highly imbalanced provenance trace datasets from the DARPA Trans-parent Computing program, where APT-like attacks constitute as little as 0.004 % of the data across multiple operating systems (Android, Linux, BSD, and Windows) and attack scenarios. Results demonstrate that APT-LLM significantly improves detection performance under extreme imbalance conditions, outperforming existing anomaly detection methods and highlighting the effectiveness of LLM-based feature extraction in cybersecurity.","2768-1831","979-8-3315-0993-4","10.1109/ISDFS65363.2025.11011912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011912","Anomaly Detection;Deep Learning;Transformers;Large Language Models;AutoEncoders;Cyber-security","Large language models;Operating systems;Computational modeling;Linux;Autoencoders;Noise reduction;Feature extraction;Transformers;Computer security;Anomaly detection","","","","12","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"Optimizing Real-Time Data Processing in Resource-Constrained Environments: a Spark and Gpu-Driven Workflow for Large Language Models","M. Yousef; G. Khoriba; T. Arafa","Center for Informatics Science, Nile University, Giza, Egypt; Center for Informatics Science, Nile University, Giza, Egypt; Center for Informatics Science, Nile University, Giza, Egypt",2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS),"17 Feb 2025","2024","","","116","121","Resource limitations on computational infrastructure reduce the effective use of Large Language Models (LLMs) for processing streaming data, especially in research. The combination of Apache Spark and Graphics Processing Units (GPUs) has revolutionized large-scale data processing but benefits primarily resource-rich computational clusters. This research explores the potential of an advanced real-time data processing pipeline built on Apache Spark Streaming, efficiently managing data flow across streaming channels. This enhanced infrastructure significantly improves the real-time analytical capabilities of LLMs, enabling them to perform scalable and context-aware batch tasks more efficiently. Our proposed pipeline expands the functionality of the Spark ecosystem, particularly in GPU-accelerated clusters, which solves the resource limitation of low computational power machines. Deploying LLMs across Spark's distributed nodes ensures their readiness for various analytical tasks. In a practical scenario, the system's performance is assessed by evaluating student programming assignments and giving feedback through LLMs. We tested the system across multiple tasks, including code generation from questions, and compared single, dual, and triple-worker configurations, focusing on time efficiency and resilience. Our results show that the pipeline achieves up to a 50% and $70-80\%$ reduction in inference time with dual and three workers, respectively, without compromising result accuracy. The system also demonstrates resilience through a failover strategy that ensures seamless task redirection to alternate nodes in case of failure, maintaining continuous operation. Including additional workers significantly enhances data processing capacity and improves load distribution, making the system scalable for more resource-intensive tasks. This adaptability benefits researchers with different resource capabilities, optimizing the performance of LLMs in diverse programming and code generation scenarios.","","979-8-3503-9121-3","10.1109/FMLDS63805.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10874044","Large Language Model (LLM);Clusteringbased implementation;Apache Spark Streaming;Resource optimization","Adaptation models;Codes;Large language models;Pipelines;Programming;Data processing;Real-time systems;Sparks;Resource management;Resilience","","","","17","IEEE","17 Feb 2025","","","IEEE","IEEE Conferences"
"FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments","F. Liu; Z. Liu; Q. Zhao; J. Jiang; L. Zhang; G. Li; Z. Sun; Z. Li; Y. Ma","State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science, Peking University, Beijing, China; State Key Laboratory of Complex & Critical Software Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; Huawei Cloud Computing Technologies Co., Ltd, Shenzhen, China; Huawei Cloud Computing Technologies Co., Ltd, Shenzhen, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","669","680","Providing personalized and timely feedback for student’s programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM’s attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67× compared to the autoregressive decoding algorithm.CCS CONCEPTS• Software and its engineering; • Computing methodologies → Artificial intelligence;","2643-1572","979-8-4007-1248-7","","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765070","Automated Program Repair;Large Language Models;Programming Education;Inference Acceleration","Large language models;Computer bugs;Software algorithms;Education;Maintenance engineering;Inference algorithms;Software;Decoding;Programming profession;Software engineering","","","","41","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Integrating Graphs With Large Language Models: Methods and Prospects","S. Pan; Y. Zheng; Y. Liu","Griffith University, Gold Coast, Queens, Australia; Monash University, Melbourne, Vic, Australia; Monash University, Melbourne, Vic, Australia",IEEE Intelligent Systems,"28 Feb 2024","2024","39","1","64","68","Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.","1941-1294","","10.1109/MIS.2023.3332242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10453398","","Predictive models;Transformers;Prediction algorithms;Cognition;Task analysis;Intelligent systems;Large language models;Graphical models;Query processing;Learning systems","","9","","10","IEEE","28 Feb 2024","","","IEEE","IEEE Magazines"
"Investigating the Proficiency of Large Language Models in Formative Feedback Generation for Student Programmers","S. S. Kumar; M. A. Lones; M. Maarek; H. Zantout","Heriot-Watt University, Dubai, United Arab Emirates; Heriot-Watt University, Edinburgh, United Kingdom; Heriot-Watt University, Edinburgh, United Kingdom; Heriot-Watt University, Dubai, United Arab Emirates",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","88","93","Generative AI has considerably altered traditional workplace practice across numerous industries. Ever since the emergence of large language models (LLMs), their potential to generate formative feed-back for introductory programming courses has been extensively researched. However, most of these studies have focused on Python. In this work, we examine the bug-fixing and feedback-generation abilities of Code Llama and ChatGPT for Java programming assignments using our new Java benchmark called CodeWBugs. The results indicate that ChatGPT performs reasonably well, and was able to fix 94.33% programs. By comparison, we observed high variability in the results from Code Llama. We further analyzed the impact of different types of prompts and observed that prompts that included task descriptions and test inputs yielded better results. In most cases, the LLMs precisely localized the bugs and also offered guidance on how to proceed. Nevertheless, we also noticed incorrect responses generated by the LLMs, emphasizing the need to validate responses before disseminating feedback to learners.CCS CONCEPTS• Applied computing → Computer-assisted instruction; • Computing methodologies → Machine translation; Natural language generation.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734619","Large language models (LLM);GPT-4;Feedback;Java Programming;Program Repair","Java;Codes;Large language models;Natural language generation;Benchmark testing;Maintenance engineering;Chatbots;Machine translation;Programming profession;Python","","1","","42","CCBY","30 Oct 2024","","","IEEE","IEEE Conferences"
"Refining Large Language Models for Tabular Data Analysis in Business Domain by Laymen Text","T. Nirusanan; S. Prasanth; K. Banujan; S. Kumara","Department of Computing and Information Systems, Sabaragamuwa University of Sri Lanka, Belihuloya, Sri Lanka; Department of Physical Sciences and Technology, Sabaragamuwa University of Sri Lanka, Belihuloya, Sri Lanka; Independent Researcher, Australia; Department of Computing and Information Systems, Sabaragamuwa University of Sri Lanka, Belihuloya, Sri Lanka",2024 International Research Conference on Smart Computing and Systems Engineering (SCSE),"11 Jun 2024","2024","7","","1","5","Organizations face various challenges when analyzing tabular data. One of the key challenges is the complexity of business data analytics tasks. These tasks involve handling large volumes of data, organizing and structuring it, and extracting valuable insights. Additionally, employees who work in financial analysis often encounter a significant workload on tools such as Excel, SAP, PowerBI, and Tabula. This workload can result in increased effort and time required to analyze and make sense of the data. Organizations must address these challenges to ensure efficient and effective analysis of tabular data. Organizations spend more money to perform business tasks. Nowadays, there are many efficient models in artificial intelligence to perform text, audio, video, and image-based tasks, but no efficient models are available to perform tabular-based tasks specifically. Pandas Python library provides various functionalities and APIs that are useful for business data analysis. This research solved the problem with tabular data using Python Pandas code generation. Here, the researchers used two datasets, each with 50 records. The Large Language Model (LLM) is a supervised Learning Pre-Trained Foundation Model (SSL PFM) category based on text generation. The SSL PFM helps the language models learn the context of language and world knowledge. During this research, models such as LLaMA-2, Falcon, CodeLlama, and Mistral were considered for analysis. Each of these models consists of a Billion parameters. Moreover, Quantization techniques were incorporated to reduce model size, enabling models to load with minimal hardware. After quantizing the model, Parameter Efficient Fine Tuning (PEFT) trains the dataset using only a few model layers; other layers are frozen. Well-known experts with Pandas evaluate the fine-tuned models of chosen language models. Finally, mistral-7B produced prominent results in analyzing business tasks and producing summarized results.","2613-8662","979-8-3503-7568-8","10.1109/SCSE61872.2024.10550809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550809","DPO;Generative AI;Large Language Model;Pretrained Foundation Model;Self-supervised learning","Data analysis;Codes;Computational modeling;Organizations;Predictive models;Hardware;Libraries","","1","","20","IEEE","11 Jun 2024","","","IEEE","IEEE Conferences"
"MAD-CTI: Cyber Threat Intelligence Analysis of the Dark Web Using a Multi-Agent Framework","S. Shah; V. K. Madisetti","College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA, USA",IEEE Access,"7 Mar 2025","2025","13","","40158","40168","The dark web is a host to illicit activities where hacker forums, blogs, and articles provide significant insights into Cyber Threat Intelligence (CTI) that are frequently unavailable on the surface web. The increasing incidence of security breaches underscores the necessity for advanced CTI solutions to defend against emerging threats. This paper introduces MAD-CTI, a novel multi-agent framework based on Large Language Models (LLM) designed to extract insights from dark web sources. It independently scrapes, analyzes, and classifies content related to vulnerabilities, malware, and hacking, by leveraging a multi-agent architecture to improve efficiency, scalability, and consistency. By utilizing state-of-the-art LLM models and agents, we demonstrate how organizations can adopt this methodology to enhance the accuracy and efficiency of CTI.","2169-3536","","10.1109/ACCESS.2025.3547172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908603","Cybersecurity defense;cyber threat intelligence;dark web;hack;large language models (LLMs);malware;multi-agent systems (MAS);predictive intelligence;vulnerability","Dark Web;Cyber threat intelligence;Malware;Translation;Oral communication;Computer hacking;Blogs;Accuracy;Large language models;Computer architecture","","","","31","CCBY","3 Mar 2025","","","IEEE","IEEE Journals"
"DiCE: Distributed Code Generation and Execution","K. Rao; G. Coviello; S. Chakradhar","NEC Laboratories America, Princeton, NJ; NEC Laboratories America, Princeton, NJ; NEC Laboratories America, Princeton, NJ",2024 IEEE Conference on Pervasive and Intelligent Computing (PICom),"19 Dec 2024","2024","","","8","15","Generative artificial intelligence (GenAI), specifically, Large Language Models (LLMs), have shown tremendous potential in automating several tasks and improving human productivity. Recent works have shown them to be quite useful in writing and summarizing text (articles, blogs, poems, stories, songs, etc.), answering questions, brainstorming ideas, and even writing code. Several LLMs have emerged specifically targeting code generation. Given a prompt, these LLMs can generate code in any desired programming language. Many tools like ChatGPT, CoPilot, CodeWhisperer, Cody, DeepSeek Coder, StarCoder, etc. are now routinely being used by software developers. However, most of the prior work in automatic code generation using LLMs is focused on obtaining “correct” and working code, and mainly runs on a single computer (serial code). In this paper, we take this to the next level, where LLMs are leveraged to generate code for execution on a distributed infrastructure. We propose a novel system called DiCE, which takes serial code as input and automatically generates distributed version of the code and efficiently executes it on a distributed setup. DiCE consists of two main components (a) LLM-based tool (Synthia) to understand dependencies in serial code and automatically generate distributed version of the code using specialized programming model and semantics, and (b) Runtime (Hermod) to understand the semantics in the distributed code and realize efficient execution on a cluster of machines (distributed infrastructure). DiCE currently focuses on visual programs synthesized by tools like ViperGPT [1] and VisReP [2] (serial code), automatically identifies higher-level task parallelism opportunities (e.g., parallel object detection), transforms the code to exploit the parallelism, and finally efficiently executes it on a cluster of machines. Through our experiments using 100 examples from the GQA dataset [3], we show that the serial codes generated by ViperGPT are successfully transformed into distributed codes which are then efficiently executed on a cluster of machines by DiCE. We note that DiCE correctly identifies opportunities for parallelism and distributes tasks on separate GPUs within the cluster. We observe an average speed-up of 2X, 2.95X, and 3.7X, and an average efficiency of 1, 0.74 and 0.48 for a cluster of 2 nodes, 4 nodes, and 8 nodes, respectively.","","979-8-3315-2274-2","10.1109/PICom64201.2024.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795392","Generative Artificial Intelligence (GenAI);Large Language Models (LLM);code generation;code optimization;parallel computing;distributed computing;distributed systems;distributed runtime","Visualization;Codes;Runtime;Generative AI;Semantics;Transforms;Parallel processing;Writing;Programming;Software","","1","","24","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"LSTM-Based Assistance for People with Alzheimer’s Disease","P. K; V. B. Chitla; A. Aftab; S. Kamath","Dept. of ISE, Nitte Meenakshi Institute of Technology, Bengaluru, India; Dept. of ISE, Nitte Meenakshi Institute of Technology, Bengaluru, India; Dept. of ISE, Nitte Meenakshi Institute of Technology, Bengaluru, India; Dept. of ISE, Nitte Meenakshi Institute of Technology, Bengaluru, India","2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)","14 Mar 2025","2025","","","1","5","The study emphasizes the utilization of AI-based technologies to support individuals affected by Alzheimer’s Disease (AD) and their caregivers. The following presents a succinct overview of research papers that tackle the obstacles confronted by AD patients, along with innovative AI solutions. Explored topics encompass the creation of mobile applications, deployment of deep learning algorithms, integration of voice-activated technologies, and application of systems biology approaches. These technological interventions provide a spectrum of features including memory-boosting games, medication reminders, location tracking, cognitive exercises, and emotionalsupport. Through harnessing AI advancements, these solutions aspire to augment patient independence, aid in daily activities, enhance communication, and contribute to overall well-being. Key findings underscore the importance of user-friendly interfaces, personalized support, and seamless integration with existing healthcare systems. Additionally, privacy, security, and effective integration with clinical data emerge as crucial considerations. Ongoing research in the domain holds the promise of further advancements to positively impact the lives of those affected by AD.","","979-8-3315-1591-1","10.1109/IITCEE64140.2025.10915385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915385","Large Language Models (LLMs);Long short term memory (LSTM);Machine learning;Prompt Engineering;Alzheimer’s Disease (AD);Cognitive activities","Electric potential;Systems biology;Medical services;Games;Mobile applications;Security;Prompt engineering;Alzheimer's disease;Artificial intelligence;Long short term memory","","","","15","IEEE","14 Mar 2025","","","IEEE","IEEE Conferences"
"Automated Repair of Programs from Large Language Models","Z. Fan; X. Gao; M. Mirchev; A. Roychoudhury; S. H. Tan","National University of Singapore, Singapore; Beihang University, Beijing, China; National University of Singapore, Singapore; National University of Singapore, Singapore; Southern University of Science and Technology, Shenzhen, China",2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE),"14 Jul 2023","2023","","","1469","1481","Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.","1558-1225","978-1-6654-5701-9","10.1109/ICSE48619.2023.00128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172854","Large Language Model;Program Repair","Location awareness;Training;Analytical models;Codes;Semantics;Maintenance engineering;Programming","","138","","53","IEEE","14 Jul 2023","","","IEEE","IEEE Conferences"
"Leveraging Open Source LLMs for Software Engineering Education and Training","J. Pereira; J. -M. López; X. Garmendia; M. Azanza","Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain; Dept. of Computer Languages and Systems, University of the Basque Country, UPV/EHU, Donostia, Spain",2024 36th International Conference on Software Engineering Education and Training (CSEE&T),"10 Sep 2024","2024","","","1","10","Generative AI, particularly Large Language Models (LLMs), presents innovative opportunities to enhance software engineering education. Open source LLMs such as LLaMA and Mistral leverage the potential of generative AI offering distinct advantages over proprietary options including transparency, customizability, collaboration, and cost savings. This paper de-velops a catalog of LLM prompt examples tailored for software engineering training, mapped to knowledge areas from the Soft-ware Engineering Body of Knowledge (SWEBoK) framework. Example prompts demonstrate LLMs' capabilities in eliciting requirements, diagram generation, API simulation, effort esti-mation through role-playing, and other areas. The methodology involves evaluating prompt responses from ChatGPT, Mistral, and LLaMA on representative tasks. Quantitative and qualitative analysis assesses quality, usefulness, and correctness. Findings show ChatGPT and Mistral outperforming LLaMA overall, but no model perfectly executes complex interactions. We examine implications and challenges of integrating open source LLMs into classrooms, emphasizing the need for oversight, verification, and prompt design aligned with pedagogical objectives.","2377-570X","979-8-3503-7897-9","10.1109/CSEET62301.2024.10663055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663055","Software Engineering Education;Open Source AI Models;Large Language Models;Prompt Engineering","Training;Knowledge engineering;Costs;Generative AI;Large language models;Collaboration;Chatbots","","3","","28","IEEE","10 Sep 2024","","","IEEE","IEEE Conferences"
"Enhancing University Students’ Activities with Interactive Immediate Feedback Using Customized LLMs","V. Andonov","Department of Informatics, Faculty of Applied Informatics and Statistics, University of National and World Economy, Sofia, Bulgaria",2024 12th International Scientific Conference on Computer Science (COMSCI),"11 Dec 2024","2024","","","1","5","One of the significant issues for the students is getting timely feedback on their work so that they can understand their mistakes and improve their knowledge iteratively. There have been significant results in this area through automated evaluation, but they require a strongly defined structure of the expected results, which has its limitations. We propose using customized large language models as the core of an assignment evaluation software architecture that is used for automated evaluation and feedback generation on a variety of tasks in an introductory Java programming course. The model is implemented and tested, achieving significantly better results than using general-purpose models.","","979-8-3503-9216-6","10.1109/COMSCI63166.2024.10778529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778529","education;learning systems;large language models","Java;Software architecture;Computational modeling;Large language models;Software;Programming profession;Context modeling","","","","17","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Enhancing Network Traffic Classification with Large Language Models","H. Zhou; X. Huang; L. Deng","Department of Computer and Information Sciences, Towson University, Maryland, USA; Department of Computer and Information Sciences, Towson University, Maryland, USA; Department of Computer and Information Sciences, Towson University, Maryland, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","7282","7291","The growing complexity and volume of modern network traffic, driven by the rise of connected devices and cloud services, present significant challenges to traditional classification methods. These methods often fail to adapt to the dynamic and multifaceted nature of today’s network environments, which can compromise security and efficiency. In this paper, we present a novel approach that leverages Large Language Models (LLMs) to classify network traffic. Our proposed methodology utilizes the advanced capabilities of LLMs to understand and categorize network traffic based on their inherent patterns, enhancing the accuracy and efficiency of network analysis. First, we preprocess network traffic data by organizing it into formats compatible with LLMs. Next, we evaluate various LLMs, employing different prompts to determine their effectiveness in accurately classifying network traffic. Finally, we demonstrate the application of this LLM-driven approach in real-world scenarios, showcasing its potential to revolutionize network traffic classification. Our approach achieves an average F1-score of 0.952. In comparison with traditional machine learning-based methods, particularly Naïve Bayes, SVM, and MLP, our method outperforms them. It highlights the significant advancements in network traffic analysis achievable through the integration of LLMs, paving the way for more robust and intelligent network security solutions.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825308","Bureau of Justice Assistance; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825308","Network Traffic Analysis;Large Language Models;Machine Learning","Support vector machines;Learning systems;Intelligent networks;Accuracy;Large language models;Telecommunication traffic;Network analyzers;Network security;Complexity theory;Bayes methods","","","","26","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"An LLM-based Cross-Domain Fault Localization in Carrier Networks","J. Ma; S. Han; G. Wang; Z. Wang; F. Fan; S. Ye","University of Chinese Academy of Sciences, Beijing, China; China Unicom Research Institute, Beijing, China; China Unicom Research Institute, Beijing, China; China Unicom Research Institute, Beijing, China; China Unicom Research Institute, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)","4 Mar 2025","2024","","","731","736","The increasing complexity of carrier networks poses significant challenges for root cause analysis (RCA) and fault localization across diverse domains, including Optical Transport Networks (OTN) and IP Radio Access Networks (IPRAN). Traditional approaches, predominantly reliant on rule-based systems and manual expertise, are often inadequate to manage the scale and intricacies of modern network environments. In this paper, an intelligent method, CrossRCA, is presented to address cross-domain fault localization and RCA in carrier networks. Powered by large language models (LLMs), CrossRCA is designed to dynamically align incoming alerts with relevant diagnostic workflows, aggregate critical context-aware diagnostic information, predict incident root cause categories, and generate explanatory narratives to support network engineers. Pre-trained LLMs, augmented with prompt engineering and Retrieval Augmented Generation (RAG) techniques, are utilized to encapsulate domain-specific knowledge and enable sophisticated contextual reasoning across heterogeneous network domains. As a result, accurate and automated fault diagnosis is facilitated. Experimental evaluations conducted on real-world network fault datasets demonstrate significant improvements in diagnostic accuracy and reductions in troubleshooting time, particularly in complex cross-domain scenarios. The transformative potential of LLM-driven frameworks to advance autonomous network management and enhance operational efficiency in carrier networks is highlighted in this paper.","","979-8-3315-3122-5","10.1109/ICAIRC64177.2024.10900110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10900110","root cause analysis;large language models;fault localization","Location awareness;Knowledge engineering;Fault diagnosis;Root cause analysis;Accuracy;Retrieval augmented generation;Real-time systems;Prompt engineering;Robots;Radio access networks","","","","19","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests","A. Deljouyi; R. Koohestani; M. Izadi; A. Zaidman","Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1449","1461","Automated unit test generators, particularly searchbased software testing tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines searchbased software testing and large language models to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33 % more bugs and use up to 20 % less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bugfixing process.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00032","NWO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029767","Automated Test Generation;Large Language Models;Unit Testing;Readability;Understandability","Software testing;Industries;Large language models;Retrieval augmented generation;Computer bugs;Writing;Software;Generators;Test pattern generators;Software engineering","","","","74","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Complex Motion Planning for Quadruped Robots Using Large Language Models","X. Zhang; R. He; K. Tong; S. Man; J. Tong; H. Li; H. Zhuang","Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China",2024 IEEE International Symposium on Circuits and Systems (ISCAS),"2 Jul 2024","2024","","","1","5","Large language models (LLMs) have shown dominant performance in various language tasks, including code-writing, machine translation, and semantic comprehension. With prompt engineering, LLMs can also comprehend complex tasks and translate them into executable code. These powers offer great potential for controlling the motion of robots. In this paper, we focus on leveraging the ability of LLMs, prompt engineering, and predefined robot action APIs to facilitate high-level motion planning for quadruped robots. With LLMs, we enable the robot to autonomously plan and execute sophisticated actions based on the comprehension of effective prompts. Through various experiments and evaluations, we demonstrate the effectiveness and adaptability of our approach in handling intricate motion tasks. Our research contributes to the advancement of intelligent robotics and paves the way for more versatile quadruped robots in real-world scenarios.","2158-1525","979-8-3503-3099-1","10.1109/ISCAS58744.2024.10558349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10558349","","Codes;Accuracy;Circuits and systems;Semantics;Natural languages;Planning;Quadrupedal robots","","","","24","IEEE","2 Jul 2024","","","IEEE","IEEE Conferences"
"Licoeval: Evaluating LLMs on License Compliance in Code Generation","W. Xu; K. Gao; H. He; M. Zhou","School of Computer Science, Peking University, Beijing, China; University of Science and Technology Beijing, Beijing, China; Carnegie Mellon University, Pittsburgh, USA; School of Computer Science, Peking University, Beijing, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1665","1677","Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for “striking similarity” that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain opensource code. Based on this standard, we propose LiCoEval, to evaluate the license compliance capabilities of LLMs, i.e., the ability to provide accurate license or copyright information when they generate code with striking similarity to already existing copyrighted code. Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88 % to 2.01 %) of code strikingly similar to existing open-source implementations. Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses. These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks. Our study provides a foundation for future research and development to improve license compliance in AIassisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00052","National Natural Science Foundation of China(grant numbers:62332001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029777","code generation;llm;open-source license","Codes;Accuracy;Production;Licenses;Benchmark testing;Protection;Standards;Research and development;Software engineering;Software development management","","","","92","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Automatic Generation of Test Cases Based on Bug Reports: a Feasibility Study with Large Language Models","L. Plein; W. C. Ouédraogo; J. Klein; T. F. Bissyandé",University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg,2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","360","361","Tests suites are a key ingredient in various software automation tasks. Recently, various studies [4] have demonstrated that they are paramount in the adoption of latest innovations in software engineering, such as automated program repair (APR) [3]. Test suites are unfortunately often too scarce in software development projects. Generally, they are provided for regression testing, while new bugs are discovered by users who then describe them infor-mally in bug reports. In recent literature, a new trend of research in APR has attempted to leverage bug reports in generate-and-validate pipelines for program repair. Even in such cases, when an APR tool generates a patch candidate, if test cases are unavailable, developers must manually validate the patch, leading to a threat to validity.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3643119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554908","","Technological innovation;Computer bugs;Pipelines;Maintenance engineering;Market research;Software;Task analysis","","4","","5","CCBY","20 Jun 2024","","","IEEE","IEEE Conferences"
"Data-Secure Natural Language Interfaces to Databases via Large Language Models","D. Cakir; V. Illik; K. Y. Koc; E. Deniz","Software Engineering, Bahcesehir University, Istanbul, Turkey; Software Engineering, IBSS Technology and Software Inc., Istanbul, Turkey; Software Engineering, IBSS Technology and Software Inc., Istanbul, Turkey; Software Engineering, IBSS Technology and Software Inc., Istanbul, Turkey",2024 8th International Symposium on Innovative Approaches in Smart Technologies (ISAS),"22 Jan 2025","2024","","","1","6","Natural Language Interfaces to Databases (NLIDB) aim to allow users to query databases using natural language, eliminating the need for proficiency in formal query languages like SQL. Recent advancements in Large Language Models (LLMs) have shown promise in connecting natural language and structured query generation. This paper proposes a novel approach utilizing an LLM system to translate natural language queries into optimized SQL statements while ensuring data security by not exposing database contents to external models. The system leverages the database schema. The challenges of working with morphologically rich languages like Turkish are also discussed, along with how the proposed approach addresses these issues.","","979-8-3315-4010-4","10.1109/ISAS64331.2024.10845720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845720","Natural Language Processing;Large Language Models;SQL Query Generation;Data Security;Natural Language Interfaces to Databases","Structured Query Language;Data privacy;Translation;Databases;Large language models;Data security;Natural languages;Data models;Robustness;Personnel","","","","21","IEEE","22 Jan 2025","","","IEEE","IEEE Conferences"
"Large Language Models for Urban Mobility","Y. Hussein; M. Hemdan; M. F. Mokbel","Department of Computer Science and Engineering, University of Minnesota, Minnesota, USA; Department of Computer Science and Engineering, University of Minnesota, Minnesota, USA; Department of Computer Science and Engineering, University of Minnesota, Minnesota, USA",2025 26th IEEE International Conference on Mobile Data Management (MDM),"8 Jul 2025","2025","","","175","178","This Advanced Seminar provides a comprehensive overview of the research landscape of employing Large Language Models (LLMs) for Urban Mobility applications. The presented work in this seminar is categorized based on how LLMs are employed to serve various urban mobility applications. This goes from employing LLMs as a black box with a bit of prompt engineering, to fine-tuning LLMs to fit urban mobility applications, to completely retrain a vanilla LLM architecture with urban mobility data, to modifying the internal LLM loss function to fit urban mobility applications. The seminar concludes by presenting a set of benchmarking and evaluation work while pointing out to research gaps, open problems, and future research directions for employing LLMs to urban mobility applications.","2375-0324","979-8-3315-2569-9","10.1109/MDM65600.2025.00041","NSF(grant numbers:IIS-2203553); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11058433","","Seminars;Large language models;Benchmark testing;Prompt engineering","","","","53","IEEE","8 Jul 2025","","","IEEE","IEEE Conferences"
"Aligning the Objective of LLM-Based Program Repair","J. Xu; Y. Fu; S. H. Tan; P. He","The Chinese University of Hong Kong, Shenzhen, China; Chongqing University, China; Concordia University, Canada; The Chinese University of Hong Kong, Shenzhen, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2548","2560","Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations. In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10 % and reduces the patch sampling number by 90 %. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00169","Guangdong Basic and Applied Basic Research Foundation(grant numbers:2024A1515010145); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029731","Automated Program Repair;Large Language Model;Objective Alignment","Training;Location awareness;Fault diagnosis;Large language models;Computer bugs;Debugging;Maintenance engineering;Object recognition;Software engineering","","","","73","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Integrating Large Language Models for Task Planning in Robots ‐ A case Study with NAO","S. Abpeikar; M. Garratt; K. Kasmarik; S. Anavatti; R. Islam","School of Engineering and Technology, University of New South Wales, Canberra, Australia; School of Engineering and Technology, University of New South Wales, Canberra, Australia; School of Systems and Computeing, University of New South Wales, Canberra, Australia; School of Engineering and Technology, University of New South Wales, Canberra, Australia; School of Engineering and Technology, University of New South Wales, Canberra, Australia",2024 6th International Conference on Control and Robotics (ICCR),"25 Mar 2025","2024","","","217","225","This paper uses prompt engineering to integrate large language models for verbal and physical task planning in robots via human verbal commands. Recently, large language models have garnered significant interest in diverse research areas. Due to the potential of large language models, their utilization in robot task planning, and controlling robots, is growing fast. This paper presents a mechanism consisting of three subsystems that enable robots to interact with human verbal commands, and respond by verbal and physical task planning. The large language model uses speech recognition to receive human requests. Then it decides to perform a ‘VERBAL’ or ‘PHYSICAL’ action in response. LLM then generates an answer for a ‘VERBAL’ action or a behavior tree for a ‘PHYSICAL’ action. The behavior tree subsequently translates into robot physical movement using an execution module. This module provides a joint setup to enable the robot to perform low-level actions. The experiments show that this mechanism accomplishes verbal and physical task planning for the NAO humanoid robot as a case study.","","979-8-3315-1815-8","10.1109/ICCR64365.2024.10927585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10927585","Large Language Models;Behavior Tree;Human-Robot Interaction;Task Planning;Humanoid Robot","Translation;Large language models;Humanoid robots;Human-robot interaction;Speech recognition;Planning;Prompt engineering","","","","32","IEEE","25 Mar 2025","","","IEEE","IEEE Conferences"
"A Digital Twin Modeling Code Generation Framework based on Large Language Model","J. Dong; L. Ren","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China",IECON 2024 - 50th Annual Conference of the IEEE Industrial Electronics Society,"10 Mar 2025","2024","","","1","4","Digital twin has made significant achievements in applications across various fields by bridging the gap between virtual and physical world. However, the modeling of digital twin faces challenges of labor-intensive and time-consuming manual modeling or coding. With the emergence of artificial intelligence foundation models, the integration of large language models (LLMs) and digital twin may contribute to enhancing modeling efficiency significantly. Therefore, in this paper, a framework of digital twin modeling code generation is proposed based on LLMs. Given instructions in natural language prompts, modeling code in digital twin software will be generated from LLMs. Firstly, a tree diagram-based code generation method is proposed to generate main section of modeling code. Additionally, a randomized code completion method is proposed to complete missing parts which are unspecified or unclear in prompts. Finally, a case study is given in NVIDIA Omniverse for a digital twin construction based on framework proposed in this paper.","","978-1-6654-6454-3","10.1109/IECON55916.2024.10905976","Research and Development; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10905976","digital twin;large language model;modeling and simulation;code generation;AI foundation model","Industrial electronics;Solid modeling;Codes;Three-dimensional displays;Foundation models;Large language models;Natural languages;Manuals;Software;Digital twins","","","","13","IEEE","10 Mar 2025","","","IEEE","IEEE Conferences"
"Deductive Software Architecture Recovery via Chain-of-thought Prompting","S. A. Rukmono; L. Ochoa; M. R. V. Chaudron","Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands",2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),"29 Oct 2024","2024","","","92","96","As software evolves, software architecture recovery techniques can help for effective maintenance. We envision a deductive software architecture recovery approach supported by Large Language Models (LLMs). Unlike existing inductive (bottom-up) recovery techniques, which reconstruct architecture by considering the properties observed at implementation level, our top-down approach starts with architectural properties and seeks their manifestations in the implementation. It employs a known Reference Architecture (RA) and involves two phases: RA definition and code units classification. A proof-of-concept with GPT-4 emulates deductive reasoning via chain-of-thought prompting. It demonstrates the deductive SAR approach, applying it to the Android application K-9 Mail and achieving a 70% accuracy in classifying 54 classes and 184 methods. The future plans focus on evaluating and refining the approach through ground-truth assessments, deeper exploration of reference architectures, and advancing toward automated human-like software architecture explanations. We highlight the potential for LLMs in achieving more comprehensive and explainable software architecture recovery.","2832-7632","979-8-4007-0500-7","10.1145/3639476.3639776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10727142","software architecture;software architecture recovery;deductive SAR;chain-of-thought prompting","Accuracy;Software architecture;Source coding;Natural languages;Semantics;Computer architecture;Syntactics;Software;System implementation;Postal services","","2","","23","CCBY","29 Oct 2024","","","IEEE","IEEE Conferences"
"Automatic Prompt Generation Based on Combinatorial Relationships in Reasoning Problems","T. Wu; H. He","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; State Key Lab of Advanced Optical Communication System and Network School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China",2024 6th International Conference on Natural Language Processing (ICNLP),"4 Oct 2024","2024","","","192","196","The effectiveness of Large Language Models (LLMs) in tasks involving reasoning is significantly influenced by the structure and formulation of the prompts, contemporary research in prompt engineering aims to help LLMs better understand the paradigms of reasoning question (e.g., CoT). However, these efforts have either struggled to effectively incorporate external knowledge into single prompt or integrating entire corpus information, often fails to significantly enhance the reasoning capabilities of LLMs. This paper introduces a novel prompting method that incorporates implicit hints that represent logical combinatorial relationships between known conditions in reasoning problems, guiding LLMs to think correctly in the initial steps of reasoning for such problems. Extensive and comprehensive experiment results on four different reasoning problem datasets indicate that our proposed method improved accuracy while maintaining efficiency.","","979-8-3503-4911-5","10.1109/ICNLP60986.2024.10692467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10692467","Large Language Models;Prompt Engineering;Natural Language Reasoning","Accuracy;Large language models;Predictive models;Cognition;Natural language processing;Prompt engineering","","","","17","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"CLAF-IoT: Context-Aware LLMs-Enhanced Authentication Framework for Internet of Things","A. Rehman; K. A. Awan; M. U. Hassan; A. Shaikh; A. Alqazzaz; K. Cengiz","Department of Information Technology, University of Haripur, Haripur, Pakistan; Department of Information Technology, University of Haripur, Haripur, Pakistan; Department of Computer Skills Deanship of Preparatory Year, Najran University, Najran, Saudi Arabia; Department of Information Systems and the Emerging Technologies Research Laboratory (ETRL), College of Computer Science and Information Systems, Najran University, Najran, Saudi Arabia; College of Computing and Information Technology, University of Bisha, Bisha, Saudi Arabia; Department of Electrical Engineering, Prince Mohammad Bin Fahd University, Al Khobar, Saudi Arabia",IEEE Internet of Things Journal,"8 Jul 2025","2025","12","14","28639","28646","The significant increase in the number of Internet of Things (IoT) devices in various domains requires robust and adaptive authentication mechanisms. Existing methods often fail to address the dynamic and heterogeneous nature of the IoT ecosystem, resulting in significant security vulnerabilities. This article presents a context-aware LLM-enhanced authentication framework (CLAF-IoT) that dynamically adjusts authentication protocols based on real-time environmental and user-specific contexts. Using the advanced contextual understanding and generation capabilities of large language models (LLMs), the proposed framework enhances both security and usability in highly dynamic IoT environments. Key components include environmental context sensing, user behavior analysis, adaptive authentication protocols, real-time threat detection, and federated learning integration for continuous improvement and privacy preservation. Experimental evaluations demonstrate that CLAF-IoT achieves higher authentication accuracy in different scenarios, 11.11% false acceptance rate and 9.09% false rejection rate.","2327-4662","","10.1109/JIOT.2025.3567634","Deanship of Graduate Studies and Scientific Research at University of Bisha through the Fast-Track Research Support Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10990157","Authentication;federated learning;Internet of Things (IoT);large language models (LLMs);privacy;security","Internet of Things;Authentication;Security;Protocols;Real-time systems;Biometrics;Measurement;Sensors;Training;Privacy","","","","26","IEEE","7 May 2025","","","IEEE","IEEE Journals"
"Towards Automated Verification of IP and COTS: Leveraging LLMs in Pre- and Post-Silicon Stages","S. Paria; A. Dasgupta; S. Bhunia","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",2025 IEEE 43rd VLSI Test Symposium (VTS),"10 Jun 2025","2025","","","1","5","Modern computing systems rely on System-on-Chips (SoCs) to integrate multiple Intellectual Property (IP) cores developed in-house or acquired from third-party vendors with varying trust levels. Commercial Off-The-Shelf (COTS) components, such as microcontrollers and FPGAs, offer ready-made solutions but introduce security risks, especially in an untrusted supply chain. Effective verification of both IP cores and COTS components is essential for ensuring functionality, security, and reliability. Traditional IP verification techniques are often complex and error-prone due to over-reliance on manual efforts, while COTS verification poses significant challenges due to their inherent black-box nature and diverse integrity issues. The emergence of Large Language Models (LLMs) significantly enhances hardware verification by automating tasks such as code generation and bug fixing. In this paper, we present a review of LLM-based IP verification methods and discuss challenges in current verification practices. Next, we propose an LLM-driven workflow that generates test programs for COTS verification and demonstrate its effectiveness through experimental analysis on open-source COTS processors.","2375-1053","979-8-3315-2144-8","10.1109/VTS65138.2025.11022781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022781","LLMs;Hardware Verification;IP;SVA;Formal Verification;COTS;Architectural Events","Program processors;Reviews;Microcontrollers;Supply chains;Very large scale integration;Hardware;System-on-chip;Security;Reliability;Prompt engineering","","","","33","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques","A. Lee; S. Moon; M. Jhon; J. -W. Kim; D. -K. Kim; J. E. Kim; K. Park; E. Jeon","Honam Research Division, Electronics and Telecommunications Research Institute; Honam Research Division, Electronics and Telecommunications Research Institute; Department of Psychiatry, Chonnam National University Hwasun Hospital; Department of Psychiatry, Chonnam National University Hospital; Department of Psychiatry, Chonnam National University Hospital; Honam Research Division, Electronics and Telecommunications Research Institute; MegaWorks Co., Ltd.; Honam Research Division, Electronics and Telecommunications Research Institute",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","7080","7082","Recent advancements in large language models (LLMs) have opened new avenues in psychological counseling. This study leverages LLMs to develop chatbots capable of conducting empathetic and personalized counseling sessions by applying various prompt engineering techniques, including zero-shot, few-shot, meta-learning, Chain of Thought, and our newly developed Empathetic Meta-Chain (EMC) method. The EMC method demonstrated superior performance in empathy, response accuracy, interaction continuity, fluency, and understanding, as confirmed by expert evaluations. By integrating advanced empathetic strategies, the EMC chatbot significantly enhances its ability to support users' mental well-being through natural and engaging counseling interactions. These findings highlight the potential of LLM-based counseling chatbots to serve as effective tools in mental health support.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822158","Electronics and Telecommunications Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822158","prompt engineering;large language model;chatbot","Employee welfare;Metalearning;Measurement;Refining;Mental health;Oral communication;Chatbots;Electromagnetic compatibility;Prompt engineering;Mirrors","","","","10","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"Optimizing NLP Model Deployment: A Comparative Analysis of Fine-Tuning, Few-Shot Learning, and Prompt Engineering Strategies","A. M. Jain; A. F. Augustine; B. S. Telaprolu","Independent Researcher, Seattle, USA; Cloud Engineering, Pegasystems, Washington, DC, USA; Northeastern University, CA, USA","2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","23 Jun 2025","2025","","","1892","1903","As large language models continue to revolutionize natural language processing, the trade-offs between performance, computational efficiency, and resource requirements become increasingly critical. This paper presents a comprehensive evaluation of three prominent approaches: fine-tuning, few-shot learning, and zero-shot learning using sentiment analysis as a case study. We benchmark the efficacy of these methods across multiple dimensions, including accuracy, inference time, computational cost, and adaptability to new domains. Our experiments, conducted on a state-of-the-art NVIDIA H100 GPU and extrapolated to AWS cloud infrastructure, reveal nuanced insights into the practical implications of deploying these approaches. Fine-tuning achieved the highest accuracy ($\mathbf{7 0. 0 6 \%}$) but at substantial computational cost ($344.10$ for 5 iterations). Few-shot learning emerged as a compelling alternative, reaching 65.05 % accuracy with minimal resource usage ($17.95$ for 5 iterations) and demonstrating strong adaptability. Surprisingly, zero-shot learning with prompts showed decreased performance compared to no-prompt baselines, challenging common assumptions about prompt engineering while using a fine tuned model. These findings have far-reaching implications for the deployment of language models across various applications. We provide a decision-making framework that balances performance gains against resource constraints, offering valuable guidance for practitioners and researchers alike. Our research not only contributes to the ongoing dialogue about the most effective ways to leverage large language models but also highlights the need for continued innovation in resource-efficient NLP techniques. As the field evolves, the insights from this study will help inform strategies for developing more adaptable, efficient, and powerful language processing systems.","","979-8-3315-2228-5","10.1109/AINIT65432.2025.11035028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035028","Natural Language Processing;Large Language Models;Fine-tuning;few-shot learning;zero-shot learning;prompt engineering;cost optimization;performance evaluation;resource allocation;Sentiment Analysis;Cost-benefit Analysis","Sentiment analysis;Adaptation models;Technological innovation;Accuracy;Large language models;Zero shot learning;Organizations;Computational efficiency;Prompt engineering;Few shot learning","","","","23","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Prompt Driven Image Creation: A Comparative Evaluation of Generative AI Tools","S. Das; D. Santra; T. Chhari; S. Roy; S. Mukherjee","Department of Computer Science and Engineering, Institute of Engineering and Management (IEM), University of Engineering and Management Kolkata (UEM); Department of Computer Science and Engineering, Institute of Engineering and Management (IEM), University of Engineering and Management Kolkata (UEM); Department of Computer Science and Engineering, Institute of Engineering and Management (IEM), University of Engineering and Management Kolkata (UEM); Department of Computer Science and Engineering, Institute of Engineering and Management (IEM), University of Engineering and Management Kolkata (UEM); Department of Computer Science and Engineering, Institute of Engineering and Management (IEM), University of Engineering and Management Kolkata (UEM)","2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)","16 Apr 2025","2025","","","1","6","This paper presents a comparative analysis of four generative AI models namely ChatGPT, Gemini, Copilot, and Stable Diffusion - evaluated on metrics such as visual quality, prompt adherence, creativity, usability, and processing time. Visual and quantitative results highlight Gemini and Copilot's superiority in artistic and imaginative tasks, while Stable Diffusion excels in customization for advanced users. ChatGPT demonstrates ease of use but is limited in complexity. We also provides the best practices in selecting and using such tools to improve creative work. This study emphasizes the importance of evaluating generative AI tools based on diverse requirements and practical use cases.","2767-9934","979-8-3315-2076-2","10.1109/IEMENTech65115.2025.10959444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10959444","Generative AI;Image Generation;Prompt Engineering;Digital Art;Visual Creativity;AI Tools Comparison","Measurement;Visualization;Image synthesis;Digital art;Chatbots;Complexity theory;Prompt engineering;Usability;Creativity;Best practices","","","","14","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"AI-Powered Code Reviews: Leveraging Large Language Models","M. S. S. Chowdhury; M. N. U. R. Chowdhury; F. F. Neha; A. Haque","Department of Computer Science, Utah State University, Logan, UT, USA; Department of Computer Science, New Mexico Tech, Socorro, NM, USA; Department of Applied Mathematics, University of Dhaka, Dhaka, Bangladesh; Department of Computer Sceince, New Mexico Tech, Socorro, New Mexico, USA",2024 International Conference on Signal Processing and Advance Research in Computing (SPARC),"10 Jan 2025","2024","1","","1","6","As the complexity and volume of software development continue to grow, the need for efficient and thorough code review processes becomes increasingly critical. This paper explores the integration of Large Language Models (LLMs), such as ChatGPT and Bard, into code review workflows to enhance software quality and security. By leveraging the natural language processing capabilities of LLMs, we aim to streamline the identification of code issues, detect potential security vulnerabilities, and provide developers with actionable feedback. Through a comprehensive analysis of current literature, case studies, and experimental data, this study evaluates the impact of AI-assisted code reviews on developer productivity and code quality. We also address the challenges and limitations of relying on LLMs, including context comprehension and potential biases. Our findings suggest that while LLMs offer significant advantages in automating and improving code reviews, they should complement rather than replace human expertise. This paper provides insights into best practices for integrating LLMs into development workflows, ultimately contributing to more robust and secure software systems.","","979-8-3503-8520-5","10.1109/SPARC61891.2024.10829223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829223","Large Language Model;Code Review;Software Engineering;Artificial Intelligence;Machine Learning;Natural Language Processing","Ethics;Technological innovation;Codes;Reviews;Large language models;Collaboration;Software quality;Software systems;Security;Software development management","","1","","21","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"Prompting Literature Review: The Impact of Transformer Models and ChatGPT on Prompt Engineering and AI Research Publications","D. C. Leal; A. Capetillo; D. G. Castorena","School of Engineering and Sciences, Tecnologico de Monterrey, Monterrey, N.L., México; School of Engineering and Sciences, Tecnologico de Monterrey, Monterrey, N.L., México; School of Engineering and Sciences, Tecnologico de Monterrey, Monterrey, N.L., México",2025 Institute for the Future of Education Conference (IFE),"13 Jun 2025","2025","","","1","6","Artificial intelligence (AI) has significantly impacted our daily lives, particularly through advancements in conversational interfaces enabled by large language models. This state-of-the-art literature review examines the evolution of the prompt phenomenon, focusing on key developments such as the Transformer model and ChatGPT. By analyzing the growth in related research publications from the Scopus database across three periods—pre-Transformer (1967–2016), post-Transformer (2017–2021), and post-ChatGPT (2022-2024)—this study highlights the exponential growth of prompt engineering as a critical area of AI research. The findings reveal how prompt engineering has evolved from a basic computational concept into a recognized engineering discipline, with implications for both technical and social sciences. The review also proposes future research directions, including the exploration of non-linear prompting and prompt-oriented behavior engineering, to further enhance human-AI interactions and optimize generative AI systems.","","979-8-3503-5523-9","10.1109/IFE63672.2025.11024600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024600","Prompt;Prompt engineering;AI;Transformer model;ChatGPT;Literature review","Computational modeling;Large language models;Social sciences;Education;Focusing;Transformers;Chatbots;Prompt engineering;Artificial intelligence;Systematic literature review","","","","10","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Multi-agent based systems on micro grid — A review","P. Kiran; K. R. M. V. Chandrakala; T. N. P. Nambiar","Department of Electrical and Electronics Engineering, Amrita University, India; Department of Electrical and Electronics Engineering, Amrita University, India; Department of Electrical and Electronics Engineering, Amrita University, India",2017 International Conference on Intelligent Computing and Control (I2C2),"22 Mar 2018","2017","","","1","6","The intelligence required for the smart operation of micro grid can be realized by using Multi-Agent Systems (MAS). Micro grids will be an integral part of future electricity grid offering integration of DERs, advanced storage and demand management. This paper presents a detailed survey and review report of Multi-Agent based systems on Microgrid. Various advantages and challenges on Multi-Agent Technology in research years is discussed. This survey report will help the researchers to know the recent developments undertaken in this application.","","978-1-5386-0374-1","10.1109/I2C2.2017.8321880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8321880","Micro grid;Multi-Agent;Java Agent Development Frame;Foundation of Intelligent and Physical Agent;Distributed Energy Resources;Deregulation;Electricity Market","Microgrids;Electricity supply industry;Multi-agent systems;Reliability;Tools;Load modeling;Computational modeling","","11","","68","IEEE","22 Mar 2018","","","IEEE","IEEE Conferences"
"Security Protection in Cooperative Control of Multi-agent Systems","S. Sun; Y. Mo","Department of Automation and BNRist, Tsinghua University, Beijing, P. R. China; Department of Automation and BNRist, Tsinghua University, Beijing, P. R. China",2021 40th Chinese Control Conference (CCC),"6 Oct 2021","2021","","","4696","4701","Due to the wide application of average consensus algorithm, its security and privacy problems have attracted great attention. In this paper, we consider the system threatened by a set of unknown agents that are both ""malicious"" and ""curious"", who add additional input signals to the system in order to perturb the final consensus value or prevent consensus, and try to infer the initial state of other agents. At the same time, we design a privacy-preserving average consensus algorithm equipped with an attack detector with a time-varying exponentially decreasing threshold for every benign agent, which can guarantee the initial state privacy of every benign agent, under mild conditions. The attack detector will trigger an alarm if it detects the presence of malicious attackers. An upper bound of false alarm rate in the absence of malicious attackers and the necessary and sufficient condition for there is no undetectable input by the attack detector in the system are given. Specifically, we show that under this condition, the system can achieve asymptotic consensus almost surely when no alarm is triggered throughout the execution, and an upper bound of convergence rate and some quantitative estimates about the error of final consensus value are given. Finally, numerical case is used to illustrate the effectiveness of some theoretical results.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549998","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549998","multi-agent systems;average consensus;security protection;intrusion detection","Privacy;Sufficient conditions;Upper bound;Detectors;Consensus algorithm;Control systems;Security","","2","","17","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Requirements Verification Through the Analysis of Source Code by Large Language Models","J. O. Couder; D. Gomez; O. Ochoa","Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America",SoutheastCon 2024,"24 Apr 2024","2024","","","75","80","In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.","1558-058X","979-8-3503-1710-7","10.1109/SoutheastCon52093.2024.10500073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500073","Verification;Software Engineering;Software Requirements;ChatGPT;Large Language Model;GPT-3.5","Codes;Software design;Source coding;Software;Mathematical models;Robustness;Planning","","2","","33","IEEE","24 Apr 2024","","","IEEE","IEEE Conferences"
"Augmenting the Generality and Performance of Large Language Models for Software Engineering","F. C. Peña","Faculty of Computer Science and Mathematics, University of Passau",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","196","198","Large Language Models (LLMs) are revolutionizing software engineering (SE), with special emphasis on code generation and analysis. However, their applications to broader SE practices including conceptualization, design, and other non-code tasks, remain partially underexplored. This research aims to augment the generality and performance of LLMs for SE by (1) advancing the understanding of how LLMs with different characteristics perform on various non-code tasks, (2) evaluating them as sources of foundational knowledge in SE, and (3) effectively detecting hallucinations on SE statements. The expected contributions include a variety of LLMs trained and evaluated on domain-specific datasets, new benchmarks on foundational knowledge in SE, and methods for detecting hallucinations. Initial results in terms of performance improvements on various non-code tasks are promising.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00059","German Research Foundation(grant numbers:524228075); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024366","Large Language Model (LLM);software engineering;benchmarking;hallucination detection","Codes;Large language models;Benchmark testing;Software engineering","","","","30","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Adaptive Probabilistic Operational Testing for Large Language Models Evaluation","A. Asgari; A. Guerriero; R. Pietrantuono; S. Russo","Department of Software Technology, Delft University of Technology, Delft, The Netherlands; Dipartimento di Ingegneria Elettrica e Delle Tecnologie Dell’Informazione, Università Degli Studi di Napoli Federico II, Napoli, Italy; Dipartimento di Ingegneria Elettrica e Delle Tecnologie Dell’Informazione, Università Degli Studi di Napoli Federico II, Napoli, Italy; Dipartimento di Ingegneria Elettrica e Delle Tecnologie Dell’Informazione, Università Degli Studi di Napoli Federico II, Napoli, Italy",2025 IEEE/ACM International Conference on Automation of Software Test (AST),"21 Jul 2025","2025","","","103","113","Large Language Models (LLM) empower many modern software systems, and are required to be highly accurate and reliable. Evaluating LLM poses challenges due to the high costs of manual labeling and of validation of labeled data.This study investigates the suitability of probabilistic operational testing for effective and efficient evaluation of LLM, focusing on a case study with DistilBERT. To this aim, we adopt an existing framework (DeepSample) for Deep Neural Network (DNN) testing and adapt it to the LLM domain by introducing auxiliary variables tailored to LLM and classification tasks.Through a comprehensive evaluation, we demonstrate how sampling-based operational testing can yield reliable LLM accuracy estimates and effectively expose failures, or, under testing budget constraints, it can find a trade off between accuracy estimation and failure exposure. The experimental results, using DistilBERT on three sentiment analysis datasets, show that sampling-based methods can provide cost effective and reliable operational accuracy assessment for LLM. These findings offer practical insights for testers and help address critical gaps in current LLM evaluation practices.","2833-9061","979-8-3315-0179-2","10.1109/AST66626.2025.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081715","Software testing;Large Language Models;Sampling;LLM evaluation","Software testing;Accuracy;Costs;Large language models;Artificial neural networks;Probabilistic logic;Minimization;Software systems;Entropy;Testing","","","","20","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Formal Trust and Threat Modeling Using Large Language Models","Z. Yao","Computer Science Department, New Jersey Institute of Technology, Newark, New Jersey, USA",2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops),"18 Mar 2025","2024","","","232","239","Security modeling, including trust and threat modeling, is a critical process of modern system design and analysis. However, the models are often described in imprecise natural languages, and their inconsistent interpretations and implementations can lead to cybersecurity incidents. In this work, we first introduce an extended Linear Temporal Logic to model the multi-faceted security model of a system to capture its temporal and spatial properties and security guarantees. Then, we manually write 10 security model formulas of real-world systems and attack scenarios. Finally, we fine-tune a large language model with our manually written models. We evaluate the fine-tuned model with another set of 9 recent system designs to validate its capability in accurately capturing their security models. Our work provides a formal approach to system security modeling, and it demonstrates the benefits of using large language models in capturing the models of real-world systems.","","979-8-3315-3281-9","10.1109/ACSACW65225.2024.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917671","Large Language Models;Trust Modeling;Threat Modeling;Formal Methods","Threat modeling;Annotations;Computational modeling;Large language models;Conferences;Natural languages;Security;Logic;Computer security;System analysis and design","","","","72","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Research on Distributed Node Resource Optimization Mechanism for Multi-Agent Systems Combined with Blockchain Technology","Z. Yin; B. Bai; Y. Liu; T. Cheng","China Academy of Information and Communications Technology, Beijing, China; China Institute of Communications, Beijing, China; China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China",2024 IEEE International Conference on Blockchain (Blockchain),"18 Sep 2024","2024","","","536","541","This paper explores integrating blockchain technology into multiagent systems (MAS) to enhance distributed node resource optimization. Key challenges addressed include task decision-making, task allocation, and resource scheduling, with a focus on minimizing energy consumption and latency. Blockchain ensures secure, efficient coordination among nodes, mitigating issues like data privacy leaks and system failures. The study also leverages federated learning for secure decentralized machine learning model training. Simulation results demonstrate the enhanced performance, security, and scalability of MAS with blockchain, paving the way for more efficient distributed computing environments.","2834-9946","979-8-3503-5159-0","10.1109/Blockchain62396.2024.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664344","Blockchain;MultiAgent Systems (MAS);Distributed Resource Optimization;Federated Learning;Secure Coordination","Energy consumption;Federated learning;Simulation;Scalability;Decision making;Blockchains;Resource management","","","","12","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations","C. Tony; M. Mutas; N. E. D. Ferreyra; R. Scandariato","Institute of Software Security, Hamburg University of Technology, Germany; Institute of Software Security, Hamburg University of Technology, Germany; Institute of Software Security, Hamburg University of Technology, Germany; Institute of Software Security, Hamburg University of Technology, Germany",2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR),"12 Jul 2023","2023","","","588","592","Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE’s Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by LLMs. As a practical application, we show how LLMSecEval can be used for evaluating the security of snippets automatically generated from NL descriptions.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174231","LLMs;code security;NL prompts;CWE","Computer languages;Codes;Natural languages;Programming;Software;Security;Data mining","","15","","18","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence","J. Shao; J. Tong; Q. Wu; W. Guo; Z. Li; Z. Lin; J. Zhang","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong 999077, China",Journal of Communications and Information Networks,"2 Jul 2024","2024","9","2","99","112","The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in large language models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.","2509-3312","","10.23919/JCIN.2024.10582827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10582827","large language models;multi-modal models;wireless communications;power allocation;spectrum sensing;protocol understanding","Wireless communication;Adaptation models;Wireless sensor networks;Data models;Communication system security;Task analysis;Sensors","","16","","","","2 Jul 2024","","","PTP","PTP Journals"
"Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics","S. Jia; R. Lyu; K. Zhao; Y. Chen; Z. Yan; Y. Ju; C. Hu; X. Li; B. Wu; S. Lyu","University at Buffalo, State University of New York, Buffalo, USA; Williamsville East High School, Buffalo, USA; The Chinese University of Hong Kong, Shenzhen, China; The Chinese University of Hong Kong, Shenzhen, China; The Chinese University of Hong Kong, Shenzhen, China; University at Buffalo, State University of New York, Buffalo, USA; University at Albany, State University of New York, Albany, USA; University at Albany, State University of New York, Albany, USA; The Chinese University of Hong Kong, Shenzhen, China; University at Buffalo, State University of New York, Buffalo, USA",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"27 Sep 2024","2024","","","4324","4333","DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.","2160-7516","979-8-3503-6547-4","10.1109/CVPRW63382.2024.00436","National Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10677972","Deepfake Detection;Multimodal Large Language Models;Media Forensics;GPT4V","Deepfakes;Machine learning algorithms;Forensics;Large language models;Conferences;Training data;Focusing","","11","","42","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"Prompt Chain Engineering for Customer Satisfaction Analysis via Large Language Models","X. Wang; X. Wang; D. Zheng","Sichuan University of Media and Communications, China; Sichuan University of Media and Communications, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, China",2025 8th World Conference on Computing and Communication Technologies (WCCCT),"13 Jun 2025","2025","","","463","467","In the contemporary era of technological evolution, the swift progress of large language models (LLMs) has democratized the realm of data analysis, rendering it more accessible to a broader audience. This work delves into the innovative application of LLMs within the sphere of satisfaction analysis, with a primary objective of empowering non-specialists to harness the potential of these advanced models for their analytical endeavors. The core of this study lies the concept of prompt chain and prompt engineering, which serves as a strategic approach to guide the interaction between users and LLM. By meticulously delineating the operational procedures, parameter configurations, and nuanced use cases of LLMs in satisfaction analysis, this paper establishes a comprehensive prompt framework. This framework is designed to be user-friendly, enabling non-experts to navigate the complexities of LLMs with relative ease. To further validate the efficacy of this approach, we integrate SPSS statistical analysis as a benchmark for comparison. The performance and outcomes of satisfaction analysis conducted via the designed prompt chain are rigorously compared with those obtained through traditional SPSS methods.","","979-8-3315-1262-0","10.1109/WCCCT65447.2025.11027978","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11027978","Large language models;Satisfaction analysis;Prompt chain engineering for non-experts","Data analysis;Navigation;Large language models;Customer satisfaction;Linear regression;Rendering (computer graphics);Stability analysis;Software;Reliability;Prompt engineering","","","","12","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Cross-Level Requirements Tracing Based on Large Language Models","C. Ge; T. Wang; X. Yang; C. Treude","Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; Faculty of Computing, Harbin Institute of Technology, Harbin, China; School of Computing and Information Systems, Singapore Management University, Singapore",IEEE Transactions on Software Engineering,"17 Jul 2025","2025","51","7","2044","2066","Cross-level requirements traceability, linking high-level requirements (HLRs) and low-level requirements (LLRs), is essential for maintaining relationships and consistency in software development. However, the manual creation of requirements links necessitates a profound understanding of the project and entails a complex and laborious process. Existing machine learning and deep learning methods often fail to fully understand semantic information, leading to low accuracy and unstable performance. This paper presents the first approach for cross-level requirements tracing based on large language models (LLMs) and introduces a data augmentation strategy (such as synonym replacement, machine translation, and noise introduction) to enhance model robustness. We compare three fine-tuning strategies—LoRA, P-Tuning, and Prompt-Tuning—on different scales of LLaMA models (1.1B, 7B, and 13B). The fine-tuned LLMs exhibit superior performance across various datasets, including six single-project datasets, three cross-project datasets within the same domain, and one cross-domain dataset. Experimental results show that fine-tuned LLMs outperform traditional information retrieval, machine learning, and deep learning methods on various datasets. Furthermore, we compare the performance of GPT and DeepSeek LLMs under different prompt templates, revealing their high sensitivity to prompt design and relatively poor result stability. Our approach achieves superior performance, outperforming GPT-4o and DeepSeek-r1 by 16.27% and 16.8% in F-measure on cross-domain datasets. Compared to the baseline method that relies on prompt engineering, it achieves a maximum improvement of 13.8%.","1939-3520","","10.1109/TSE.2025.3572094","High Quality Development Special Project(grant numbers:CEIEC-2024-ZM02-0067); National Natural Science Foundation of China(grant numbers:61977020); Key technical projects of ShenZhen(grant numbers:JSGG2021110892802003); Basic Research Project(grant numbers:JCKY2021204B025); Natural Science Foundation of Hei Longjiang Province(grant numbers:LH2019F046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008781","Requirements tracing;large language models;fine-tuning;data augmentation;software requirements","Feature extraction;Semantics;Deep learning;Information retrieval;Data augmentation;Software;Vectors;Training;Large language models;Accuracy","","","","35","IEEE","21 May 2025","","","IEEE","IEEE Journals"
"Semantic API Alignment: Linking High-Level User Goals to APIs","R. Feldt; R. Coppola","Chalmers University of Technology, Göteborg, Sweden; Politecnico di Torino, Turin, Italy",2025 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE),"13 Jun 2025","2025","","","17","20","Large Language Models (LLMs) are becoming key in automating and assisting various software development tasks, including text-based tasks in requirements engineering but also in coding. Typically, these models are used to automate small portions of existing tasks, but we present a broader vision to span multiple steps from requirements engineering to implementation using existing libraries. This approach, which we call Semantic API Alignment (SEAL), aims to bridge the gap between a user's high-level goals and the specific functions of one or more APIs. In this work in progress paper, we propose a system architecture where a set of LLM-powered “agents” match such high-level objectives with appropriate API calls. This system could facilitate automated programming by finding matching links or, alternatively, explaining mismatches to guide manual intervention or further development. As an initial pilot, our paper demonstrates this concept by applying LLMs to Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for aligning with REST API specifications, specifically through a case study involving a GitHub statistics API. We discuss the potential of our approach to enhance complex tasks in software development and requirements engineering and outline future directions for research.","","979-8-3315-3864-4","10.1109/NLBSE66842.2025.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029426","Large Language Models;Goal-Oriented Requirements Engineering;API Alignment","Large language models;Semantics;Systems architecture;Manuals;Computer architecture;Seals;Software;Requirements engineering;Software engineering;Software development management","","","","9","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Assessment of Pre-Trained Large Language Models for Hardware Trojan Detection in RTL Designs","G. -W. Wan; S. -Z. Wong; D. Liu; X. Wang",NCTIEDA; NCTIEDA; Amazon Web Services; Southeast University,2024 IEEE LLM Aided Design Workshop (LAD),"1 Oct 2024","2024","","","1","1","With the successful application of large language models (LLMs) in hardware design and bug fixing, pre-trained LLMs may offer a potential solution for HT detection on the user side. For this purpose, we have carefully selected and implemented a lightweight HT benchmark for LLM at the RTL level based on Trust-Hub, which we call HTEval-mini, and includes 12 designs. We designed a unified prompt template and selected four LLMs, including the SOTA model Claude3, for multiple iterations pass@10 tests. Among all models, the highest detection accuracy and classification success rate were shown to be 66.67% and 50%, respectively. Finally, We have analyzed the results and provided insights for future development.","","979-8-3503-7608-1","10.1109/LAD62341.2024.10691751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691751","","Accuracy;Large language models;Conferences;Computer bugs;Benchmark testing;Hardware;Trojan horses","","1","","3","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Adversarial Prompt Optimization in LLMs: HijackNet’s Approach to Robustness and Defense Evasion","L. Fu; K. Shih; J. Cao; S. Wu; Y. Jin; Z. Yang","Independent Researcher, San Jose, USA; Tsinghua University, Beijing, China; San Francisco State University, San Francisco, USA; University of Rochester, Scarborough, Canada; University of Illinois at Urbana-Champaign, Champaign, USA; University of Illinois at Urbana-Champaign, Champaign, USA",2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT),"3 Jun 2025","2025","","","2081","2085","This paper introduces HijackNet, an adversarial prompt hijacking framework designed to improve hijacking success rates and generalization in multi-lingual tasks. Combining Neural Architecture Search (NAS), Reinforcement Learning (RL), and Multi-Objective Optimization (MOO), HijackNet generates adversarial prompts that adapt to target tasks while bypassing model defense mechanisms. This framework optimizes hijacking success, defense evasion, and task completion through a combination of a prompt generator and adversarial optimizer. We introduce a Defense Robustness Score (DRS) to evaluate the stability of prompts in adversarial settings. HijackNet improves adversarial prompt generation, advancing the security and robustness of large language models in adversarial environments.","","979-8-3315-4285-6","10.1109/ISCAIT64916.2025.11010431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11010431","Adversarial Prompt Hijacking;Large Language Models;Neural Architecture Search;Reinforcement Learning;Multi-Objective Optimization","Large language models;Reinforcement learning;Robustness;Natural language processing;Stability analysis;Neural architecture search;Multilingual;Security;Information technology;Optimization","","","","9","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Extracting Domain Models from Textual Requirements in the Era of Large Language Models","S. Arulmohan; M. -J. Meurs; S. Mosser","McMaster University, CAS & McSCert, Hamilton, Canada; Université du Québec à Montréal, CIRST, Montreal, Canada; McMaster University, CAS & McSCert, Hamilton, Canada",2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),"22 Dec 2023","2023","","","580","587","Requirements Engineering is a critical part of the software lifecycle, describing what a given piece of software will do (functional) and how it will do it (non-functional). Requirements documents are often textual, and it is up to software engineers to extract the relevant domain models from the text, which is an error-prone and time-consuming task. Considering the recent attention gained by Large Language Models (LLMs), we explored how they could support this task. This paper investigates how such models can be used to extract domain models from agile product backlogs and compare them to (i) a state-of-practice tool as well as (ii) a dedicated Natural Language Processing (NLP) approach, on top of a reference dataset of 22 products and 1, 679 user stories. Based on these results, this paper is a first step towards using LLMs and/or tailored NLP to support automated requirements engineering thanks to model extraction using artificial intelligence.","","979-8-3503-2498-3","10.1109/MODELS-C59198.2023.00096","Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350787","Domain Modeling;Natural Language Processing;Large Language Models;Concept Extraction;User stories","Training;Visualization;Costs;Natural language processing;Software;Model driven engineering;Requirements engineering","","30","","17","IEEE","22 Dec 2023","","","IEEE","IEEE Conferences"
"RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs","N. Jiang; Y. Wu","Purdue University, West Lafayette, USA; Purdue University, West Lafayette, USA",2024 IEEE/ACM International Workshop on Automated Program Repair (APR),"2 Sep 2024","2024","","","58","60","Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM -generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.","","979-8-4007-0577-9","10.1145/3643788.3648020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10653037","Automated Program Repair;Large Language Model","Codes;Large language models;Conferences;Computer bugs;Maintenance engineering;Market research;Python","","","","27","CCBY","2 Sep 2024","","","IEEE","IEEE Conferences"
"Align Is Not Enough: Multimodal Universal Jailbreak Attack Against Multimodal Large Language Models","Y. Wang; W. Hu; Y. Dong; J. Liu; H. Zhang; R. Hong","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Institute of Automation, Chinese Academy of Science, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China",IEEE Transactions on Circuits and Systems for Video Technology,"9 Jun 2025","2025","35","6","5475","5488","Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.","1558-2205","","10.1109/TCSVT.2025.3526248","National Natural Science Foundation of China(grant numbers:62306098,U23B2031); Open Projects Program of State Key Laboratory of Multimodal Artificial Intelligence Systems; Fundamental Research Funds for the Central Universities(grant numbers:JZ2024HGTB0256); National Natural Science Foundation, Singapore under its AI Singapore Programme(grant numbers:AISG2-RP-2021-022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829683","Multimodal large language models;adversarial attack;jailbreak attack","Safety;Large language models;Electronic mail;Watermarking;Robustness;Biological system modeling;Visualization;Circuits and systems;Social networking (online);Reviews","","","","45","IEEE","6 Jan 2025","","","IEEE","IEEE Journals"
"LLMs for Microservice Generation: Capabilities, Challenges, and Advancements","R. Spista; B. Crispo; P. Giorgini; A. Marchetto; G. Riccardi","Computer Science and Information Engineering Department, University of Trento, Italy; Computer Science and Information Engineering Department, University of Trento, Italy; Computer Science and Information Engineering Department, University of Trento, Italy; Computer Science and Information Engineering Department, University of Trento, Italy; Computer Science and Information Engineering Department, University of Trento, Italy","2025 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)","7 Aug 2025","2025","","","25","32","This paper examines the transformative potential of Large Language Models (LLMs) in automating the generation of microservices, highlighting their capabilities, applications, and associated challenges. While LLMs such as GPT-3, GPT-4, and GPT-4o demonstrate remarkable advancements in automating software engineering tasks, offering enhanced productivity, scalability, and efficiency, critical challenges persist. These include dependency management, adherence to architectural patterns, and mitigating security vulnerabilities like SQL Injection and improper error handling. Through systematic experimentation, this paper evaluates the performance of LLMs across properties such as correctness, scalability, security, and efficiency. Key contributions include the demonstration of GPT-4o’s notable advancements in generating scalable and secure microservices, driven by enhanced training methodologies, curated datasets, and security-aware prompts. The paper also emphasizes strategies for overcoming remaining challenges, proposing a roadmap for advancing LLMs as reliable tools in modern software development practices, particularly in security-critical and scalable microservice architectures.","2834-8249","979-8-3315-8649-2","10.1109/IAICT65714.2025.11100313","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11100313","Large Language Models;Microservices;Automated Code Generation","Training;Productivity;Codes;Systematics;Scalability;Large language models;Microservice architectures;Software reliability;Security;Software engineering","","","","55","IEEE","7 Aug 2025","","","IEEE","IEEE Conferences"
"Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study","K. Tamberg; H. Bahsi","School of Information Technologies, Tallinn University of Technology, Tallinn, Estonia; School of Information Technologies, Tallinn University of Technology, Tallinn, Estonia",IEEE Access,"18 Feb 2025","2025","13","","29698","29717","Despite various approaches being employed to detect software vulnerabilities, the number of reported software vulnerabilities shows an upward trend over the years. This suggests the problems are not caught before the code is released, which could be caused by many factors, like lack of awareness, limited efficacy of the existing vulnerability detection tools or the tools not being user-friendly. To help combat some issues with traditional vulnerability detection tools, we propose using large language models (LLMs) to assist in finding vulnerabilities in source code. LLMs have shown a remarkable ability to understand and generate code, underlining their potential in code-related tasks. The aim is to test multiple state-of-the-art LLMs and identify the best prompting strategies, allowing extraction of the best value from the LLMs. We leverage findings from prompting-focused research, benchmarking approaches like chain of thought, tree of thought and self-consistency for vulnerability detection use-cases. We provide an overview of the strengths and weaknesses of the LLM-based approach and compare the results to those of traditional static analysis tools. We find that LLMs can pinpoint more issues than traditional static analysis tools, outperforming traditional tools in terms of recall and F1 scores. However, LLMs are more prone to generate false positive classifications than traditional tools. The experiments are conducted using the Java programming language and the results should benefit software developers and security analysts responsible for ensuring that the code is free of vulnerabilities.","2169-3536","","10.1109/ACCESS.2025.3541146","Estonian Centre of Excellence in Artificial Intelligence (EXAI); Haridus- ja Teadusministeerium(grant numbers:TK213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879492","Benchmarking;large language models;LLM;prompting;software vulnerabilities;static code analyser","Codes;Static analysis;Security;Benchmark testing;Software;Java;Costs;Manuals;Large language models;Deep learning","","5","","64","CCBY","11 Feb 2025","","","IEEE","IEEE Journals"
"The Development of CanPrompt Strategy in Large Language Models for Cancer Care","N. Ahmad; E. Mamatjan; T. Wali; Y. Mamatjan","Faculty of Science, Thompson Rivers University, Canada; MamatjanLab, Thompson Rivers University, Canada; School of Information Technology, Carleton University, Canada; Faculty of Science, Thompson Rivers University, Canada",2024 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),"8 Oct 2024","2024","","","1","6","Background: The recent revolution in Large Language Models (LLMs) is transforming industries, enhancing communication, and reshaping research methodologies. LLMs have found significant applications across various sectors, notably in finance for stock market predictions, and in healthcare, where complex medical data is analyzed for diagnosis at an early stage, improving diagnostic procedures, and personalized treatment planning. In healthcare, where complex medical data is analyzed for diagnosis at an early stage. Despite the immense potential, challenges such as overwhelming Big Data, model hallucinations, and ethical concerns about patient privacy and bias persist. Method: We implemented novel strategies like CanPrompt to mitigate the accuracy and hallucination concerns to ensure responsible deployment. The CanPrompt strategy utilizes prompt engineering combined with few-shot and in-context learning to significantly enhance model accuracy by generating more relevant answers. The models were tested against a specialized dataset from MedQuAD, focusing on cancer, and evaluated using metrics like ROUGE and BERTScore to assess the semantic and syntactic accuracy of generated responses against validated ""Gold Answers"". Through this approach, the study seeks to outline the potential and limitations of LLMs in improving cancer care. Result: After applying CanPrompt with models Mistral 7x8b, Falcon 40b, and Llama 3-8b, BERTScore results showed Mistral leading with an accuracy around 84%, Falcon slightly lower, and Llama the least, with respective precision scores also reflecting a similar trend. Conclusion: The study demonstrates the promise of LLMs in cancer care through the introduction of CanPrompt.","2994-9408","979-8-3503-5663-2","10.1109/CIBCB58642.2024.10702147","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10702147","Large Language Model (LLM);Cancer;prompt;ROGUE score;BERTScore;Mistral 7x8B;Falcon 40b;and Llama 3-8b","Accuracy;Large language models;Semantics;Medical services;Syntactics;Planning;Prompt engineering;Stock markets;Medical diagnostic imaging;Cancer","","2","","12","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Healthcare data interoperability with Large Language Models","Á. R. Tornel",Universidad de Murcia,2025 IEEE 13th International Conference on Healthcare Informatics (ICHI),"22 Jul 2025","2025","","","658","658","Data interoperability is crucial for making the most of available information, and it becomes even more valuable when it comes to healthcare data. In this field, collaboration and interaction among different medical environments are key to quickly and effectively managing diseases. Moreover, the collaboration of various medical organizations and the smooth exchange of information among them can be used to improve research quality and clinical practice. However, this task is quite complex due to the great diversity of the data collected and its different nature. This information involves both structured and unstructured data, capturing all clinical events of patients. Medical interoperability standards emerge to try to mitigate and solve this problem by representing clinical information in a standardized manner and common format. However, there is a wide range of interoperability standards in this field, which creates significant difficulty when exchanging information. Standards such as HL7 FHIR [1], OMOP, HL7 CDA, etc., lead to a lack of a clear standardization goal since each organization can choose any of the existing standards. Additionally, the process of standardizing medical data is very challenging and complex. This is because those responsible for this transformation must have deep knowledge of the medical field, as well as extensive experience and understanding of the interoperability standard to be used. For these reasons, and due to the emergence of disruptive technologies like Large Language Models (LLMs), it is proposed to leverage these tools to address this issue and try to provide a method that eases or even overcomes this challenge. In this context, this work is being developed. A methodology is being created that is capable of representing structured medical data using a medical standard, specifically, HL7 FHIR, through the use of LLMs. The methodology has been evaluated using the MIMIC-IV dataset [2], starting from a single table where each column represents an aspect of the data. Its goal is as follows: starting from a single table that compiles clinical information, where each row represents an attribute of this clinical data, the methodology transforms the data into HL7 FHIR. In this way, the corresponding HL7 FHIR model has been identified for each row; that is, which resource and attributes are needed to represent the input clinical information. To achieve this, different techniques such as unsupervised clustering and prompt engineering combined with Retrieval Augmented Generation (RAG) are implemented to interact with the LLM. The results obtained in this process show a 94% accuracy in identifying the correct HL7 FHIR resources and a 70.15% accuracy in identifying the right HL7 FHIR attributes. These are very promising results for this challenge. Currently, this methodology is being applied in a real use case to study its impact. All of this is part of the European project (https://www.resqplus.eu), which aims to standardize a stroke registry using HL7 FHIR and SNOMED CT, thereby facilitating its interoperability on an international level. However, there are several limitations and challenges that are proposed to be addressed in order to increase both the robustness and quality of the methodology. The main issue identified is the lack of a broad context that explains the source data, which hinders the performance and generalization of the LLM. It is also necessary to mention the difficulty in relating the different resources, since each HL7 FHIR resource has many interrelationships. This creates a need for a global view of the data before carrying out the process of identifying resources and attributes related to the interoperability standard.Regarding the future of this work, improvement paths are proposed, such as implementing support for a larger number of medical interoperability standards (HL7 CDA, OMOP, etc.) and allowing the use of unstructured data, which would considerably increase the tool’s adaptability. Additionally, finetuning processes for the LLM are being considered, which would give the model a greater capacity for generalization and ensure improved performance. Moreover, the use of opensource LLMs is proposed to enable the platform to scale and the tool to be more customizable. The development of a user-friendly and intuitive interface is currently underway with the aim of making a tool accessible to any type of user. With the advances described and the future directions proposed, this work proposes and implements a method for automating or semi-automating the process of medical data interoperability. Therefore, it increases the possibility of conducting studies and research using data from various medical sources, as well as improves the interaction among different health entities—ultimately enhancing the quality of treatment and patient care.","2575-2634","979-8-3315-2094-6","10.1109/ICHI64645.2025.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081538","hl7 fhir;mimic;large language model;clustering;retrieval augmented generation;data interoperability","Accuracy;Large language models;Standards organizations;Retrieval augmented generation;Collaboration;Organizations;Transforms;Robustness;Clinical diagnosis;Interoperability","","","","2","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"LLM Prompt Engineering for Automated White-Box Integration Test Generation in REST APIs","A. M. Rincon; A. M. R. Vincenzi; J. P. Faria","Campus Paraíso do Tocantins, Federal Institute of Tocantins (IFTO), Paraíso do Tocantins, Brazil; Department of Computing (DC), Federal University of São Carlos (UFSCar), São Carlos, Brazil; INESC TEC, Faculty of Engineering, University of Porto, Porto, Portugal","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","21","28","This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90% coverage, although at higher costs. Additionally, combining test sets from different models increased coverage, reaching 96% in some cases. We also compared the results with EvoMaster, a specialized tool for generating tests for REST APIs, where LLM-generated tests achieved comparable or higher coverage in the benchmark projects. Despite higher execution costs, LLMs demonstrated superior adaptability and flexibility in test generation.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962507","Prompt Engineering;Large Language Models;Automated Integration Testing;White-Box Testing;REST APIs","Software testing;Costs;Codes;Large language models;Conferences;Restful API;Benchmark testing;Prompt engineering;Test pattern generators;Glass box","","","","22","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"Comparing the Effectiveness of Generative AI for Learning and Developing Flutter Application","P. Heng; K. Yongsiriwit; P. Chaisiriprasert","College of Digital Innovation Technology, Rangsit University, Thailand; College of Digital Innovation Technology, Rangsit University, Thailand; College of Digital Innovation Technology, Rangsit University, Thailand",2024 8th International Conference on Information Technology (InCIT),"30 Dec 2024","2024","","","746","751","The rapid growth of business demands modern technological advancements, leading to an increased need for accelerated learning and development of tools. Generative AI has become a key player in enhancing these areas by aiding in code generation for application development. Specifically, Generative AI can produce functional code for various programming languages, aiding in the setup of UI components, navigation, and complex state management. This study evaluates the effectiveness of three widely-used Generative AI tools— ChatGPT, Copilot, and Codeium—chosen for their popularity and diverse approaches to code generation. Standardized prompts were used to generate Flutter code for beginner, intermediate, and advanced tasks. The results show that ChatGPT outperformed other tools, consistently generating runnable and comprehensive code, while Copilot and Codeium exhibited some limitations in handling complex tasks. These findings suggest that integrating Generative AI into Flutter development can significantly accelerate the coding process and enhance application quality.","","979-8-3503-6630-3","10.1109/InCIT63192.2024.10810490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810490","Generative AI;Code generation;Flutter;Prompt engineering;Cross-platform","Computer languages;Codes;Generative AI;Navigation;Chatbots;Encoding;Information technology;Business","","","","11","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Understanding the Effectiveness of Coverage Criteria for Large Language Models: A Special Angle from Jailbreak Attacks","S. Zhou; T. Li; K. Wang; Y. Huang; L. Shi; Y. Liu; H. Wang","Huazhong University of Science and Technology, Wuhan, China; Nanyang Technological University, Singapore; Huazhong University of Science and Technology, Wuhan, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Huazhong University of Science and Technology, Wuhan, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","730","742","Large language models (LLMs) have revolutionized artificial intelligence, but their increasing deployment across critical domains has raised concerns about their abnormal behaviors when faced with malicious attacks. Such vulnerability alerts the widespread inadequacy of pre-release testing. In this paper, we conduct a comprehensive empirical study to evaluate the effectiveness of traditional coverage criteria in identifying such inadequacies, exemplified by the significant security concern of jailbreak attacks. Our study begins with a clustering analysis of the hidden states of LLMs, revealing that the embedded characteristics effectively distinguish between different query types. We then systematically evaluate the performance of these criteria across three key dimensions: criterion level, layer level, and token level. Our research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries, aligning with our clustering experiments. Leveraging these findings, we propose three practical applications of coverage criteria in the context of LLM security testing. Specifically, we develop a realtime jailbreak detection mechanism that achieves high accuracy (93.61 % on average) in classifying queries as normal or jailbreak. Furthermore, we explore the use of coverage levels to prioritize test cases, improving testing efficiency by focusing on high-risk interactions and removing redundant tests. Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities. This study improves our understanding of LLM security testing, enhances their safety, and provides a foundation for developing more robust AI applications.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029795","large language models;coverage criteria;jailbreak attacks","Systematics;Large language models;Neurons;Buildings;Focusing;Real-time systems;Safety;Security;Testing;Software engineering","","","","49","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Automating Code Review","R. Tufano","SEART @ Software Institute, Università della Svizzera italiana (USI), Switzerland",2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"12 Jul 2023","2023","","","192","196","Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, code review comes at the cost of spending developers' time on reviewing their teammates' code. The goal of this research is to investigate the possibility of using Deep Learning (DL) to automate specific code review tasks. We started by training vanilla Transformer models to learn code changes performed by developers during real code review activities. This gives the models the possibility to automatically (i) revise the code submitted for review without any input from the reviewer; and (ii) implement changes required to address a specific reviewer's comment. While the preliminary results were encouraging, in this first work we tested DL models in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. Thus, in a subsequent work, we exploited a pre-trained Text- To- Text-Transfer-Transformer (T5) to overcome some of these limitations and experiment DL models for code review automation in more realistic and challenging scenarios. The achieved results show the improvements brought by T5 both in terms of applicability (i.e., scenarios in which it can be applied) and performance. Despite this, we are still far from performance levels making these techniques deployable in practice, thus calling for additional research in this area, as we discuss in our future work agenda.","2574-1934","979-8-3503-2263-7","10.1109/ICSE-Companion58688.2023.00053","European Research Council (ERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172885","Code Review;Deep Learning","Training;Deep learning;Codes;Automation;Costs;Computer bugs;Transformers","","3","","35","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Transformer and Large Language Models for Automatic Multiple-Choice Question Generation: A Systematic Literature Review","H. W. Awalurahman; R. Fathoni Aji; I. Budi","Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia",IEEE Access,"25 Jul 2025","2025","13","","127100","127112","Developing multiple-choice questions manually requires a lot of time and effort. Automatic multiple-choice question generation is one of the solutions to alleviate the problem. The research in automatic multiple-choice question generation has been growing with the recent use of Transformer and Large-Language Models. However, existing literature reviews have not thoroughly covered the recent advances in methods and evaluation conducted on the multiple-choice question generation domain. This research conducted a systematic literature review on multiple-choice question generation using Transformer and Large Language Models. This research aims to discover recent methods and evaluation strategies that have been used in the domain. We obtained 28 primary studies. We presented a taxonomy covering strategy of using the Transformer and Large Language Models for multiple-choice question generation, including fine-tuning and prompt engineering with zero-shot, few-shot, chain-of-thought, and retrieval augmented generation. Primary studies used either or both automatic and manual evaluation for the generated questions from Transformer and LLM. We found that studies are still primarily in English, with few studies utilizing learning components such as learning objective, limited use of chain-of-thought, retrieval augmented generation, and open problem in automatic evaluation.","2169-3536","","10.1109/ACCESS.2025.3590423","Kementerian Pendidikan Tinggi, Sains, dan Teknologi Republik Indonesia through Pendidikan Magister menuju Doktor untuk Sarjana Unggul (PMDSU); Kemdiktisaintek(grant numbers:070/C3/DT.05.00/PL/2025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11084803","Transformer;large language models;multiple-choice question generation;systematic literature review","Transformers;Question generation;Systematic literature review;Large language models;Deep learning;Data models;Databases;Planning;Quality assessment;Libraries","","","","42","CCBY","18 Jul 2025","","","IEEE","IEEE Journals"
"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance","H. Jin; Z. Sun; H. Chen",University of Sydney; University of Sydney; University of Sydney,2024 IEEE International Conference on Agents (ICA),"24 Dec 2024","2024","","","136","141","Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent research in prompt engineering have enhanced LLMs’ understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs’ need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers seldom produce flawless code on their first attempt. Instead, iterative feedback and debugging are heavily leveraged to refine the programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a $9.8 \%$ improvement on the HumanEval dataset and a $\mathbf{1 6. 2 \%}$ improvement on the MBPP dataset compared to the state-of-theart approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs’ ability to generate and refine code autonomously.","","979-8-3315-3991-7","10.1109/ICA63002.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807454","Large Language Models;Code Generation;Automatic Debugging;Multi-Agent System;Code Debugging","Codes;Accuracy;Large language models;Refining;Failure analysis;Debugging;Computer architecture;Iterative methods;Prompt engineering;Software development management","","","","39","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models","H. Yao; J. Lou; Z. Qin","Zhejiang University, Hangzhou, China; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou, China; Zhejiang University, Hangzhou, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","7745","7749","Prompts have significantly improved the performance of pre-trained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model’s normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present PoisonPrompt, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of PoisonPrompt through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446267","Prompt learning;Backdoor attacks;Large language model","Signal processing;Predictive models;Robustness;Acoustics;Security;Task analysis;Speech processing","","15","","18","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Brand Voice Alignment in Chatbots: Comparison of Prompt Engineering and LLM Fine-tuning","S. Sharma; N. Afzal","ZoomInfo, Waltham, USA; Takeda Pharmaceuticals, Cambridge, USA",2025 IEEE International Conference on Electro Information Technology (eIT),"12 Aug 2025","2025","","","249","255","This paper examines two primary methodologies for achieving brand voice consistency in AI chatbots: prompt engineering and fine-tuning of Large Language Models (LLMs). As organizations increasingly use chatbots for customer interaction, maintaining a consistent brand voice has become crucial for customer experience and brand identity and brand reputation. We compare the effectiveness, scalability, and implementation challenges of prompt engineering versus LLM fine-tuning approaches in achieving authentic brand voice representation. Through experimental analysis, we evaluate factors such as tone accuracy, response consistency, and implementation complexity. Our findings provide insights into the optimal methodology selection based on organizational needs, resource availability, and desired brand voice complexity.","2154-0373","979-8-3315-3233-8","10.1109/eIT64391.2025.11103650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11103650","Natural language processing (NLP);large language models (LLMs);fine-tuning;chatbots","Large language models;Scalability;Organizations;Oral communication;Chatbots;Complexity theory;Prompt engineering;Cost benefit analysis;Information technology;Investment","","","","23","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Fine Tuning of large language Models for Arabic Language","A. Tamer; A. -A. Hassan; A. Ali; N. Salah; W. Medhat","Nile University, Giza, Egypt; Nile University, Giza, Egypt; Nile University, Giza, Egypt; Nile University, Giza, Egypt; Nile University, Giza, Egypt",2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA),"2 Apr 2024","2023","","","1","4","In recent years, Long language models have made significant progress, enabling machines to interpret and process human language. However, the Arabic language presents unique challenges due to its rich morphology and diverse sentence structures. The development of specialized language models for Arabic question answering has implications for improved human- computer interaction, cultural preservation, and accessibility. This paper aims to enhance the comprehension and contextual understanding of Arabic-posed questions by leveraging the capabilities of the LLaMa language model and the XLNet transformer. The ARCD dataset, which mainly consists of an Arabic dataset for question-answering, was used to fine- tune the LLaMa 2.0 and XLNet. By utilizing LLaMA and XLNet transformers separately, This paper contributes to the construction of an NLP pipeline that can properly understand and process Arabic text to provide answers depending on a particular Arabic context by using LLaMA and XLNet transformers individually. It is important to note that Arabic datasets were not previously used to train the LLaMa language model. The LLaMA language model received accuracy scores of 93.70","2161-5330","979-8-3503-1943-9","10.1109/AICCSA59173.2023.10479346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479346","Language Model;Arabic Language;Prompt Engineering","Human computer interaction;Computational modeling;Morphology;Transformers;Question answering (information retrieval);Cultural differences;Reliability","","1","","11","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Responsible Generative AI for Software Development Life Cycle","J. A. Shah; G. Bajpai","Cyber Security Engineering Canada DevOps Community of Pracitce & IEEE YP Toronto, Ontario, Canada; Sofware Development & AI Engineering Canada DevOps Community of Pracitce Ottawa, Ontario, Canada",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0056","0061","Software practitioners are driving a paradigm shift in software engineering practices by integrating generative AI technology into software development and lifecycle management. Integration of generative AI to plan, design, develop, test and maintain software brings productivity gains and enables rapid software releases, however it also presents ethical challenges. This paper examines strategies for developing software through integration of responsible Generative AI that endures, emphasizing primarily the ethical considerations, and the responsible use of Generative technology. It covers the benefits and challenges of collaborative development with responsible Generative AI technologies. The paper focuses on responsible use of generative AI considerations which are likely to induce software integrity and trust. The paper presents best practices, audits, assessments and benchmarking concepts for Gen AI integrated software development and lifecycle management. Subsequently, the paper highlights the importance of safeguarding the integrity of the software development lifecycle through incorporating responsible AI principles, mainly fairness, bias mitigation, privacy and data security, transparency and accountability. Lastly, presenting recommendation for built-in and add-on capabilities for responsible use of GenAI integration into SDLC which paves the way to the trusted ecosystem of GenAI integration for software practitioners.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105309","Artificial Intelligence;Generative AI;Software Development Life Cycle;Software Engineering;Security","Productivity;Ethics;Technological innovation;Generative AI;Reviews;Biological system modeling;Organizations;Software;Software development management;Software engineering","","","","13","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Prompt Engineering ChatGPT for Codenames","M. Sidji; M. Stephenson","Faculty of Engineering and Information Technology, The University of Melbourne, Melbourne, Australia; College of Science and Engineering, Flinders University, Adelaide, Australia",2024 IEEE Conference on Games (CoG),"28 Aug 2024","2024","","","1","4","The word association game Codenames challenges the AI community with its requirements for multimodal language understanding, theory of mind, and epistemic reasoning. Previous attempts to develop AI agents for the game have focused on word embedding techniques, which while good with other models using the same technique, can sometimes suffer from brittle performance when paired with other models. Recently, Large Language Models (LLMs) have demonstrated enhanced capabilities, excelling in complex cognitive tasks, including symbolic and common sense reasoning. In this paper, we compare a range of recent prompt engineering techniques for GPT-based Codenames agents. While there was no significant game score improvement over the baseline agent, we did observe qualitative changes in agents’ strategies suggesting that further refinement has potential for score improvement. We also propose a revised Codenames AI competition specifically focusing on the use of LLM agents.","2325-4289","979-8-3503-5067-8","10.1109/CoG60054.2024.10645591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645591","Codenames;ChatGPT;Prompt Engineering;Game Playing Agents;AI","Large language models;Focusing;Games;Benchmark testing;Chatbots;Prompt engineering;Task analysis","","","","22","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Fin-TuneNet: A Hierarchical Fine-Tuning Framework for Domain-Specific NLP Optimization","F. Lei; Q. Xu; Y. Jin; Y. Tian; K. Shih; K. Lu","TikTok, San Jose, USA; University at Buffalo, The State University of New York, New York, USA; University of Illinois at Urbana-Champaign, Champaign, USA; Independent Researcher, Redmond, USA; Tsinghua University, Beijing, China; Independent Researcher, Sunnyvale, USA","2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","15 Jul 2025","2025","","","1553","1557","The challenges of financial natural language processing (NLP) are significant. Specialized terminology, complex dependencies, and strict compliance rules make deploying large language models (LLMs) in finance difficult. Existing models like GPT-3, FinBERT, and T5-Financial show potential but fail to generalize well across financial tasks. This paper presents Fin-TuneNet, a hierarchical fine-tuning framework to improve LLMs for financial applications. It integrates task-specific adapters, multi-scale attention mechanisms, and domain-guided prompt engineering. Fin-TuneNet aligns better with tasks and achieves robustness while keeping computational costs low. This frame-work is an efficient and domain-specific solution for applying LLMs in financial NLP.","","979-8-3315-0796-1","10.1109/NNICE64954.2025.11064437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11064437","Kerwords-Financial NLP;Task-Specific Adapters;Multi-Scale Attention;Domain-Guided Prompt Engineering;Large Language Models","Adaptation models;Attention mechanisms;Terminology;Large language models;Finance;Natural language processing;Robustness;Computational efficiency;Prompt engineering;Optimization","","","","11","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"LLM Based Biological Named Entity Recognition from Scientific Literature","S. J. Jung; H. Kim; K. S. Jang","Division of DG and AI Business, En-Core Inc., Seoul, South Korea; Division of DT Business, En-Core Inc., Seoul, South Korea; Division of DG and AI Business, En-Core Inc., Seoul, South Korea",2024 IEEE International Conference on Big Data and Smart Computing (BigComp),"11 Apr 2024","2024","","","433","435","Recently, the application of Large Language Models (LLMs) in the field of natural language processing has witnessed remarkable growth, revolutionizing the field of bioinformatics by automating the extraction of biological entities from scientific literature. This study presents the development and evaluation of a Biological Named Entity Recognizer (BNER) using a pre-trained Large Language Model (LLM) refined through prompt engineering. The BNER was tailored to identify proteins, genes, and small molecules within scientific texts, specifically targeting the context of p53 protein-related research. To assess the BNER's efficacy, we curated a dataset comprising ten paragraphs extracted from the abstracts and significant sections of five high-relevance scientific papers. The system's performance was quantified through an entity recognition task, resulting in 51 true positives (TP), 10 false positives (FP), and 3 false negatives (FN). The BNER achieved an F1 score of 0.887, demonstrating a high degree of precision and recall. These results validate the utility of LLMs in bioinformatics and highlight the BNER's potential to support and accelerate scientific discovery by providing accurate, structured data outputs suitable for comprehensive analysis.","2375-9356","979-8-3503-7002-7","10.1109/BigComp60711.2024.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488292","Biological Named Entity Recognition (BNER);Large Language Models (LLM);Prompt Engineering;p53 Protein","Proteins;Pathology;Annotations;Biological system modeling;Computational modeling;System performance;Data models","","2","","9","IEEE","11 Apr 2024","","","IEEE","IEEE Conferences"
"The Plastic Surgery Hypothesis in the Era of Large Language Models","C. S. Xia; Y. Ding; L. Zhang","University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","522","534","Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00047","NSF(grant numbers:CCF-2131943,CCF-2141474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298499","","Training;Codes;Computer bugs;Surgery;Manuals;Static analysis;Maintenance engineering","","12","","92","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Invited Paper: VerilogEval: Evaluating Large Language Models for Verilog Code Generation","M. Liu; N. Pinckney; B. Khailany; H. Ren",NVIDIA Corporation; NVIDIA Corporation; NVIDIA Corporation; NVIDIA Corporation,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),"30 Nov 2023","2023","","","1","8","The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.","1558-2434","979-8-3503-2225-5","10.1109/ICCAD57390.2023.10323812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323812","","Codes;Design automation;Benchmark testing;Hardware;Distance measurement;Combinational circuits;Hardware design languages","","61","","34","IEEE","30 Nov 2023","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models for High-Level Synthesis","A. Rizvi; N. Simon; J. Tocho; A. Yongaci; S. Abi-Karam; C. Hao",NA; NA; NA; NA; NA; NA,2024 IEEE Opportunity Research Scholars Symposium (ORSS),"2 Oct 2024","2024","","","49","52","Large language models (LLMs) are comprehensive tools, that are capable of modeling natural languages, such as human reasoning, and code structures, and even more recently, using expansive collective databases are even now able to create generated information based on a given user prompts. Given the extensive capabilities of LLMs and their diverse knowledge in many fields, this paper explores the use of LLMs for hardware design. Specifically improving LLM’s capability for code editing and generation of C++ code targeting high level synthesis (HLS) for FPGA design, as well as creating generated code based on a programmable prompt. We built a frame work to interact with LLM tools. Results indicate that there is success in creating an external framework for zero-shot code editing, indicating that the approach is valid with minor model performance. Future work looks to improve the framework’s accuracy for the current model and expand usage to other models as well as code generation.","","979-8-3503-9069-8","10.1109/ORSS62274.2024.10697938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10697938","","Codes;Databases;Large language models;Natural languages;Hardware;Data models;Cognition;Planning;High level synthesis;Field programmable gate arrays","","1","","7","IEEE","2 Oct 2024","","","IEEE","IEEE Conferences"
"YABLoCo: Yet Another Benchmark for Long Context Code Generation","A. Valeev; R. Garaev; V. Lomshakov; I. Piontkovskaya; V. Ivanov; I. Adewuyi","Research Center of the Artificial Intelligence Institute, Innopolis University, Russia; Research Center of the Artificial Intelligence Institute, Innopolis University, Russia; St. Petersburg Department of the Steklov, Institute of Mathematics, Russia; Huawei Noah’s Ark Lab; Research Center of the Artificial Intelligence Institute, Innopolis University, Russia; Research Center of the Artificial Intelligence Institute, Innopolis University, Russia",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","88","95","Large Language Models (LLMs) demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code (LoC). At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions’ bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C/C++. The dataset as well as the code for the evaluation pipeline are available at https://github.com/yabloco-codegen/yabloco-benchmark","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00016","Innopolis University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028318","Large Language Models for Code;Code Generation Benchmarks;Large Repository Context;C/C++","Measurement;Visualization;Codes;Large language models;Conferences;Pipelines;Benchmark testing;Programming;Metadata;Software","","1","","19","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Enhancing Educational Websites With AI Chatbots: Design Considerations for Safety","D. Gupta; P. Burke; V. Phalke; N. Khanduja","The Harker School, San Jose, California, USA; University of California at Berkeley, Berkeley, California, USA; RAFT, San Jose, California, USA; RAFT, San Jose, California, USA","2024 Conference on AI, Science, Engineering, and Technology (AIxSET)","3 Dec 2024","2024","","","327","329","Generative AI and Large Language Models provide an immense opportunity to add value to educational organizations and websites. However, a software engineering challenge exists to integrate GenAI while factoring in the requirements of educational sites. We describe a pipeline for testing generative AI chatbots for an academic site [1]. We describe our requirements, which we believe will be represented in other educational sites, and our approach to test case generation, prompt engineering, and response assessments across various Large Language Models. We also present case study results and pipeline code in an open-source repository [14].","","979-8-3503-9099-5","10.1109/AIxSET62544.2024.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10770876","Generative AI;Chatbots;Large Language Models;Software Engineering;Education","Codes;Large language models;Pipelines;Organizations;Chatbots;Safety;Prompt engineering;Testing;Software engineering","","","","14","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Lessons from the Use of Natural Language Inference (NLI) in Requirements Engineering Tasks","M. Fazelnia; V. Koscinski; S. Herzog; M. Mirakhorli","Department of Computer Science, University of Hawaii at Manoa; Global Cybersecurity Institute, Rochester Institute of Technology; Global Cybersecurity Institute, Rochester Institute of Technology; Department of Computer Science, University of Hawaii at Manoa",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","103","115","We investigate the use of Natural Language Inference (NLI) in automating requirements engineering tasks. In particular, we focus on three tasks: requirements classification, identification of requirements specification defects, and detection of conflicts in stakeholders' requirements. While previous research has demonstrated significant benefit in using NLI as a universal method for a broad spectrum of natural language processing tasks, these advantages have not been investigated within the context of software requirements engineering. Therefore, we design experiments to evaluate the use of NLI in requirements analysis. We compare the performance of NLI with a spectrum of approaches, including prompt-based models, conventional transfer learning, Large Language Models (LLMs)-powered chatbot models, and probabilistic models. Through experiments conducted under various learning settings including conventional learning and zero-shot, we demonstrate conclusively that our NLI method surpasses classical NLP methods as well as other LLMs-based and chatbot models in the analysis of requirements specifications. Additionally, we share lessons learned characterizing the learning settings that make NLI a suitable approach for automating requirements engineering tasks.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628475","Requirements Engineering;Natural Language Inference;Large Language Models;Software Requirements Classification;Specification Defects;Requirements Conflict Detection","Analytical models;Large language models;Transfer learning;Chatbots;Probabilistic logic;Software;Requirements engineering","","4","","67","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs","Z. Li; D. Shin","University of Sheffield, Sheffield, UK; University of Sheffield, Sheffield, UK",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","150","159","Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.CCS CONCEPTS• Software and its engineering → Software testing and debug-ging; Empirical software validation.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556138","Large Language Models;Software Engineering;Mutation Analysis","Software testing;Training;Analytical models;Codes;Sensitivity;Semantics;Benchmark testing","","2","","47","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Does Prompt Engineering Help Turkish Named Entity Recognition?","A. S. Pektezol; A. B. Ulugergerli; V. Öztoklu; S. Demir","Department of Computer Engineering, MEF University, İstanbul, Türkiye; Department of Computer Engineering, MEF University, İstanbul, Türkiye; Department of Computer Engineering, MEF University, İstanbul, Türkiye; Department of Computer Engineering, MEF University, İstanbul, Türkiye",2024 9th International Conference on Computer Science and Engineering (UBMK),"11 Dec 2024","2024","","","233","237","The extraction of entity mentions in a text (named entity recognition) has been traditionally formulated as a sequence labeling problem. In recent years, this approach has evolved from recognizing entities to answering formulated questions related to entity types. The questions, constructed as prompts, are used to elicit desired entity mentions and their types from large language models. In this work, we investigated prompt engineering in Turkish named entity recognition and studied two prompting strategies to guide pretrained language models toward correctly identifying mentions. In particular, we examined the impact of zero-shot and few-shot prompting on the recognition of Turkish named entities by conducting experiments on two large language models. Our evaluations using different prompt templates revealed promising results and demonstrated that carefully constructed prompts can achieve high accuracy on entity recognition, even in languages with complex morphology.","2521-1641","979-8-3503-6588-7","10.1109/UBMK63289.2024.10773605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773605","Named entity recognition;prompt engineering;Turkish","Computer science;Accuracy;Large language models;Morphology;Named entity recognition;Prompt engineering;Labeling","","","","26","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Research on trustworthy Software Testing Techniques Based on Large Models","Q. Han; Z. Shi; Z. Zhao","School of Computer Scinece and Engineering, North Minzu University, Yinchuan, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China","2024 10th International Symposium on System Security, Safety, and Reliability (ISSSR)","21 Jun 2024","2024","","","524","525","This paper focuses on trustworthy software testing techniques enabled by large language models. We first give an overview of large language models and their applications in software engineering. Then we analyze the challenges of applying these models to software testing, which include overfitting, bias and trust issues. We evaluate LMTester on both synthetic and real-world programs compared with baseline testing tools. The results demonstrate its advantages in improving test coverage, detecting bugs, and assuring trustworthiness. We also discuss the limitations of current techniques and point out several future directions. Our work is expected to facilitate trustworthy applications of large language models in software testing and quality assurance.","2835-2823","979-8-3503-6293-0","10.1109/ISSSR61934.2024.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10562113","Software Testing;Large Models;Software Trustworthiness","Software testing;Analytical models;Quality assurance;Computer bugs;Software;Software reliability;Safety","","","","12","IEEE","21 Jun 2024","","","IEEE","IEEE Conferences"
"Steering Large Language Models for Vulnerability Detection","J. Li; L. Cui; J. Zhang; H. Fei; Y. Chen; H. Zhu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Quancheng Lab, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Vulnerability detection remains a critical challenge in the field of security. Many existing approaches extract code representations for vulnerability detection. However, these methods often focus on the overall semantics of the code, neglecting to specifically target vulnerability-related semantics. To address this limitation, we propose a novel LLM steering method designed to steer LLMs to focus on vulnerability concepts, thereby enhancing their performance in vulnerability detection. Specifically, we introduce a vulnerability steering vector that represents the concept of vulnerability in the representation space. This vector is generated using a paired vulnerability-patch function dataset, effectively capturing the essence of vulnerabilities. Experimental results demonstrate that the proposed method significantly improves LLMs' performance and notably outperforms existing SOTA methods in vulnerability detection tasks. Furthermore, we validate the cross-language transferability of the steering vector and explore the explainability of vulnerability detection.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10887736","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887736","security;vulnerability detection;large language model;steering vector;transferability","Codes;Large language models;Semantics;Signal processing;Vectors;Acoustics;Security;Speech processing","","","","35","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"ChipMnd: LLMs for Agile Chip Design","F. Firouzi; D. Z. Pan; J. Gu; B. Farahani; J. Chaudhuri; Z. Yin; P. Ma; P. Domanski; K. Chakrabarty",Arizona State University; University of Texas at Austin; Arizona State University; Shahid Beheshti University; Arizona State University; Arizona State University; Arizona State University; Arizona State University; Arizona State University,2025 IEEE 43rd VLSI Test Symposium (VTS),"10 Jun 2025","2025","","","1","10","The increasing complexity of semiconductor design, along with stringent performance, power, and time-to-market requirements, has outpaced the capabilities of traditional Electronic Design Automation (EDA) methodologies. Conventional design workflows rely on manual intervention for critical tasks such as hardware description, synthesis optimization, and verification, leading to inefficiencies and scalability limitations. Large Language Models (LLMs) present a transformative approach by automating key stages of the design pipeline, enabling intelligent synthesis tuning, test generation, and security analysis. This paper introduces ChipMind, an LLM-driven framework comprising specialized agents and modules for digital and analog chip design. ChipMind integrates AI-driven methodologies to enhance design efficiency, accelerate prototyping, and optimize key design trade-offs, thereby addressing fundamental challenges in modern semiconductor development.","2375-1053","979-8-3315-2144-8","10.1109/VTS65138.2025.11022936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022936","AI for chip design;Large Language Models (LLMs);Electronic Design Automation (EDA);hardware security","Design automation;Design methodology;Scalability;Large language models;Hardware security;Very large scale integration;Complexity theory;Test pattern generators;Chip scale packaging;Optimization","","","","56","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Trustworthiness of Large Language Models for Code","D. Khati","William & Mary, Williamsburg, Virginia, USA",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","208","210","In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot and Google's CodeBot exemplify how LLMc contributes to substantial time and effort savings in software development. However, the widespread application of these models has raised critical concerns regarding their trustworthiness. The lack of well-defined trust metrics beyond mere accuracy poses significant risks, including potential security vulnerabilities and compromised data integrity. This dissertation proposes solving this pressing need by developing a comprehensive framework to evaluate LLMc's trustworthiness. We aim to establish contextualized definitions of trust, distrust, and trustworthiness specific to LLMc, identify key influencing factors, and create a standardized evaluation framework encompassing both model-based attributes and human-centric considerations. We will validate the framework's effectiveness through rigorous empirical studies and user evaluations and provide insights for targeted improvements in LLMc development. This dissertation seeks to enhance the reliability and transparency of LLMc, fostering their responsible integration into software engineering practices and paving the way for more trustworthy AI-assisted code generation.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024370","Software Engineering;LLM;trust;LLM4SE","Codes;Translation;Reviews;Large language models;Pressing;Reliability engineering;Software reliability;Security;Software engineering;Software development management","","","","46","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"ChatAssert: LLM-Based Test Oracle Generation With External Tools Assistance","I. Hayet; A. Scott; M. d'Amorim","North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA",IEEE Transactions on Software Engineering,"13 Jan 2025","2025","51","1","305","319","Test oracle generation is an important and challenging problem. Neural-based solutions have been recently proposed for oracle generation but they are still inaccurate. For example, the accuracy of the state-of-the-art technique teco is only 27.5% on its dataset including 3,540 test cases. We propose ChatAssert, a prompt engineering framework designed for oracle generation that uses dynamic and static information to iteratively refine prompts for querying large language models (LLMs). ChatAssert uses code summaries and examples to assist an LLM in generating candidate test oracles, uses a lightweight static analysis to assist the LLM in repairing generated oracles that fail to compile, and uses dynamic information obtained from test runs to help the LLM in repairing oracles that compile but do not pass. Experimental results using an independent publicly-available dataset show that ChatAssert improves the state-of-the-art technique, teco, on key evaluation metrics. For example, it improves Acc@1 by 15%. Overall, results provide initial yet strong evidence that using external tools in the formulation of prompts is an important aid in LLM-based oracle generation.","1939-3520","","10.1109/TSE.2024.3519159","National Science Foundation(grant numbers:2319472,2349961); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10804561","Test oracle generation;large language models (LLMs);tool-augmented LLMs;prompt engineering framework","Chatbots;Codes;Measurement;Prompt engineering;Maintenance engineering;Large language models;Accuracy;Static analysis;Standards;Semantics","","1","","99","IEEE","16 Dec 2024","","","IEEE","IEEE Journals"
"DetectBERT: Code Vulnerability Detection","S. S. Gujar","Department of CyberSecurity, Golisano College of Computing and Information Sciences, Rochester Institute Of Technology, Rochester, NY",2024 Global Conference on Communications and Information Technologies (GCCIT),"6 Feb 2025","2024","","","1","21","DetectBERT is a deep learning approach for identifying vulnerabilities in source code at the statement level. Utilizing transformer encoder models extracts features and learns complex relationships without predefined graph structures. DetectBERT addresses data distribution challenges using real-world code and normalizing user-defined names. Demonstrated effectiveness in Python using public datasets like CVEFixes and VUDENC, DetectBERT offers scalable, robust, and granular vulnerability detection.","","979-8-3503-8891-6","10.1109/GCCIT63234.2024.10862235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10862235","Vulnerability detection;Deep learning;Large Language Models (LLMs);Static Application Security Testing (SAST);Transformer encoders;Code analysis;Common Vulnerabilities and Exposures (CVE);Common Weakness Enumeration (CWE);Generative Pre-trained Transformers (GPT);Graph Neural Networks (GNN);Code normalization;Secure software development;Python code analysis;Control Flow Graph (CFG);AI in cybersecurity;Software security;Explainable AI (XAI);Cybersecurity automation;Adversarial robustness","Codes;Source coding;Neural networks;Transformers;Feature extraction;Robustness;Information and communication technology;Testing;Software development management;Python","","1","","0","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites","G. M. Santos; R. Maria Da Silva Julia; M. Z. Do Nascimento","Computer Science Faculty, Federal University of Uberlândia, São Paulo, Brazil; Computer Science Faculty, Federal University of Uberlândia, São Paulo, Brazil; Computer Science Faculty, Federal University of Uberlândia, São Paulo, Brazil",2025 IEEE Congress on Evolutionary Computation (CEC),"24 Jun 2025","2025","","","1","8","Prompt engineering is essential for optimizing large language models (LLMs), yet the link between prompt structures and task performance remains underexplored. This work introduces an evolutionary approach that combines context-free grammar (CFG) with the MAP-Elites algorithm to systematically explore the prompt space. Our method prioritizes quality and diversity, generating high-performing and structurally varied prompts while analyzing their alignment with diverse tasks by varying traits such as the number of examples (shots) and reasoning depth. By systematically mapping the phenotypic space, we reveal how structural variations influence LLM performance, offering actionable insights for task-specific and adaptable prompt design. Evaluated on seven BigBench Lite tasks across multiple LLMs, our results underscore the critical interplay of quality and diversity, advancing the effectiveness and versatility of LLMs.","","979-8-3315-3431-8","10.1109/CEC65147.2025.11043024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11043024","Prompt Evolution;MAP-Elites;Prompt Design;Quality-Diversity;Large Language Models","Large language models;Evolutionary computation;Cognition;Space exploration;Grammar;Prompt engineering","","","","40","IEEE","24 Jun 2025","","","IEEE","IEEE Conferences"
"Can Large Language Models Discover Metamorphic Relations? A Large-Scale Empirical Study","J. Zhang; C. -a. Sun; H. Liu; S. Dong","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Department of Computing Technologies, Swinburne University of Technology, Melbourne, Australia; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","24","35","Software testing is a mainstream approach for software quality assurance. One fundamental challenge for testing is that in many practical situations, it is very difficult to verify the correctness of test results given inputs for Software Under Test (SUT), which is known as the oracle problem. Metamorphic Testing (MT) is a software testing technique that can effectively alleviate the oracle problem. The core component of MT is a set of Metamorphic Relations (MRs), which are basically the necessary properties of SUT, represented in the form of relationship among multiple inputs and their corresponding expected outputs. Different methods have been proposed to support the systematic MR identification. However, most of them still rely heavily on test engineers' understanding of the SUT and involve massive manual work. Although a few preliminary studies have shown LLMs' viability in generating MRs, there does not exist a thorough and in-depth investigation on their capability in MR identification. We are thus motivated to conduct a comprehensive and large-scale empirical study to systematically evaluate the performance of LLMs in identifying appropriate MRs for a wide variety of software systems. This study makes use of 37 SUTs collected from previous MT studies. Prompts are constructed for two LLMs, gpt-3.5-turbo-1106 and gpt-4-1106-preview, to perform the MR identification for each SUT. The empirical results demonstrate that both LLMs can generate a large amount of MR candidates (MRCs). Among them, 29.86% and 43.79% of all MRCs are identified as the MRs valid for the corresponding SUT, respectively. In addition, 24.59% and 38.63% of all MRCs are MRs that had never been identified in previous studies. Our study not only reinforces LLM-based MR identification as a promising research direction for MT, but also provides some practical guidelines for how to further improve LLMs' performance in generating good MRs.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00011","National Natural Science Foundation of China(grant numbers:62272037); Australian Research Council(grant numbers:DP210102447); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992399","Metamorphic testing;metamorphic relation;large language models;oracle problem;empirical study","Software testing;Systematics;Large language models;Software quality;Manuals;Software systems;Guidelines","","","","65","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Lookup Table Refactoring: Towards Efficient Logarithmic Number System Addition for Large Language Models","X. Geng; S. Liu; H. Wang; J. Han; H. Jiang","Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Alberta, Alberta, Canada; Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China","2025 Design, Automation & Test in Europe Conference (DATE)","21 May 2025","2025","","","1","7","Compared to integer quantization, logarithmic quantization aligns more effectively with the long-tailed distribution of data in large language models (LLMs), resulting in lower quantization errors. Moreover, the logarithmic number system (LNS) employs a fixed-point adder to perform multiplication, indicating a potential reduction in computational complexity for LLM accelerators that require extensive multiply-accumulate (MAC) operations. However, a key bottleneck is that LNS addition requires complex nonlinear functions, which are typically approximated using lookup tables (LUTs). This study aims to reduce the hardware resources needed for LUTs in LNS addition while maintaining high precision. Specifically, we investigate the specific nature of addition operations within LLMs; the relationship between the hardware parameters of the LUT and the computing errors is then mathematically derived. Based on these insights, we propose LUT refactoring to optimize the LUT for enhanced efficiency in LNS addition. With 10.93% and 19.78% reductions in area-delay product (ADP) and power-delay product (PDP), respectively, LUT refactoring results in an accuracy improvement of up to 33.5% in LLM benchmarks compared to the naive design. When compared to integer quantization, our method achieves higher accuracy while reducing area by 18.27% and power by 42.61%.","1558-1101","978-3-9826741-0-0","10.23919/DATE64628.2025.10993215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10993215","logarithmic number system;lookup table;large language model;approximate computing;error analysis","Quantization (signal);Accuracy;Heavily-tailed distribution;Error analysis;Large language models;Europe;Hardware;Table lookup;Adders;Optimization","","","","17","","21 May 2025","","","IEEE","IEEE Conferences"
"Optimizing Transformer Models for Prompt Jailbreak Attack Detection in AI Assistant Systems","L. A. Tien; P. Van Huong","Information Technology Department, Academy of Cryptography Techniques, Hanoi, Vietnam; Information Technology Department, Academy of Cryptography Techniques, Hanoi, Vietnam",2024 1st International Conference On Cryptography And Information Security (VCRIS),"27 Dec 2024","2024","","","1","4","Integrating AI-powered assistants in different areas has changed how people retrieve information. Users can now get quick responses, use the service whenever needed, and handle inquiries well. However, relying on these systems has increasingly raised worries about their safety and reliability, especially when facing threats like prompt injection or jailbreak attacks. These attacks take advantage of the weaknesses of large language models (LLMs) by changing input prompts to create harmful, biased, or misleading results. This paper examines prompt jailbreak attacks in ChatGPT-based assistant chatbots, especially in education. It examines the weaknesses in these systems and suggests ways to detect these attacks by fine-tuning various Transformer models. The research also includes adding prompt jailbreak attack detection in a virtual assistant application for university use. This aims to ensure teachers and students can interact safely and rely on the system. Through rigorous experimentation and evaluation, we demonstrate the practicality and effectiveness of our detection methods, providing reassurance and confidence in the face of AI security challenges. Our studies emphasize the need for robust AI chatbots and offer practical solutions to preserve the integrity of AI-driven tools.","","979-8-3315-2994-9","10.1109/VCRIS63677.2024.10813380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10813380","chatbot;prompt engineering;prompt injection attack;transformer;LLM","Training;Virtual assistants;Large language models;Refining;Information security;Transformers;Chatbots;Reliability engineering;Safety;Faces","","","","18","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open Radio Access Networks","P. Gajjar; V. K. Shah","NextG Wireless Lab, North Carolina State University; NextG Wireless Lab, North Carolina State University",2025 IEEE 22nd Consumer Communications & Networking Conference (CCNC),"5 May 2025","2025","","","1","4","Large Language Models (LLMs) can revolutionize how we deploy and operate Open Radio Access Networks (0-RAN) by enhancing network analytics, anomaly detection, and code generation and significantly increasing the efficiency and reliability of a plethora of 0- RAN tasks. In this paper, we present ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the performance of Large Language Models (LLMs) within the context of O-RAN. Our benchmark consists of 13,952 meticulously curated multiple-choice questions generated from 116 O-RAN specification documents. We leverage a novel three- stage LLM framework, and the questions are categorized into three distinct difficulties to cover a wide spectrum of 0 RAN- related knowledge. We thoroughly evaluate the performance of several state-of-the-art LLMs, including Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a Retrieval- Augmented Generation (RAG)-based pipeline that demonstrates superior performance on ORAN-Bench-13K compared to other tested closed-source models. Our findings indicate that current popular LLM models are not proficient in O-RAN, highlighting the need for specialized models. We observed a noticeable performance improvement when incorporating the RAG-based ORANSight pipeline, with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on average 21.55% and 22.59% better than the other tested LLMs.","2331-9860","979-8-3315-0805-0","10.1109/CCNC54725.2025.10975994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10975994","O-RAN;LLMs;Benchmarks;LLM-Benchmark;Dataset;ChatGPT;Gemini;RAG","Accuracy;Codes;Large language models;Pipelines;Retrieval augmented generation;Open RAN;Benchmark testing;Chatbots;Reliability;Anomaly detection","","","","24","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions","S. Javaid; R. A. Khalil; N. Saeed; B. He; M. -S. Alouini","Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Engineering Requirement Unit, College of Engineering, United Arab Emirates University, Al-Ain, UAE; Department of Electrical and Communication Engineering, College of Engineering, United Arab Emirates University, Al-Ain, UAE; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science And Technology, Thuwal, Saudi Arabia",IEEE Open Journal of the Communications Society,"8 Jan 2025","2025","6","","399","432","Integrated satellite, aerial, and terrestrial networks (ISATNs) represent a sophisticated convergence of diverse communication technologies to ensure seamless connectivity across different altitudes and platforms. This paper explores the transformative potential of integrating Large Language Models (LLMs) into ISATNs, leveraging advanced Artificial Intelligence (AI) and Machine Learning (ML) capabilities to enhance these networks. We outline the current architecture of ISATNs and highlight the significant role LLMs can play in optimizing data flow, signal processing, and network management to advance 5G/6G communication technologies through advanced predictive algorithms and real-time decision-making. A comprehensive analysis of ISATN components is conducted, assessing how LLMs can effectively address traditional data transmission and processing bottlenecks. The paper delves into the network management challenges within ISATNs, emphasizing the necessity for sophisticated resource allocation strategies, traffic routing, and security management to ensure seamless connectivity and optimal performance under varying conditions. Furthermore, we examine the technical challenges and limitations associated with integrating LLMs into ISATNs, such as data integration for LLM processing, scalability issues, latency in decision-making processes, and the design of robust, fault-tolerant systems. The study also identifies critical future research directions for fully harnessing LLM capabilities in ISATNs, which is important for enhancing network reliability, optimizing performance, and achieving a truly interconnected and intelligent global network system.","2644-125X","","10.1109/OJCOMS.2024.3522103","National Natural Science Foundation of China(grant numbers:62088101); United Arab Emirates University (UAEU) through the UAEU Program for Advanced Research(grant numbers:12N174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10813004","Integrated satellite-aerial-terrestrial networks;large language models;5G/6G communication;resource allocation;intelligent networks","Artificial intelligence;Satellites;Satellite broadcasting;Resource management;Reliability;Computational modeling;Complexity theory;Bandwidth;Scalability;STEM","","12","","306","CCBY","25 Dec 2024","","","IEEE","IEEE Journals"
"Large Language Models for Anomaly Detection in Computational Workflows: From Supervised Fine-Tuning to In-Context Learning","H. Jin; G. Papadimitriou; K. Raghavan; P. Zuk; P. Balaprakash; C. Wang; A. Mandal; E. Deelman","Mathematics and Computer Science Division Argonne National Laboratory, Lemont, IL, USA; Information Sciences Institute University of Southern California, Los Angeles, CA, USA; Mathematics and Computer Science Division Argonne National Laboratory, Lemont, IL, USA; Information Sciences Institute University of Southern California, Los Angeles, CA, USA; Computing and Computational Sciences Directorate Oak Ridge National Laboratory, Oak Ridge, TN, USA; RENCI University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; RENCI University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Information Sciences Institute University of Southern California, Los Angeles, CA, USA","SC24: International Conference for High Performance Computing, Networking, Storage and Analysis","24 Dec 2024","2024","","","1","17","Anomaly detection in computational workflows is critical for ensuring system reliability and security. However, traditional rule-based methods struggle to detect novel anomalies. This paper leverages large language models (LLMs) for workflow anomaly detection by exploiting their ability to learn complex data patterns. Two approaches are investigated: (1) supervised fine-tuning (SFT), where pretrained LLMs are fine-tuned on labeled data for sentence classification to identify anomalies, and (2) in-context learning (ICL), where prompts containing task descriptions and examples guide LLMs in few-shot anomaly detection without fine-tuning. The paper evaluates the performance, efficiency, and generalization of SFT models and explores zeroshot and few-shot ICL prompts and interpretability enhancement via chain-of-thought prompting. Experiments across multiple workflow datasets demonstrate the promising potential of LLMs for effective anomaly detection in complex executions.","","979-8-3503-5291-7","10.1109/SC41406.2024.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10793185","anomaly detection;large language models;supervised fine-tuning;in-context learning;computational workflows","Training;Analytical models;Accuracy;Large language models;Computational modeling;High performance computing;Transfer learning;Reliability;Security;Anomaly detection","","2","","42","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"Large Language Models in Data Governance: Multi-source Data Tables Merging","L. Li; H. Chen; Z. Qiu; L. Luo","School of SCSE, University of Electronic Science and Technology of China, Chengdu, China; School of SCSE, University of Electronic Science and Technology of China, Chengdu, China; School of SCSE, University of Electronic Science and Technology of China, Chengdu, China; School of SCSE, University of Electronic Science and Technology of China, Chengdu, China",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","3965","3974","With the explosive growth of data volumes, enterprises face unprecedented challenges in data governance. As a critical step in data modeling, multi-source data table merging enables the integration of multiple tables containing the same entities from different business system databases into a standardized entity table, thereby enhancing data quality and consistency. However, this process relies heavily on manual judgment, leading to inefficiencies. In recent years, the performance of large language models (LLMs) in natural language processing has provided new insights for addressing data governance issues. This paper proposes a ""mutation-generation-tuning"" approach that enhances the performance of small parameter LLMs (e.g., Baichuan2-7B) in multi-source data table merging tasks through secondary pre-training and fine-tuning. The specific methods include data augmentation and generating intermediate reasoning processes using large parameter LLMs. Experimental results indicate that the optimized MDTM-GPT model exhibits performance on par with high-parameter models in multi-source data table merging tasks while maintaining low cost and data security. The contributions of this paper lie in presenting a novel approach for small parameter LLMs and providing practical validation for applications in the field of data governance.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10826092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10826092","Large Language Models;Data Governance;Data Augmentation","Costs;Databases;Large language models;Merging;Manuals;Data augmentation;Data models;Cognition;Data governance;Few shot learning","","1","","29","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Zero-Shot Detection and Sanitization of Data Poisoning Attacks in Wearable AI Using Large Language Models","W. K. M. Mithsara; A. R. Shahid; N. Yang","School of Computing, Southern Illinois University, Carbondale, IL, USA; School of Computing, Southern Illinois University, Carbondale, IL, USA; School of Computing, Southern Illinois University, Carbondale, IL, USA",2024 International Conference on Machine Learning and Applications (ICMLA),"4 Mar 2025","2024","","","1510","1515","Wearable AI systems, particularly in Human Activity Recognition (HAR), are becoming integral to applications in healthcare, security, and personal fitness due to the widespread adoption of smart devices and wearable technologies. However, the increasing reliance on machine learning models in HAR introduces significant risks, especially from poisoning attacks that compromise system reliability and data integrity. This paper explores the potential of Large Language Models (LLMs) to detect and sanitize poisoning attacks in wearable AI systems. Building on ongoing research into integrating LLMs within cyber-physical systems, we focus on sensor-based interactions with the physical world. Our case study seeks to answer the following question: How effective are LLMs in detecting and sanitizing poisoning attacks on human activity sensor data? Through zero-shot learning, we evaluate the performance of models such as ChatGPT 3.5, ChatGPT 4, and Gemini, providing insights into the viability of LLMs for real-time defense and data integrity in wearable AI systems.","1946-0759","979-8-3503-7488-9","10.1109/ICMLA61862.2024.00233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903508","Large Language Models;Data Poisoning attack","Large language models;Data integrity;Zero shot learning;Machine learning;Chatbots;Data models;Human activity recognition;Security;Reliability;Smart devices","","","","18","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Fine-Tuning Large Language Models for Solving Math Word Problems: A Case Study with LLaMA-3","H. Nie","SWJTU-Leeds Joint School, Southwest Jiaotong University, Chengdu, China","2024 4th International Signal Processing, Communications and Engineering Management Conference (ISPCEM)","14 Feb 2025","2024","","","100","104","The rapid development of Large Language Models (LLMs) has led to advances in natural language processing, but handling mathematical tasks remains a challenge due to the demands of text understanding, quantitative reasoning, and numerical computation. Although LLMs show progress in reasoning for simpler problems, they often struggle with more complex tasks requiring multi-step reasoning. To address these challenges, this study focuses on fine-tuning a pre-trained Llama-3-8B-Chinese-Chat model to enhance its ability to solve mathematical word problems (MWPs). A systematic approach was followed, involving dataset difficulty classification, prompt engineering, and LoRA fine-tuning techniques. The model's performance was evaluated to show significant improvements in equation matching and numerical accuracy after fine-tuning, particularly for complex problems. On simple, medium, and difficult test sets, End-to-End accuracy of fine-tuned model improved by 47.1%, 30.2%, and 65.7% correspondingly. However, the model still fails to reach a satisfactory accuracy while handling purely mathematical computations, which may be accounted for its poor capability of handling symbol processing and fraction conversion. Future research directions include investigating diverse mathematical reasoning tasks and integrating external symbolic computation tools to enhance computational precision. This study contributes to evaluating LLMs' mathematical reasoning capabilities and revealing the promising potential of LoRA fine-tuning.","","979-8-3315-2867-6","10.1109/ISPCEM64498.2024.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10874365","Large language model;Math word problem;Natural language processing;Llama-3;Fine-tuning","Training;Analytical models;Accuracy;Computational modeling;Large language models;Mathematical models;Cognition;Numerical models;Prompt engineering;Problem-solving","","","","11","IEEE","14 Feb 2025","","","IEEE","IEEE Conferences"
"LLM4ITD: Insider Threat Detection with Fine-Tuned Large Language Models","M. Zhang; X. Liang; F. Tian; Y. Yang; H. Yu; B. Li","China Mobile Information Technology Co., Ltd, Guangdong, China; China Mobile Information Technology Co., Ltd, Guangdong, China; China Mobile Information Technology Co., Ltd, Beijing, China; China Mobile Information Technology Co., Ltd, Guangdong, China; Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, Key Lab of Cyberspace Security Defense, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",2024 International Conference on Interactive Intelligent Systems and Techniques (IIST),"4 Nov 2024","2024","","","236","241","Recently, insider threat detection has been an important means to ensure data security. By analyzing user logs and finding significant deviations, existing methods have obtained some achievements. However, they still use handcrafted rules to transform logs into specific input forms, which is time-consuming and requires prior knowledge. Moreover, they extensively depend on a substantial amount of labeled malicious data which is hard to obtain. To address these identified issues, this paper proposes to use Large Language Models for Insider Threat Detection (LLM4ITD). Prompt construction module designs a new template to guide LLM4ITD in identifying potential malicious activities without specific rules. The fine-tuning module adopts parameter-efficient methods to fine-tune LLM4ITD, thus improving the accuracy of insider threat detection with limited labeled data. Extensive experiments on the CERT dataset demonstrate that LLM4ITD outperforms various state-of-the-art methods.","","979-8-3503-7442-1","10.1109/IIST62526.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734450","data security;insider threat detection;large language models;fine-tuning","Accuracy;Large language models;Data security;Transforms;Threat assessment;Data models;Intelligent systems","","","","19","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Large Language Models Are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks","K. Yang; X. Mao; S. Wang; Y. Wang; T. Zhang; B. Lin; Y. Qin; Z. Zhang; Y. Lu; K. Al-Sabahi","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; Sun Yat-sen University, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Banking and Financial Studies, Oman",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","298","309","Pre-trained code models are essential for various code intelligence tasks. Yet, their effectiveness is heavily influenced by the quality of the pre-training dataset, particularly human-written reference comments, which usually serve as a bridge between the programming language and natural language. One significant challenge is that such comments could become inconsistent with the corresponding code as the software evolves, leading to suboptimal model performance. Large language models (LLMs) have demonstrated superior capabilities in generating high-quality code comments. This work investigates whether substituting original human-written comments with LLM-generated ones can improve pre-training datasets for more effective pretrained code models. As existing reference-based metrics cannot evaluate the quality of human-written reference comments themselves, to enable direct comparison between LLM-generated and human reference comments, we introduce two auxiliary tasks as novel reference-free metrics, including code-comment inconsistency detection and semantic code search. Experimental results show that LLM-generated comments exhibit superior semantic consistency with the code compared to human-written reference comments. Our manual evaluation also corroborates this conclusion, which indicates the potential of utilizing LLMs to enhance the quality of the pre-training dataset. Based on this finding, we rebuilt the CodeSearchNet dataset with LLM-generated comments and re-pre-trained the CodeT5 model. Evaluations on multiple code intelligence tasks demonstrate that models pretrained by LLM-enhanced data outperform their counterparts (pre-trained by original human reference comments data) on code summarization, code generation, and code translation tasks. This research validates the feasibility of rebuilding the pre-training dataset by LLMs to advance code intelligence tasks. It advocates rethinking the reliance on human reference comments for coderelated tasks.","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00038","National Key Research and Development Program of China(grant numbers:2023YFB4503802); National Natural Science Foundation of China(grant numbers:62172426,62302515); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025927","Code Summarization;Pre-Training;Large Language Models;Code Intelligence","Measurement;Training;Computer languages;Codes;Translation;Large language models;Semantics;Manuals;Software;Data models","","","","84","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Cross-Multi-Agent Systems Identity Authentication Framework on Decentralized Identity","Z. Xu; Y. Zuo; B. Yu; Y. Zhang; Y. Gong","School of Information and Communication Engineering, Beijing Information Science and Technology University, Beijing, China; School of Information and Communication Engineering, Beijing Information Science and Technology University, Beijing, China; School of Information and Communication Engineering, Beijing Information Science and Technology University, Beijing, China; School of Information and Communication Engineering, Beijing Information Science and Technology University, Beijing, China; School of Information and Communication Engineering, Beijing Information Science and Technology University, Beijing, China",2024 3rd Conference on Fully Actuated System Theory and Applications (FASTA),"23 Jul 2024","2024","","","500","505","Multi-Agent Systems (MAS) as a means to solve complex problems by subdividing them into smaller tasks. It has become increasingly prominent in solving complex interaction problems. In the process of data exchange between MASs, the inability to verify each other’s identity information often leads to mutual distrust, compromising security. Traditional centralized identity authentication management has significant flaws under the decentralized network environment of MAS. To address this issue, this paper introduces a Cross-Multi-Agent Systems Identity Authentication Framework based on Decentralized Identity (DID), which establishes trust between MASs and ensures security and confidentiality. Our security analysis proves the effectiveness of this solution in overcoming the identified challenges and validates the feasibility of the framework.","","979-8-3503-7369-1","10.1109/FASTA61401.2024.10595187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10595187","Blockchain;Decentralized Identity;Identity Management;Multi-Agent Systems","Authentication;Security;Task analysis;Multi-agent systems","","","","15","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models","S. Tang; Z. Zhang; J. Zhou; L. Lei; Y. Zhou; Y. Xue","University of Science and Technology of China, Hefei, China; Kyushu University, Fukuoka, Japan; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Zhejiang Sci-Tech University, Hangzhou, China; University of Science and Technology of China, Hefei, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1497","1508","Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level. However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers. To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios. To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios. We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios. Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.CCS CONCEPTS• Software and its engineering → Software testing and debugging; Search-based software engineering.","2643-1572","979-8-4007-1248-7","","Anhui Provincial Department of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764952","Autonomous Driving Systems;Critical Scenario Generation;Large Language Models","Software testing;Large language models;Web and internet services;Transforms;Feature extraction;Software;Scenario generation;Data mining;Autonomous vehicles;Software engineering","","3","","50","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models","G. Crumrine; I. Alsmadi; J. Guerrero; Y. Munian; M. Al-Abdullah","Dept. of Computational Engineering and Mathematical Sciences, Texas A&M University, San Antonio; Dept. of Computational Engineering and Mathematical Sciences, Texas A&M University, San Antonio; Dept. of Computational Engineering and Mathematical Sciences, Texas A&M University, San Antonio; Dept. of Computational Engineering and Mathematical Sciences, Texas A&M University, San Antonio; Dept. of Information and Technology Management, University of Tampa",2024 4th Intelligent Cybersecurity Conference (ICSC),"25 Feb 2025","2024","","","39","47","Large language models (LLMs) have achieved groundbreaking advancements in natural language processing (NLP) that hold the promise of revolutionizing the relationship between humans and technology. However, this technological advancement has been joined by the emergence of “Mallas” (a term coined by Lin et al. [4]). These services facilitate the creation of malware, phishing attacks, deceptive websites, and most concerning, exploit code. This paper delves into the proliferation of Mallas by examining the use of various pretrained language models and their efficiency at generating vulnerabilities and exploits when being misused. Leveraging a comprehensive dataset from the Common Vulnerabilities and Exposures (CVE) program, it explores dataset creation, prompt engineering and fine-tuning methodologies needed to generate code and explanatory text related to vulnerabilities identified in the CVE database. Furthermore, this research aims to shed light on the strategies and exploitation techniques of Mallas; ultimately assisting the development of more secure and trustworthy AI applications. The paper concludes by emphasizing the critical need for further research into LLM-related cyber threat intelligence and advocating for the development of enhanced safeguards and ethical guidelines to mitigate the risks associated with these malicious applications of LLMs. We propose a novel Dynamic Ethical Boundary Reinforcement (DEBR) system as a proactive measure against LLM exploitation.","","979-8-3503-5477-5","10.1109/ICSC63108.2024.10895437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895437","Computer security;cyber threat intelligence;human computer interaction;natural language processing;question answering(information gathering)","Ethics;Technological innovation;Codes;Protocols;Large language models;Phishing;Natural language processing;Malware;Prompt engineering;Computer security","","","","6","IEEE","25 Feb 2025","","","IEEE","IEEE Conferences"
"Performance Analysis of Chinese Large Language Models in Solving Math Word Problems","S. Pan; K. Liu; W. Chen; B. He","Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China; Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China; Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China; Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, China",2024 International Conference on Intelligent Education and Intelligent Research (IEIR),"14 Apr 2025","2024","","","1","8","Recently, researchers in the field of math word problem (MWP) solving have reported performance metrics for various large language models (LLMs) on benchmark datasets, with some models consistently achieving new state-of-the-art (SOTA) results. However, the benchmark datasets are often selectively curated and subjectively refined, leading to discrepancies between the evaluation metrics and actual experiences in real-world applications. To address this issue, we constructed a comprehensive dataset named AS-Math5k to assess more authentic and reliable performance. AS-Math5k is derived from practical teaching scenarios and encompasses multiple-choice questions and word problems across K-12 grade levels. Subsequently, we developed a unified evaluation framework that includes a prompt engineering module, a problem-solving module, and a result analysis module. This framework evaluates the performance of representative Chinese LLMs on AS-Math5k and provides a detailed discussion of performance variations among LLMs when solving problems of different grade levels and types. Experimental results indicate the following: (1) There is a notable discrepancy in the performance of large language models on real-world MWPs compared to benchmark datasets; (2) LLMs generally perform better on objective problems with clear, standardized answers; and (3) LLMs demonstrate strong comprehension and problem-solving abilities in both explicit relations and logical scenarios.","","979-8-3315-1982-7","10.1109/IEIR62538.2024.10960109","National Natural Science Foundation of China; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10960109","math word problem solving;large language model evaluation;prompt engineering","Training;Adaptation models;Large language models;Benchmark testing;Reliability engineering;Cognition;Performance analysis;Problem-solving;Prompt engineering;Standards","","","","38","IEEE","14 Apr 2025","","","IEEE","IEEE Conferences"
"Federated Large Language Models for Wireless Networks","Y. Himeur; D. W. Dawoud; O. Alnaseri; S. Atalla; W. Mansoor; H. Al-Ahmad","College of Engineering and Information Technology, University of Dubai, Dubai, UAE; College of Engineering and Information Technology, University of Dubai, Dubai, UAE; Department of Electrical Engineering, DHBW University, Ravensburg, Germany; College of Engineering and Information Technology, University of Dubai, Dubai, UAE; College of Engineering and Information Technology, University of Dubai, Dubai, UAE; College of Engineering and Information Technology, University of Dubai, Dubai, UAE",2025 International Wireless Communications and Mobile Computing (IWCMC),"2 Jul 2025","2025","","","1546","1551","This paper explores the potential of federated large language models (FLLMs) for privacy-preserving artificial intelligence (AI) and intelligent management in emerging fifth-generation (5G) and sixth-generation (6G) wireless networks. Using federated learning (FL), FLLMs enable decentralized model training at the network edge while maintaining data locality and compliance with privacy regulations. Key challenges—such as communication overhead, security vulnerabilities, and the adaptability of large language models (LLMs) to dynamic network conditions—are examined. The study also highlights the role of FLLMs in optimizing resource allocation, improving cybersecurity, and supporting real-time decision making. Future directions include integration with Digital Twin frameworks, Explainable AI (XAI), and blockchain technologies for secure and scalable wireless AI ecosystems.","2376-6506","979-8-3315-0887-6","10.1109/IWCMC65282.2025.11059463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11059463","5G/6G Wireless Networks;Federated Large Language Models;Edge-based AI;Low Latency","6G mobile communication;Training;Technological innovation;Explainable AI;Wireless networks;Large language models;Decision making;Regulation;Blockchains;Resource management","","","","42","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Contextualized Data-Wrangling Code Generation in Computational Notebooks","J. Huang; D. Guo; C. Wang; J. Gu; S. Lu; J. P. Inala; C. Yan; J. Gao; N. Duan; M. R. Lyu","The Chinese University of Hong Kong, China; Sun-yat Sen University; Microsoft Research; The Chinese University of Hong Kong, China; Microsoft Research Asia; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research Asia; The Chinese University of Hong Kong, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1282","1294","Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts’ overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose DataCoder, which encodes data context and code&textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.CCS CONCEPTS• Software and its engineering → Automatic programming.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764918","code generation;data wrangling;computational notebooks;large language models","Training;Codes;Computational modeling;Source coding;Large language models;Data science;Data models;Software;Context modeling;Software engineering","","","","103","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Artificial Intelligence in Engineering and Society: Blue Skies, Black Holes, and the Job of Requirements Engineers (Keynote)","A. Ferrari","CNR-ISTI, Pisa, Italy",2023 IEEE 31st International Requirements Engineering Conference Workshops (REW),"28 Sep 2023","2023","","","67","67","The democratization of artificial intelligence (AI) has brought substantial achievements in science, engineering disciplines, and society as a whole. New technologies based on large language models, multi-modal learning, embodied AI, and the quest for artificial general intelligence (AGI) promise to further change the world's landscape as we know it. At the same time, AI's rapid and uncontrolled evolution also poses serious risks to society, such as the concentration of power, exclusion, discrimination, and manipulation of reality. The keynote will present some experiences in AI democratization, including the usage of explainable machine learning approaches for agronomists, NLP-based solutions for railway engineers, image processing techniques for the maintenance of riverbeds, and mobile data processing in road safety assessment. The talk will outline the latest technological advancements in AI, e.g., in healthcare and science, and will show how large language models like ChatGPT and Bing Chat can solve long-standing requirements engineering (RE) problems. For example, requirements completeness can be easily checked and addressed with simple prompts, and model generation from requirements becomes a one-click task. The keynote will then describe the risks that current AI development poses to society. Besides the increasingly convincing deep fakes, and the widely discussed risks for privacy and reputation, we must be aware of the uncontrolled speed of AI evolution. As AI continues to advance, it will replace many jobs that require intellectual skills. This could lead to a significant number of people losing their jobs, as they may not have the necessary skills to adapt to the new labour market. People and entire countries that cannot exploit technological developments will be excluded from the game, and this will cause resentment and the possible emergence of new fundamentalism. The race for semiconductors is already creating hot spots and rifts between the superpowers. In this context, RE researchers are called to new technical and societal challenges. With pieces of code and even entire programs that can be automatically generated with large language models, the craft of prompting becomes the new requirements specification, and the concept of structured APIs dissolves into natural language interfaces. At the societal level, AI regulations are making their first steps, and we are called to contribute to operationalise the norms while preventing over-regulation. Equipped with years of experience at the boundary of the technical and social facets of systems, RE researchers are pivotal subjects in the new golden age of AI.","2770-6834","979-8-3503-2691-8","10.1109/REW57809.2023.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260845","requirements engineering;large language models;artificial intelligence","Privacy;Natural languages;Medical services;Maintenance engineering;Road safety;Regulation;Rail transportation","","2","","0","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Ethical Challenges and Frameworks in AI-Driven Software Development and Testing","A. Donvir; G. Sharma","Application Development, Wayne, NJ, USA; AI Test Automation Solution Architect, Atlanta, GA, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00569","00576","Artificial Intelligence (AI) has revolutionized and transformed the landscape of software development and testing by introducing new efficiencies and capabilities through advancements like Generative AI (GenAI) and Large Language Models (LLMs). While these technologies bring major benefits in terms of productivity, personalization, and innovation, they also raise critical ethical challenges, such as biases, lack of transparency, data privacy concerns, and potential negative societal impacts. This paper examines the ethical considerations involved in developing such advanced AI systems as well using AI systems within software development and testing. It explores existing ethical frameworks and principles provided by leading organizations, emphasizing core concepts like human-centered design, accountability, transparency, fairness, and privacy. Practical strategies for integrating ethical practices throughout the AI development lifecycle are discussed, with a strong emphasis on the need for continuous ethical evaluation. The paper explores the ethical landscape of AI in software development, addressing challenges like algorithmic bias, data security, and broader societal impacts. Real-world case studies presented in the paper demonstrate the consequences of neglecting ethical considerations. Looking forward, the paper suggests future directions, including the development of unified ethical standards, collaborative ethical auditing, regulatory advancements, and higher societal engagement.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903892","Artificial Intelligence (AI);Ethical AI;Software Development;Software Testing;Generative AI (GenAI);Large Language Models (LLMs);Ethical Frameworks;Human-Centered Design;Accountability;Transparency;Fairness and Non-Discrimination;Data Privacy;Responsible AI;Bias Detection;Explainable AI (XAI);Ethical Auditing;Regulatory Frameworks;Societal Engagement;Case Studies in AI Ethics","Ethics;Technological innovation;Data privacy;Standards organizations;Software algorithms;Collaboration;Stakeholders;Artificial intelligence;Software development management;Testing","","1","","26","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"GPT-Powered Elicitation Interview Script Generator for Requirements Engineering Training","B. Görer; F. B. Aydemir","Microsoft, Istanbul, Türkiye; Utrecht University, Utrecht, Netherlands",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","372","379","Elicitation interviews are the most common requirements elicitation technique, and proficiency in conducting these interviews is crucial for requirements elicitation. Traditional training methods, typically limited to textbook learning, may not sufficiently address the practical complexities of interviewing techniques. Practical training with various interview scenarios is important for understanding how to apply theoretical knowledge in real-world contexts. However, there is a shortage of educational interview material, as creating interview scripts requires both technical expertise and creativity. To address this issue, we develop a specialized GPT agent for auto-generating interview scripts. The GPT agent is equipped with a dedicated knowledge base tailored to the guidelines and best practices of requirements elicitation interview procedures. We employ a prompt chaining approach to mitigate the output length constraint of GPT to be able to generate thorough and detailed interview scripts. This involves dividing the interview into sections and crafting distinct prompts for each, allowing for the generation of complete content for each section. The generated scripts are assessed through standard natural language generation evaluation metrics and an expert judgment study, confirming their applicability in requirements engineering training.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628455","large language models;prompt engineering;elicitation interview script generation;requirements engineering education","Training;Measurement;Knowledge based systems;Natural language generation;Production;Generators;Requirements engineering","","3","","46","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"LERE: A Method Based on LLMS and Enhanced Ears in Requirements Engineering","H. Han; K. Chen; J. He; Z. Huang","Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; School of Cyber Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2025 8th International Conference on Software and System Engineering (ICoSSE),"28 Jul 2025","2025","","","80","85","Normalized requirements permeate the entire requirements engineering and serve as a crucial foundation for the project. To improve the accuracy of requirements normalization in Chinese, in this study, we propose a method: LERE, which is based on LLMs and an enhanced EARS paradigm. The enhanced EARS is an optimized version of the original English EARS, incorporating elements from Chinese linguistic structures. This method employs a two-step processing prompt, with each step utilizing the enhanced EARS to perform structural element analysis on the input. The first step involves preprocessing the input, while the second step normalizes the output from the preprocessing stage by refining its syntactic structure and semantic expression. We have constructed a dataset consisting of 138 Chinese requirements, categorized into eight types of errors. The dataset and prompts have been made publicly available. Testing results on this dataset using Qwen2.5 and ChatGPT-4o show that the performance of LERE improved by approximately $\mathbf{2 0 \%}$ compared to baseline.","","979-8-3315-4267-2","10.1109/ICoSSE65712.2025.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11087952","Requirements Engineering(RE);Requirements Normalization;LLMs;NLP;EARS(Easy Approach to Requirements Syntax)","Accuracy;Data preprocessing;Semantics;Refining;Ear;Syntactics;Linguistics;Software;Requirements engineering;Testing","","","","12","IEEE","28 Jul 2025","","","IEEE","IEEE Conferences"
"(Security) Assertions by Large Language Models","R. Kande; H. Pearce; B. Tan; B. Dolan-Gavitt; S. Thakur; R. Karri; J. Rajendran","Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA; School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, Australia; Department of Electrical and Software Engineering, University of Calgary, Calgary, AB, Canada; Department of Computer Science and Engineering, New York University, New York, NY, USA; Department of Computer Science and Engineering, New York University, New York, NY, USA; Department of Computer Science and Engineering, New York University, New York, NY, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA",IEEE Transactions on Information Forensics and Security,"6 May 2024","2024","19","","4374","4389","The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.","1556-6021","","10.1109/TIFS.2024.3372809","U.S. Office of Naval Research(grant numbers:00014-18-1-2058); Intel Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458667","LLM;AI;hardware;assertion generation;hardware security;vulnerability detection;ChatGPT;Codex","Hardware;Security;Benchmark testing;Codes;Writing;Task analysis;Source coding","","27","","57","IEEE","4 Mar 2024","","","IEEE","IEEE Journals"
"DataAgent: Evaluating Large Language Models’ Ability to Answer Zero-Shot, Natural Language Queries","M. Mishra; A. Braham; C. Marsom; B. Chung; G. Griffin; D. Sidnerlikar; C. Sarin; A. Rajaram","Irvington High School, Fremont, United States; Pioneer High School, Sousse, Tunisia; Davis Senior High School, Davis, United States; The Loomis Chaffee School, Windsor, United States; Bellarmine College Preparatory, Sunnyvale, United States; Rutgers University, New Brunswick, United States; Bellarmine College Preparatory, Sunnyvale, United States; University of Maryland, Frisco, United States",2024 IEEE 3rd International Conference on AI in Cybersecurity (ICAIC),"16 Feb 2024","2024","","","1","5","Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI’s GPT-3.5 as a ""Language Data Scientist"" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.","","979-8-3503-8185-6","10.1109/ICAIC60265.2024.10433803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433803","GPT;data science;natural language processing;large language model;data processing;RefleXion;Chain-of-Thought;SayCan;action plan generation;zero-shot prompting;plain language","Natural languages;Manuals;Data science;Benchmark testing;Data models;Task analysis;Standards","","","","10","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"Optimizing Generative AI Applications: A Comparative Study of Effective Prompting Techniques","S. Jyoti; M. Tejpal; J. K. R","Department of Software Systems, School of Computer Science & Engineering, Vellore Institute of Technology, Vellore, India; Department of Software Systems, School of Computer Science & Engineering, Vellore Institute of Technology, Vellore, India; Department of Computational Intelligence, School of Computer Science & Engineering, Vellore Institute of Technology, Vellore, India",2025 5th International Conference on Pervasive Computing and Social Networking (ICPCSN),"19 Jun 2025","2025","","","389","396","Generative Artificial Intelligence (GenAI) has emerged as a pivotal technology across various industries, driving advancements in automation, decision-making, and content generation. This paper investigates the efficacy of the prompt engineering methods such as zero-shot, one-shot, and few-shot prompting—in optimizing GenAI systems for diverse applications. Through a comprehensive literature review and an empirical survey involving 13 use cases such as chatbots, content creation, and medical decision support, we evaluate the performance of these prompting methods. The findings reveal that few-shot prompting excels in complex tasks, while zero-shot and one-shot prompting are more effective for simpler tasks. These insights provide practical guidance for leveraging GenAI across different domains, contributing to the advancement of AI-driven solutions.","","979-8-3315-3519-3","10.1109/ICPCSN65854.2025.11035207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035207","Generative Artificial Intelligence (GenAI);Large Language Models (LLMs);Prompt Engineering (PE);Zero-Shot Prompting (ZSP);Few-Shot Prompting (FSP);One-Shot Prompting (OSP);Clinical Decision Support Systems (CDSS);Human Resource Management (HRM)","Surveys;Pervasive computing;Industries;Decision support systems;Social networking (online);Large language models;Chatbots;Prompt engineering;Human resource management;Systematic literature review","","","","23","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation","K. Li; Y. Yuan; H. Yu; T. Guo; S. Cao","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China",IEEE Transactions on Evolutionary Computation,"","2025","PP","99","1","1","Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with an additional test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.","1941-0026","","10.1109/TEVC.2025.3593272","National Natural Science Foundation of China(grant numbers:62202023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11098743","Large Language Models;Code Generation;Test Case Generation;Co-Evolution","Codes;Accuracy;Maintenance engineering;Evolutionary computation;Electronic mail;Software development management;Programming;Dynamic scheduling;Computer bugs;Training","","","","","IEEE","28 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Beyond the Hype: An Empirical Assessment of LLMs and Traditional ML for Code Vulnerability Detection","K. A. Rafique; S. Das; C. Grimm","Chair of Cyber-Physical Systems, University of Kaiserslautern-Landau (RPTU), Kaiserslautern, Germany; Chair of Cyber-Physical Systems, University of Kaiserslautern-Landau (RPTU), Kaiserslautern, Germany; Chair of Cyber-Physical Systems, University of Kaiserslautern-Landau (RPTU), Kaiserslautern, Germany",2025 IEEE Swiss Conference on Data Science (SDS),"18 Jul 2025","2025","","","87","94","In the era of large language models (LLMs), the detection of software vulnerabilities in code snippets remains a critical challenge, particularly in resource-constrained environments. This paper explores the efficacy of fine-tuned and zero-shot LLMs, and traditional machine learning (ML) models for classifying code snippets as “vulnerable” or “not vulnerable” based on the Software Common Weakness Enumeration (CWE) framework. We perform an evaluation of three fine-tuned LLMs: Flan-T5-small, Llama-3.2-1B-Instruct and Deepseek-llm-7B-Base—alongside three traditional ML models: Logistic Regression, Multinomial Naïve Bayes, and Linear Support Vector Classifier. We then compare these approaches with zero-shot models - Deepseek-R1 and ChatGPT-o1. Our experiments show that LLMs achieve competitive validation and test accuracies, with Llama-3.2-1B-Instruct and Deepseek-llm-7B-Base both achieving 84% accuracy on the test set. However, traditional ML models, particularly Logistic Regression and Linear SVC, demonstrate strong performance with 78 % accuracy, highlighting their viability in scenarios where computational resources are limited. Furthermore, we address the challenges of dataset imbalance, resource-intensive fine-tuning, and the practical applicability of these models in real-world vulnerability detection pipelines. Our study emphasizes the importance of considering traditional ML models as viable alternatives to LLMs, especially in resourceconstrained environments while contributing to the ongoing research on the practical applicability of AI, and data science in software security.","2835-3420","979-8-3315-9467-1","10.1109/SDS66131.2025.00019","BMBF Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081515","Code Vulnerability Detection;Large Language Models (LLMs);ChatGPT-o1;Deepseek-R1;Deepseek-7B;FlanT5;Llama-3.2;Logistic Regression;Multinomial Naïve Bayes;Linear SVC;Parameter-Efficient Fine-Tuning (PEFT)","Logistic regression;Accuracy;Codes;Computational modeling;Pipelines;Static VAr compensators;Machine learning;Predictive models;Software;Security","","","","16","IEEE","18 Jul 2025","","","IEEE","IEEE Conferences"
"Can LLMs Generate Green Code - A Comprehensive Study Through LeetCode","J. F. Tuttle; D. Chen; A. Nasrin; N. Soto; Z. Zong","Computer Science Dept., Texas State University, San Marcos, USA; Computer Science Dept., Texas State University, San Marcos, USA; Computer Science Dept., Texas State University, San Marcos, USA; Computer Science Dept., Texas State University, San Marcos, USA; Computer Science Dept., Texas State University, San Marcos, USA",2024 IEEE 15th International Green and Sustainable Computing Conference (IGSC),"29 Nov 2024","2024","","","39","44","This paper presents a comprehensive analysis of code generated by Large Language Models (LLMs) in terms of energy consumption and speed, moving beyond traditional accuracy metrics. We evaluate eight state-of-the-art models, including ChatGPT-3.5, ChatGPT-4o, CodeGemma:7b, WizardCoder:33b, Llama3:8b, Phind-CodeLlama:34b-v2, Nous-Hermes2:10.7b, and Mistral:7b, using RuntimeRatio and EnergyRatio metrics on LeetCode1 questions. To evaluate LLMs’ understanding on energy efficient concepts, we propose a three-step prompting methodology to test models on coding with runtime and energy efficiency in mind. In an overwhelming majority of cases, LLMs perform worse than the human solution in both code runtime and energy consumption. In cases where models do perform well, it is often because they are aware of the question online, and produce responses similar or identical to famous solutions. Our comparative analysis of the efficient prompt and chain of thought prompt methods to the base prompt emphasize that models are inconsistent with producing quicker and more efficient code when directly asked to assemble energy efficient code. Our findings highlight that LLMs currently lack understanding in energy efficiency in terms of code generation, pushing a need for efforts to add green computing ideas and energy efficiency for future training and fine-tuning.","2993-2084","979-8-3315-0786-2","10.1109/IGSC64514.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765804","large language models;LLM code generation;energy usage;energy efficiency;LeetCode;language model evaluation;green code","Measurement;Training;Energy consumption;Codes;Runtime;Large language models;Computational modeling;Green computing;Energy efficiency;Encoding","","","","20","IEEE","29 Nov 2024","","","IEEE","IEEE Conferences"
"SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation","I. Petrukha; Y. Kurliak; N. Stulova","MacPaw, Kyiv, Ukraine; MacPaw, Kyiv, Ukraine; MacPaw, Kyiv, Ukraine",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","73","77","In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-Xl and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-Centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-Oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052807","Program Synthesis;Code Generation Benchmark;Large Language Models;Swift Programming Language","Codes;Translation;Foundation models;Large language models;Benchmark testing;Encoding;Multilingual;Software engineering;Python","","","","31","CCBYNCND","2 Jul 2025","","","IEEE","IEEE Conferences"
"Evaluating Large Language Models Versus Traditional Tools in SQL Injection Exploit Generation","F. Jameel; W. Ahmad; M. Heydari; M. Shahpasand; V. H. F. Tafreshi","Department of Computing, Staffordshire University, London, United Kingdom; Department of Computing, Staffordshire University, London, United Kingdom; Department of Computing, Staffordshire University, London, United Kingdom; Department of Computing, Staffordshire University, London, United Kingdom; Department of Computing, Staffordshire University, London, United Kingdom",Global Congress on Emerging Technologies (GCET-2024),"26 Mar 2025","2024","","","322","329","SQL injection (SQLi) remains a critical threat to database security, as it exploits vulnerabilities that allow unauthorized access to or manipulation of database systems. Traditional tools like SQLmap are commonly used for vulnerability testing but often lack adaptability and efficiency in complex database environments. This research examines the potential of generative artificial intelligence to improve SQLi payload generation, specifically through a fine-tuned models of ChatGPT 3.5 Turbo by OpenAI and Command R by Cohere.The study began by using SQLmap to generate 1,136 SQLi payloads, which achieved a success rate of 6.51% in bypassing authentication and extracting data from a MySQL database. These payloads were then used to fine-tune Command R and ChatGPT 3.5 Turbo models, which subsequently generated 1,007 and 1,004 new payloads, respectively. The effectiveness of these payloads was tested using Burp Suite, a proxy software, targeting the OWASP Mutillidae II testbed. This involved setting parameters for username and password fields in the Burp Suite Intruder to launch attacks, resulting in ChatGPT 3.5 Turbo proved to be around 60% more effective than Command R and nearly 75% better than SQLmap. Furthermore, Command R was about 130% more effective than SQLmap.","","979-8-3315-4260-3","10.1109/GCET64327.2024.10934554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934554","SQL Injection;Generative AI;Large Language Models;ChatGPT;Command R","Generative AI;Large language models;Passwords;SQL injection;Chatbots;Software;Database systems;Data models;Payloads;Testing","","","","20","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models","Y. Feng; Z. Chen; Z. Kang; S. Wang; H. Tian; W. Zhang; M. Zhu; W. Chen","State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China",IEEE Transactions on Visualization and Computer Graphics,"","2025","PP","99","1","14","The proliferation of large language models (LLMs) has underscored concerns regarding their security vulnerabilities, notably against jailbreak attacks, where adversaries design jailbreak prompts to circumvent safety mechanisms for potential misuse. Addressing these concerns necessitates a comprehensive analysis of jailbreak prompts to evaluate LLMs' defensive capabilities and identify potential weaknesses. However, the complexity of evaluating jailbreak performance and understanding prompt characteristics makes this analysis laborious. We collaborate with domain experts to characterize problems and propose an LLM-assisted framework to streamline the analysis process. It provides automatic jailbreak assessment to facilitate performance evaluation and support analysis of components and keywords in prompts. Based on the framework, we design JailbreakLens, a visual analysis system that enables users to explore the jailbreak performance against the target model, conduct multi-level analysis of prompt characteristics, and refine prompt instances to verify findings. Through a case study, technical evaluations, and expert interviews, we demonstrate our system's effectiveness in helping users evaluate model security and identify model weaknesses.","1941-0506","","10.1109/TVCG.2025.3575694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11020711","Jailbreak attacks;visual analytics;large language models","Analytical models;Security;Safety;Visualization;Taxonomy;Training;Semantics;Ethics;Large language models;Interviews","","1","","","IEEE","2 Jun 2025","","","IEEE","IEEE Early Access Articles"
"User Centric Evaluation of Code Generation Tools (Invited Paper)","T. Miah; H. Zhu","School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK; School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK",2024 IEEE International Conference on Artificial Intelligence Testing (AITest),"25 Sep 2024","2024","","","109","119","With the rapid advance of machine learning (ML) technology, large language models (LLMs) are increasingly explored as an intelligent tool to generate program code from natural language specifications. However, existing evaluations of LLMs have focused on their capabilities in comparison with humans. It is desirable to evaluate their usability when deciding on whether to use a LLM in software production. This paper proposes a user centric method for this purpose. It includes metadata in the test cases of a benchmark to describe their usages, conducts testing in a multi-attempt process that mimics the uses of LLMs, measures LLM generated solutions on a set of quality attributes that reflect usability, and evaluates the performance based on user experiences in the uses of LLMs as a tool. The paper also reports a case study with the method in the evaluation of ChatGPT's usability as a code generation tool for the R programming language. Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks. The user experiences are good with overall average number of attempts being 1.61 and the average time of completion being 47.02 seconds. Our experiments also found that the weakest aspect of usability is conciseness, which has a score of 3.80 out of 5.","2835-3560","979-8-3503-6505-4","10.1109/AITest62860.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685197","Machine learning;Large language models;ChatGPT;Code generation;Performance evaluation;Usability;R programming language","Computer languages;Codes;Natural languages;MIMICs;Benchmark testing;Metadata;Chatbots","","1","","42","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"Aligning Crowd-sourced Human Feedback for Code Generation with Bayesian Inference","M. F. Wong; C. Wei Tan",City University of Hong Kong; Nanyang Technological University,2024 IEEE Conference on Artificial Intelligence (CAI),"30 Jul 2024","2024","","","158","163","While large language models (LLMs) excel at code generation, translating abstract descriptions into robust and functional code remains a significant challenge. Despite dedicated efforts, existing works for refining code generation with LLMs have demonstrated limitations, either constrained by the static rules or computational overhead of additional training, ultimately proving insufficient to meet the intricate demands of real-world code quality. This paper proposes a method to improve code generation ability with LLM by combining reinforcement learning from human feedback (RLHF) with crowd-sourced computation, referred to as cRLHF. Our goal is to enhance code quality through diverse end-user feedback. Traditional RLHF, relying on a single evaluator, risks biases and overlooks insights, hampering LLMs’ growth. The cRLHF framework, powered by Bayesian inference, ensures objective code evaluation from multiple evaluators. Our experiments exhibit significant improvements in code correctness, showcasing the efficacy of crowd-sourcing with reinforcement learning.","","979-8-3503-5409-6","10.1109/CAI59869.2024.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605319","AI alignment;Bayesian analysis;Reinforcement learning;Inductive bias;Code generation","Training;Ranking (statistics);Codes;Refining;Reinforcement learning;Programming;Software","","","","37","IEEE","30 Jul 2024","","","IEEE","IEEE Conferences"
"Graph Retrieval-Augmented Generation for Large Language Models: A Survey","T. T. Procko; O. Ochoa","Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, United States of America","2024 Conference on AI, Science, Engineering, and Technology (AIxSET)","3 Dec 2024","2024","","","166","169","Large Language Models (LLMs) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without retraining, are 1) prompt engineering and 2) Retrieval-Augmented Generation (RAG). RAG is a form of prompt engineering, insofar as relevant lexical snippets retrieved from RAG corpora are vectorized and aggregated with prompts. However, RAG documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pretraining a LLM on massive and general corpora is to engender a generally applicable model, RAG is not: it is a means of LLM optimization, and as such, RAG document selection must be precise, not general. For expert tasks, it is imperative that a RAG corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs (KGs) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a better understanding of this novel research area for future work.","","979-8-3503-9099-5","10.1109/AIxSET62544.2024.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771030","LLM;GPT;fine-tuning;knowledge graphs;RAG","Surveys;Training;Uncertainty;Large language models;Noise;Knowledge graphs;Noise measurement;Prompt engineering;Reliability;Optimization","","11","","35","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review","H. Edirisinghe; D. Wickramaarachchi","Department of Industrial Management Faculty of Science, University of Kelaniya, Sri Lanka; Department of Industrial Management Faculty of Science, University of Kelaniya, Sri Lanka",2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI),"20 Jan 2025","2024","","","1","6","The rapid advancements in artificial intelligence have transformed software testing, with Large Language Models (LLMs) emerging as powerful tools for automating test case generation. This paper explores Quality Assurance (QA) for LLM-generated test cases in black-box testing through a systematic literature review. Though LLMs are increasingly used for test case generation, challenges in ensuring their quality remain. Following PRISMA guidelines, relevant studies were selected from databases focusing on critical quality attributes, QA frameworks, metrics, and challenges. LLMs demonstrate high efficiency but face numerous issues. A recommendation for future research is given on addressing standardized metrics and improving human-AI collaboration for enhanced testing outcomes.","","979-8-3315-0917-0","10.1109/SLAAI-ICAI63667.2024.10844968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10844968","large language models (LLMs);black-box testing;test case generation;Quality Assurance (QA);evaluation metrics","Measurement;Software testing;Quality assurance;Databases;Large language models;Focusing;Collaboration;Faces;Systematic literature review;Guidelines","","","","73","IEEE","20 Jan 2025","","","IEEE","IEEE Conferences"
"Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers","S. Hamer; M. d’Amorim; L. Williams","Department of Computer Science, North Carolina State University, Raleigh, North Carolina; Department of Computer Science, North Carolina State University, Raleigh, North Carolina; Department of Computer Science, North Carolina State University, Raleigh, North Carolina",2024 IEEE Security and Privacy Workshops (SPW),"4 Jul 2024","2024","","","87","94","Sonatype’s 2023 report found that 97% of developers and security leads integrate generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), into their development process. Concerns about the security implications of this trend have been raised. Developers are now weighing the benefits and risks of LLMs against other relied-upon information sources, such as StackOverflow (SO), requiring empirical data to inform their choice. In this work, our goal is to raise software developers’ awareness of the security implications when selecting code snippets by empirically comparing the vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an existing Java dataset from SO with security-related questions and answers. Then, we asked ChatGPT the same SO questions, gathering the generated code for comparison. After curating the dataset, we analyzed the number and types of Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each platform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer vulnerabilities with a statistically significant difference. Additionally, ChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings suggest developers are undereducated on insecure code propagation from both platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code copied and pasted, created by AI or humans, cannot be trusted blindly, requiring good software engineering practices to reduce risk. Future work can help minimize insecure code propagation from any platform.","2770-8411","979-8-3503-5487-4","10.1109/SPW63631.2024.00014","National Science Foundation; North Carolina State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579524","Software Engineering Security;Empirical Study;Large Language Models;Software Supply Chain;Code Generation","Java;Privacy;Codes;Generative AI;Supply chains;Chatbots;Market research","","9","","56","IEEE","4 Jul 2024","","","IEEE","IEEE Conferences"
"Fine Tuning Large Language Model for Secure Code Generation","J. Li; A. Sangalay; C. Cheng; Y. Tian; J. Yang","Concordia University, Montreal, Canada; Delhi Technological University, Delhi, India; Concordia University, Montreal, Canada; Queen's University, Kingston, Canada; Concordia University, Montreal, Canada",2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","86","90","AI pair programmers, such as GitHub's Copilot, have shown great success in automatic code generation. However, such large language model-based code generation techniques face the risk of introducing security vulnerabilities to codebases. In this work, we explore the direction of fine-tuning large language models for generating more secure code. We use real-world vulnerability fixes as our fine-tuning dataset. We craft a code-generation scenario dataset (C/C++) for evaluating and comparing the pre-trained and fine-tuned models. Our experiments on GPT-J show that the fine-tuned GPT-J achieved 70.4% and 64.5% ratios of non-vulnerable code generation for C and C++, respectively, which has a 10% increase for C and a slight increase for C++ compared with the pre-trained large language model.","","979-8-4007-0536-6","10.1145/3650105.3652299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599549","Code Generation;Cybersecurity;Artificial Intelligence (AI);Common Weakness Enumerations (CWEs)","Codes;Terminology;Large language models;C++ languages;Security;Tuning;Faces","","4","","40","","30 Jul 2024","","","IEEE","IEEE Conferences"
"GenFollower: Enhancing Car-Following Prediction With Large Language Models","X. Chen; M. Peng; P. Tiu; Y. Wu; J. Chen; M. Zhu; X. Zheng","Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Intelligent Transportation Thrust, Systems Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",IEEE Transactions on Intelligent Vehicles,"","2024","PP","99","1","11","Accurate modeling of car-following behaviors is crucial for autonomous driving systems. While recent advancements in large language models (LLMs) have shown promise in various domains, their application to car-following tasks has not been fully explored. In this study, we introduce GenFollower, a novel framework that leverages LLMs for car-following modeling. GenFollower employs a specialized prompt engineering technique that integrates heterogeneous inputs into structured natural language prompts, incorporating Chain of Thought (CoT) to enhance reasoning and clarity. GenFollower prioritizes interpretability by explicitly requiring explanations into the prompts, providing valuable insights into the model's decision-making process. Through experiments on the Waymo Open datasets, we demonstrate GenFollower's superior performance and interpretability. Our findings contribute to advancing the field of car-following modeling and pave the way for more robust and reliable autonomous driving systems.","2379-8904","","10.1109/TIV.2024.3484528","National Natural Science Foundation of China(grant numbers:52302379,62373315); Guangzhou Basic and Applied Basic Research Projects(grant numbers:2023A03J0106,2024A04J4290); Guangdong Provincial Natural Science Foundation-General Project(grant numbers:2024A1515011790); Guangdong Province General Universities Youth Innovative Talents Project(grant numbers:2023KQNCX100); Guangzhou Municipal Science and Technology Project(grant numbers:2023ZD006); Nansha District Key R&D Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10726688","Car-following;autonomous driving systems;large language models;zero-shot learning;prompting;interpretability;prediction","Adaptation models;Predictive models;Data models;Autonomous vehicles;Cognition;Decision making;Mathematical models;Computational modeling;Large language models;Vehicle dynamics","","3","","","IEEE","21 Oct 2024","","","IEEE","IEEE Early Access Articles"
"Generative AI Enabled Secure Communication in Smart Grid: Challenges and Solutions","M. Zeng; M. Xie; L. Meng; H. Zhang; T. Wu; J. Wang","Guangxi Power Grid Co., Ltd, China; Guangxi Power Grid Co., Ltd, China; Guangxi Power Grid Co., Ltd, China; Guilin University Of Electronic Technology, China; University of Science and Technology Beijing, China; University of Science and Technology Beijing, China",IEEE Network,"","2025","PP","99","1","1","Generative AI has recently emerged as a promising solution to address critical challenges in smart grid communication, including security, privacy, and efficiency. In this paper, we first introduce some concepts of smart grid and generative AI, outline their current state, and discuss the potential applications of integrating generative AI into smart grid. Then, we examine traditional and non-generative AI solutions, highlighting their limitations. Subsequently, we propose a promising solution by integrating generative AI into the smart grid to develop a generative AI-based agent, aiming to enhance the security and efficiency of smart grid communication while addressing privacy concerns. Additionally, we provide an experimental evaluation to demonstrate how the proposed solution can be applied to optimize smart grid holistically. Experimental results clearly show that the proposed generative AI agent approach can protect user privacy and significantly improve the overall efficiency and security of smart grid communication. Furthermore, we highlight future research directions for generative AI in smart grid applications, including the synergistic role of generative AI in smart grids, cross-domain applications and extensions, and quantum security of smart grids.","1558-156X","","10.1109/MNET.2025.3580477","Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:No. 2022B1212010005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11037821","Generative AI;smart grid;security;privacy;generative AI agent","Smart grids;Generative AI;Security;Artificial intelligence;Privacy;Firewalls (computing);Data privacy;Data models;Synthetic data;Training","","","","","IEEE","18 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Can AI Keep You Safe? A Study of Large Language Models for Phishing Detection","R. Chataut; P. K. Gyawali; Y. Usman","School of Computing and Engineering, Quinnipiac University, Hamden, CT, USA; Department of Computer Science and Electrical Engineering, Morgantown, West Virginia, USA; School of Computing and Engineering, Quinnipiac University, Hamden, CT, USA",2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC),"13 Feb 2024","2024","","","0548","0554","Phishing attacks continue to be a pervasive challenge in cybersecurity, with threat actors constantly developing new strategies to penetrate email inboxes and compromise sensitive data. In this study, we investigate the effectiveness of Large Language Models (LLMs) in the crucial task of phishing email detection. With the growing sophistication of these attacks, we assess the performance of three distinct LLMs: GPT-3.5, GPT-4, and a customized ChatGPT, against a carefully curated dataset containing both phishing and legitimate emails. Our research reveals the proficiency of LLMs in identifying phishing emails, with each model showing varying levels of success. The paper outlines the strengths and limitations of GPT-3.5, GPT-4, and the custom ChatGPT, illuminating their respective suitability for practical applications in email security. These results underscore the potential of LLMs in effectively identifying phishing emails and their significant implications for enhancing cybersecurity measures and safeguarding users from the risks of online fraud.","","979-8-3503-6013-4","10.1109/CCWC60891.2024.10427626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427626","Artificial Intelligence;ChatGPT;LLMs;phishing attacks;security","Phishing;Organizations;Chatbots;Electronic mail;Fraud;Task analysis;Tides","","16","","19","IEEE","13 Feb 2024","","","IEEE","IEEE Conferences"
"Decentralized e-Bidding for B2B Procurement using Blockchain and AI Autonomous Agents","C. Manno; G. Manno; E. Tramontana","Department of Mathematics and Informatics, University of Catania, Catania, Italy; Department of Electric, Electronic and Informatics Engineering, University of Catania, Catania, Italy; Department of Mathematics and Informatics, University of Catania, Catania, Italy",2025 33rd International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),"28 Jul 2025","2025","","","1","6","Modern supply chains still rely on centralized procurement platforms; hence they struggle with inefficiencies, lack of transparency, and limited adaptability to disruptions and market changes. This paper proposes a decentralized e-bidding platform for Business-to-Business (B2B) procurement, integrating blockchain technology and artificial intelligence driven Intelligent Autonomous Agents (IAAs) to ensure trustworthiness and efficiency. Blockchain’s immutable ledger provides transparency and security, while IAAs, powered by large language models (LLMs), automate operational bid evaluation and support strategic supplier selection. Simulations of two supply chain scenarios with varying information sharing levels have demonstrated that higher transparency enhances efficiency. Results confirm the suitability of IAAs for real-time decision-making and blockchain for secure automation in supply chain applications.","","979-8-3315-6512-1","10.1109/WETICE67341.2025.11091982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091982","large language models;blockchain;intelligent autonomous agents;procurement;e-bidding","Procurement;Automation;Costs;Large language models;Supply chains;Information sharing;Real-time systems;Autonomous agents;Blockchains;Sustainable development","","","","28","IEEE","28 Jul 2025","","","IEEE","IEEE Conferences"
"Securing UAV Delivery Systems with Blockchain and Large Language Models: An Innovative Logistics Solution","C. Dong; N. Syed; F. Jiang; R. Elphick-Darling; S. Chen; J. Zhang; M. Lu; X. Liu","School of Information Technology Faculty of Sci Eng & Built Env, Deakin University, Geelong, Australia; School of Information Technology Faculty of Sci Eng & Built Env, Deakin University, Geelong, Australia; School of Information Technology Faculty of Sci Eng & Built Env, Deakin University, Geelong, Australia; Centre for Regional and Rural Futures Faculty of Sci Eng & Built Env, Deakin University, Geelong, Australia; Data61, CSIRO, Sydney, Australia; Hanyang University ERICA, Zhejiang Normal University, Melbourne Victoria University, Melbourne, Australia; School of Artificial Intelligence, Guangxi Minzu University, Nanning, China; School of Information Technology Faculty of Sci Eng & Built Env, Deakin University, Geelong, Australia",2024 11th International Conference on Machine Intelligence Theory and Applications (MiTA),"15 Nov 2024","2024","","","1","8","In the evolving landscape of logistics and supply chain management, Unmanned Aerial Vehicles (UAVs) have emerged as a promising solution to enhance delivery efficiency and reach. However, the integration of UAVs into mainstream logistics faces significant challenges, including security, data integrity, and scalable customer interaction. This study proposes an innovative framework that leverages blockchain technology and large language models to address these challenges, offering a secure, transparent, and intelligent UAV delivery system. By integrating blockchain, we ensure the immutability and transparency of delivery transactions, enhancing trust in UAV-operated logistics. Concurrently, the employment of large language models facilitates intelligent route optimization, dynamic task allocation, and real-time customer service through natural language processing capabilities. We present a comprehensive analysis of the proposed system’s architecture, security features, and operational efficiency through simulated delivery scenarios. Our findings indicate a notable improvement in delivery times, reduced operational costs, and enhanced customer satisfaction. This research not only contributes to the theoretical understanding of blockchain and AI applications in logistics but also provides a practical roadmap for implementing secure and efficient UAV delivery systems in the future.","","979-8-3503-8567-0","10.1109/MiTA60795.2024.10751689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10751689","Blockchain;UAV delivery;Large Language Models;Natural Language Processing","Supply chain management;Large language models;Scalability;Autonomous aerial vehicles;Chatbots;Blockchains;Security;Resource management;Vehicle dynamics;Logistics","","1","","21","IEEE","15 Nov 2024","","","IEEE","IEEE Conferences"
"Oracle-Guided Vulnerability Diversity and Exploit Synthesis of Smart Contracts Using LLMs","M. Eshghie; C. Artho","KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2240","2244","Many smart contracts are prone to exploits, which has given rise to analysis tools that try to detect and fix vulnerabilities. Such analysis tools are often trained and evaluated on limited data sets, which has the following drawbacks: 1. The ground truth is often based on the verdict of related tools rather than an actual verification result; 2. Data sets focus on low-level vulnerabilities like reentrancy and over-flow; 3. Data sets lack concrete exploit examples. To address these shortcomings, we introduce XploGen, which uses a model-based oracle specification of the business logic of the smart contracts to synthesize valid exploits using LLMs. Our experiments, involving 104 synthesized vulnerability-exploit pairs, demonstrated a 57% success rate in exploiting targeted aspects of the contract. ley achieved exploit efficiency with an average of only 3.5 transactions per exploit, highlighting the effectiveness of our methodology.CCS CONCEPTS• Security and privacy → Software and application security; • Software and its engineering → Software verification and validation.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765027","Exploit Synthesis;Smart Contract;Vulnerability;LLM;Large Language Models","Privacy;Systematics;Smart contracts;Software;Application security;Logic;Security;Business;Software engineering;Context modeling","","","","36","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Automated Program Repair for Introductory Programming Assignments","H. Wan; H. Luo; M. Li; X. Luo","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China",IEEE Transactions on Learning Technologies,"11 Jun 2024","2024","17","","1705","1720","Automatic program repair (APR) tools are valuable for students to assist them with debugging tasks since program repair captures the code modification to make a buggy program pass the given test-suite. However, the process of manually generating catalogs of code modifications is intricate and time-consuming. This article proposes contextual error model repair (CEMR), an automated program repair tool for introductory programming assignments. CEMR is designed to learn program code modifications from incorrect–correct code pairs automatically. Then, it utilizes these code modifications along with CodeBERT, a generative AI, to repair students' new incorrect programs in the same programming assignment. CEMR builds on the observation that code edits performed by students in pairs of incorrect–correct code can be used as input–output examples for learning code modifications. The key idea of CEMR is to leverage the wisdom of the crowd: it uses the existing code modifications of incorrect–correct student code pairs to repair the new incorrect student attempts. We chose three of the most related APR tools, Refazer, Refactory, and AlphaRepair, as the baselines to compare against CEMR. The experimental results demonstrate that, on public and real classroom datasets, CEMR achieves higher repair rates than the baselines. Through further analysis, CEMR has demonstrated promising effectiveness in addressing semantical and logical errors while its performance in fixing syntactical errors is limited. In terms of time for repairing buggy programs, CEMR costs approximately half as much as AlphaRepair requires. We opine that CEMR not only be seen as a program repair method that achieves good results with incorrect–correct code pairs but also be further utilized to generate hints to better assist students in learning programming.","1939-1382","","10.1109/TLT.2024.3403710","National Natural Science Foundation of China(grant numbers:61907002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535720","Large language model;program repair;programming education","Codes;Maintenance engineering;Programming profession;Context modeling;Syntactics;Computer bugs;Problem-solving","","3","","44","IEEE","21 May 2024","","","IEEE","IEEE Journals"
"Enhancing Supply Chain Efficiency Through Retrieve-Augmented Generation Approach in Large Language Models","B. Zhu; C. Vuppalapati","Global Supply Chain, Intel Corp., Hillsboro, Oregon, USA; Computer Engineering, San Jose State University, San Jose, USA",2024 IEEE 10th International Conference on Big Data Computing Service and Machine Learning Applications (BigDataService),"29 Oct 2024","2024","","","117","121","This paper delves into the fascinating integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) for optimizing supply chain management operations. RAG combines the robust retrieval capabilities of information retrieval systems with the generative prowess of neural language models to create a powerful tool that bolsters data protection while expanding the knowledge base to capture supply chain intricacies. This innovative methodology revolves around a dual-component system that employs a retrieval module to pinpoint relevant information from a knowledge base, while a generation module crafts contextualized responses using large language models. Through iterative retrieval strategies and tailored chunk optimization techniques, RAG enables contextualized analysis, predictive insights, and data-driven decision-making that streamlines processes from demand forecasting to inventory optimization. An experimental setup mimicking enterprise data classification assesses RAG's efficacy, employing recursive retrieval, multi-hop querying, and integration of generative and retrieval processes. Results showcase RAG's potential to revolutionize supply chain logistics, enhancing operational agility, minimizing disruptions, and fortifying data security. The impacts span improved forecasting accuracy, inventory level optimization, supplier risk assessment, and comprehensive supply chain reporting. However, RAG necessitates stringent ethical considerations and robust countermeasures against exploitation. Future work centers on system scalability, advanced evaluation metrics, and interdisciplinary collaboration between machine learning, retrieval systems, and supply chain domains. Overall, this paper presents a groundbreaking approach to optimizing supply chain management operations that could significantly impact the industry's future.","2690-828X","979-8-3503-6638-9","10.1109/BigDataService62917.2024.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10730655","Deep Learning;RAG;Supply Chain Operations;Unstructured Big Data;LLM","Ethics;Technological innovation;Supply chain management;Large language models;Scalability;Supply chains;Knowledge based systems;Decision making;Machine learning;Optimization","","1","","11","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"Designing Prompts and Creating Cleaned Scientific Text for Retrieval Augmented Generation for More Precise Responses from Generative Large Language Models","R. Lakatos; E. K. Urbán; Z. J. Szabó; J. Pozsga; E. Csernai; A. Hajdu","Data Science and Visualization Faculty of Informatics, University of Debrecen, Debrecen, Hungary; Data Science and Visualization Faculty of Informatics, University of Debrecen, Debrecen, Hungary; Data Science and Visualization Faculty of Informatics, University of Debrecen, Debrecen, Hungary; Data Science and Visualization Faculty of Informatics, University of Debrecen, Debrecen, Hungary; GE HealthCare, Chicago, Illinois, United States; Data Science and Visualization Faculty of Informatics, University of Debrecen, Debrecen, Hungary",2024 IEEE 3rd Conference on Information Technology and Data Science (CITDS),"17 Dec 2024","2024","","","1","6","This paper presents a comprehensive methodology for extracting and processing data from the scientific literature to improve the performance of generative language models in the case of the application of Retrieval Augmented Generation. We show how a knowledge-based system can be created to extract information from scientific literature using generative language models. The methodology involves a two-phase approach, utilizing the GROBID PDF processing system for initial data extraction, followed by refinement through a custom text-cleaning module. The processed data is formatted into JSON for integration into a semantic search engine, facilitating efficient searching and retrieval. Additionally, the setup of prompts and management of generative language models are meticulously detailed to optimize response quality. Evaluation of response performance using metrics such as BLEU, ROUGE, METEOR, and cosine similarity demonstrates the efficiency of the proposed methodology. As a result, the efficiency of generative language models using data embedded and cleaned in our knowledge-based system improves by 9 % in terms of cosine similarity and by 27 %, 6%, and 2% in the case of BLEU, ROUGE, METEOR scores, in contrast to the direct usage of the data extracted by GROBID. Overall, this work showcases the methodology's effectiveness in improving the quality of responses generated by language models and lays the groundwork for further advancements in natural language processing and semantic search systems within the scientific literature domain.","","979-8-3503-8788-9","10.1109/CITDS62610.2024.10791382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10791382","large language models;prompt engineering;retrieval augmented generation;text-cleaning;knowledge-based systems;semantic search;natural language processing","Measurement;Semantic search;Large language models;Knowledge based systems;Data models;Natural language processing;Data mining;Meteors;Engines;Context modeling","","","","19","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"A Systematic Survey on Black-box Attacks in Large Language Models Within Communication Networks","W. Du; J. Xue; W. Guo; X. Yang; Y. Fu; Y. Wang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China",IEEE Network,"","2025","PP","99","1","1","Large Language Models (LLMs) exhibit extraordinary competence in language comprehension and generation. However, as these models find increasing adoption in communication systems, they also become vulnerable to adversarial threats. Such attacks exploit model response mechanisms to generate malicious content or execute jailbreak attempts. Given the paramount importance of reliability and security in communication networks, this issue has garnered considerable attention. Of particular concern are black-box attacks, which circumvent conventional defense strategies by exploiting input-output interactions without requiring internal model knowledge or parameter access. These attacks are highly clandestine and demonstrate substantial practical feasibility, with possible repercussions such as data compromise, the production of harmful content, and the interruption of standard operations. Although relevant research efforts have achieved notable breakthroughs, a comprehensive examination of the topic, especially a systematic review within the realm of communication networks, remains insufficient. This article seeks to offer a comprehensive survey of contemporary black-box attack strategies aimed at LLMs. We begin by retracing the development and applications of such attacks across diverse fields, then propose a taxonomy pertinent to black-box attacks within communication networks, classifying them into three principal categories: scenario and context manipulation attacks, transformation and evasion attacks, and automated and optimized generation attacks. In addition, we examine the associated impacts and potential risks, synthesize limitations in existing research, and present prospective directions and challenges for future research. Our overarching goal is to provide substantial insights that advance the security and reliability of LLMs while promoting the stable evolution of these models within communication network environments.","1558-156X","","10.1109/MNET.2025.3578610","Beijing Institute of Technology(grant numbers:62172042); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030592","Large Language Model;Black-box Attack;Network Security;Generative AI","Closed box;Security;Communication networks;Perturbation methods;Training;Context modeling;Large language models;Vectors;Surveys;Computational modeling","","","","","IEEE","11 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Large Language Models for Networking: Workflow, Advances and Challenges","C. Liu; X. Xie; X. Zhang; Y. Cui","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China",IEEE Network,"","2024","PP","99","1","1","The networking domain is characterized by its high complexity and rapid iteration, requiring extensive expertise to accomplish network tasks, ranging from network design, configuration, diagnosis and security. The inherent complexity of these tasks, coupled with the ever-changing landscape of networking technologies and protocols, poses significant hurdles for traditional methods. However, the recent emergence of large language models (LLMs) has sparked a new wave of possibilities in addressing these challenges. LLMs have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. These models, trained on extensive data, can benefit the networking domain. Some efforts have already explored the application of LLMs in the networking domain and revealed promising results. To describe the fundamental process involved in applying LLM for networking, we first propose a unified workflow that encompasses a majority of the previous work. We then introduce the highlights of existing works by category and explain in detail how they operate at different stages of the workflow. Furthermore, we delve into the challenges encountered, discuss potential solutions, and outline future research prospects. We hope that this survey will provide insight for researchers and practitioners, promoting the development of this interdisciplinary research field.","1558-156X","","10.1109/MNET.2024.3510936","National Natural Science Foundation of China(grant numbers:62132009,62221003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10777581","Large Language Models;Networking;LLMN Workflow","Data models;Surveys;Large language models;Knowledge engineering;Cognition;Accuracy;Training;Planning;Network security;Grammar","","6","","","IEEE","4 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools","S. Bhatia; T. Gandhi; D. Kumar; P. Jalote","IIIT Delhi, Delhi, India; IIIT Delhi, Delhi, India; IIIT Delhi, Delhi, India; IIIT Delhi, Delhi, India",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","54","61","Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.CCS Concepts• Software and its engineering → Software testing and debugging; • Computing methodologies → Artificial intelligence.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734640","Large Language Models;Unit Test Generation;ChatGPT;Generative AI","Software testing;Codes;Accuracy;Large language models;Scalability;Semantics;Chatbots;Software;Test pattern generators;Software development management","","2","","35","","30 Oct 2024","","","IEEE","IEEE Conferences"
"SECURE: Benchmarking Large Language Models for Cybersecurity","D. Bhusal; M. T. Alam; L. Nguyen; A. Mahara; Z. Lightcap; R. Frazier; R. Fieblinger; G. L. Torales; B. A. Blakely; N. Rastogi","Rochester Institute of Technology, Rochester, USA; Rochester Institute of Technology, Rochester, USA; Rochester Institute of Technology, Rochester, USA; RIT, Rochester, USA; RIT, Rochester, USA; RIT, Rochester, USA; RIT, Rochester, USA; RIT, Rochester, USA; Argonne National Lab, USA; Rochester Institute of Technology (RIT), Rochester, USA",2024 Annual Computer Security Applications Conference (ACSAC),"18 Mar 2025","2024","","","15","30","Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness. Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks. To address this gap, we introduce the SECURE (Security Extraction, Understanding & Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios. SECURE includes six datasets focused on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources. Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts. We also offer recommendations for improving LLMs reliability as cyber advisory tools and release our benchmark datasets and framework for community use at https://github.com/aiforsec/SECURE.","2576-9103","979-8-3315-2088-5","10.1109/ACSAC63791.2024.00019","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917682","Large Language Models;Cybersecurity;Benchmarking;Dataset;Industrial Control System","Large language models;Industrial control;Benchmark testing;Cognition;Reliability;Computer security;Computer crime;Context modeling","","1","","68","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation","A. Lazarev; D. Sedov","Department of Control and Applied Mathematics, Moscow Institute of Physics and Technology, Moscow, Russia; Institute for Statistical Studies and Economics of Knowledge, HSE University, Moscow, Russia","2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)","27 Dec 2024","2024","","","317","321","The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.","","979-8-3315-3217-8","10.1109/SUMMA64428.2024.10803746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803746","LLM;Gemini;Large Language Model;NLP;text summarization","Navigation;Large language models;Finance;Market research;Technology forecasting;Mathematical models;Internet;Prompt engineering;Data mining;Software development management","","","","6","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Context Matters: Investigating Its Impact on ChatGPT's Bug Fixing Performance","X. Qu; F. Zuo; X. Li; J. Rhee","University of Central Oklahoma, Edmond, Oklahoma, USA; University of Central Oklahoma, Edmond, Oklahoma, USA; Microsoft, Redmond, Washington, USA; University of Central Oklahoma, Edmond, Oklahoma, USA","2024 IEEE/ACIS 22nd International Conference on Software Engineering Research, Management and Applications (SERA)","26 Sep 2024","2024","","","17","23","In this study, we explore the role of contextual information in enhancing ChatGPT's capabilities in bug fixing. Our focus is specifically on the “Wrong Answer” problem, where a program executes without error but fails to produce the correct output. Our approach draws inspiration from human debugging practices, which heavily rely on understanding both the intended task of the program and the specific scenarios in which it fails, such as unit test cases. We evaluate ChatGPT's performance with various types and levels of contextual data. The results reveal three key insights. First, providing the model with a mix of correct and incorrect test cases sharpens its debugging skills. Second, giving ChatGPT detailed descriptions of the problems substantially enhances its ability to identify and resolve errors. Third, merging detailed problem descriptions with various test cases leads to a synergistic outcome. This combined approach significantly elevates the efficiency of the bug-fixing process compared to employing each type of contextual information individually. Our paper presents a thorough analysis based on these findings. It offers an extensive exploration of why and how contextual information can be strategically utilized to enhance ChatGPT's debugging effectiveness. Furthermore, this investigation enriches our comprehension of the underlying mechanisms by which contextual cues amplify the model's capacity for solving problems.","2770-8209","979-8-3503-9134-3","10.1109/SERA61261.2024.10685568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685568","Bug Fixing;Contextual Information;Large Language Models;Problem Descriptions;Test Cases","Computer bugs;Merging;Debugging;Chatbots;Context modeling;Software engineering","","","","28","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Navigating the Risks: A Review of Safety Issues in Large Language Models","H. Wang; Y. Li; Y. Wang; P. Liu; P. Li","Institute of Network Technology (Yantai), Yantai, China; Institute of Network Technology (Yantai), Yantai, China; Institute of Network Technology (Yantai), Yantai, China; Faculty of Business Information, Shanghai Business School, Shanghai, China; National Computer Network Emergency Response Technical Team/Coordination Center, Beijing, China","2024 IEEE 24th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","29 Oct 2024","2024","","","74","83","Since the advent of Large Language Models (LLMs) such as ChatGPT, natural language processing has seen significant progress, providing timely, context-aware, and intelligent solutions across various domains. While users benefit greatly from these advancements, increasing safety concerns have surfaced, highlighting vulnerabilities and their impact on daily tasks. It is crucial to understand the implications of LLMs and not simply trust their outputs blindly. This paper reviews literature on LLM safety issues including data privacy, security, adversarial attacks, and compliance with laws and ethical guidelines.. Specifically, limitations, risks, testing methods, and evaluation criteria from a safety perspective are discussed. Based on these insights, it proposes suggestions to mitigate current safety challenges. The goal is to aid researchers and developers in designing safer, more reliable LLMs and to enhance user interactions with these technologies.","2693-9371","979-8-3503-6565-8","10.1109/QRS-C63300.2024.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10727077","Large Language Models;safety;testing;evaluation;review","Reviews;Navigation;Large language models;Software quality;Reliability engineering;Safety;Software reliability;Security;Surface treatment;Testing","","","","71","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"Large Language Models are Autonomous Cyber Defenders","S. R. Castro; R. Campbell; N. Lau; O. Villalobos; J. Duan; A. A. Cardenas","University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz",2025 IEEE Conference on Artificial Intelligence (CAI),"7 Jul 2025","2025","","","1125","1132","Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents.","","979-8-3315-2400-5","10.1109/CAI64502.2025.00195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050539","Autonomous Cyber Defense;Incident Response;Large Language Models;Reinforcement Learning;Ai Agents","Training;Bridges;Protocols;Large language models;Human-machine systems;Reinforcement learning;Cognition;Security;Computer crime;Tuning","","","","32","IEEE","7 Jul 2025","","","IEEE","IEEE Conferences"
"Using LLMs in Software Requirements Specifications: An Empirical Evaluation","M. Krishna; B. Gaur; A. Verma; P. Jalote","Department of Computer Science, IIIT Delhi; Department of Computer Science, IIIT Delhi; Department of Computer Science, IIIT Delhi; Department of Computer Science, IIIT Delhi",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","475","483","The creation of a Software Requirements Specification (SRS) document is important for any software development project. Given the recent prowess of Large Language Models (LLMs) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. We assess the performance of GPT-4 and CodeLlama in drafting an SRS for a university club management system and compare it against human benchmarks using eight distinct criteria. Our results suggest that LLMs can match the output quality of an entry-level software engineer to generate an SRS, delivering complete and consistent drafts. We also evaluate the capabilities of LLMs to identify and rectify problems in a given requirements document. Our experiments indicate that GPT-4 is capable of identifying issues and giving constructive feedback for rectifying them, while CodeLlama's results for validation were not as encouraging. We repeated the generation exercise for four distinct use cases to study the time saved by employing LLMs for SRS generation. The experiment demonstrates that LLMs may facilitate a significant reduction in development time for entry-level software engineers. Hence, we conclude that the LLMs can be gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628461","Requirements engineering;software requirements specifications;empirical research;large language models","Productivity;Accuracy;Large language models;Impedance matching;Natural languages;Benchmark testing;Software","","15","","41","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets","M. Jahanshahi; A. Mockus","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; University of Tennessee, Knoxville, USA",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","104","111","A critical part of creating code suggestion systems is the pre-training of Large Language Models (LLMs) on vast amounts of source code and natural language text, often of questionable origin, quality, or compliance. This may contribute to the presence of bugs and vulnerabilities in code generated by LLMs. While efforts to identify bugs at or after code generation exist, it is preferable to pre-train or fine-tune LLMs on curated, high-quality, and compliant datasets. The need for vast amounts of training data necessitates that such curation be automated, minimizing human intervention.We propose an automated source code autocuration technique that leverages the complete version history of open-source software (OSS) projects to improve the quality of training data. The proposed approach leverages the version history of all OSS projects to: (1) identify training data samples that have ever been modified, (2) detect samples that have undergone changes in at least one OSS project, and (3) pinpoint a subset of samples that include fixes for bugs or vulnerabilities. We evaluate this method using ""The Stack"" v2 dataset, comprising almost 600M code samples, and find that 17% of the code versions in the dataset have newer versions, with 17% of those representing bug fixes, including 2.36% addressing known CVEs. The clean, deduplicated version of Stack v2 still includes blobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the dataset were never modified after creation, suggesting they likely represent software with minimal or no use. Misidentified blob origins present an additional challenge, as they lead to the inclusion of non-permissively licensed code, raising serious compliance concerns.By deploying these fixes and addressing compliance issues, the training of new models can avoid perpetuating buggy code patterns or license violations. We expect our results to inspire process improvements for automated data curation, a critical component of AI engineering, with the potential to significantly enhance the quality and reliability of outputs generated by AI tools.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00018","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028470","Large Language Models (LLMs);The Stack v2 Dataset;Open Source Software (OSS);LLMs for Code (LLM4Code);Software Supply Chains;World of Code (WoC);Security Vulnerability;Open Source Licensing","Training;Codes;Source coding;Large language models;Computer bugs;Supply chains;Training data;Security;History;Open source software","","","","23","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?","L. Y. -H. Lo; H. Qu","Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong",IEEE Transactions on Visualization and Computer Graphics,"25 Nov 2024","2025","31","1","1116","1125","In this study, we address the growing issue of misleading charts, a prevalent problem that undermines the integrity of information dissemination. Misleading charts can distort the viewer's perception of data, leading to misinterpretations and decisions based on false information. The development of effective automatic detection methods for misleading charts is an urgent field of research. The recent advancement of multimodal Large Language Models (LLMs) has introduced a promising direction for addressing this challenge. We explored the capabilities of these models in analyzing complex charts and assessing the impact of different prompting strategies on the models' analyses. We utilized a dataset of misleading charts collected from the internet by prior research and crafted nine distinct prompts, ranging from simple to complex, to test the ability of four different multimodal LLMs in detecting over 21 different chart issues. Through three experiments–from initial exploration to detailed analysis–we progressively gained insights into how to effectively prompt LLMs to identify misleading charts and developed strategies to address the scalability challenges encountered as we expanded our detection range from the initial five issues to 21 issues in the final experiment. Our findings reveal that multimodal LLMs possess a strong capability for chart comprehension and critical thinking in data interpretation. There is significant potential in employing multimodal LLMs to counter misleading information by supporting critical thinking and enhancing visualization literacy. This study demonstrates the applicability of LLMs in addressing the pressing concern of misleading charts.","1941-0506","","10.1109/TVCG.2024.3456333","HK RGC GRF(grant numbers:16210722); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10679256","Deceptive Visualization;Large Language Models;Prompt Engineering","Data visualization;Visualization;Fake news;Education;Libraries;Large language models;Technological innovation","Computer Graphics;Humans;Programming Languages;Natural Language Processing","5","","39","IEEE","12 Sep 2024","","","IEEE","IEEE Journals"
"JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models","J. Cao; Z. Chen; J. Wu; S. -C. Cheung; C. Xu","The Hong Kong University of Science and Technology, Hong Kong, China; Nanjing University, Nanjing, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; Nanjing University, Nanjing, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","870","882","Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs’ capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs’ capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM’s capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.CCS CONCEPTS• Software and its engineering → Automatic programming.","2643-1572","979-8-4007-1248-7","","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765035","Large Language Model;Program Synthesis;Object-Oriented Programming","Measurement;Java;Codes;Systematics;Object oriented modeling;Benchmark testing;Software;Skeleton;Python;Software engineering","","","","65","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Intelligent Scientific Cabinet System Based on Large-Scale Language Model and RFID Technology","M. Zhang; T. Xu; J. Zheng; M. Yu; S. Ye","School of Artificial Intelligent, Guangzhou Huashang College, Guangzhou, China; School of Artificial Intelligent, Guangzhou Huashang College, Guangzhou, China; School of Artificial Intelligent, Guangzhou Huashang College, Guangzhou, China; School of Artificial Intelligent, Guangzhou Huashang College, Guangzhou, China; School of Artificial Intelligent, Guangzhou Huashang College, Guangzhou, China",2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA),"1 Jul 2025","2025","","","1783","1786","This study proposes an intelligent scientific cabinet system that integrates large language models and radio frequency identification (RFID) technology, aiming to enhance the management efficiency and safety of hazardous chemicals. Current traditional hazardous chemical storage cabinets, such as Ronghai Tongda and Sanling Hazardous Chemical Intelligent Storage Cabinets, lack real-time monitoring and intelligent early warning mechanisms, making it difficult to adapt to complex safety standards and regulatory requirements. This system achieves closed-loop management of hazardous chemicals throughout their entire lifecycle through real-time monitoring, data analysis, behavior analysis, and environmental monitoring. The hardware part uses a carbon steel spray-painted shell, integrated with an Android industrial host and RFID reading and writing device. The software architecture combines LLM with RFID technology, integrating information at the terminal, and through prompt engineering and retrieval enhancement generation, it achieves precise management and information generation. Experimental results show that the system can effectively improve management efficiency, ensure compliance, and enhance safety.","","979-8-3315-0976-7","10.1109/AIITA65135.2025.11047648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11047648","Retrieval Enhancement;Large Language Models;Radio Frequency Identification;Prompt Engineering;Full Lifecycle Management","Training;Data analysis;Large language models;Writing;Real-time systems;Safety;Prompt engineering;Environmental monitoring;Chemicals;Radiofrequency identification","","","","13","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Resilience Assessment of Large Language Models under Transient Hardware Faults","U. K. Agarwal; A. Chan; K. Pattabiraman","The University of British Columbia, Canada; The University of British Columbia, Canada; The University of British Columbia, Canada",2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE),"2 Nov 2023","2023","","","659","670","Large Language Models (LLMs) are transforming the field of natural language processing and revolutionizing the way machines interact with humans. LLMs like ChatGPT and Google’s Bard have already made significant strides in conversational AI, enabling machines to understand natural language and respond in a more human-like manner. In addition to typical applications like sentiment analysis and text generation, LLMs are also used in safety-critical applications such as code generation and speech comprehension in autonomous driving vehicles, where reliability is important.In this work, we investigate the resilience of LLMs under transient hardware faults. Specifically, we used IR-level fault injection (FI) to assess the reliability of five popular LLMs, including Bert, GPT2, and T5, under transient hardware faults. Moreover, we also investigate how the resilience of LLMs varies with different pre-training, fine-tuning objectives, and the number of encoder and decoder blocks. We find that LLMs are quite resilient to transient faults overall. We also find that the behavior of the LLM under transient faults varies significantly with the input, LLM’s architecture, and the type of task (e.g., translation vs. fill-in-the-blank). Finally, we find that the Silent Data Corruption (SDC) rate varies with different fine-tuning objectives, and for the fill-mask fine-tuning objective, the SDC rate also increases with the model size. Overall, our findings indicate that the use of LLMs in safety-critical applications needs further investigation.","2332-6549","979-8-3503-1594-3","10.1109/ISSRE59848.2023.00052","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301253","Error resilience;LLMS;Soft Errors","Sentiment analysis;Speech coding;Computer architecture;Hardware;Data models;Software reliability;Decoding","","4","","63","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"TAPO: Task-Referenced Adaptation for Prompt Optimization","W. Luo; W. Wang; X. Li; W. Zhou; P. Jia; X. Zhao",City University of Hong Kong; City University of Hong Kong; City University of Hong Kong; City University of Hong Kong; City University of Hong Kong; City University of Hong Kong,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Prompt engineering can significantly improve the performance of large language models (LLMs), with automated prompt optimization (APO) gaining significant attention due to the time-consuming and laborious nature of manual prompt design. However, much of the existing work in APO overlooks task-specific characteristics, resulting in prompts that lack domain specificity and are not well-suited for task-specific optimization. In this paper, we introduce TAPO, a multitask-aware prompt optimization framework composed of three key modules. First, a task-aware metric selection module is proposed to enhance task-specific prompt generation capabilities. Second, we present a multi-metrics evaluation module to jointly evaluate prompts from multiple perspectives. Third, an evolution-based optimization framework is introduced for automatic prompt refinement, which improves adaptability across various tasks. Extensive experiments on six datasets demonstrate the effectiveness of our approach, and our code is publicly available 1.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888677","Impact Fund; Innovation and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888677","Prompt Engineering;Automated Prompt Optimization;Large Language Models;Multi-Task Learning","Feedback loop;Large language models;Manuals;Signal processing;Cognition;Distance measurement;Prompt engineering;Problem-solving;Speech processing;Optimization","","","","37","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"ChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?","M. Fu; C. K. Tantithamthavorn; V. Nguyen; T. Le","Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia",2023 30th Asia-Pacific Software Engineering Conference (APSEC),"2 Apr 2024","2023","","","632","636","Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4) exhibited remarkable advancement in a range of software engineering tasks associated with source code such as code review and code generation. In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair. We compare ChatGPT with state-of-the-art language models designed for software vulnerability purposes. Through an empirical assessment employing extensive real-world datasets featuring over 190,000 C/C++ functions, we found that ChatGPT achieves limited performance, trailing behind other language models in vulnerability contexts by a significant margin. The experimental outcomes highlight the challenging nature of vulnerability prediction tasks, requiring domain-specific expertise. Despite ChatGPT's substantial model scale, exceeding that of source code-pre-trained language models (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuning remains imperative for ChatGPT to generalize for vulnerability prediction tasks. We publish the studied dataset, experimental prompts for ChatGPT, and experimental results at https://github.com/awsm-research/ChatGPT4Vul.","2640-0715","979-8-3503-4417-2","10.1109/APSEC60848.2023.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479409","ChatGPT;Large Language Models;Cybersecurity;Software Vulnerability;Software Security","Codes;Reviews;Source coding;Predictive models;Maintenance engineering;Chatbots;Software","","42","","21","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"OpenRAG: Open-source Retrieval-Augmented Generation Architecture for Personalized Learning","R. Shan","Department of Data Science, North Carolina School of Science and Mathematics, Durham, NC, USA","2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)","4 Mar 2025","2024","","","212","216","This paper introduces OpenRAG, an open-source Retrieval-Augmented Generation (RAG) system architecture designed to enhance GenAI applications in personalized learning. The architecture is modular with loosely coupled components: Generator, User Interface, Indexing subsystem, Retriever, and Orchestration module. The research applies cutting-edge design patterns for retrieval, generation, indexing, storage, user interactions, and performance optimization, whereas the data flow and processing pipeline are seamlessly integrated for customization and adaptability. A case study demonstrates a proof-of-concept implementation of OpenRAG used in an online learning platform, resulting in significant increases in learner engagement and education efficiency. Key design concerns such as computational efficiency, content accuracy, learning styles, and user experience are discussed along with their solutions.","","979-8-3315-3122-5","10.1109/ICAIRC64177.2024.10900069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10900069","Retrieval-Augmented Generation (RAG);open source;architecture;system design;vector database;orchestration;information retrieval;Natural Language Processing (NLP);large language models (LLMs);personalized learning","Retrieval augmented generation;Pipelines;Systems architecture;Learning (artificial intelligence);Computer architecture;User interfaces;User experience;Robots;Optimization;Indexing","","1","","22","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"A Model Driven Method to Design and Analyze Secure Architectures of Systems-of-Systems","J. E. Hachem; T. A. Khalil; V. Chiprianov; A. Babar; P. Aniorte","UNIV PAU & PAYS ADOUR, PAU, FRANCE; Antonine University, Lebanon; UNIV PAU & PAYS ADOUR, PAU, FRANCE; UNIV PAU & PAYS ADOUR, PAU, FRANCE; UNIV PAU & PAYS ADOUR, PAU, FRANCE",2017 22nd International Conference on Engineering of Complex Computer Systems (ICECCS),"15 Feb 2018","2017","","","166","169","Context: Systems-of-Systems (SoS) is becoming the major paradigm for engineering next generation solutions such as smart cities, health-care and emergency response. However, SoS differentiating characteristics, such as emergent behavior, may introduce specific issues that make ensuring their security a critical challenge. Objective: the aim of this study is to investigate how Software Engineering approaches can be extended to model and analyze secure SoS solutions for discovering high impact cascading attacks at the architecture stage. Method: in order to achieve our objective, we followed the guidelines of Model Driven Engineering to propose a method, Systems-of-Systems Security (SoSSec), that comprises: (1) an architectural description language for modeling SoS and its vulnerabilities and (2) a MultiAgent System for security analysis of SoS architectures.","","978-1-5386-2431-9","10.1109/ICECCS.2017.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292818","Systems-of-Systems Security;Security Analysis;Model Driven Engineering;Software Architecture;Agent-based Simulation;Multi-Agent Systems;Smart Buildings","Unified modeling language;Security;Analytical models;Cascading style sheets;Computer architecture;Tools;Semantics","","8","","12","IEEE","15 Feb 2018","","","IEEE","IEEE Conferences"
"Understanding the Conceptual Structure of Large Language Models through Bibliographical Network","V. Duarte-Martínez; I. J. Perez; P. Ducange; M. J. Cobo","Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, ESPOL Polytechnic University, Guayaquil, Ecuador; Dpt. of Computer Science and Artificial Intelligence Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Granada, Spain; Dpt. of Information Engineering, University of Pisa, Italy; Dpt. of Computer Science and Artificial Intelligence Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Granada, Spain",2024 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS),"26 Jun 2024","2024","","","1","7","Large Language Models represent a transformative technology at the forefront of artificial intelligence and natural language processing, with applications spanning diverse domains. This study conducts a comprehensive science mapping analysis of the LLMs research field, leveraging bibliometric techniques to uncover its thematic structure, trends, and global actors involved. Utilizing data from the Web of Science, a corpus of 1303 research documents from 2010 to 2023 is analyzed, revealing a notable surge in research activity, particularly in recent years. Key thematic areas driving research within the field are identified, including chatbot, code generation, augmented reality, transformers, and machine learning paradigms. Foundational technologies such as transformers are pivotal in shaping the research landscape, while emerging themes like prompt learning hint at future directions. This study offers valuable insights for researchers, practitioners, and policymakers seeking to navigate the dynamic landscape of LLMs research and harness its full potential for societal benefit.","2473-4691","979-8-3503-6623-5","10.1109/EAIS58494.2024.10569112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569112","","Technological innovation;Codes;Navigation;Collaboration;Market research;Transformers;Chatbots","","","","19","IEEE","26 Jun 2024","","","IEEE","IEEE Conferences"
"A Framework for Using LLMs for Repository Mining Studies in Empirical Software Engineering","V. De Martino; J. Castaño; F. Palomba; X. Franch; S. Martínez-Fernández","Software Engineering (SeSa) Lab, University of Salerno, Italy; Universitat Politécnica de Catalunya, Spain; Software Engineering (SeSa) Lab, University of Salerno, Italy; Universitat Politécnica de Catalunya, Spain; Universitat Politécnica de Catalunya, Spain",2025 IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE),"10 Jul 2025","2025","","","6","11","Context: The emergence of Large Language Mod-els (LLMs) has significantly transformed Software Engineering (SE) by providing innovative methods for analyzing software repositories. Objectives: Our objective is to establish a practical framework for future SE researchers needing to enhance the data collection and dataset while conducting software repository mining studies using LLMs. Method: This experience report shares insights from two previous repository mining studies, focusing on the methodologies used for creating, refining, and validating prompts that enhance the output of LLMs, particularly in the context of data collection in empirical studies. Results: Our research packages a framework, coined Prompt Refinement and Insights for Mining Empirical Software repositories (PRIMES), consisting of a checklist that can improve LLM usage performance, enhance output quality, and minimize errors through iterative processes and comparisons among different LLMs. We also emphasize the significance of reproducibility by implementing mechanisms for tracking model results. Conclusion: Our findings indicate that standardizing prompt engineering and using PRIMES can enhance the reliability and reproducibility of studies utilizing LLMs. Ultimately, this work calls for further research to address challenges like hallucinations, model biases, and cost-effectiveness in integrating LLMs into workflows.","","979-8-3315-0225-6","10.1109/WSESE66602.2025.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11071447","Large Language Models;Mining Software Repositories;Prompt Engineering;LLM Reproducibility","Large language models;Refining;Data collection;Reliability engineering;Software;Reproducibility of results;Software reliability;Data mining;Prompt engineering;Software engineering","","","","26","IEEE","10 Jul 2025","","","IEEE","IEEE Conferences"
"DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models","M. Yuan; J. Chen; Z. Xing; A. Quigley; Y. Luo; T. Luo; G. Mohammadi; Q. Lu; L. Zhu","University of New South Wales, Australia; CSIRO's Data61, Australia; CSIRO's Data61, Australia; University of New South Wales, Australia; The Hong Kong University of Science and Technology, (Guangzhou), China; The Hong Kong University of Science and Technology, (Guangzhou), China; University of New South Wales, Australia; University of New South Wales, Australia; University of New South Wales, Australia",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2483","2494","The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel's v0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google's Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030228","Frontend Code Repair;Design Guideline;UI Design;Large Language Models","Codes;Large language models;Design methodology;Knowledge based systems;Maintenance engineering;User experience;Usability;System analysis and design;Software engineering;Software development management","","","","46","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"An Intra-lingual Stylistic Comparative Research for Function Evaluation of Large Language Models","K. Liu; Y. Du; L. Kong; P. Ji","Department of General Education, Clinical College of Anhui Medical University, Hefei, China; Department of General Education, Clinical College of Anhui Medical University, Hefei, China; Department of General Education, Clinical College of Anhui Medical University, Hefei, China; The Second School of Preschool Education Nantong Normal College, Rugao, China","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","24 Jun 2025","2025","","","137","141","The functional evaluation of large language models encompasses aspects such as semantic comprehension, reasoning, text generation, and translation. To provide methodological references for the functional evaluation of different large language models, five models including ERNIE 4.0 Turbo were selected. Initially, the same instruction was issued to these five models via prompt engineering to generate five English texts with similar styles, themes, and word counts. Subsequently, the meta-function theory of systemic functional linguistics was employed to analyze the differences in transitivity processes, mood types, and theme types among these texts, and a comparison was made with a reference text to explore whether stylistic differences exist and their technical motivations. The results revealed that, in general, the generated texts from domestic large language models closely approximate the functional semantic configurations of the original English reference samples, yet there are still hierarchical differences. This is because different models vary in their ability to balance computational power with interpretability, whereas foreign large language models can achieve rapid convergence in small-sample reasoning. Stylistic contrastive research based on semantic configuration features is conducive to comparing the functions of different large language models and optimizing their implementation technologies, thereby fostering customer-friendly technological models.","","979-8-3315-1091-6","10.1109/AEMCSE65292.2025.11042316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042316","Large Language Model;Semantic Configuration;Functional Stylistic Contrast;Functional Evaluation;Prompt Engineering","Translation;Mood;Large language models;Computational modeling;Semantics;Linguistics;Cognition;Prompt engineering;Software engineering;Convergence","","","","14","IEEE","24 Jun 2025","","","IEEE","IEEE Conferences"
"Leveraging RAG for Enhanced Business Intelligence with Local LLMs","A. Donvir; P. Yadav; S. Panyam; R. Joshi","Application Development, Wayne, NJ, United States; Independent Researcher, Surrey, BC, Canada; Cloud Computing, Data Engineering, Sunnyvale, United States; Independent Researcher, Staff Engineer, Seattle, USA",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0655","0661","This research addresses critical limitations in current business intelligence (BI) insights generation. It focuses on real-time data updates, accuracy, data privacy, security and more to build a system that is practically viable for modern-day businesses. Leveraging the proposed solution gives organizations a competitive edge in the fast-paced market. Research leverages emerging Large Language Models (LLMs) to derive relevant actionable insights while eliminating typical hallucinations. The research proposes system architecture that combines Retrieval-Augmented Generation (RAG) technology with local LLMs and real-time data streaming via Kafka to ensure data privacy, factual accuracy, and timely insights. Research presents quantitative and qualitative analysis for insights generation based on RAG vs. Direct LLM request. Moreover, research is executed on two different families of LLMs - thinking models and traditional models for detailed validation. Empirical testing reveals that RAG-based approaches outperform direct LLM queries in response time, CPU efficiency, and factual accuracy for insights generation. By leveraging local LLMs instead of cloud-based solutions, research aims to protect data and intellectual property of organizations.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105349","Retrieval-Augmented Generation (RAG);Business Intelligence (BI);Large Language Models;Data Privacy;Real-time Analytics;Vector Embeddings;Kafka;Local LLMs;Hallucination Mitigation;Natural Language Processing;ChromaDB;DeepSeek-R1;Gemma","","","","","27","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI and Structured Prompt Engineering","S. Ahmed; A. B. M Mohaimenur Rahman; M. M. Alam; M. S. Islam Sajid","Department of Computer and Information Sciences, Towson University, Towson, Maryland, USA; Department of Software and Information Systems, UNC Charlotte, Charlotte, North Carolina, USA; School of Cybersecurity, Old Dominion University, Norfolk, Virginia, USA; Department of Computer and Information Sciences, Towson University, Towson, Maryland, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","01007","01013","The rapid evolution of modern malware presents significant challenges to the development of effective defense mechanisms. Traditional cyber deception techniques often rely on static or manually configured parameters, limiting their adaptability to dynamic and sophisticated threats. This study leverages Generative AI (GenAI) models to automate the creation of adaptive cyber deception ploys, focusing on structured prompt engineering (PE) to enhance relevance, actionability, and deploy-ability. We introduce a systematic framework (SPADE) to address inherent challenges large language models (LLMs) pose to adaptive deceptions, including generalized outputs, ambiguity, under-utilization of contextual information, and scalability constraints. Evaluations across diverse malware scenarios using metrics such as Recall, Exact Match (EM), BLEU Score, and expert quality assessments identified ChatGPT-4o as the top performer. Additionally, it achieved high engagement (93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini demonstrated competitive performance, with Llama3.2 showing promise despite requiring further optimization. These findings highlight the transformative potential of GenAI in automating scalable, adaptive deception strategies and underscore the critical role of structured PE in advancing real-world cybersecurity applications.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903748","Cyber Deception;Generative AI;LLM;Malware","Measurement;Systematics;Limiting;Scalability;Large language models;Focusing;Malware;Quality assessment;Prompt engineering;Optimization","","","","27","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering","R. Shen; N. Inoue; K. Shinoda","Institute of Science, Tokyo; Institute of Science, Tokyo; Institute of Science, Tokyo",2024 IEEE International Conference on Image Processing (ICIP),"27 Sep 2024","2024","","","430","436","Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on visual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code. To overcome these challenges, this paper introduces PyramidCoder, a novel prompting framework for PVQA models. PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggregation. Notably, PyramidCoder utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures. Compared to the state-of-the-art PVQA model, our approach improves accuracy by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.","2381-8549","979-8-3503-4939-9","10.1109/ICIP51287.2024.10648180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10648180","Visual question answering;Large language models;Code generation;Prompting methods","Training;Visualization;Codes;Accuracy;Image processing;Large language models;Natural languages","","","","31","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"A Methodology for Risk Management of Generative AI based Systems","R. H. Filho; D. Colares","Postgraduate Program in Applied Informatics, University of Fortaleza, Fortaleza, Brazil; Postgraduate Program in Applied Informatics, University of Fortaleza, Fortaleza, Brazil","2024 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","23 Oct 2024","2024","","","1","6","This paper addresses cybersecurity challenges in integrating Generative Artificial Intelligence (GenAI) into applications. It proposes a risk analysis approach that focuses on identifying threats and vulnerabilities exploited by malicious actors, which is validated through a case study on prompt injection attacks. The results demonstrated the efficacy of our approach in identifying and quantifying vulnerabilities in Large Language Models (LLMs).","1847-358X","978-953-290-138-2","10.23919/SoftCOM62040.2024.10721790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721790","Cyber Security;Risk Management;Generative AI;Large Language Models","Generative AI;Computational modeling;Large language models;Software;Computer networks;Telecommunications;Risk management;Computer crime;Best practices","","","","14","","23 Oct 2024","","","IEEE","IEEE Conferences"
"Generative AI for Diagrams as Code and Code as Diagrams","M. Trifan; B. Ionescu; D. Ionescu","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada",2025 IEEE 19th International Symposium on Applied Computational Intelligence and Informatics (SACI),"23 Jun 2025","2025","","","01","06","Modern software projects often involve extensive codebases that are difficult to navigate, document, and keep consistent across multiple teams and repositories. Traditional documentation often becomes outdated, leading to inconsistencies and unclear architectural insights. To address this, Generative AI and Large Language Models (LLMs) such as ChatGPT and Claude automate the bidirectional transformation between code and diagrams, seamlessly integrating into software workflows. The proposed framework uses PlantUML and TikZ, to improve debugging, documentation, and versioning. Neo4j is used for natural language retrieval, and advanced queries using Cypher as well as similarity searches. This hybrid approach mitigates LLMrelated challenges like hallucinations by combining database integration with prompt engineering. By consolidating code, diagrams, and metadata into a unified resource, the framework aims to improve maintainability, collaboration, and transparency, as demonstrated in initial qualitative use cases.","2765-818X","979-8-3315-1547-8","10.1109/SACI66288.2025.11030105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030105","Diagrams as Code;Code as Diagrams;LLMs;Claude;ChatGPT;eXplainable AI (XAI);C4;Neo4j;PlantUML;TikZ","Visualization;Codes;Source coding;Collaboration;Documentation;Chatbots;Real-time systems;Prompt engineering;Software engineering;Software development management","","","","22","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"The Use and Misuse of Pre-Trained Generative Large Language Models in Reliability Engineering","Y. Hu; Y. Goktas; D. D. Yellamati; C. De Tassigny",Schneider Electric; Schneider Electric; Schneider Electric; Schneider Electric,2024 Annual Reliability and Maintainability Symposium (RAMS),"18 Mar 2024","2024","","","1","7","Generative Large Language Models (LLMs) have garnered significant attention since the release of ChatGPT in November 2022. Researchers are actively exploring diverse applications to leverage the capabilities of these LLM systems. Within the field of reliability engineering there exists a potential for fruitful utilization of such models. In this paper, we delve into the applications of Large Language Models in reliability engineering, specifically focusing on their impressive language processing capabilities beyond traditional Natural Language Processing (NLP) tasks. Our study aims to evaluate the LLMs' potential in answering complex engineering questions and offering solutions to intricate problems. Additionally, we investigate the limitations of LLMs to understand their boundaries in providing accurate and reliable outputs. The paper emphasizes the significance of prompt engineering to enhance the accuracy and reliability of LLMs for improved performance in quantitative tasks. By incorporating minor prompt engineering techniques, the Large Language Models (LLMs), especially GPT-4, exhibited promising performance in answering Certified Reliability Engineer (CRE) exam questions. Our study involves an analysis of the errors made by the LLMs, allowing for a understanding of their limitations. Drawing from our findings, we provide recommendations on the appropriate application and areas to exercise caution when employing LLMs in the field of reliability engineering. These insights aim to guide practitioners in maximizing the benefits of LLMs while being mindful of their limitations and potential pitfalls. It is important to note that Generative AI and LLMs are rapidly evolving, and the evaluation conducted in this study reflects the test results at the time of writing. We anticipate that LLM responses may vary in the future. We are currently conducting research on developing applications based on LLMs to support the daily tasks of reliability engineers. We are excited about the possibilities and look forward to sharing our outcomes and contributing to the community.","2577-0993","979-8-3503-0769-6","10.1109/RAMS51492.2024.10457630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457630","Large Language Model;Machine Learning;Reliability;FMEA","Generative AI;Random access memory;Focusing;Writing;Reliability engineering;Chatbots;Task analysis","","4","","8","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Algorithmic Inversion: A Learnable Algorithm Representation for Code Generation","Z. Shi; F. Wu; W. Zeng; Y. Kong; S. Shen; Y. Wu","Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; Chinese Academy of Sciences, Institute of Software, Beijing, China",2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC),"17 Jun 2025","2025","","","431","441","The prevalent fine-tuning paradigm for large language models (LLMs) has demonstrated strong performance on various code generation tasks. However, these models still fall short when confronted with algorithmic programming problems, where precise algorithmic reasoning is required. Humans typically adopt diverse algorithmic techniques to tackle complex programming problems, enabling general analysis and accurate implementation. Building on this observation, we propose a method that learns compact, LLM-friendly representations of algorithmic knowledge, termed Algorithmic Inversion (AI), which aims to aid LLMs in understanding programming problems. Specifically, we apply a lightweight fine-tuning process on codeoriented models to automatically learn algorithm embeddings. When concatenated with the inputs, the algorithm embeddings act as instructive signals, guiding LLMs in generating correct code solutions by providing contextual algorithmic hints. We apply our approach to models of three different parameter sizes and evaluate them on three algorithmic programming benchmarks. Our extensive experiments show that applying AI to small (1.5B parameters) models results in absolute improvements of up to 1.8 on Pass@1, while large models (1 5 B parameters) achieve improvements of up to 1.4, compared to Prompt-Tuning. Additionally, our method outperforms traditional full fine-tuning approaches by a significant margin across all tested benchmarks. Furthermore, our analysis of the generated code reveals that AI effectively enhances the model's problem-solving process by providing clear algorithmic guidance. Codes and datasets are available11https://github.com/joeysbase/Algorithmic-Inversion .","2643-7171","979-8-3315-0223-2","10.1109/ICPC66645.2025.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025867","Code Generation;Algorithmic Knowledge Representation;Large Language Models","Codes;Large language models;Semantics;Pipelines;Knowledge representation;Programming;Benchmark testing;Vectors;Encoding;Problem-solving","","","","32","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"VToT: Automatic Verilog Generation via LLMs with Tree of Thoughts Prompting","Y. Zhou; R. Chen; X. Li; J. Wang; Z. Fang; B. Wang; W. Bai; Q. Cao; L. Wang","National University of Defense Technology, Changsha, China; Qiyuan Laboratory, Beijing, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; Defense Innovation Institute, Academy of Military Sciences, Beijing, China","2025 Design, Automation & Test in Europe Conference (DATE)","21 May 2025","2025","","","1","2","The automatic generation of Verilog code using Large Language Models (LLMs) presents a compelling solution to enhance the efficiency of hardware design flow. However, the state-of-the-art performance of LLMs in Verilog generation remains limited compared to programming languages such as Python. Previous research, Chain of Thought (CoT), has demonstrated that incorporating intermediate reasoning steps can significantly improve the performance of LLMs in code generation. In this paper, we propose the Verilog Tree of Thoughts (VToT) method. This structured prompting technique addresses the abstraction gap between Verilog and CoT by embedding hierarchical design constraints within the prompt. Experimental results on the VerilogEval and RTLLM benchmarks demonstrate that VToT prompting enhances both the syntactic and functional correctness of the generated code. Specifically, according to the RTLLM benchmark, VToT achieved a correctness rate of 75.9% at pass@5, representing an improvement of 10.4%. Furthermore, in the VerilogEval benchmark, VToT achieved state-of-the-art performance with a correctness rate of 52.4% at pass@1 (an increase of 8.9%) and 65.4% at pass@5 (an increase of 9.6%).","1558-1101","978-3-9826741-0-0","10.23919/DATE64628.2025.10993029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10993029","Verilog;LLMs;Chain-of-Thought;Tree-of-Thoughts;Integrated Circuit Design Automatization","Integrated circuit synthesis;Codes;Large language models;Europe;Benchmark testing;Syntactics;Hardware;Cognition;Hardware design languages;Python","","","","5","","21 May 2025","","","IEEE","IEEE Conferences"
"Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation","D. Jin; Q. Fu; Y. Li","UNSW Sydney, Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO) Data61, Australia; UNSW Sydney, Australia",2025 IEEE Security and Privacy Workshops (SPW),"3 Jul 2025","2025","","","278","282","Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though 's minimal errors highlight the potential for LLM-driven AEG advancements.","2770-8411","979-8-3315-6643-2","10.1109/SPW67851.2025.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050034","Exploitability;Software Vulnerability;Auto-mated Exploit Generation;Large Language Model","Resistance;Privacy;Systematics;Large language models;Conferences;Benchmark testing;Software;Security","","","","17","IEEE","3 Jul 2025","","","IEEE","IEEE Conferences"
"Fine-Tuned Mistral Model for Multi-Agent Mental Health Counseling System","A. Patel; P. Lohumi; V. Shah; M. Dash; M. S. Arya","Research Dept, Accrete AI, Mumbai, India; Research Dept, Accrete AI, Mumbai, India; Research Dept, Accrete AI, Mumbai, India; Research Dept, Accrete AI, Mumbai, India; Research Dept, Accrete AI, Mumbai, India",2025 11th International Conference on Computing and Artificial Intelligence (ICCAI),"11 Aug 2025","2025","","","317","324","In today’s digital world, mental health issues such as stress, depression, and anxiety are often expressed through social media. We propose a multi-agent system utilizing the fine-tuned Mistral-7B model to detect and provide personalized counseling for mental health concerns. This paper discusses the development and deployment of a multi-agent system consisting of diagnostic and counseling agents. By fine-tuning Mistral with the Interpretable Mental Health Instruction (IMHI) dataset, the system offers personalized mental health support, improving scalability and accessibility to mental health services.","","979-8-3315-2491-3","10.1109/ICCAI66501.2025.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11106008","Large Language Models (LLMs);Prompt engineering;fine-tuning;mental health illness;social media;interpretability;multi-agent systems","Employee welfare;Social networking (online);Scalability;Mental health;Medical services;Real-time systems;Safety;Prompt engineering;Multi-agent systems;Testing","","","","18","IEEE","11 Aug 2025","","","IEEE","IEEE Conferences"
"Tackling Big Data Challenges in Large Language Models with An Emphasis on Sharing, Security, and Scalability","B. Reddy; C. M; V. K. Kolekar; G. Arun Francis; S. Sharma; M. B","Chitkara Centre for Research and Development, Chitkara University, Himachal Pradesh, India; School of Sciences, Physics & Electronics, JAIN (Deemed to be University), Bangalore, Karnataka, India; Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Electronics and Communication Engineering, Karpagam College of Engineering, Coimbatore, India; Centre of Research Impact and Outcome, Chitkara University, Rajpura, Punjab, India; Department of EEE, Prince Shri Venkateshwara Padmavathy Engineering College, Chennai, India",2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Mar 2025","2024","","","1","6","This paper proposes a novel approach to huge data challenges in large language models (LLMs) that prioritises sharing, security, and scale. Our solution leverages modern encryption algorithms, distributed computing tools, and edge computing principles to secure data and speed up computers. We routinely outperform previous methods in accuracy, precision, memory, and model fitting rates. Our research found that the strategy protects private data, reduces interaction costs, and speeds up model training. Cutting-edge technology allows accurate forecasts and seamless computer operations in many contexts. Ablation studies also demonstrate how our approach's components work together to improve performance. This paper advances AI-driven LLMs by demonstrating a secure, affordable, and scalable method for today's data issues. Future research may concentrate on blockchain and shared learning's larger applications and advancements as data security and computer scale become more important. Our strategy is a huge step towards creating safe and dependable AI systems that can manage today's data-driven situations.","","979-8-3315-1898-1","10.1109/ICTBIG64922.2024.10911841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911841","AI;Big Data;Encryption;Language Models;Privacy;Scalability;Security;Sharing;Technology","Computers;Accuracy;Computational modeling;Large language models;Scalability;Data security;Big Data;Data models;Encryption;Edge computing","","","","23","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Automated Codebase Reconciliation using Large Language Models","A. Gandhi; S. De; M. Chechik; V. Pandit; M. Kiehn; M. C. Chee; Y. Bedasso","Computer Science, University of Toronto, Canada; Advanced Micro Devices, AMD, Canada; Computer Science, University of Toronto, Canada; Advanced Micro Devices, AMD, Canada; Advanced Micro Devices, AMD, Canada; Advanced Micro Devices, AMD, Canada; Advanced Micro Devices, AMD, Canada",2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge),"2 Jul 2025","2025","","","1","11","Large-scale software projects frequently encounter the challenge of manually propagating code changes across branches—a process that is error-prone due to code divergence, conflicting dependencies, and branch-specific modifications. Automating code porting can streamline development workflows, accelerate development cycles, and improve team collaboration. However, achieving this automation presents significant hurdles, particularly in maintaining consistency and resolving conflicts during codebase integration. We propose a novel approach that integrates algorithmic analysis with artificial intelligence-driven code generation, leveraging multi-agent systems to automate the identification of porting requirements and the development of ‘context-aware’ modifications. Our comprehensive, end-to-end framework starts by extracting recent commits to evaluate divergence. It subsequently assesses the necessity for porting changes and employs large language model (LLM) based systems to generate adaptive code suggestions tailored to files exhibiting inconsistencies. Experimental results suggest a substantial decrease in manual work through pipeline-generated pull requests. Despite these promising outcomes, integrating LLMs into complex workflows presents challenges, such as handling intricate dependencies and ensuring alignment with a company’s software development issue tracking and change management systems. This paper explores the potential and limitations of LLMs in advancing automation within software engineering and suggests future directions for enhancing these models to achieve industry-grade reliability.","","979-8-3315-0211-9","10.1109/Forge66646.2025.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052825","Code Porting;Large Language Model;Multi-Agent Systems;Software Engineering Workflows;Version Control System","Codes;Automation;Large language models;Manuals;Reliability engineering;Software;Software reliability;Software engineering;Multi-agent systems;Software development management","","","","32","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Large Language Models for Vulnerability Detection in Static Code Analysis: A Survey","S. Jin","Meituan, China",2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD),"21 Jul 2025","2025","","","473","479","As software systems grow increasingly complex, security vulnerabilities pose escalating threats to digital infrastructure. This survey examines how Large Language Models (LLMs) enhance static vulnerability detection and systematically categorizes adaptation strategies into five approaches: Base Application, Static Analysis Augmentation, Knowledge Enhancement, Fine-tuning Based, and Hybrid Feature Fusion. Through standardized benchmark comparisons, we demonstrate these strategies' differential impact on detection performance and reveal complementary strengths between encoder-only and decoder-only architectures for varying security priorities. Despite advancements, challenges persist in model robustness, generalization, and adaptation to emerging vulnerabilities. Our comprehensive analysis establishes a foundation for future research while providing practical guidance for selecting and implementing LLM adaptation strategies in security-critical applications.","2769-3554","979-8-3315-1936-0","10.1109/ICAIBD64986.2025.11082007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082007","large language models;code auditing;vulnerability detection;source code vulnerability","Surveys;Adaptation models;Analytical models;Codes;Large language models;Static analysis;Computer architecture;Benchmark testing;Robustness;Security","","","","35","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"5G Specifications Formal Verification with Over-the-Air Validation: Prompting is All You Need","T. Wray; Y. Wang","Stevens Institute of Technology, Hoboken, NJ; Stevens Institute of Technology, Hoboken, NJ",MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM),"6 Dec 2024","2024","","","412","418","The critical role of 5G and other complex systems in infrastructure necessitates rigorous protocol verification and system validation to ensure security and reliability. This paper explores the application of applying Large Language Model enabled auto Formal Verification with Real-world Prompting on Large Language Models (LLMs) for 5G and NextG protocols, addressing ambiguities and security concerns in network infrastructure protocol and specification design. By leveraging generative transformer-based LLMs, we present a formal approach to prompt engineering that validates complex specifications and implements formal verification techniques to detect and eliminate hallucinations. Our approach is agnostic to specific LLMs, with performance comparisons across currently popular models. We thoroughly examine the human processes involved to identify entry points where Prompt Engineering can reduce process overhead. We have developed a novel framework for iterative prompting and self-monitoring to aid in formal verification using 5G reasoner, enabling closed-loop automatic 5G protocol verification. Focusing on the RRC layer of 5G release 17, specifically sections 5.3.3.3, 5.3.3.4, and 5.3.5.3, we examined the liveness properties and detected a total of seven vulnerabilities, including variations of Null Cipher, Denial of Service (DoS), Lullaby, and Incarceration attacks. Further, we established a general testing framework that spans conception, virtualization, and over-the-air testing, providing a holistic approach to security assessment. This comprehensive framework underscores the importance of robust protocol verification and system validation in the deployment of critical infrastructure technologies.","2155-7586","979-8-3503-7423-0","10.1109/MILCOM61039.2024.10773849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773849","5G;Artificial Intelligence;Prompt Engineering;Large Language Models;Security","Protocols;5G mobile communication;Wireless networks;Security;Reliability;Prompt engineering;System validation;Virtualization;Formal verification;Testing","","","","28","IEEE","6 Dec 2024","","","IEEE","IEEE Conferences"
"LLMs: A Promising New Tool for Improving Healthcare in Low-Resource Nations","A. Gangavarapu","Safety4XR, Sammamish, WA, USA",2023 IEEE Global Humanitarian Technology Conference (GHTC),"20 Dec 2023","2023","","","252","255","This paper explores the potential of large language models (LLMs) in addressing healthcare inequalities, particularly in underserved nations with provider shortages, limited resources, and funding constraints. The UN’s Sustainable Development Goal 3 aims to achieve health and well-being for all, but disparities persist. Recent advancements in LLMs, exemplified by OpenAI’s GPT-4, demonstrate their ability to surpass human performance on certain medical exams, offering an opportunity to augment the limited number of physicians and meet unmet healthcare needs affordably. By customizing LLMs for healthcare, various applications become possible, including automating patient screening, assisting with diagnosis and treatment, enabling virtual health assistants to track health indicators and educate communities, supporting frontline health workers in addressing basic healthcare needs, translating medical knowledge for accessibility, and aiding multilingual healthcare providers. However, challenges such as inadequate data and infrastructure, risks of bias and privacy breaches, cost and access barriers, and security threats must be addressed to ensure that the benefits of LLMs reach communities facing the greatest challenges and threats. This paper highlights the potential benefits, challenges, and the need for collaboration to harness the power of LLMs effectively in healthcare and contribute to achieving the UN’s Sustainable Development Goal 3.","2473-5728","979-8-3503-2132-6","10.1109/GHTC56179.2023.10354650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354650","LLMs;healthcare;multilingual;low-resource regions","Costs;Sociology;Medical services;Linguistics;Privacy breach;Security;Reliability","","11","","8","IEEE","20 Dec 2023","","","IEEE","IEEE Conferences"
"L2 · M = C2 Large Language Models Are Covert Channels","S. Gaure; S. Koffas; S. Picek; S. Rønjom","Norwegian National Security Authority, Norway; Cybersecurity Group, Delft University of Technology, The Netherlands; Digital Security Group, Radboud University, The Netherlands; Norwegian National Security Authority, Norway","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Large Language Models (LLMs) are susceptible to various attacks but can also improve the security of diverse systems. However, how well do open source LLMs behave as covertext distributions to, e.g., facilitate censorship-resistant communication? In this paper, we explore open-source LLM-based covert channels. We empirically measure the security vs. capacity of two open-source LLM models (Llama-7B and GPT-2) to assess their performance as covert channels. Although our results indicate that such channels are not likely to achieve high practical bitrates, we also show that the chance for an adversary to detect covert communication is low. To ensure our results can be used with the least effort as a general reference, we employ a conceptually simple and concise scheme and only assume public models.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10887756","The Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887756","","Large language models;Computational modeling;Bit rate;Watermarking;Bandwidth;Signal processing;Data models;Cryptography;Speech processing;Fake news","","","","21","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models","K. Xu; G. L. Zhang; X. Yin; C. Zhuo; U. Schlichtmann; B. Li","Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany; Hardware for Artificial Intelligence Group, Technical University of Darmstadt, Darmstadt, Germany; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Chair of Electronic Design Automation, Technical University of Munich (TUM), Munich, Germany; Research Group of Digital Integrated Systems, University of Siegen, Siegen, Germany",2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),"6 Nov 2024","2024","","","1","9","In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular $\mathrm{C} / \mathrm{C}++$ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.","","979-8-3503-6356-2","10.1109/MLCAD62225.2024.10740262","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740262","","Training;Codes;Costs;Large language models;Manuals;Machine learning;Maintenance engineering;Software;Iterative methods;Optimization","","5","","29","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Intrusion Detection Technology Based on Large Language Models","H. Lai","Pittsburgh Institute, Sichuan University, Chengdu, China",2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT),"22 Jan 2024","2023","","","1","5","This paper explores the application of large language models, particularly BERT, to intrusion detection technology. The urgency of developing sophisticated intrusion detection systems (IDS) has grown with the rise of complex cyber threats. The paper proposes a novel framework that employs BERT to extract meaningful features from network data and identify anomalous behavior. By transforming network data into natural language text format, the model effectively discerns patterns often overlooked by traditional IDS. The research findings reveal that the BERT-based IDS significantly outperforms conventional machine learning methods in terms of accuracy, detection rate, and false positive rate. Experimental results demonstrate the model's robustness in various network environments, showcasing its versatility. Specifically, the model excels in identifying unknown threats, reducing false positives, and enhancing attack detection accuracy. These findings emphasize the potential value of large language models in the field of network security.","","979-8-3503-1341-3","10.1109/EASCT59475.2023.10393509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393509","Intrusion detection systems;BERT;large language models;cybersecurity;network data;machine learning","Computational modeling;Natural languages;Intrusion detection;Machine learning;Evolutionary computation;Network security;Feature extraction","","8","","12","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"APTSniffer: Detecting APT Attack Traffic Using Retrieval-Augmented Large Language Models","H. Xu; C. Si; Zhouzhou; C. Wang; P. Sun; Q. Liu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing; National Computer Network Emergency Response Technical Team/Coordination Center of China, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing; Institute of Information Engineering, Chinese Academy of Sciences, Beijing; Institute of Information Engineering, Chinese Academy of Sciences, Beijing; Institute of Information Engineering, Chinese Academy of Sciences, Beijing","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Advanced Persistent Threats (APT) differ from traditional attacks by using more complex and covert strategies for long-term assaults, posing a severe threat to organizational and national security. Due to problems like the shortage of APT traffic data and encrypted traffic obfuscation, existing methods cannot accurately identify APT traffic with just a few traffic samples. To overcome the above limitation, we propose a novel encrypted APT traffic detection model, APTSniffer, which combines large language models (LLM) and retrieval-augmented technology. APTSniffer utilizes the few-shot inference and generalization abilities of large language models by converting raw traffic data into natural language inference examples understandable by the LLM. Experimental results show that, compared to other baseline models, APTSniffer exhibits SOTA performance. It achieves F1 scores above 97% on three APT datasets, making it practically applicable for APT traffic detection tasks.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888022","Encrypted Traffic Classification;Large Language Models;Retrieval-Augmented Techniques;Advanced Persistent Threats","Large language models;Natural languages;Training data;Signal processing;Cognition;Acoustics;Cryptography;Speech processing;National security","","","","25","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Towards a Conversational Invoice Issuance LLM-Based Agent","R. Nie; H. Wu; L. Ma; Z. Liu; Z. Wang; P. Zhang","Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China; Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China; Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China; Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China; Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China; Dept. AI of Research Institute, Aisino Co., Ltd., Beijing, China","2024 7th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI)","3 Mar 2025","2024","","","1","5","The traditional invoice issuance process within tax administration is labor-intensive and prone to errors, necessitating a shift towards digitalization. Despite the advent of digital invoicing systems that streamline invoice generation and automate rule-based audits, integration with existing financial accounting systems remains a challenge. Particularly in the hospitality and bookkeeping sectors, the adoption of these systems is hindered by the lack of standardized software, high costs, and the absence of technical expertise among small and micro enterprises. The integration of digital invoicing systems with diverse financial software presents significant barriers to uniform adaptation. Furthermore, the complexity of tax regulations and the dynamic nature of tax categories require advanced understanding beyond the capabilities of standard Large Language Models (LLMs). The need for a specialized system that can comprehend finance and tax contexts, securely handle sensitive information, and adapt to user interactions is paramount. This paper introduces an autonomous agent based on a finance and tax-specific Large Language Model (LLM) designed to address the aforementioned challenges. The system includes a Specialized Training Framework to enhance domain comprehension, a Hierarchical Memory Architecture for dynamic user interaction, and a Tax Domain Security Module to ensure compliance with tax regulations. The proposed agent aims to improve the efficiency and accuracy of the invoice issuance process, providing a robust solution for tax administration in the digital era.","","979-8-3315-2931-4","10.1109/ACAI63924.2024.10899737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10899737","Large Language Models;Autonomous Agents;E-Invoice System;Natural Language Processing","Training;Large language models;Memory architecture;Finance;Collaboration;Aerodynamics;Regulation;Software;Autonomous agents;Security","","","","11","IEEE","3 Mar 2025","","","IEEE","IEEE Conferences"
"Generative AI for Real-Time Cloud Security: Advanced Anomaly Detection Using GPT Models","C. K. Akiri; K. Jayabalan; J. Lopes; S. A. Kareem; A. Tabbassum","Tennessee Tech University, USA; IEEE, USA; IEEE, USA; IEEE, USA; IEEE, USA",2025 IEEE Conference on Computer Applications (ICCA),"27 May 2025","2025","","","1","6","As cloud infrastructures become increasingly complex and integral to modern enterprises, the demand for advanced, real-time security solutions has grown significantly. Traditional anomaly detection systems often struggle to keep pace with the rapid evolution of cyber threats in these dynamic environments, particularly when faced with novel or sophisticated attacks. Such systems typically rely on predefined rules or signature-based detection, which limits their effectiveness in identifying emerging security risks. This paper explores the potential of generative AI models, specifically LLaMA and OpenAI’s GPT architectures, to enhance real-time cloud security. By leveraging the advanced pattern recognition and adaptive capabilities of these models, we propose a framework that can analyze vast amounts of cloud data, including logs, network traffic, user behavior, and system activities, to detect abnormal patterns indicative of security threats. The real-time anomaly detection offered by generative AI provides a significant advantage over traditional methods, as it continuously learns from new data, thereby improving its ability to identify novel threats in complex cloud environments.This research addresses key gaps in current cloud security practices, highlighting the limitations of existing systems in detecting previously unknown threats. The proposed approach introduces generative AI as a highly adaptive and scalable solution for cloud anomaly detection, capable of responding to evolving threats in real-time. By using models such as GPT, which are known for their ability to generate coherent predictions based on diverse inputs, the framework offers a novel means of safe-guarding cloud infrastructure. This study not only underscores the benefits of employing generative AI for security purposes but also provides a robust methodology for integrating these models into cloud security systems. The paper concludes with an assessment of the practical deployment of these AI models in large-scale cloud environments, demonstrating their potential to significantly enhance the resilience and adaptability of modern cloud security frameworks.","","979-8-3315-3459-2","10.1109/ICCA65395.2025.11011269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011269","Generative AI;Real-time Anomaly Detection;Cloud Security;GPT and LLaMA Models;Zero-Day Attack Detection","Adaptation models;Generative AI;Cloud computing security;Scalability;Organizations;Telecommunication traffic;Real-time systems;Data models;Security;Anomaly detection","","","","13","IEEE","27 May 2025","","","IEEE","IEEE Conferences"
"Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities","A. Khare; S. Dutta; Z. Li; A. Solko-Breslin; R. Alur; M. Naik","University of Pennsylvania, Philadelphia, USA; Cornell University, Ithaca, USA; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","103","114","Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection techniques have made promising progress, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect security vulnerabilities. In this paper, we perform a more comprehensive study by examining a larger and more diverse set of datasets, languages, and LLMs, and qualitatively evaluating detection performance across prompts and vulnerability classes. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples-1,000 randomly selected each from five diverse security datasets. These balanced datasets encompass synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Our results show that LLMs across all scales and families show modest effectiveness in end-to-end reasoning about vul-nerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across all datasets. LLMs are significantly better at detecting vulnerabilities that typically only need intra-procedural reasoning, such as OS Command Injection and NULL Pointer Dereference. Moreover, LLMs report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by up to 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications (e.g., sources and sinks) and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We believe our insights can motivate future work on LLM-augmented vulnerability detection systems.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988968","","Software testing;Java;Codes;Accuracy;Large language models;Scalability;Static analysis;Cognition;Software;Security","","9","","64","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"LLMs and the Future of Chip Design: Unveiling Security Risks and Building Trust","Z. Wang; L. Alrahis; L. Mankali; J. Knechtel; O. Sinanoglu","New York University, USA; New York University Abu Dhabi, UAE; New York University, USA; New York University Abu Dhabi, UAE; New York University Abu Dhabi, UAE",2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),"25 Sep 2024","2024","","","385","390","Chip design is about to be revolutionized by the integration of large language, multimodal, and circuit models (collectively LxMs). While exploring this exciting frontier with tremendous potential, the community must also carefully consider the related security risks and the need for building trust into using LxMs for chip design. First, we review the recent surge of using LxMs for chip design in general. We cover state-of-the-art works for the automation of hardware description language code generation and for scripting and guidance of essential but cumbersome tasks for electronic design automation tools, e.g., design-space exploration, tuning, or designer training. Second, we raise and provide initial answers to novel research questions on critical issues for security and trustworthiness of LxM-powered chip design from both the attack and defense perspectives.","2159-3477","979-8-3503-5411-9","10.1109/ISVLSI61997.2024.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10682661","Electronic Design Automation;Integrated Circuits;Large Language Models;Hardware Security","Training;Reviews;Hardware security;Buildings;Very large scale integration;Chip scale packaging;Integrated circuit modeling","","8","","68","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"SSCM: Self-Supervised Critical Model for Reducing Hallucinations in Chinese Financial Text Generation","K. Jin; Y. Wang; L. Santos; T. Fang; X. Yang; S. K. Im","Faculty of Applied Sciences, Macao Polytechnic University, China; Faculty of Applied Sciences, Macao Polytechnic University, China; Computer Science and Communication Research Centre, Polytechnic of Leiria, Leiria, Portugal; Hithink RoyalFlush Information Network Co., Ltd., Hangzhou, China; Faculty of Applied Sciences, Macao Polytechnic University, China; Macao Polytechnic University, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Large Language Models (LLMs) show strong performance in natural language processing tasks, but their application in the financial domain is limited. Current methods rely on large datasets and manual prompt engineering, resulting in high data demands, long inference times, and frequent hallucinations. To address these limitations, we propose a novel self-supervised prompt optimization framework tailored for the financial domain. Our approach involves training a critical model that evaluates and ranks generated outputs using both good and bad answers generated from various revised prompts. Experiments on a large Chinese financial corpus show that our framework significantly improves performance on tasks such as summarization and event-based question answering, as evidenced by higher scores on both automated metrics like ROUGE, BLEU, and BERTScore, and also through human evaluations. These results validate the effectiveness of our method in reducing hallucinations and improving the quality of financial text generation.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10887684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887684","Large Language Models (LLMs);Hallucination;Financial Applications;Critical Model","Training;Measurement;Large language models;Contrastive learning;Signal processing;Question answering (information retrieval);Multilingual;Prompt engineering;Speech processing;Optimization","","","","33","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Identification and Optimization of Redundant Code Using Large Language Models","S. T. Cynthia","Department of Computer Science, University of Saskatchewan, Saskatoon, Canada",2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN),"16 Jun 2025","2025","","","261","263","Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.","","979-8-3315-0219-5","10.1109/CAIN66642.2025.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029992","code redundancy;LLM;code optimization","Codes;Large language models;Redundancy;Computer bugs;Encoding;Complexity theory;Optimization;Software engineering;Software development management","","","","26","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair","S. Fatima; H. Hemmati; L. C. Briand","School of EECS, University of Ottawa, Ottawa, ON, Canada; Electrical Engineering and Computer Science Department, York University, Toronto, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada",IEEE Transactions on Software Engineering,"11 Dec 2024","2024","50","12","3146","3171","Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.","1939-3520","","10.1109/TSE.2024.3472476","Research Grant from Huawei Technologies Canada, Mitacs Canada; Canada Research Chair and Discovery Grant programs of the Natural Sciences and Engineering Research Council of Canada (NSERC); NSERC and Alberta Innovates; Science Foundation Ireland(grant numbers:13/RC/2094-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10704582","Flaky tests;fix category;test repair;large language models;code models;few shot learning;software testing","Codes;Predictive models;Maintenance engineering;Analytical models;Production;Large language models;Java;Few shot learning;Python;Manuals","","4","","93","CCBYNCND","2 Oct 2024","","","IEEE","IEEE Journals"
"PACGBI: A Pipeline for Automated Code Generation from Backlog Items","M. Sarschar; G. Zhang; A. Nowak","Hochschule für Technik und Wirtschaft Berlin, Germany; Hochschule für Technik und Wirtschaft Berlin, Germany; Capgemini Deutschland GmbH, Germany",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2338","2341","While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables non-technical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764968","ci/cd;generative ai;automated software development","Codes;Costs;Source coding;Large language models;Pipelines;Agile software development;Manuals;Software;Stakeholders;Software engineering","","","","15","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Sallm: Security Assessment of Generated Code","M. L. Siddiq; J. C. Da Silva Santos; S. Devareddy; A. Muller","University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA",2024 39th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),"28 Nov 2024","2024","","","54","65","With the growing popularity of Large Language Models (LLMs) in software engineers’ daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate LLMs do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. Second, existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations. Therefore, in this paper, we described Sallm, a framework to benchmark LLMs’ abilities to generate secure code systematically. This framework has three major components: a novel dataset of security-centric Python prompts, configurable assessment techniques to evaluate the generated code, and novel metrics to evaluate the models’ performance from the perspective of secure code generation.CCS Concepts• Security and privacy → Software security engineering; • Software and its engineering → Software verification and validation; • Computing methodologies → Natural language processing.","2151-0849","979-8-4007-1249-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765115","security evaluation;large language models;pre-trained transformer model;metrics","Measurement;Codes;Large language models;Programming;Transformers;Software;Natural language processing;Security;Software engineering;Python","","1","","85","","28 Nov 2024","","","IEEE","IEEE Conferences"
"Automatic Generation of Learning Material for Residential Students in ED: Prompt Engineering and Evaluation","A. Fontana; G. Sinatti; S. Cicchinelli; S. Santini; V. Caputo; I. Letteri; P. Vittorini; C. Balsano","Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY; Dep. of Life, Health and Environmental Sciences, University of L'Aquila, L'Aquila, ITALY",2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET),"16 Jan 2025","2024","","","1","8","Emergency Department (ED) physicians must manage all types of pathologies and quickly implement the appropriate diagnostic and therapeutic pathways. Flashcards can be a valuable support and consultation tool because they are simple and effective learning methods. Accordingly, we started a project that aims to develop an app that provides flashcards to help resident students in the ED learn the correct diagnostic and therapeutic guidelines. However, developing the appropriate flashcard content is long, complex, and expensive. This research discusses how we tackled this issue through automated content generation: we first report on the prompt engineering phase and then on the evaluation of the learning material. The prompt engineering phase returned some valuable lessons learned that can be useful to researchers involved in a similar process. The (intrinsic and extrinsic) evaluation showed that the automated material was of good quality, even if the lack of in-depth contextual knowledge by the Large Language Model (LLM) of the specific domain of ED was one of the significant problems, and minor revisions remained necessary. In conclusion, the research summarized in this paper provides additional evidence about the pros and cons of using LLMs for automated content generation in the specific context of medical education.","2473-2060","979-8-3315-1663-5","10.1109/ITHET61869.2024.10837651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837651","Large Language Models;Prompt Engineering;Education","Training;Accuracy;Reviews;Emergency medicine;Surgery;Production;Medical services;Prompt engineering;Medical diagnostic imaging;Guidelines","","","","25","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Prompting Is All You Need: Automated Android Bug Replay with Large Language Models","S. Feng; C. Chen","Monash University, Melbourne, Australia; Monash University, Melbourne, Australia",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","803","815","Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current au-tomated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and predefined vocabulary lists. In-spired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a devel-oper. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3608137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548487","automated bug replay;large language model;prompt engineering","Training;Software maintenance;Vocabulary;Computer bugs;Manuals;Cognition;Natural language processing","","39","","86","","14 Jun 2024","","","IEEE","IEEE Conferences"
"LASP: LLM Assisted Security Property Generation for SoC Verification","A. Ayalasomayajula; R. Guo; J. Zhou; S. K. Saha; F. Farahmandi","University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA",2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD),"6 Nov 2024","2024","","","1","7","As the complexity of System-on-Chips (SoCs) increases, ensuring their security presents escalating challenges. Formal property verification is one of the most robust methods to model and check security behaviors using model checkers. However, the generation of these security properties is a labor-intensive endeavor. Large language models (LLMs) have been applied in multiple fields due to their excellent ability to understand natural language. Hence, this paper presents a novel framework that utilizes LLMs to automate the generation of security properties directly from Register Transfer Level (RTL) designs. By extracting critical features and security assets from both the design specifications and RTL, the framework systematically produces tailored security properties for specific hardware designs. These properties are systematically cataloged in a security property database, providing an essential resource for ongoing and future hardware verification efforts. The effectiveness of this innovative framework is validated through its application to various open-source hardware designs, confirming its ability to significantly improve SoC security verification by efficiently generating robust security properties.","","979-8-3503-6356-2","10.1109/MLCAD62225.2024.10740198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740198","System-on-Chip Security;Formal Property Verification;Large Language Models;Security Property Generation","Threat modeling;Solid modeling;Databases;Hardware security;Static analysis;Feature extraction;System-on-chip;Registers;Open source hardware;Security","","3","","25","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Extending a Multi-Agent Systems Simulation Architecture for Systems-of-Systems Security Analysis","J. El Hachem; V. Chiprianov; V. V. Graciano Neto; P. Aniorte","UNIV PAU & PAYS ADOUR, LIUPPA, PAU, FRANCE; UNIV PAU & PAYS ADOUR, LIUPPA, PAU, FRANCE; University of São Paulo, São Carlos, Brazil; UNIV PAU & PAYS ADOUR, LIUPPA, PAU, FRANCE",2018 13th Annual Conference on System of Systems Engineering (SoSE),"9 Aug 2018","2018","","","276","283","Security is an important concern for software-intensive Systems-of-Systems (SoS). Architectural analysis for SoS secturity assessment should be performed at early stages of development. Such activity could prevent vulnerabilities and avoid potential cascading attack emergent behaviors, i.e., a succession of security vulnerabilities that emerge from individual constituents security fragilities, potentially causing interruption and collapse of SoS operation. Model simulation can prevent these issues by predicting, at design-time, how SoS will behave regarding its reaction to potential attacks. As security is a quality attribute, i.e., a property that comes up from the relation between software parts, software architecture analysis and simulation are an additional support for the prediction of SoS security. However, despite recent advances in such area, few simulation approaches have tackled simulation of secure SoS architectures where the basis of the described models are the SoS behavior or the interactions among the SoS Constituent Systems (CS). The main contribution of this paper is offering a big picture of how recent advances on SoS security analysis via simulations can form a robust framework for SoS security prediction. We argue the pertinence of Multi-Agent Systems (MAS) for SoS simulation due to similarities between MAS and SoS concepts, and we report how MAS simulation enables the visualization of emergent behaviors and how they impact the SoS security. Our results to foster SoS security analysis include (i) an extension of a MAS conceptual model and platform to include security concepts, (ii) a Model-Driven Engineering (MDE) approach that adopts automatic mappings between secure SoS architecture modeled using an existing SysML-based modeling language, namely the SoSSecML, and (iii) a MAS platform to support such analysis.","","978-1-5386-4876-6","10.1109/SYSOSE.2018.8428776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428776","","Security;Analytical models;Cascading style sheets;Safety;Predictive models;Tools;Multi-agent systems","","5","","30","IEEE","9 Aug 2018","","","IEEE","IEEE Conferences"
"LLM-based and Retrieval-Augmented Control Code Generation","H. Koziolek; S. Grüner; R. Hark; V. Ashiwal; S. Linsbauer; N. Eskandani","ABB Corporate Research, Germany; ABB Corporate Research, Germany; ABB Corporate Research, Germany; ABB Corporate Research, Germany; ABB Corporate Research, Germany; ABB Corporate Research, Germany",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","22","29","Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.CCS CONCEPTS• Software and its engineering Automatic programming; Command and control languages; • Applied computing → Computer-aided design; • Computing methodologies → Natural language processing.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734438","Large language models;code generation;IEC 61131-3;industrial automation;PLC;DCS;ChatGPT;GPT-4","Codes;Automation;Large language models;Process control;Libraries;Vectors;Logic;IEC Standards;Programming environments;Testing","","4","","29","","30 Oct 2024","","","IEEE","IEEE Conferences"
"Navigating the Pitfalls: Analyzing the Behavior of LLMs as a Coding Assistant for Computer Science Students—A Systematic Review of the Literature","F. A. Pirzado; A. Ahmed; R. A. Mendoza-Urdiales; H. Terashima-Marin","School of Engineering and Sciences Monterrey, Tecnológico de Monterrey, Nuevo León, Monterrey, Mexico; School of Computer Science and Engineering, University of Electronic Science and Technology of China—UESTC, Sichuan, China; School of Engineering and Sciences Monterrey, Tecnológico de Monterrey, Nuevo León, Monterrey, Mexico; School of Engineering and Sciences Monterrey, Tecnológico de Monterrey, Nuevo León, Monterrey, Mexico",IEEE Access,"21 Aug 2024","2024","12","","112605","112625","In recent years, large language models (LLMs) have been employed significantly in different domains of computing education. Nevertheless, these models have been focused on essential adherence to their integration as coding assistants in computing education. However, attention has been switched to thoroughly examining and analyzing LLM behavior, particularly in computing education for programming tasks such as code generation, code explanation, and programming error message explanation. Therefore, it becomes imperative to understand their behavior to examine potential pitfalls. This article addresses this gap systematically and details how different LLM-based coding chatbots, such as ChatGPT, Codex, Copilot, and others, react to various coding inputs within computing education. To achieve this objective, we collected and analyzed articles from 2021 to 2024, and 72 studies were thoroughly examined. These objectives include investigating the existing limitations and challenges associated with utilizing these systems for coding tasks, assessing their responses to prompts containing coding syntax, examining the impact of their output on student learning, and evaluating their performance as debugging tools. The findings of this review highlight that it is premature to incorporate these systems into computing education due to their limitations that may limit their effectiveness as comprehensive coding assistants for computer science students. These limitations include issues with handling prompts containing code snippets, potential negative impacts on student learning, limited debugging capabilities, and other ineffectiveness. The finding also reports multiple research directions that can be considered in future research related to LLMs in computing education.","2169-3536","","10.1109/ACCESS.2024.3443621","Tecnológico de Monterrey through the Ecosystem for Scaling Up Computational Thinking and Reasoning for Complexity Within the “Challenge-Based Research Funding Program 2022”(grant numbers:I003-IFE001-C2-T3-T E4C&CT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10636140","Large language models;computing education;code generation;code explanation;programming error messages explanation","Codes;Encoding;Chatbots;Programming profession;Task analysis;Surveys;Large language models;Computer science education;Error analysis","","6","","99","CCBYNCND","14 Aug 2024","","","IEEE","IEEE Journals"
"Bipartite Synchronization of Multi-Agent Systems With Signed Graph Under Deception Attacks and Application in Speech Communication","Y. Liu; S. Sheng; J. Zhang; G. Lu","School of Electrical Engineering, Nantong University, Nantong, China; School of Electrical Engineering, Nantong University, Nantong, China; School of Electrical Engineering, Nantong University, Nantong, China; School of Electrical Engineering, Nantong University, Nantong, China",IEEE Access,"22 Feb 2023","2023","11","","15954","15967","This paper addresses the bipartite synchronization of multi-agent systems with time-varying delays and signed graph under deception attacks, where the systems contain both the self-structure delayed term and the coupling delayed term. The deception attacks are assumed to be randomly launched at each impulse instant. By using the gauge transformation, Lyapunov function method, Halanay differential inequality and linear matrix inequality techniques, several sufficient conditions are newly established to guarantee the achievement of bipartite synchronization, in which some special cases of multi-agent systems without delay or deception attacks are further considered. Two numerical examples are provided to illustrate the effectiveness of the proposed results. Finally, a new multi-agent speech communication system is constructed based on the bipartite synchronization of the multi-agent systems, where two agents for point-to-point communication can generate synchronous chaotic encryption/decryption key streams. The experimental results show that the proposed multi-agent speech communication system has the advantages of high security against some classical attacks.","2169-3536","","10.1109/ACCESS.2022.3222069","National Natural Science Foundation of China(grant numbers:62073180); Doctoral Research Startup Fund Project of Nantong University; Postgraduate Research & Practice Innovation Program of Jiangsu Province(grant numbers:KYCX22_3348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950051","Multi-agent systems;bipartite synchronization;signed graph;deception attacks;speech communication","Multi-agent systems;Synchronization;Delays;Couplings;Chaotic communication;Oral communication;Linear matrix inequalities;Speech recognition","","1","","38","CCBY","14 Nov 2022","","","IEEE","IEEE Journals"
"Coded Secure Control for Multi-Agent Systems","R. Ji; Y. Zhang; S. Sam Ge","Electrical and Computer Engineering, National University of Singapore, Singapore; Electrical and Computer Engineering, National University of Singapore, Singapore; Electrical and Computer Engineering, National University of Singapore, Singapore",2024 14th Asian Control Conference (ASCC),"19 Sep 2024","2024","","","599","604","This paper proposes a coded secure control for the consensus of Multi-agent Systems under external disturbances. To address the challenges of conserving communication bits while enhancing communication security, a novel coded event-triggered algorithm is designed from an encoding-decoding viewpoint. Upon the satisfaction of our algorithm, only an encrypted L-length codeword, instead of the original agent state, is transmitted, which economizes on bandwidth usage. The proposed algorithm can be implemented to more general base-$p$ number systems and it achieves a greater balance between signal distortion and communication frequency. As no sensitive agent information over the network, the communication security is thereby enhanced. As a result, the designed coded secure control guarantees uniformly bounded consensus outcomes and comparative simulations are given to illustrate the effectiveness of our control design.","2770-8373","978-9-8875-8159-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10665627","Consensus control;event-triggered control;communication security;multi-agent systems","Control design;Bandwidth;Distortion;Cryptography;Multi-agent systems","","","","42","","19 Sep 2024","","","IEEE","IEEE Conferences"
"AI and Generative AI-Driven Automation for Multi-Cloud and Hybrid Cloud Architectures: Enhancing Security, Performance, and Operational Efficiency","D. K. Seth; K. K. Ratra; A. P. Sundareswaran","Walmart Global Tech, Sunnyvale, CA, USA; Walmart Global Tech, Sunnyvale, CA, USA; Walmart Global Tech, Sunnyvale, CA, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00784","00793","The emergence of cloud and hybrid cloud structures presents eCommerce firms with the adaptability and robustness needed to manage expansion and varying user requirements effectively. However, this also brings about challenges concerning security enhancements, distribution of workloads, and cost-effectiveness optimization. Traditional cloud management models often need help to meet these evolving demands efficiently. This research presents a system that leverages Artificial Intelligence (AI) and Generative AI (Gen AI) to effectively automate and enhance cloud and hybrid infrastructures for ecommerce websites. The system adapts infrastructure to traffic times like holidays or sales events by utilizing AI to scale resources as needed. It conserves resources during low user activity periods such as overnight. Ensuring optimal system performance and availability during peak traffic times while cutting costs during traffic periods is essential for cost-effectiveness and efficient resource management. In addition, AI-powered security automation safeguards against changing cyber dangers, and compliance automation guarantees conformity with rules like PCI DSS for payment handling. This report also delves into merging Gen AI into cloud coordination systems, facilitating workflows, and enhancing eCommerce processes. The outcome is a significant drop in operational expenses, a quicker service rollout, and decreased security breaches. Through real-world eCommerce case studies, this paper provides actionable insights for cloud engineers and architects on leveraging AI-driven cloud management to enhance performance, security, and cost-efficiency in multi-cloud and hybrid environments, ensuring seamless user experiences and business continuity.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903928","AI-driven automation;Generative AI;Multi-cloud architecture;Hybrid cloud;Cloud security;Performance optimization;Operational efficiency;Cloud orchestration;Infrastructure automation;Cloud scalability;Machine learning in cloud;AI-based cloud management;Cloud performance monitoring;Cloud security automation;Resource allocation optimization Introduction","Cloud computing;Technological innovation;Automation;Generative AI;System performance;Hybrid power systems;Robustness;Security;Electronic commerce;Optimization","","2","","9","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning","L. Puccioni; A. Farshin; M. Scazzariello; C. Wang; M. Chiesa; D. Kostić",Spotify; NVIDIA; RISE Research Institutes of Sweden; KTH Royal Institute of Technology; KTH Royal Institute of Technology; KTH Royal Institute of Technology,2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","185","192","Large Language Models (LLMs) have demonstrated their exceptional performance in various complex code generation tasks. However, their broader adoption is limited by significant computational demands and high resource requirements, particularly memory and processing power. To mitigate such requirements, model pruning techniques are used to create more compact models with significantly fewer parameters. However, current approaches do not focus on the efficient extraction of programming-language-specific sub-models. In this work, we explore the idea of efficiently deriving coding-specific sub-models through unstructured pruning (i.e., Wanda). We investigate the impact of different domain-specific calibration datasets on pruning outcomes across three distinct domains and extend our analysis to extracting four language-specific sub-models: Python, Java, C++, and JavaScript. We demonstrate that it is possible to efficiently extract programming-language-specific sub-models using appropriate calibration datasets while maintaining acceptable accuracy w.r.t. full models. We are also the first to provide analytical evidence that domain-specific tasks activate distinct regions within LLMs, supporting the creation of specialized sub-models through unstructured pruning. We believe that this work has significant potential to enhance LLM accessibility for coding by reducing computational requirements to enable local execution on consumer-grade hardware, and supporting faster inference times critical for real-time development feedback.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00028","Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028176","Large Language Models;LLMs;pruning;code","Training;Codes;Translation;Accuracy;Large language models;Computational modeling;Memory management;Real-time systems;Calibration;Python","","","","67","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"AskIt: Unified Programming Interface for Programming with Large Language Models","K. Okuda; S. Amarasinghe","CSAIL, MIT, Cambridge, USA; CSAIL, MIT, Cambridge, USA",2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO),"28 Feb 2024","2024","","","41","54","Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 % reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively.","2643-2838","979-8-3503-9509-9","10.1109/CGO57630.2024.10444830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444830","domain specific language;code generation;large language model;software engineering;artificial intelligence","Codes;Programming;Benchmark testing;DSL;Task analysis;Python;Domain specific languages","","2","","28","IEEE","28 Feb 2024","","","IEEE","IEEE Conferences"
"Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs","N. Forouzandehmehr; Y. Cao; N. Thakurdesai; R. Giahi; L. Ma; N. Farrokhsiar; J. Xu; E. Korpeoglu; K. Achan","Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA; Personalization Team, Walmart Global Tech, Sunnyvale, CA, USA",2023 IEEE International Conference on Big Data (BigData),"31 Jan 2024","2023","","","1","7","The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models and improves the customer’s approach to fashion by generating personalized recommendations. With experiments and case studies, we demonstrate the effectiveness of our solution from multiple dimensions.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10416943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416943","Outfit Generation;Recommendation Systems;Large Language Models;Generative AI;Stable Diffusion","Visualization;Social networking (online);Big Data;Motion pictures;Data models;Data mining;Task analysis","","2","","39","IEEE","31 Jan 2024","","","IEEE","IEEE Conferences"
"Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes","Z. Xu; D. Zhao; Q. Zou; J. Xiao; Y. Jiang; Z. Yuan; Q. Li","Tsinghua Shenzhen International Graduate School, China; Pengcheng Laboratory, China; Tsinghua Shenzhen International Graduate School, China; The Chinese University of Hong Kong, Hong Kong, SAR, China; Tsinghua Shenzhen International Graduate School, China; University of Warwick, United Kingdom; Pengcheng Laboratory, China",IEEE Internet of Things Magazine,"","2025","PP","99","1","7","In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance.","2576-3199","","10.1109/MIOT.2025.3575803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11026877","Smart Homes;Large Language Models;Data Synthesis","Smart homes;Internet of Things;Adaptation models;Synthetic data;Large language models;Data models;Security;Hidden Markov models;Anomaly detection;Training","","","","","IEEE","6 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Benchmarking Large Language Models for Automated Verilog RTL Code Generation","S. Thakur; B. Ahmad; Z. Fan; H. Pearce; B. Tan; R. Karri; B. Dolan-Gavitt; S. Garg",New York University; New York University; New York University; New York University; University of Calgary; New York University; New York University; New York University,"2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","6","Automating hardware design could obviate a signif-icant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). We release our training/evaluation scripts and LLM checkpoints as open source contributions.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10137086","NSF(grant numbers:1553419,1646671,2039607); ARO(grant numbers:77191NC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137086","Transformers;Verilog;GPT;LLM","Codes;Digital systems;Syntactics;Benchmark testing;Transformers;Hardware;Functional analysis","","75","","15","","2 Jun 2023","","","IEEE","IEEE Conferences"
"Leveraging ChatGPT for GDPR Compliance in Requirements Engineering: A Pilot Study","A. -J. Aberkane; S. v. Broucke; G. Poels; G. Georgiadis","Department of Business Informatics & Operations Management, Ghent University, Ghent, Belgium; Department of Business Informatics & Operations Management, Ghent University, Ghent, Belgium; Department of Business Informatics & Operations Management, Ghent University, Ghent, Belgium; Department of Business Informatics & Operations Management, Ghent University, Ghent, Belgium","2024 IEEE International Conference on Security, Privacy, Anonymity in Computation and Communication and Storage (SpaCCS)","25 Apr 2025","2024","","","34","41","Large Language Models (LLMs) have significantly impacted various industries, offering enhanced processes and decision-making capabilities. Among these models, ChatGPT, an intelligent conversational agent, has found applications in multiple fields, including software development. This study explores how ChatGPT can benefit requirements engineering (RE), specifically in facilitating compliance with the General Data Protection Regulation (GDPR) principle of data protection by design and default. In particular, our research objective is to evaluate ChatGPT's ability to support professionals in assessing GDPR compliance within user stories, thus enhancing requirement quality. We obtain insights into ChatGPT's strengths and limitations in this context through an experiment and survey pilot study with experts. The experiment's outcome suggests that respondents generally agreed with ChatGPT's evaluation of user stories on GDPR compliance, agreeing with almost 73% of ChatGPT's evaluations. Furthermore, the survey results showed reservations regarding recommending ChatGPT to peers or colleagues, suggesting a cautious stance within the professional community. This study provides insights into ChatGPT's utility as an assistive tool for GDPR compliance in RE, presenting a starting point to address GDPR challenges in RE using LLMs.","","979-8-3315-2107-3","10.1109/SpaCCS63173.2024.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973563","general data protection regulation;data protection by design and by default;requirements engineering;large language models","Surveys;Industries;Large language models;Decision making;Chatbots;Requirements engineering;Security;General Data Protection Regulation;Software development management","","","","27","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Observer-Based Event-Triggered Composite Anti-Disturbance Control for Multi-Agent Systems Under Multiple Disturbances and Stochastic FDIAs","X. -G. Guo; D. -Y. Zhang; J. -L. Wang; J. H. Park; L. Guo","Autonomous Intelligent Systems Department, Hangzhou Innovation Institute of Beihang University, Hangzhou, China; School of Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Autonomous Intelligent Systems Department, Hangzhou Innovation Institute of Beihang University, Hangzhou, China; Department of Electrical Engineering, Yeungnam University, Gyeongsan, South Korea; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China",IEEE Transactions on Automation Science and Engineering,"5 Jan 2023","2023","20","1","528","540","This article aims to investigate the security consensus and composite anti-disturbance problems for a class of nonlinear multi-agent systems subjected to stochastic false data injection attacks (FDIAs) and multiple disturbances under a directed communication topology. To attenuate and reject of the negative effects of two types of disturbances, a disturbance observer (DO) is designed to counteract the disturbance produced by exogenous system, and the  $\mathcal {H}_\infty $  control method is adopted to attenuate the bounded errors and variables caused by the other type of disturbances and FDIAs simultaneously. To ensure the consensus performance of MASs, an observer-based control strategy is designed, and a novel adaptive compensation technique is proposed to not only evaluate the upper bounds of the unknown but bounded disturbances but also improve the accuracy of the state observer. Furthermore, a novel event-triggered mechanism (ETM) without requiring continuous communication among neighboring agents is developed to reduce the controller update frequency and the communication burden. Meanwhile, Zeno behavior is excluded. Finally, numerical simulations are provided to verify the availability of the designed method. Note to Practitioners—In multi-agent systems, network security is very important. For example, in smart power grid systems, it is necessary to use the method of state estimation to observe the system to guarantee its safe operation. However, the measured value of the instrument may be affected by FDIAs in the transmission process, thus changing the result of state estimation and causing misjudgment of the system. Similarly, in multi-vehicle systems, FDIAs may destroy the location information of vehicles and cause serious accidents. In addition, the system will be subjected to different types of disturbances in practice, thus reducing the performance of the system. In view of the threat of FDIAs and disturbances to the MAS, a composite anti-disturbance method and an observer-based control strategy are proposed. Meanwhile, to avoid the limitation of communication bandwidth in reality, a novel ETM is developed to save network resources.","1558-3783","","10.1109/TASE.2022.3162651","National Natural Science Foundation of China(grant numbers:62173028,61773056); Scientific and Technological Innovation Foundation, Shunde Graduate School, USTB(grant numbers:BK19AE018); Fundamental Research Funds for the Central Universities of USTB(grant numbers:FRF-TP-20-09B,230201606500061,FRF-BD-19-002A); National Natural Science Foundation of China(grant numbers:62173024); Zhejiang Natural Science Foundation(grant numbers:LD21F030001); National Research Foundation of Korea (NRF); Korean Government through the Ministry of Science and Information and Communications Technology (MSIT)(grant numbers:2020R1A2B5B02002002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745491","Multi-agent systems (MASs);composite anti-disturbance technique;event-triggered control;multiple disturbances;false data injection attacks (FDIAs)","Monitoring;Control systems;Technological innovation;Security;Multi-agent systems;Electrical engineering;Directed graphs","","101","","41","CCBYNCND","31 Mar 2022","","","IEEE","IEEE Journals"
"Leveraging Large Language Models for Exploiting ASR Uncertainty","P. Dighe; Y. Su; S. Zheng; Y. Liu; V. Garg; X. Niu; A. Tewfik",Apple; Apple; Apple; Apple; Apple; Apple; Apple,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12231","12235","While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM’s accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM’s ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters [1] on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446132","large language models;prompting;LoRA finetuning;speech recognition;intent detection;keyword spotting","Voice activity detection;Uncertainty;Costs;Intent recognition;Signal processing;Natural language processing;Task analysis","","5","","37","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"CIPTA: Contrastive-based Iterative Prompt-tuning Using Text Annotation from Large Language Models","Y. Yan; W. Du; D. Yang; D. Yin","College of Information Network Security, People’s Public Security University of China, Beijing, China; College of Information Network Security, People’s Public Security University of China, Beijing, China; College of Information Network Security, People’s Public Security University of China, Beijing, China; College of Information Network Security, People’s Public Security University of China, Beijing, China",2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI),"13 Jul 2023","2023","","","174","178","In recent years, public opinion analysis has become increasingly important due to the widespread use of social media platforms and the growing influence of online information on public security. Prompt tuning, a typical few-shot learning method, ensures that the model quickly adapts to opinion analysis with different classification rules. However, existing prompt tuning for opinion analysis cannot guarantee the effectiveness of the model in zero-shot or one-shot cases. In this study, we propose the Contrastive-based Iterative Prompt-tuning method using Text-Annotation from Large Language Models (LLMs), CIPTA, for low-resource public opinion analysis. Specifically, with a small amount of manually labeled data, CIPTA leverages the knowledge from LLMs to text annotation and utilizes unsupervised contrastive embedding training to optimize text representation. Based on the prompt tuning method and the iterative training over unlabeled data, the model further utilizes the knowledge from the pre-training stage. Experiment results on tweet data show that our CIPTA achieves encouraging performance in public opinion analysis.","","979-8-3503-1449-6","10.1109/ICECAI58670.2023.10176586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176586","Public Opinion Analysis;Few-shot Learning;Prompt Tuning;Large Language Model;Contrastive Learning","Training;Analytical models;Adaptation models;Sentiment analysis;Annotations;Social networking (online);Data models","","1","","15","IEEE","13 Jul 2023","","","IEEE","IEEE Conferences"
"Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study","S. Deo; D. Hinge; O. S. Chavan; Y. Olivia Wang; M. W. Mkaouer","Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; University of Michigan-Flint, Flint, Michigan, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","207","211","In recent years, Large Language Models (LLMs) have witnessed a remarkable ascent, with OpenAI’s ChatGPT, introduced in 2022, garnering substantial attention. ChatGPT’s rapid adoption in the software development community has opened up new avenues for exploring its qualitative and quantitative impact on Developer-ChatGPT conversations. In this paper, we delve into a rich dataset from GitHub and Hacker News to perform a thorough analysis. Our objectives include characterizing the nature of these interactions and evaluating the use of ChatGPT in refactoring. To achieve these goals, we employ a combination of exploratory data analysis and data annotation, utilizing relevant keyword filters to extract pertinent information. Our examination encompasses the identification and analysis of code refactorings facilitated by ChatGPT. Through a meticulous exploration of these conversations, our goal is to illuminate the potential of ChatGPT to enhance software development practices. This research promises to provide valuable insights into the evolving role of ChatGPT in the world of software development.CCS CONCEPTS• Software Engineering → Software Quality; Refactoring.","2574-3864","979-8-4007-0587-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555866","Refactoring documentation;ChatGPT;mining software repositories","Codes;Filters;Focusing;Oral communication;Software quality;Chatbots;Data mining","","","","15","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Evolution and Perspectives of Speech Synthesis Technology: From Parametric Synthesis to the Era of Large Language Models","Y. Guo; G. Li; C. Xie; Q. Sun","Key Laboratory of Language and Cultural Computing, Ministry of Education line, Northwest Minzu University, Lanzhou, China; Key Laboratory of Language and Cultural Computing, Ministry of Education line, Northwest Minzu University, Lanzhou, China; Key Laboratory of Language and Cultural Computing, Ministry of Education line, Northwest Minzu University, Lanzhou, China; Key Laboratory of Language and Cultural Computing, Ministry of Education line, Northwest Minzu University, Lanzhou, China","2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)","17 Jun 2025","2025","","","350","353","Speech synthesis technology has evolved over six decades from rule-based to data-driven and now knowledge-driven approaches. This paper reviews its development, from early parametric methods (e.g., Formant, LPC) to modern Large Language Models (LLMs), highlighting key breakthroughs and paradigm shifts. Traditional methods relied on handcrafted acoustic rules, while deep learning (e.g., WaveNet, Tacotron) enabled end-to-end natural-sounding synthesis. Recently, LLMs like VALL-E and Voicebox have transformed speech generation into conditional language modeling, enhancing zero-shot cloning and cross-language synthesis. We propose a three-stage” division of this evolution and discuss three key features of speech synthesis in the LLM era: (1) joint text-to-speech modeling, (2) dynamic style control, and (3) adaptive generation with limited samples. Despite achieving near-human naturalness (MOS 4.5), current systems face challenges like high computational cost, insufficient emotional expressiveness, and security risks. Future trends include lightweight deployment, multimodal generation, and self-learning systems, with ethical guidelines needed to address deepfake risks.","","979-8-3315-1066-4","10.1109/ICAID65275.2025.11034615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11034615","speech synthesis;text-to-speech (TTS);deep learning;large language modeling;technology evolution","Deep learning;Adaptation models;Reviews;Large language models;Speech enhancement;Market research;Text to speech;Internet;Security;Guidelines","","","","28","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Robustness Evaluation of Cloud-Deployed Large Language Models against Chinese Adversarial Text Attacks","Y. Zhang; L. Ye; B. Li; H. Zhang","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; Antiy Labs Antiy, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China",2023 IEEE 12th International Conference on Cloud Networking (CloudNet),"9 Apr 2024","2023","","","438","442","In the evolving digital realm, Large Language Models (LLMs) like ChatGPT, which recently achieved state-of-the-art results across diverse NLP tasks, are extensively used. Deployed on the cloud, ChatGPT allows interaction via its API, providing rich and high-quality solutions. However, its vulnerability to adversarial attacks, potentially compromising the quality and reliability of cloud services and leading to information leakage, raises security concerns. Investigating the robustness of ChatGPT against adversarial attacks enables a preliminary understanding of its weaknesses and facilitates the subsequent integration of targeted defensive mechanisms into the cloud framework. Most current research on the robustness of LLMs against adversarial attacks focuses on BERT, with few studies on ChatGPT under similar conditions. This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement. Experiments conducted on the real-world dataset, THUCNews, examined the robustness of Chinese BERT and ChatGPT against adversarial attacks generated via various Chinese adversarial text generation methods. A multidimensional assessment revealed that both models are susceptible to attacks, leading to decreased text classification accuracy. The attack success rate on ChatGPT reached nearly 45%.","2771-5663","979-8-3503-1306-2","10.1109/CloudNet59005.2023.10490064","Natural Science Foundation of Heilongjiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10490064","adversarial attack;adversarial example;Large Language Model;ChatGPT;robustness","Text categorization;Chatbots;Information leakage;Robustness;Security;Task analysis","","","","13","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Fine-Tuning Large Language Models for Sentiment Classification of AI-Related Tweets","M. J. A. Riad; R. Debnath; M. R. Shuvo; F. J. Ayrin; N. Hasan; A. A. Tamanna; P. Roy","Master Of Science In Information Technology (MSIT), Washington University of Science and Technology; Department of Computer Science, Prairie View A & M University, Texas, United States; Department of Management Information System, International American University, Los Angeles, California; Department of Computer Science and Engineering, Chittagong University (CU); Department of Management Information System, International American University, Los Angeles, California; Department of CSSE, University Of AIUB; Department of Computer Science, Prairie View A & M University, Texas, United States",2024 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE),"13 Mar 2025","2024","","","186","191","Fine-tuning large language models (LLMs) for sentiment analysis presents a promising avenue for enhancing the understanding of public sentiment surrounding artificial intelligence (AI) topics. This study explores the fine-tuning of Large Language Models (LLMs) for sentiment analysis on AI-related topics using social media data. We collected approximately 20,000 tweets containing hashtags and comments related to AI concerns, including ‘AI Danger,’ ‘AI Ethics,’ ‘AI Problems,’ ‘AI Creating Problem on Jobs,’ and ‘AI Threat.’ The dataset captures public sentiment surrounding potential negative impacts of artificial intelligence. Three cutting-edge LLMs, Llama 2, Gemma, Mistral, and Phi 3, served as our foundational models for fine-tuning. To enhance model performance, we implemented advanced prompt engineering techniques, including Chain-of-Thought (COT), Algorithm-of-Thoughts (AOT), Fewshot and EmotionPrompt. Our results demonstrate that Llama 2 achieves consistent performance across all techniques with accuracies ranging from 82.3% to 83.9%. Phi-3 showed the best performance with EmotionPrompt and Fewshot techniques, achieving accuracies of 87.8% and 87.4% respectively. Gemma 7B performed optimally with Fewshot at 84.0% accuracy, while Mistral achieved 85.0% accuracy using the same technique. The research reveals that Fewshot and AOT consistently produce reliable results across models, with Emotion-Prompt showing model-specific effectiveness. Our findings have applications for enhancing the precision and dependability of sentiment analysis instruments that track public opinion and AI-related speech.","2837-8245","979-8-3315-3547-6","10.1109/WIECON-ECE64149.2024.10914746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10914746","AOT;LLMs;COT;AI Danger","Sentiment analysis;Ethics;Accuracy;Social networking (online);Large language models;Computational modeling;Instruments;Speech enhancement;Reliability;Prompt engineering","","","","24","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Generative AI in Phishing Detection: Insights and Research Opportunities","O. Perera; J. Grob","College of Business and IS, Dakota State University, Madison, SD, USA; Data Integration Security Architect, The Aerospace Corporation, El Segundo, CA, USA",2024 Cyber Awareness and Research Symposium (CARS),"13 Dec 2024","2024","","","1","5","Phishing is a method of cyberattack that exploits social engineering tactics to deceive individuals into revealing sensitive information. As phishing tactics become more sophisticated, phishing detection gains a new perspective in cyber security research. This paper explores the potential of Generative AI and Large Language Models (LLMs), in enhancing phishing detection capabilities. This study conducted a literature search in arXiv and IEEE Xplore digital databases with key phrases ""LLM phishing detection"", ""Generative AI phishing detection"" to discover insights and identify novel research opportunities in phishing detection. The results outline the current state of research, offering relevant insights to cybersecurity professionals and researchers.","","979-8-3503-8641-7","10.1109/CARS61786.2024.10778758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778758","Generative AI;LLM;Phishing Detection;Artificial Intelligence","Privacy;Generative AI;Databases;Phishing;Scalability;Large language models;Automobiles;Cyberattack","","","","25","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"JailbreakTracer: Explainable Detection of Jailbreaking Prompts in LLMs Using Synthetic Data Generation","M. F. A. Sayeedi; M. Bin Hossain; M. K. Hassan; S. Afrin; M. M. S. Hossain; M. S. Hossain","Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh",IEEE Access,"18 Jul 2025","2025","13","","123708","123723","The emergence of Large Language Models (LLMs) has revolutionized natural language processing (NLP), enabling remarkable advancements across various applications. However, these models remain susceptible to adversarial prompts, commonly referred to as jailbreaks, which exploit their vulnerabilities to bypass ethical and safety constraints. These prompts manipulate LLMs to produce harmful or forbidden outputs, posing serious ethical and security challenges. In this study, we propose JailbreakTracer, a novel framework leveraging synthetic data generation and Explainable AI (XAI) to detect and classify jailbreaking prompts. We first construct two comprehensive datasets: a Toxic Prompt Classification Dataset, combining real-world and synthetic jailbreak prompts, and a Forbidden Question Reasoning Dataset, categorizing forbidden queries into 13 distinct scenarios with clear reasoning labels. Synthetic toxic prompts are generated using a fine-tuned GPT model, achieving an attack success rate of 95.1%, effectively addressing the class imbalance. Using transformer-based architectures, we train classifiers that achieved 97.25% accuracy in detecting jailbreak prompts and 100% accuracy in categorizing forbidden questions. Our approach integrates XAI techniques, such as LIME, to ensure interpretability and transparency in the model’s predictions. Extensive evaluations demonstrate the efficacy of JailbreakTracer in detecting and reasoning about jailbreak prompts, providing a critical step toward enhancing the safety and accountability of LLMs. The dataset and code are available on GitHub: https://github.com/faiyazabdullah/JailbreakTracer","2169-3536","","10.1109/ACCESS.2025.3579996","Institute for Advanced Research Publication Grant of United International University(grant numbers:IAR-2025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11036671","Natural language processing;large language models;jailbreaking;text classification;synthetic data;generative AI;explainable AI","Ethics;Cognition;Synthetic data;Natural language processing;Artificial intelligence;Adaptation models;Security;Robustness;Prevention and mitigation;Passwords","","","","32","CCBY","16 Jun 2025","","","IEEE","IEEE Journals"
"Ethics in the Era of Generative AI: A Framework for Responsible Development and Use","V. S. Anand; A. V. V.","Department of Computer Science, College of Engineering, Thiruvananthapuram, India; Department of Computer Science, College of Engineering, Thiruvananthapuram, India",2025 International Conference on Innovative Trends in Information Technology (ICITIIT),"27 Jun 2025","2025","","","1","5","Generative AI (GenAl) has influenced almost all fields known to mankind. With the advancements in Large Language Models (LLMs), the influence has never been greater. Fields like E-Commerce, Finance, Healthcare and Entertainment have incorporated GenAl in every aspect of day-to-day operations. This situation raises questions about the security and privacy of individuals and corporations. The concerns are primarily based on the obscurity and ambiguity of the infrastructure, process, and data handling behind these models, especially the popular LLMs. Another aspect of the concerns is based on bias, lack of robustness, low legality, and less human-centric behavior of GenAl. These concerns call upon establishing certain frameworks on a socio-technological level such that it ensures GenAl doesn't evolve into a rogue tool.","","979-8-3315-3638-1","10.1109/ICITIIT64777.2025.11041291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11041291","Generative AI;Bias;Privacy;Authentic;Securitv;Socio-Technological;Framework","Privacy;Ethics;Generative AI;Large language models;Finance;Medical services;Market research;Robustness;Security;Information technology","","","","29","IEEE","27 Jun 2025","","","IEEE","IEEE Conferences"
"Innovative Prompting Strategies and Holistic Evaluation of LLM Movie Recommender","Y. Zhu; Y. Liu","Social Science Research Institute, Duke University, Durham, United States; Electrical & Computer Engineering, Duke University, Durham, United States",2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN),"14 Feb 2025","2024","","","1","5","Recommendation systems play a crucial role in to-day's digital environment, and the rise of Large Language Models (LLMs) has created new opportunities to enhance their effectiveness. LLMs have shown great promise in improving recommendation systems by better understanding user preferences and generating personalized suggestions. However, zero-shot in-context learning often yields suboptimal performance, while fine-tuning requires significant resources. This study creatively develops four distinct prompting strategies: Basic, Context-based, Personalized, and Chain-of-Thought. Each strategy takes advantage of LLMs for movie recommendations, integrating collaborative filtering techniques with reasoning methods. Our innovative approach outperforms traditional deep learning methods without the need for fine-tuning. Unlike previous research that focused solely on recommendation accuracy, we use a comprehensive evaluation framework that includes accuracy, diversity, and novelty metrics. This holistic analysis highlights the specific strengths of each prompting strategy across different LLM architectures, providing valuable insights into balancing recommendation quality with user experience.","","979-8-3315-2922-2","10.1109/ICNGN63705.2024.10871391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871391","Recommendation system;Large Language Models (LLM);Prompt Engineering;In-context Learning;Zero-shot;Chain-of-Thought (CoT)","Measurement;Deep learning;Accuracy;Large language models;Focusing;Computer architecture;Motion pictures;User experience;Recommender systems;Next generation networking","","","","16","IEEE","14 Feb 2025","","","IEEE","IEEE Conferences"
"Rethinking Legal Compliance Automation: Opportunities with Large Language Models","S. Hassani; M. Sabetzadeh; D. Amyot; J. Liao","University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; University of Ottawa, Ottawa, Canada; New Software, Toronto, Canada",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","432","440","As software-intensive systems face growing pressure to comply with laws and regulations, providing automated support for compliance analysis has become paramount. Despite advances in the Requirements Engineering (RE) community on legal compliance analysis, important obstacles remain in developing accurate and generalizable compliance automation solutions. This paper highlights some observed limitations of current approaches and examines how adopting new automation strategies that leverage Large Language Models (LLMs) can help address these shortcomings and open up fresh opportunities. Specifically, we argue that the examination of (textual) legal artifacts should, first, employ a broader context than sentences, which have widely been used as the units of analysis in past research. Second, the mode of analysis with legal artifacts needs to shift from classification and information extraction to more end-to-end strategies that are not only accurate but also capable of providing explanation and justification. We present a compliance analysis approach designed to address these limitations. We further outline our evaluation plan for the approach and provide preliminary evaluation results based on data processing agreements (DPAs) that must comply with the General Data Protection Regulation (GDPR). Our initial findings suggest that our approach yields substantial accuracy improvements and, at the same time, provides justification for compliance decisions.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00051","MITACS(grant numbers:IT37879); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628503","Legal Compliance;Legal Requirements;Large Language Models;GPT-4;GDPR","Automation;Accuracy;Law;Large language models;Information retrieval;Data processing;Regulation","","3","","54","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier","M. B. Moumoula; A. K. Kabore; J. Klein; T. F. Bissyande","University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2474","2475","Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem.This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pre-trained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets.Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764826","Cross-Language Pairs;Code Clone Detection;Large Language Model;Prompt Engineering;Embedding Model","Training;Computer languages;Codes;Large language models;Cloning;Machine learning;Programming;Software engineering;Software development management","","","","16","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Care-Ring: Harnessing Generative AI for Women's Safety and Protection","S. G. Y; N. B. N; S. K; L. S","Dept of MTech CSE (5 years integrated), Sri Sai Ram Engineering College, Chennai, India; Dept of MTech CSE (5 years integrated), Sri Sai Ram Engineering College, Chennai, India; Dept of MTech CSE (5 years integrated), Sri Sai Ram Engineering College, Chennai, India; Dept of MTech CSE (5 years integrated), Sri Sai Ram Engineering College, Chennai, India",2025 International Conference on Computing and Communication Technologies (ICCCT),"10 Jun 2025","2025","","","1","3","In today's world., the safety and security of women have become a critical concern., with alarming statistics revealing the high rate of physical, mental, and verbal abuse they endure. Studies show that globally, around 1 in 3 women experience some form of abuse in their lifetime, with millions of cases going unreported due to fear and societal stigma. To combat this escalating issue, we propose Safe Circle, an innovative mobile application designed to safeguard women in vulnerable situations. The key feature of the app is Carering, an AI-powered chatbot that monitors users' safety in real-time. Through advanced location tracking and automated checkins, Care-ring intervenes by notifying emergency contacts and local authorities if the user is in danger or unresponsive. By utilizing Generative AI, Care-ring ensures rapid response, offering a sense of security and support to those at risk. Safe Circle aims to be a proactive solution, leveraging technology to empower women and reduce the incidence of abuse.","2995-3197","979-8-3315-3757-9","10.1109/ICCCT63501.2025.11019205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11019205","Safety;Security;Physical abuse;Carering;AI-powered chatbot;Emergency contacts;Generative AI","Accuracy;Generative AI;Local government;Chatbots;Real-time systems;Regulation;Safety;Mobile applications;Protection;Monitoring","","","","7","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Cybercrime and Privacy Threats of Large Language Models","N. Kshetri","University of North Carolina at Greensboro, Greensboro, NC, USA",IT Professional,"6 Jul 2023","2023","25","3","9","13","This article explores the privacy and security issues associated with large language models (LLMs). It takes a look at nefarious actors’ possible use of LLM tools. Also analyzed are behaviors and practices of the developers, operators, and users of such models from privacy and security standpoints.","1941-045X","","10.1109/MITP.2023.3275489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174273","","Privacy;Analytical models;Behavioral sciences;Security;Computer crime","","33","","11","IEEE","6 Jul 2023","","","IEEE","IEEE Magazines"
"Enhancing patient Comprehension: An effective sequential prompting approach to simplifying EHRs using LLMs","M. K. H. Dehkordi; S. Zhou; Y. Perl; F. P. Deek; A. J. Einstein; G. Elhanan; Z. He; H. Liu","Department of Computer Science, NJIT, Newark, NJ, USA; Department of Computer Science, NJIT, Newark, NJ, USA; Department of Computer Science, NJIT, Newark, NJ, USA; Department of Informatics, NJIT, Newark, NJ, USA; Dept. of Medicine, Cardiology Division, Dept. of Radiology Columbia University Irving Medical Center, New York, NY, USA; Center for Genomic Medicine School of Medicine, University of Nevada, Reno, NV, USA; College of Communication & Information, Florida State University, Tallahassee, FL, USA; Department of Computer Science, Montclair State University, Montclair, NJ, USA",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","6370","6377","Electronic Health Record (EHR) notes often contain complex medical language, making them difficult to understand for patients lacking medical background. Simplifying EHR notes to a 6th-grade reading level is recommended by the American Medical Association to enhance patient comprehension and engagement. Large Language Models (LLMs) show promise in achieving this goal but also face challenges, such as missing and generating false information. In our previous work, we have shown that providing LLMs with highlighted EHRs, where the important information is highlighted, results in more accurate summaries compared to summarizing unhighlighted notes. In this study, we simplify highlighted EHRs with LLMs, specifically ChatGPT-4o, using two approaches: two-step simplification (sequential) and one-step (CoT-based) simplification. In the sequential approach, we generate a structured summary of the highlighted EHR, as a first step, and then we convert this summary into language suitable for a 6th-grade reader, as a second step. In the CoT-based approach, we convert the highlighted EHR into a structured summary understandable for a 6th-grade reader in one step. Evaluating the simplified notes obtained from the two approaches, the sequential approach shows higher completeness (82.35% vs. 75.89%) and correctness, as well as better readability scores (FKGL: 7.72 vs. 10.73; Flesch: 67.71 vs. 45.31) and higher average understandability ratings from ChatGPT-4 (3.92 vs. 3.28), demonstrating its overall superiority in simplifying notes.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822313","EHR simplification;Medical text simplification;Highlighted EHR notes;Prompt engineering;Large Language Models","Measurement;Accuracy;Large language models;Biological system modeling;Fake news;Electronic medical records;Bioinformatics;Faces","","1","","31","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"Towards AI-Augmented Co-Compilation for Smart IoT Systems Through Domain-Specific LLMs","F. Ciccozzi","Mälardalen University, Sweden",2025 10th International Conference on Smart and Sustainable Technologies (SpliTech),"30 Jul 2025","2025","","","1","4","The explosion of smart, interconnected IoT devices is driving demand for higher performance, real-time processing, and self-* capabilities (self-adaptation, self-reconfiguration, self-healing) across heterogeneous platforms. CPUs, GPUs, and FPGAs now provide exceptional compute power but greatly complicate software toolchains. We envision AI-COMPILER, an LLM-driven, domain-specific framework that automates co-compilation and optimization of heterogeneous applications in smart cities, smart transportation, and real-time analytics. Tackling multi-objective goals—power efficiency, reliability, and dynamic reconfiguration—AI-COMPILER aims to transform how IoT and edge infrastructures are developed, deployed, and maintained. We examine critical issues such as generalization to new hardware, human-AI collaborative tuning, and semantic coherence across multiple DSLs, arguing that AI-COMPILER can usher in more agile, resilient IoT ecosystems through flexible, context-aware code generation and adaptation.","","978-953-290-142-9","10.23919/SpliTech65624.2025.11091755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091755","Smart IoT;compiler optimization;heterogeneous computing;domain-specific languages;self-adaptation;large language models;artificial intelligence;MLIR","Smart cities;Computer architecture;Transforms;Real-time systems;Heterogeneous networks;Hardware;Internet of Things;Vehicle dynamics;Optimization;Tuning","","","","16","","30 Jul 2025","","","IEEE","IEEE Conferences"
"Generative AI with Amazon Bedrock: Build, scale, and secure generative AI applications using Amazon Bedrock","S. Kwatra; B. Kaushik",NA; NA,"Generative AI with Amazon Bedrock: Build, scale, and secure generative AI applications using Amazon Bedrock","","2024","","","","","Become proficient in Amazon Bedrock by taking a hands-on approach to building and scaling generative AI solutions that are robust, secure, and compliant with ethical standardsKey FeaturesLearn the foundations of Amazon Bedrock from experienced AWS Machine Learning Specialist ArchitectsMaster the core techniques to develop and deploy several AI applications at scaleGo beyond writing good prompting techniques and secure scalable frameworks by using advanced tips and tricksPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe concept of generative artificial intelligence has garnered widespread interest, with industries looking to leverage it to innovate and solve business problems. Amazon Bedrock, along with LangChain, simplifies the building and scaling of generative AI applications without needing to manage the infrastructure. Generative AI with Amazon Bedrock takes a practical approach to enabling you to accelerate the development and integration of several generative AI use cases in a seamless manner. You’ll explore techniques such as prompt engineering, retrieval augmentation, fine-tuning generative models, and orchestrating tasks using agents. The chapters take you through real-world scenarios and use cases such as text generation and summarization, image and code generation, and the creation of virtual assistants. The latter part of the book shows you how to effectively monitor and ensure security and privacy in Amazon Bedrock. By the end of this book, you’ll have gained a solid understanding of building and scaling generative AI apps using Amazon Bedrock, along with various architecture patterns and security best practices that will help you solve business problems and drive innovation in your organization.What you will learnExplore the generative AI landscape and foundation models in Amazon BedrockFine-tune generative models to improve their performanceExplore several architecture patterns for different business use casesGain insights into ethical AI practices, model governance, and risk mitigation strategiesEnhance your skills in employing agents to develop intelligence and orchestrate tasksMonitor and understand metrics and Amazon Bedrock model responseExplore various industrial use cases and architectures to solve real-world business problems using RAGStay on top of architectural best practices and industry standardsWho this book is forThis book is for generalist application engineers, solution engineers and architects, technical managers, ML advocates, data engineers, and data scientists looking to either innovate within their organization or solve business use cases using generative AI. A basic understanding of AWS APIs and core AWS services for machine learning is expected.","","9781804618585","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769229.pdf&bkn=10769228&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"An Adversarial Toxicity Prompt Generator Exploiting Multilingual Code-Switching in LLMs","V. S. Wadgaonkar; N. Ratha","Department of Computer Science, University at Buffalo, Buffalo, NY, USA; Department of Computer Science, University at Buffalo, Buffalo, NY, USA",2025 IEEE Conference on Artificial Intelligence (CAI),"7 Jul 2025","2025","","","884","887","The widespread adoption of Large Language Models (LLMs) has heightened concerns about their potential to generate toxic, biased, or harmful content. These risks are magnified in multilingual contexts involving code-switching, culturally nuanced slang, and adversarial prompt engineering, which often evade conventional moderation. Existing detection models struggle due to static datasets and limited adaptability to evolving linguistic patterns. To address these challenges, we propose two innovations: a Dynamic Multilingual Toxicity Dataset that extends PolygloToxicityPrompts using subject-predicate permutations and translation averaging; and a novel Adversarial Prompt Generation Framework based on Reinforcement Learning with Adversarial Feedback (RLAF), designed to elicit toxic responses from target LLMs via reward-driven refinement. Enhanced with BLOOM fine-tuning, Model-Agnostic Meta-Learning (MAML), and dynamic few-shot learning, our framework offers robust stress-testing under multilingual, adversarial conditions [15], [16], [20].","","979-8-3315-2400-5","10.1109/CAI64502.2025.00156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050730","Multilingual toxicity detection;adversarial prompt generation;code-switching;LLM safety;RLAF;MAML;BLOOM fine-tuning;stress-testing LLMs","Metalearning;Adaptation models;Technological innovation;Toxicology;Translation;Reinforcement learning;Generators;Multilingual;Few shot learning;Testing","","","","20","IEEE","7 Jul 2025","","","IEEE","IEEE Conferences"
"Prompt Injection in Large Language Model Exploitation: A Security Perspective","J. K. Joseph; E. Daniel; V. Kathiresan; M. M.A.P","Division Of Computer Science And Engineering, Karunya Institute of Technology and Sciences, Coimbatore, India; Division Of Computer Science And Engineering, Karunya Institute of Technology and Sciences, Coimbatore, India; Specialist - Technical, HCL Technologies Limited, Chennai, India; Division Of Eletronics and Communication Engineering, Karunya Institute of Technology and Sciences, Coimbatore, India","2025 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)","11 Jul 2025","2025","","","1","8","With the rapid growth of AI technologies, ensuring the security of open-source Large Language Models (LLMs) is crucial to maintaining their reliability and trustworthiness. The paper presents a detailed framework to evaluate the security risks of these models in today's fast-changing technological world. This framework includes generators, probes, detectors, and evaluation methods, all centered around the Prompt Inject framework, which helps identify vulnerabilities and possible attacks on LLMs. By using this approach, organizations, researchers, and developers can assess how easily these models can be manipulated and take steps to improve their security. The paper highlights important applications like security testing, penetration testing, compliance checks, and continuous monitoring, ensuring that LLMs remain safe and reliable. This research is valuable for strengthening cybersecurity efforts in the era of open-source AI, helping to build safer and more trustworthy AI systems.","","979-8-3315-2162-2","10.1109/ICECCC65144.2025.11064209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11064209","Prompt Injection;Large Language Models;Security Vulnerability;Adversarial Attacks;Open-source AI;Ethical AI;Threat Detection;Prompt Security","Analytical models;Privacy;Reviews;Large language models;Prevention and mitigation;Threat assessment;Robustness;Artificial intelligence;Probes;Testing","","","","15","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"DiCE-M: Distributed Code Generation and Execution for Marine Applications - An Edge-Cloud Approach","G. Coviello; K. Rao; G. Mellone; C. G. De Vita; S. Chakradhar","NEC Laboratories America, Inc., Princeton, NJ; NEC Laboratories America, Inc., Princeton, NJ; NEC Laboratories America, Inc., Princeton, NJ; NEC Laboratories America, Inc., Princeton, NJ; NEC Laboratories America, Inc., Princeton, NJ",2024 IEEE/ACM Symposium on Edge Computing (SEC),"1 Jan 2025","2024","","","468","475","Edge computing has emerged as a transformative technology that reduces application latency, improves cost efficiency, enhances security, and enables large-scale deployment of applications across various domains. In environmental monitoring, systems such as MegaSense[49], use low-cost sensors to gather and process real-time air quality data through edge-cloud collaboration, highlighting the critical role of edge computing in enabling scalable, efficient solutions. Similarly, marine science increasingly requires real-time processing and analysis of marine data from remote, resource-constrained environments. In this paper, we extend the power of edge computing by integrating it with Generative Artificial Intelligence(GenAI),specifically large language models (LLMs), to address challenges in marine science applications. We propose DiCE-M (Distributed Code generation and Execution for Marine applications), a robust system that uses LLM to generate distributed code for marine applications and then utilizes a runtime to efficiently execute it on an edge+cloud computing infrastructure. Specifically, DiCE-M leverages edge computing to execute lightweight AI models locally on unmanned surface vehicles(USVs)while offloading complex tasks to the cloud, thus balancing computational load and enabling realtime monitoring in marine environments. We use marine litter identification as an example application to demonstrate the utility of DiCE-M. Our results show that DiCE-M reduces latency by more than 2X when marine litter is not detected and cuts cloud computing costs by more than half compared to traditional cloud-based approaches. By selectively cropping and transmitting relevant image portions, DiCE-M further improves bandwidth efficiency, making it a reliable and cost-effective solution for deploying AI-drivenapplications on resource-constrained USVs in dynamic marine environments.","2837-4827","979-8-3503-7828-3","10.1109/SEC62691.2024.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818183","Edge computing;Cloud computing;Distributed computing;Generative artificial intelligence;Large language models;Code generation;Runtime;Vision applications;Marine science","Cloud computing;Costs;Codes;Spectral efficiency;Image edge detection;Computational modeling;Real-time systems;Oceanography;Vehicle dynamics;Edge computing","","2","","57","IEEE","1 Jan 2025","","","IEEE","IEEE Conferences"
"An Adaptive End-to-End IoT Security Framework Using Explainable AI and LLMs","S. Baral; S. Saha; A. Haque","Department of Computer Science, Western University, London, Canada; Department of Computer Science, University of Northern British Columbia, Prince George, Canada; Department of Computer Science, Western University, London, Canada",2024 IEEE 10th World Forum on Internet of Things (WF-IoT),"30 Dec 2024","2024","","","469","474","The exponential growth of the Internet of Things (IoT) has significantly increased the complexity and volume of cybersecurity threats, necessitating the development of advanced, scalable, and interpretable security frameworks. This paper presents an innovative, comprehensive framework for real-time IoT attack detection and response that leverages Machine Learning (ML), Explainable AI (XAI), and Large Language Models (LLM). By integrating XAI techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) with a model-independent architecture, we ensure our framework’s adaptability across various ML algorithms. Additionally, the incorporation of LLMs enhances the interpretability and accessibility of detection decisions, providing system administrators with actionable, human-understandable explanations of detected threats. Our end-to-end framework not only facilitates a seamless transition from model development to deployment but also represents a real-world application capability that is often lacking in existing research. Based on our experiments with the CIC-IOT-2023 dataset [1], Gemini and OPENAI LLMS demonstrate unique strengths in attack mitigation: Gemini offers precise, focused strategies, while OPENAI provides extensive, in-depth security measures. Incorporating SHAP and LIME algorithms within XAI provides comprehensive insights into attack detection, emphasizing opportunities for model improvement through detailed feature analysis, fine-tuning, and the adaptation of misclassifications to enhance accuracy.","2768-1734","979-8-3503-7301-1","10.1109/WF-IoT62078.2024.10811456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10811456","IoT Security;XAI;LLMs;Intrusion Detection Systems;Machine Learning","Adaptation models;Machine learning algorithms;Explainable AI;Large language models;Prevention and mitigation;Predictive models;Feature extraction;Real-time systems;Internet of Things;Computer security","","2","","14","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Next-Generation Refactoring: Combining LLM Insights and IDE Capabilities for Extract Method","D. Pomian; A. Bellur; M. Dilhara; Z. Kurbatova; E. Bogomolov; T. Bryksin; D. Dig","JetBrains Research, University of Colorado Boulder, USA, Serbia; JetBrains Research, University of Colorado Boulder, USA, Serbia; JetBrains Research, University of Colorado Boulder, USA, Serbia; JetBrains Research, University of Colorado Boulder, USA, Serbia; JetBrains Research, the Netherlands, Cyprus, University of Colorado, Boulder, USA; JetBrains Research, the Netherlands, Cyprus, University of Colorado, Boulder, USA; JetBrains Research, the Netherlands, Cyprus, University of Colorado, Boulder, USA",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","275","287","Long methods that encapsulate multiple responsibilities within a single method are challenging to maintain. Choosing which statements to extract into new methods has been the target of many research tools. Despite steady improvements, these tools often fail to generate refactorings that align with developers' preferences and acceptance criteria. Given that Large Language Models (LLMs) have been trained on large code corpora, if we harness their familiarity with the way developers form functions, we could suggest refactorings that developers are likely to accept. In this paper, we advance the science and practice of refactoring by synergistically combining the insights of LLMs with the power of IDEs to perform Extract Method (EM). Our formative study on 1752 EM scenarios revealed that LLMs are very effective for giving expert suggestions, yet they are unreliable: up to 76.3% of the suggestions are hallucinations. We designed a novel approach that removes hallucinations from the candidates suggested by LLMs, then further enhances and ranks suggestions based on static analysis techniques from program slicing, and finally leverages the IDE to execute refactorings correctly. We implemented this approach in an IntelliJ IDEA plugin called EM-Assist. We empirically evaluated EM-Assist on a diverse corpus that replicates 1752 actual refactorings from open-source projects. We found that EM-Assist outperforms previous state of the art tools: EM-Assist suggests the developer-performed refactoring in 53.4% of cases, improving over the recall rate of 39.4% for previous best-in-class tools. Furthermore, we conducted firehouse surveys with 16 industrial developers and suggested refactorings on their recent commits. 81.3% of them agreed with the recommendations provided by EM-Assist. This shows the usefulness of our approach and ushers us into a new era when LLMs become effective AI assistants for refactoring.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00034","NSF(grant numbers:CNS-1941898,CNS-2213763); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795061","Refactoring;LLMs;Code smells;Long Methods;Java;Kotlin","Surveys;Software maintenance;Automation;Codes;Static analysis;Programming;Reliability;Best practices;Next generation networking;Python","","2","","70","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Enhancing IT Security with LLM-Powered Predictive Threat Intelligence","B. Bokkena","Clarivate, New York, USA",2024 5th International Conference on Smart Electronics and Communication (ICOSEC),"24 Oct 2024","2024","","","751","756","The primary objective of this research is to explore the integration of Large Language Models (LLMs) into IT security frameworks for enhancing predictive threat intelligence capabilities. Traditional threat detection systems often struggle with the dynamic nature of modern cyber threats. LLMs, with their advanced natural language processing abilities, present a novel approach by predicting and identifying potential security threats through pattern recognition and anomaly detection in vast datasets. This research employed a systematic methodology involving extensive data collection from diverse IT security logs and public threat databases. The selected LLM was trained on these datasets, focusing on recognizing linguistic and non-linguistic patterns indicative of potential security threats. Validation was conducted through a series of controlled tests comparing the LLM’s performance against traditional rule-based and machine learning models in simulated and real-world environments. The results demonstrate that the LLM significantly outperformed existing models in terms of detection speed, accuracy, and the ability to identify zero-day exploits. Notably, the LLM achieved an accuracy rate exceeding 95%, with substantial improvements in false positive reductions. The study concludes that leveraging LLMs in threat intelligence systems can profoundly enhance predictive capabilities and dynamically adapt to evolving cyber threats. This advancement not only bolsters security postures but also supports proactive security management, underscoring the critical role of LLMs in future IT security strategies.","","979-8-3315-0440-3","10.1109/ICOSEC61587.2024.10722712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10722712","Predictive Threat Intelligence;Large language models (LLMs);Cybersecurity Analytics;Anomaly Detection;Zero-Day Exploit Identification","Analytical models;Accuracy;Large language models;Transforms;Predictive models;Threat assessment;Pattern recognition;Security;Anomaly detection;Testing","","3","","26","IEEE","24 Oct 2024","","","IEEE","IEEE Conferences"
"Software Testing of Generative AI Systems: Challenges and Opportunities","A. Aleti","Faculty of Information Technology, Monash University, Melbourne, Australia",2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE),"4 Mar 2024","2023","","","4","14","Software Testing is a well-established area in software engineering, encompassing various techniques and methodologies to ensure the quality of software systems. However, with the arrival of generative artificial intelligence (GenAI) systems, new challenges arise in the testing domain. These systems, capable of generating novel and creative outputs, introduce unique complexities that require novel testing approaches. In this paper, I aim to explore the challenges posed by GenAI systems and discuss potential opportunities for future research in the area of testing. I will touch on the specific characteristics of GenAI systems that make traditional testing techniques inadequate or insufficient. By addressing these challenges and pursuing further research, we can enhance our understanding of how to safeguard GenAI and pave the way for improved quality assurance in this rapidly evolving area.","","979-8-3503-2496-9","10.1109/ICSE-FoSE59343.2023.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449663","Generative AI;Software testing;oracle;test suite adequacy","Software testing;Training;Uncertainty;Systematics;Generative AI;Complexity theory;Software engineering","","10","","97","IEEE","4 Mar 2024","","","IEEE","IEEE Conferences"
"The Effectiveness of Compact Fine-Tuned LLMs in Log Parsing","M. Mehrabi; A. Hamou-Lhadj; H. Moosavi","ECE, Concordia University, Montreal, QC, Canada; ECE, Concordia University, Montreal, QC, Canada; Cisco Systems, Ottawa, ON, Canada",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","438","448","Log parsing is defined as the process of extracting structured information from unstructured log data. It is an important step prior to many log analytics tasks. The emergence of Large Language Models (LLMs), like Generative Pre-trained Transformers (GPTs), has driven the development of novel log parsing methods. Existing studies have examined the effectiveness of large-scale general-purpose LLMs in log parsing. In this paper, we argue that the long-term adoption of such LLMs pose challenges of data privacy, cost, and tool integration. To address these challenges, we explore the viability of supervised fine-tuning of an open-source compact LLM for log parsing as a prospective alternative. To this end, we fine-tune the Mistral-7B-Instruct LLM on a diverse set of log files and evaluate its performance, in terms of both accuracy and robustness, against OpenAI's GPT-4-Turbo using different configuration settings. We apply two evaluation approaches, namely metric-based and LLM-based. Our overall findings show that fine-tuning a compact LLM such as Mistral-7B provides similar and sometimes better results than using a large-scale LLM, in our case GPT-4-Turbo. These findings are important because they enable companies to use a smaller LLM that they can readily adapt to parsing their log data, and integrate into their log analytics tools, without the need to rely on third-party LLM providers.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00047","Canadian Department of National Defence; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795057","Log Parsing;Large Language Models;Machine Learning;Software Maintenance and Evolution","Software maintenance;Data privacy;Accuracy;Costs;Large language models;Companies;Transformers;Robustness;Maintenance","","","","38","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in Real-World Projects","H. Nunes; E. Figueiredo; L. Rocha; S. Nadi; F. Ferreira; G. Esteves","Federal University of Minas Gerais, Belo Horizonte, Brazil; Federal University of Minas Gerais, Belo Horizonte, Brazil; State University of Bahia, Alagoinhas, Brazil; New York University Abu Dhabi, Abu Dhabi, United Arab Emirates; Federal University of Itajubá, Itajubá, Brazil; Federal University of Minas Gerais, Belo Horizonte, Brazil","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","669","680","Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9 % of the methods, while Copilot Chat and Llama zero-shot fixed 32.29 % and 30 %, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63 % of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992379","maintainability;large language models;refactoring","Codes;Large language models;Software;Encoding;Software development management","","2","","49","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Development of Dialogue Feature between Participants and ChatGPT in Network Security Exercise System","Y. Tateiwa","Graduate School of Engineering, Nagoya Institute of Technology, Nagoya, Japan",2024 7th International Conference on Information and Computer Technologies (ICICT),"3 Jun 2024","2024","","","479","484","The author confirmed that ChatGPT can provide advice including specific implementation examples on computer network management by dialogue with ChatGPT. According to this confirmation, it is expected that in network security exercises, ChatGPT will be able to resolve participants' network troubles in place of instructors or teaching assistants. In the network security exercises provided by the author, participants perform various communication experiments in a virtual network consisting of virtual machines as nodes. This paper describes a method for collecting network configuration information and conveying it to ChatGPT based on a custom YANG model, as well as a user interface for facilitating dialogue between participants and ChatGPT.","2769-4542","979-8-3503-8562-5","10.1109/ICICT62343.2024.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541497","ChatGPT;Network;Security;e-learning;Exercise;YANG model;Large Language Models;Virtual Machine;Dialogue","Performance evaluation;Computational modeling;Unified modeling language;Prototypes;Network security;User interfaces;Chatbots","","1","","15","IEEE","3 Jun 2024","","","IEEE","IEEE Conferences"
"Large Language Models for Telecom","M. Debbah","Khalifa University of Science and Technology, Abu Dhabi, UAE",2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC),"8 Nov 2023","2023","","","3","4","Large Language Models (LLMs) have shown remarkable success in natural language processing (NLP) tasks, such as language translation, text summarization, and sentiment analysis. They can also help in identifying network faults, improving network security, and facilitating spectrum sharing. LLM-based solutions can be trained on largescale datasets to capture the heterogeneity and diversity of wireless networks. These models can be deployed on resource-limited devices, such as smartphones, to provide intelligent wireless services. In this talk, we will discuss our recent progress on LLM features and the potential of LLM in enabling intelligent wireless communication systems.","","979-8-3503-1697-1","10.1109/FMEC59375.2023.10305960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305960","","Fault diagnosis;Sentiment analysis;Multi-access edge computing;Wireless networks;Computational modeling;Network security;Telecommunications","","3","","0","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study","A. Velasco; D. Rodriguez-Cardenas; L. R. Alif; D. N. Palacio; D. Poshyvanyk","Department of Computer Science, William & Mary; Department of Computer Science, William & Mary; Department of Software Engineering, University of Dhaka; Department of Computer Science, William & Mary; Department of Computer Science, William & Mary",2025 IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),"10 Jun 2025","2025","","","96","100","Large Language Models (LLMs) have shown significant potential in automating software engineering tasks, particularly in code generation. However, current evaluation benchmarks, which primarily focus on accuracy, fall short in assessing the quality of the code generated by these models, specifically their tendency to produce code smells. To address this limitation, we introduce CodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for generating code smells. Our benchmark includes a novel metric: Propensity Smelly Score (PSC), and a curated dataset of method-level code smells: CodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case study with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal that both models tend to generate code smells, such as simplifiable-condition and consider-merging-isinstance. These findings highlight the effectiveness of our benchmark in evaluating LLMs, providing valuable insights into their reliability and their propensity to introduce code smells in code generation tasks.","2832-7632","979-8-3315-3711-1","10.1109/ICSE-NIER66352.2025.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023972","LLMs;Smells;Benchmark;Interpretability","Measurement;Codes;Accuracy;Large language models;Benchmark testing;Software reliability;Software engineering","","","","45","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Securing connected and autonomous vehicles: analysing attack methods, mitigation strategies, and the role of large language models","M. Kumar; R. Rani; G. Epiphaniou; C. Maple","Secure Cyber System Research Group, WMG, University of Warwick, Coventry, UK; University of Southampton, Southampton, UK; Secure Cyber System Research Group, WMG, University of Warwick, Coventry, UK; Secure Cyber System Research Group, WMG, University of Warwick, Coventry, UK",International Conference on AI and the Digital Economy (CADE 2024),"30 Sep 2024","2024","2024","","89","94","Connected and Autonomous Vehicles (CAVs) are revolutionising transportation, promising increased safety, efficiency, and convenience. However, their reliance on digital connectivity introduces new cybersecurity vulnerabilities. This paper comprehensively analyses various attack methods targeting CAVs. We discuss the potential consequences of these attacks and propose mitigation strategies to enhance CAV security. Furthermore, the paper explores the potential of Large Language Models (LLMs) as a novel approach to fortifying CAV cybersecurity. Finally, we examine future challenges and opportunities within this rapidly developing field.","","978-1-83724-184-2","10.1049/icp.2024.2534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10700906","","","","","","","","30 Sep 2024","","","IET","IET Conferences"
"Optimizing Confidence Scoring in RAG-Based LLM Chatbots for Technical Support Services: A Prompt Engineering Approach","W. T. Chan; K. Hung; R. Ho; G. Man-Tat Man","School of Science and Technology, Hong Kong Metropolitan University, Hong Kong, China; School of Science and Technology, Hong Kong Metropolitan University, Hong Kong, China; OptiMicro Technologies Inc, Canada; School of Science and Technology, Hong Kong Metropolitan University, Hong Kong, China",2025 IEEE 15th Symposium on Computer Applications & Industrial Electronics (ISCAIE),"22 Jul 2025","2025","","","540","545","The global shortage of skilled personnel in technical support has spurred significant interest in AI-driven solutions, particularly Large Language Model (LLM)-based customer service chatbots. However, a critical challenge in deploying these systems lies in addressing AI hallucination, wherein models generate responses that are plausible yet factually incorrect. This study investigates a prompt engineering approach to enhance confidence estimation and mitigate AI hallucinations in LLM chatbots. Three distinct prompt strategies—Basic, Advanced, and Combo prompts–are systematically evaluated to improve response reliability. Given that LLMs inherently lack the ability to explicitly express uncertainty (e.g., by stating “I don't know”), a structured confidence scoring mechanism is employed to refine accuracy and reduce the Expected Calibration Error (ECE). Experimental results reveal that Basic prompts achieve an accuracy of 69.33% (ECE: 23.33), Advanced prompts improve accuracy to 75.33% (ECE: 14.87), and Combo prompts further enhance accuracy to 81.33% while reducing ECE to 8.4. These findings underscore the efficacy of prompt engineering in mitigating AI hallucinations and advancing the performance of LLM chatbots in real-world customer support applications.","2836-4317","979-8-3315-2166-0","10.1109/ISCAIE64985.2025.11081158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081158","LLM Chatbot;Confidence Score;AI Hallucination;Prompt Engineering;Customer Support","Industrial electronics;Accuracy;Uncertainty;Large language models;Refining;Estimation;Chatbots;Reliability engineering;Prompt engineering;Personnel","","","","11","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation","S. Ye; Z. Sun; G. Wang; L. Guo; Q. Liang; Z. Li; Y. Liu","Beijing University of Chemical Technology, Beijing, China; National Key Laboratory of Space Integrated Information System, Institute of Software, Chinese Academy of Sciences, Beijing, China; Peking University, Beijing, China; Beijing University of Chemical Technology, Beijing, China; Peking University, Beijing, China; Beijing University of Chemical Technology, Beijing, China; Beijing University of Chemical Technology, Beijing, China",IEEE Transactions on Software Engineering,"","2025","PP","99","1","22","Code generation has gained increasing attention as a task to automate software development by transforming high-level descriptions into executable code. While large language models (LLMs) are effective in generating code, their performance heavily relies on the quality of input prompts. Current prompt engineering methods involve manual effort in designing prompts, which can be time-consuming and yield inconsistent results, potentially constraining the efficacy of LLMs in practical applications. This paper introduces Prochemy, a novel approach for automatically refining prompts iteratively to enhance code generation. Prochemy addresses the limitations of manual prompt engineering by automating the optimization process, ensuring prompt consistency during inference, and aligning with multi-agent systems. It iteratively refines prompts based on model performance, using an optimized final prompt to improve consistency and reliability across tasks. We evaluate Prochemy on both natural language-based code generation and code translation tasks using three series of LLMs. Results show that when combining Prochemy with existing approaches, it outperforms baseline prompting methods. It achieves improvements of 5.0% (GPT-3.5-Turbo) and 1.9% (GPT-4o) over zero-shot baselines on HumanEval. For the state-of-the-art LDB, Prochemy + LDB outperforms standalone methods by 1.2–1.8%. For code translation, Prochemy elevates GPT-4o’s performance on Java-to-Python (AVATAR) from 74.5 to 84.1 (+12.9%) and Python-to-Java from 66.8 to 78.2 (+17.1%). Furthermore, considering that the o1-mini model integrates prompt engineering techniques, Prochemy can continue to show good performance among it, further validating its effectiveness in code generation and translation tasks. Additionally, Prochemy is designed to be plug-and-play, optimizing prompts with minimal human intervention and seamlessly bridging the gap between simple prompts and complex frameworks.","1939-3520","","10.1109/TSE.2025.3589634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082010","Code Generation;Prompt Refinement;Automation","Codes;Prompt engineering;Optimization;Natural language processing;Electronic mail;Translation;Training;Encoding;Cognition;Chemical technology","","","","","IEEE","16 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Boosting self-repair workflow with brainstorming for code generation","Z. Jin; Z. Shi","Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, China; Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, China",2024 IEEE 33rd Asian Test Symposium (ATS),"14 Mar 2025","2024","","","1","6","Utilizing large language models (LLMs) for code generation has greatly enhanced software development. However, despite the advances in automated code generation frameworks that incorporate self-repair strategies, successful outcomes are not always guaranteed. This led us to explore a different approach to improving code quality. So we implemented a brainstorm-select-repair framework. For a natural language query, our framework first generates multiple foundational code snippets. Then these snippets are tested on test cases, and a text-similarity-based algorithm is used to identify the most accurate code. If none of the generated code snippets successfully fulfills the test cases, our proposed self-repair strategy is used to rectify any flawed code snippets until a code snippet that can pass the test case is identified and then provided to the user. Based on ChatGPT3, our framework reached 89% Pass@1 on HumanEval dataset (at least 3% higher than ChatGPT-4 the best model to date on this dataset). Our framework also surpasses state-of-the-art methods and delivers superior performance on the HumanEval-ET, MBPP, and MBPP-ET datasets. To further validate our design rationale, we conducted comprehensive experiments and analyzed the impact of each component.","2377-5386","979-8-3315-2916-1","10.1109/ATS64447.2024.10915518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915518","large language model;AI for software engineering;code generation;self repair","Codes;Heuristic algorithms;Large language models;Natural languages;Maintenance engineering;Brain modeling;Problem-solving;Particle swarm optimization;Software development management;Software engineering","","","","22","IEEE","14 Mar 2025","","","IEEE","IEEE Conferences"
"Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies","Y. Liu; S. Tao; W. Meng; J. Wang; W. Ma; Y. Chen; Y. Zhao; H. Yang; Y. Jiang","Huawei, China; Huawei, China; Huawei, China; Beijing University of Posts and Telecommunications, China; Huawei, China; University of Science and Technology of China, China; Huawei, China; Huawei, China; Huawei, China",2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC),"18 Jun 2024","2024","","","35","46","Automated log analysis is crucial in modern software-intensive systems for facilitating program comprehension throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts’ comprehension of program status and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel interpretable log analysis approach for online scenarios. LogPrompt employs large language models (LLMs) to perform online log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs’ performance by up to 380.7% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite requiring no in-domain training, outperforms existing approaches trained on thousands of logs by up to 55.9%. We also conduct a human evaluation of LogPrompt’s interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment. Code of LogPrompt is available at https://github.com/lunyiliu/LogPrompt.CCS CONCEPTS • Computer systems organization $\rightarrow$ Reliability; • Social and professional topics $\rightarrow$ Software maintenance.","2643-7171","979-8-4007-0586-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556497","large language model;prompt engineering;log analysis;interpretability;online scenario","Training;Software maintenance;Analytical models;Systematics;Codes;Training data;Organizations","","6","","45","","18 Jun 2024","","","IEEE","IEEE Conferences"
"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pretrained Models","H. Yu; B. Shen; D. Ran; J. Zhang; Q. Zhang; Y. Ma; G. Liang; Y. Li; Q. Wang; T. Xie","School of Software and Microelectronics, Peking University, China; Huawei Cloud Computing Technologies Co., Ltd., China; Key Lab of HCST (PKU) MOE, SCS Peking University, China; Huawei Cloud Computing Technologies Co., Ltd., China; Huawei Cloud Computing Technologies Co., Ltd., China; Huawei Cloud Computing Technologies Co., Ltd., China; Huawei Cloud Computing Technologies Co., Ltd., China; School of Software and Microelectronics, Peking University, China; Huawei Cloud Computing Technologies Co., Ltd., China; Key Lab of HCST (PKU) MOE, SCS Peking University, Beijing, China",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","428","439","Code generation models based on the pretraining and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code). To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation problems carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation problems from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the target function but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (Code-Gen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating stan-dalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3623322","National Key Research and Development Program(grant numbers:2021YFF0704202); National Natural Science Foundation of China(grant numbers:62161146003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548523","Code Generation;Large Language Models;Benchmark","Industries;Codes;Benchmark testing;Chatbots;Libraries;Standards;Pragmatics","","1","","30","","14 Jun 2024","","","IEEE","IEEE Conferences"
"LLMNDC: A Novel Approach for Network Device Configuration based on Fine-tuned Large Language Models","Y. Fang; K. Lu; J. Xue; F. Li; Z. Lyu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; China Unicom, Beijing, China; China Unicom, Beijing, China",2024 5th International Conference on Computer Engineering and Intelligent Control (ICCEIC),"11 Dec 2024","2024","","","283","289","As networking demands continue to expand, there is an increasing need for enhanced capabilities in network device configuration. Managing network device configurations has become increasingly critical due to the expansion of network scales and complexity. Correct configurations ensure network security, performance, and reliability, yet traditional methods struggle with numerous parameters and complex dependencies. This complexity is exacerbated by varied configuration syntax and structures across different devices, further complicating management. To address these challenges, we propose a novel approach utilizing specially fine-tuned Large Language Models (LLMs) to assist in automating network device configuration. This study integrates artificial intelligence and machine learning to provide configuration recommendations, enhancing the automation level in network device management. Our experiments demonstrate the model's capability in querying general data communication knowledge and providing specific configuration suggestions tailored to manufacturers such as Huawei and Cisco. These results enhance the automation and intelligence of network device configuration processes, providing crucial data support and introducing new technical approaches to the field.","","979-8-3315-0799-2","10.1109/ICCEIC64099.2024.10775345","National Natural Science Foundation of China(grant numbers:61929104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10775345","Network Device Configuration;Large Language Models;Artificial Intelligence","Performance evaluation;Knowledge engineering;Automation;Large language models;Configuration management;Syntactics;Network security;Reliability engineering;Complexity theory;Network systems","","","","25","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Plausible Deniable Medical Image Encryption by Large Language Models and Reversible Content-Aware Strategy","Y. Wu; X. Liu; L. Cascone; M. Nappi; S. Wan","Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; Department of Computer Science, University of Salerno, Fisciano, SA, Italy; Department of Computer Science, University of Salerno, Fisciano, SA, Italy; Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen, China",IEEE Journal of Biomedical and Health Informatics,"","2025","PP","99","1","11","There is a rising concern about healthcare system security, where data loss could bring lots of damages to patients and hospitals. As a promising encryption method for medical images, DNA encoding own characteristics of high speed, parallelism computation, minimal storage, and unbreakable cryptosystems. Inspired by the idea of involving Large Language Models(LLMs) to improve DNA encoding, we propose a medical image encryption method with LLM-enhanced DNA encoding, which consists of LLM enhancing module and content-aware permutation&diffusion module. Regarding medical images generally have plain backgrounds with low-entropy pixels, the first module compresses pixels into highly compact signals with features of probabilistic varying and plausibly deniability, serving as another LLM-based layer of defense against privacy breaches before DNA encoding. The second module not only adds permutation by randomly sampling from a redundant correlation between adjacent pixels to break the internal links between pixels but also performs a DNAbased diffusion process to greatly increase the complexity of cracking. Experiments on ChestXray-14, COVID-CT and fcon-1000 datasets show that the proposed method outperforms all comparative methods in sensitivity, correlation and entropy.","2168-2208","","10.1109/JBHI.2025.3565271","National Key R&D Program of China(grant numbers:2023YFC3006501,2021YFB3900601); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20242050); Major Science and Technology Program of the Ministry of Water Resources of China(grant numbers:SKS2022072); High Performance Computing Platform, Hohai University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10979869","Medical Image Encryption;Large Language Models;Plausible Deniability;DNA Coding","Encryption;Biomedical imaging;Image coding;DNA;Medical services;Complexity theory;Probabilistic logic;Training;Electronic mail;Correlation","","","","","IEEE","29 Apr 2025","","","IEEE","IEEE Early Access Articles"
"A Robust Aggregation of Federated Large Language Models for Multimodal Knowledge Discovery in Computational Social Systems","J. Chen; C. Chakraborty; A. Polavarapu; Y. Qiu; Q. Zhao; O. Alfarraj; K. Yu","Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; School of Electronics Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; RIKEN Center for Advanced Intelligence Project (AIP), Tokyo 103-0027, Japan; RIKEN Center for Advanced Intelligence Project (AIP), Tokyo 103-0027, Japan; Computer Science Department, Community College, King Saud University, Riyadh, Saudi Arabia; Graduate School of Science and Engineering, Hosei University, Tokyo, Japan",IEEE Transactions on Computational Social Systems,"","2025","PP","99","1","16","Amid a rapidly evolving information era, large-scale multimodal knowledge discovery in computational social systems emerges as a key research domain. Large language models (LLMs) play a crucial role in this field, providing contextual understanding and task adaptability. Yet, centralized training of LLM raises privacy concerns. Federated learning (FL) offers a distributed alternative, but it struggles with data heterogeneity and security issues related to model parameters. To this end, we propose a robust aggregation method that leverages the relative total distance of models to improve global model performance in heterogeneous settings, complemented by Cheon-Kim-Kim-Song (CKKS) encryption to secure parameters against parameter stealing without performance loss. Extensive numeric results show our approach excels in LLM testing, scoring 3.74 on MTBenchmark and 8.17 on Vicuna, outperforming state-of-the-art FL methods against data heterogeneity challenges. It also achieves consistent gains on image datasets such as SVHN, CIFAR10, MNIST, TinyImageNet200, and CIFAR100, TinyImageNet200. In summary, our method offers an effective solution for secure multimodal data analysis in computational social systems.","2329-924X","","10.1109/TCSS.2025.3575623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11073569","Computational social systems;federated learning;large language models;large-scale knowledge discovery;privacy-preserving","Training;Computational modeling;Data models;Adaptation models;Knowledge discovery;Privacy;Servers;Large language models;Encryption;Numerical models","","","","","IEEE","8 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models","A. Islam; M. R. Biswas; W. Zaghouani; S. B. Belhaouari; Z. Shah","College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; College of Humanities and Social Sciences, Hamad Bin Khalifa University, Doha, Qatar; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar","2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS)","2 Jan 2024","2023","","","1","5","The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85%, 100%, 77%, and 79% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55%, which significantly improved to 83% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.","2831-7343","979-8-3503-1890-6","10.1109/SNAMS60348.2023.10375440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375440","Large Language Models;Large Multimodal Models;Prompt Engineering;Classification","Training;Visualization;Analytical models;Technological innovation;Zero-shot learning;Social networking (online);Data models","","3","","17","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"GestLLM: Advanced Hand Gesture Interpretation via Large Language Models for Human-Robot Interaction","O. Kobzarev; A. Lykov; D. Tsetserukou",Skolkovo Institute of Science and Technology; Skolkovo Institute of Science and Technology; Skolkovo Institute of Science and Technology,2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"30 Apr 2025","2025","","","1413","1417","This paper introduces GestLLM, an advanced system for human-robot interaction that enables intuitive robot control through hand gestures. Unlike conventional systems, which rely on a limited set of predefined gestures, GestLLM leverages large language models and feature extraction via MediaPipe [1] to interpret a diverse range of gestures. This integration addresses key limitations in existing systems, such as restricted gesture flexibility and the inability to recognize complex or unconventional gestures commonly used in human communication. By combining state-of-the-art feature extraction and language model capabilities, GestLLM achieves performance comparable to leading vision-language models while supporting gestures underrepresented in traditional datasets. For example, this includes gestures from popular culture, such as the “Vulcan salute” from Star Trek, without any additional pretraining, prompt engineering, etc. This flexibility enhances the naturalness and inclusivity of robot control, making interactions more intuitive and user-friendly. GestLLM provides a significant step forward in gesture-based interaction, enabling robots to understand and respond to a wide variety of hand gestures effectively. This paper outlines its design, implementation, and evaluation, demonstrating its potential applications in advanced human-robot collaboration, assistive robotics, and interactive entertainment.","","979-8-3503-7893-1","10.1109/HRI61500.2025.10973846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973846","LLM;gesture recognition;robot control","Hands;Meters;Accuracy;Service robots;Large language models;Robot control;NASA;Human-robot interaction;Gesture recognition;Feature extraction","","1","","18","IEEE","30 Apr 2025","","","IEEE","IEEE Conferences"
"AI and Security - What Changes with Generative AI","R. Sasaki","Tokyo Denki University, Tokyo, Japan","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","19 Feb 2024","2023","","","208","215","The authors clarified in 2020 that the relationship between AI and security can be classified into four categories: (a) attacks using AI, (b) attacks by AI itself, (c) attacks to AI, and (d) security measures using AI, and summarized research trends for each. Subsequently, ChatGPT became available in November 2022, and the various potential applications of ChatGPT and other generative AIs and the associated risks have attracted attention. In this study, we examined how the emergence of generative AI affects the relationship between AI and security. The results show that (a) the need for the four perspectives of AI and security remains unchanged in the era of generative AI, (b) The generalization of AI targets and automatic program generation with the birth of generative AI will greatly increase the risk of attacks by the AI itself, (c) The birth of generative AI will make it possible to generate easy-to-understand answers to various questions in natural language, which may lead to the spread of fake news and phishing e-mails that can easily fool many people and an increase in AI-based attacks. In addition, it became clear that (1) attacks using AI and (2) responses to attacks by AI itself are highly important. Among these, the analysis of attacks by AI itself, using an attack tree, revealed that the following measures are needed: (a) establishment of penalties for developing inappropriate programs, (b) introduction of a reporting system for signs of attacks by AI, (c) measures to prevent AI revolt by incorporating Asimov's three principles of robotics, and (d) establishment of a mechanism to prevent AI from attacking humans even when it becomes confused.","2693-9371","979-8-3503-5939-8","10.1109/QRS-C60940.2023.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430001","Artificial Intelligence;Machine Learning;Generative AI;AI and Security;Attack Tree;Research Issues","Generative AI;Phishing;Software quality;Chatbots;Software reliability;Security;Robots","","4","","15","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"Generative AI for Automated Security Operations in Cloud Computing","A. Patel; P. Pandey; H. Ragothaman; R. Molleti; D. R. Peddinti",Broadcom; Tiffany Co; Athenahealth; Options Clearing Corporation; Independent Researcher,2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC),"29 Jan 2025","2025","","","1","7","New opportunities in cloud computing have brought many new risks that require effective protection of dynamic distributed environments. Introducing a new formative technology, generative AI, to cloud security has far-reaching benefits for automating threat detection, real-time incident addressing, and vulnerability management. This paper focuses on extending generative AI with cloud security tools like AWS GuardDuty and Google Cloud Security Command Center; the contemplation of accuracy enhancement and response efficiency highlights its aim. Concerning actual applications such as SOAR systems, the study demonstrates how media industry giants, such as Netflix and JPMorgan Chase, have used AI to minimize risk factors while increasing operational efficiency. The paper also discusses the significant increase in response time, enhanced detection accuracy, and the shift to proactive security strategies brought by generative AI. Drawing attention to AI systems’ opportunities, the study examines the subsequent issues connected with AI applications, including over-dependence on AI tools, adversarial risk to models, and the complex nature of decisionmaking in the context of AI systems. The present study also highlights the importance of generative AI in strengthening the defense of the cloud environment, but, at the same time, it recognizes the significance of preventive efforts and planned action plans to manage these technologies efficiently.","","979-8-3315-1888-2","10.1109/ICAIC63015.2025.10849302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849302","cloud security;generative AI;incident response;soar systems","Technological innovation;Accuracy;Generative AI;Cloud computing security;Organizations;Real-time systems;Threat assessment;Time factors;Protection;Monitoring","","1","","23","IEEE","29 Jan 2025","","","IEEE","IEEE Conferences"
"LLM-based Control Code Generation using Image Recognition","H. Koziolek; A. Koziolek","ABB Research, Germany; Karlsruhe Institute of Technology, Germany",2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"30 Oct 2024","2024","","","38","45","LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P&IDs) using image recognition. We have evaluated the method in three case study with industrial P&IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.CCS CONCEPTS• Software and its engineering→Automatic programming; Command and control languages; • Applied computing → Computer-aided design; • Computing methodologies →Natural language processing.","","979-8-4007-0579-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734612","Large language models;code generation;P&IDs;IEC 61131-3;image recognition;industrial case study;industrial automation;PLC;DCS;ChatGPT;GPT4","Codes;Image recognition;Automation;Source coding;Process control;Manuals;Programming;Software;Logic;Testing","","4","","24","","30 Oct 2024","","","IEEE","IEEE Conferences"
"Membership-Function-Dependent Reachable Set Synthesis of Takagi-Sugeno Fuzzy Singular Multi-Agent Systems Subject to Deception Attacks","L. Zhang; X. Zhang; X. Zhao; N. Zhao","College of Control Science and Engineering, Bohai University, Jinzhou, Liaoning, China; College of Control Science and Engineering, Bohai University, Jinzhou, Liaoning, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, Liaoning, China; College of Control Science and Engineering, Bohai University, Jinzhou, Liaoning, China",IEEE Transactions on Automation Science and Engineering,"21 Mar 2025","2025","22","","7506","7515","This paper focuses on the membership-functions-dependent reachable set (RS) synthesis of a class of Takagi-Sugeno (T-S) fuzzy singular multi-agent systems subject to deception attacks. To do this, with the help of the T-S fuzzy model, the nonlinear system model is transformed into a quasi-linear system. Then, the control method in the linear system is used to solve this kind of nonlinear problem. Firstly, two suitable non-singular value matrices M and N are obtained by singular value decomposition, and then the singular system is decomposed into algebraic subsystem and differential subsystem by matrices M and N. Secondly, a consensus topology with communication delay under deception attack is designed. In this cases, the sufficient conditions of RS for the simultaneous existence of external disturbances and deception attacks are given, and the specific value of the RS is obtained by solving the linear matrix inequality using the matrix decoupling method. Finally, the effectiveness of the proposed method is verified by taking the actual DC motor built on the physical platform and a class of nonlinear single-species bio-economic system as examples.Note to Practitioners—In the context of contemporary huge industry and engineering, while rapid development, security is also one of the issues that must be considered. However, the modern engineering system has the characteristics of complexity and variability, and a single module cannot operate it completely, so multiple separate modules need to work together through a unique connection. For this reason, security issues will become much more difficult. In addition, in modern network engineering, there are often some criminals who destroy the system, resulting in its normal operation, economic losses, serious situations, or endanger personal safety. Such behavior is usually called network attacks. Deception attacks are part of network attacks, which can harm the system by spreading false information. Therefore, in researching security issues, deception attacks need to be paid attention. Based on the above analysis, the membership-functions-dependent reachable set synthesis of a class of Takagi-Sugeno fuzzy singular multi-agent systems subject to deception attacks is studied in this paper.","1558-3783","","10.1109/TASE.2024.3464580","National Natural Science Foundation of China(grant numbers:62203065); Applied Basic Research Program of Liaoning Province(grant numbers:2023021211-JH26/103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10697260","Reachable set synthesis;deception attacks;membership-functions-dependent;T-S fuzzy singular multi-agent systems","Delays;Topology;Stability analysis;Nonlinear systems;Multi-agent systems;Estimation;Autonomous aerial vehicles;Takagi-Sugeno model;Linear systems;Economics","","20","","52","IEEE","27 Sep 2024","","","IEEE","IEEE Journals"
"Using Large Language Models for Bug Localization and Fixing","T. D. Viet; K. Markov","The University of Aizu, Fukushima, Japan; The University of Aizu, Fukushima, Japan",2023 12th International Conference on Awareness Science and Technology (iCAST),"21 Dec 2023","2023","","","192","197","As part of their learning journey, students frequently encounter challenges and make errors, especially with algorithmic programming questions. Regrettably, providing tailored solutions for these mistakes can impose a significant burden on instructors in terms of time and effort. To address this, automated program repair (APR) techniques have been explored to generate such fixes automatically. Previous research has investigated the use of symbolic and neural approaches for APR in the educational domain. However, both types of approaches necessitate substantial engineering endeavors or extensive data and training. In this study, we propose the utilization of a large language model trained on code to construct an APR system specifically designed for student programs. Our system has the capability to rectify semantic errors by employing a few-shot example generation pipeline solely based on the input code. We assess the performance of our system on one dataset of algorithm implementations, namely QuixBugs. The results demonstrate that the novel example generation pipeline not only enhances the overall system’s performance but also ensures its stability.","2325-5994","979-8-3503-2469-3","10.1109/iCAST57874.2023.10359304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10359304","bug localization;bug fixing;program repair;large language model;few-shot prompting;in-context learning","Location awareness;Training;Codes;Computer bugs;Semantics;Pipelines;Maintenance engineering","","5","","33","IEEE","21 Dec 2023","","","IEEE","IEEE Conferences"
"Generative AI and the Metaverse: A Scoping Review of Ethical and Legal Challenges","A. Tabassum; E. Elmahjub; A. I. Padela; A. Zwitter; J. Qadir","Department of Computer Science and Engineering, College of Engineering, Qatar University, Doha, Qatar; Public Law Department, College of Law, Qatar University, Doha, Qatar; Department of Emergency Medicine, Medical College of Wisconsin, Milwaukee, WI, USA; Department of Governance and Innovation, University of Groningen, Groningen, The Netherlands; Department of Computer Science and Engineering, College of Engineering, Qatar University, Doha, Qatar",IEEE Open Journal of the Computer Society,"25 Feb 2025","2025","6","","348","359","The metaverse, a pioneering digital realm merging virtual and augmented realities with Artificial Intelligence (AI), represents a transformative environment where digital and physical realities converge seamlessly. Generative AI (GenAI) is indispensable in powering the metaverse's dynamic and immersive experiences, enabling the autonomous generation of diverse digital content. Large Language Models (LLMs), as a component of GenAI, play a critical role by facilitating real-time communication, multilingual translation, and personalized interactions, enhancing user engagement in shared virtual spaces. This scoping review explores the interdependence between GenAI and the metaverse and the unique ethical and legal challenges that emerge from their integration. It identifies key ethical and legal issues, such as bias in AI-generated content, misinformation, and data privacy concerns, related to the deployment of GenAI and LLMs, and offers strategic recommendations for addressing these challenges responsibly. Emphasizing the transformative potential of these technologies, this review highlights the necessity of developing tailored ethical and legal frameworks to manage their convergence responsibly, ensuring equitable and sustainable growth within the metaverse.","2644-1268","","10.1109/OJCS.2025.3536082","Qatar University; Qatar University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857306","Ethics;extended reality;generative AI (GenAI);LLMs;metaverse;privacy;regulation;security","Metaverse;Ethics;Law;Reviews;Databases;Internet;Generative AI;Translation;Real-time systems;Three-dimensional displays","","4","","56","CCBY","29 Jan 2025","","","IEEE","IEEE Journals"
"Prevention of Prompt Injection Attacks Over Financial Applications Integrated with LLM","T. Joshi; V. Naik; I. Mistry; R. Mangrulkar","Dept. of Information Technology, DJ Sanghvi College of Engineering, Mumbai, India; Dept. of Information Technology, DJ Sanghvi College of Engineering, Mumbai, India; Dept. of Information Technology, DJ Sanghvi College of Engineering, Mumbai, India; Dept. of Information Technology, DJ Sanghvi College of Engineering, Mumbai, India",2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT),"28 May 2025","2025","","","225","230","Prompt injection is a growing concern in financial applications integrated with Large Language Models. These attacks pose a critical risk to financial applications leading to data breaches, confidential data loss and financial losses by manipulating input prompts to cause unintended or harmful outputs. Existing defense strategies include utilizing filters f or input and output, and delimiters, however, these techniques have been proven inadequate. This paper builds upon existing mechanisms and proposes an enhanced security system that incorporates role-based access, jailbreak attack detection, validating inputs and securely passing prompts to LLM. This is done by developing a pre-processing layer at both ends-the client and LLMs-through the identification of critical as well as destructive keywords. The experimental results show that blocking of that malicious prompt ensures that only authenticated and authorized commands are processed. The measures, therefore, allow financial institutions to significantly reduce the probability of prompt injection attacks besides increasing data protection and confidence i n AI-based financial applications.","","979-8-3315-4389-1","10.1109/InCACCT65424.2025.11011372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011372","Authorization;Finance;Jailbreak;LLMs;Prompt Engineering;Security;Signing","Authorization;Filters;Large language models;Prevention and mitigation;Computational modeling;Finance;Data protection;Data breach;Prompt engineering;Computer security","","","","18","IEEE","28 May 2025","","","IEEE","IEEE Conferences"
"Generating Phishing Attacks and Novel Detection Algorithms in the Era of Large Language Models","J. Fairbanks; E. Serra","Boise State University, Boise, Idaho, USA; Boise State University, Boise, Idaho, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2314","2319","Phishing is a significant cybersecurity threat, with the financial impact of email security breaches and lack of awareness estimated to be between $50-100 billion in 2022. The advent of Large Language Models (LLMs) has further automated and intensified phishing attacks, posing greater challenges for defenders, especially large organizations being targeted by Advanced Persistent Threats (APT) at scale, such as Department of Energy National Labs. This study presents the development of two innovative algorithms. The first algorithm improves the efficacy of phishing attacks, while the second algorithm counteracts and defends against phishing attacks that leverage LLMs. The attack method takes detectable malicious phishing emails and rewrites them using an innovative LLM-based automatic output optimization technique, which includes Reflection and Beam Search, while preserving the original semantic meaning and Indicators Of Compromise (IOC). This approach bypasses most-commonly used institutional security tools, NLP and other LLM phishing detection systems. The results indicate that this attack algorithm increases the success rate of phishing attacks by up to 98%. The defensive algorithm presented in this research is also employed for defensive measures. When the proposed defensive algorithm is applied, it identifies malicious emails with 97% greater accuracy. The research detailed in this paper demonstrates that these algorithm serve dual purposes: one is utilized as an attack mechanism by altering the output, and the other as a defensive measure against phishing attacks by modifying the defensive prompt. Taking these algorithms and implementing them in the Department of Energy Laboratory (DOE) has demonstrated the effectiveness of applying these approaches to real world applications, and has been implemented into large-scale production environments.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825007","Email Phishing;Artificial Intelligence (AI);Large Language Model (LLM);Beam Search;Reflection;Big Data;Agentic AI","Technological innovation;Phishing;Large language models;Laboratories;Semantics;Production;Reflection;Electronic mail;Optimization;Penetration testing","","","","44","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"An Investigation of Large Language Models for Real-World Hate Speech Detection","K. Guo; A. Hu; J. Mu; Z. Shi; Z. Zhao; N. Vishwamitra; H. Hu",University at Buffalo; Union County Magnet High School; East Chapel Hill High School; East Chapel Hill High School; University at Buffalo; University of Texas at San Antonio; University at Buffalo,2023 International Conference on Machine Learning and Applications (ICMLA),"19 Mar 2024","2023","","","1568","1573","Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets. We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech. By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech. Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques. Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection.","1946-0759","979-8-3503-4534-6","10.1109/ICMLA58977.2023.00237","National Science Foundation(grant numbers:2129164,2114982,2228617,2120369,2237238,2245983); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459901","hate speech;large language model;prompt engineering;few-shot learning","Training;Hate speech;Knowledge based systems;Natural languages;Machine learning;Benchmark testing;Cognition","","5","","39","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Friends, Not Foes: Enhancing Attack Surface Metrics of Web Applications With Large Language Models","A. Tortumlu; K. Bicakci","Department of Defence Technologies, Istanbul Technical University, Istanbul, Türkiye; Informatics Institute and Department of Computer Engineering, Istanbul Technical University, Istanbul, Türkiye",2024 17th International Conference on Information Security and Cryptology (ISCTürkiye),"11 Dec 2024","2024","","","1","6","The attack surface refers to all potential points of entry in a system that an attacker could exploit. In this work, we present an advanced metric for calculating the attack surface of web applications by leveraging the capabilities of large language models (LLMs) and introducing new features based on recent evaluations of the OWASP Top 10 security risks. Incorporating 28 parameters across 9 main categories and drawing on past studies to address previously identified limitations, this metric provides a comprehensive assessment of the attack surface. The Euclidean norm of the metric allows for quantitative comparisons, enabling precise evaluation and monitoring of security risks. Additionally, we develop a tool to facilitate the calculation of attack surface parameters, with the source code available as open source. The tool features an interface that visualizes the attack surface vector, presenting the metrics of web applications in graphical form. Among its many uses, web security analysts can employ our metric and tool to monitor changes in an application's attack surface across different versions. We also provide recommendations for further improving attack surface metric calculations using LLMs.","","979-8-3315-3364-9","10.1109/ISCTrkiye64784.2024.10779256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10779256","attack surface;web application security;web security;large language model;chatGPT;LLM","Measurement;Visualization;Large language models;Source coding;Vectors;Security;Monitoring","","","","16","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Generative AI based Interpretation of Human Ultrametrics and Analysis","A. L. N. S; A. S. Nair; S. Sudhakar","Department of ECE, Humanitarian Technology Labs, Amrita School of Engineering, India; Department of Computer Engineering, Model Engineering College, Kochi, India; Dept Of Oral & Maxillofacial Pathology, PMS College of Dental Science and Research, Thiruvananthapuram, India",2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST),"24 Jun 2025","2025","1","","1","5","This paper presents a biometric identification system based on tongue print recognition with the help of machine learning techniques. The system is named Generative-AI-based Interpretation of Human Ultrametrics and Analysis (GIHUA). Traditional biometric systems like fingerprints, facial recognition and iris scans suffer the challenges of computational complexity, forgery risks and age-related changes. Tongue print, on the contrary has the advantages that it remains unchanged throughout the life of a person, unaffected by trauma or diseases and is very difficult to forge, due to difficulty in its acquisition without the awareness of the individual. Tongue-print based systems mostly rely on image-based dataset whereas this work focuses on a numerical dataset which contributes to computational efficiency. GIHUA uses ultrametric distances to extract the features and this makes it ideal for applying directly into any machine learning model, regardless of being linear or non-linear. To overcome the scarcity of labeled tongue-print datasets, GIHUA creates synthetic labels with the help of K-means clustering and generative AI-based data augmentation technique. Classification is performed using random forest algorithm. Results show an accuracy of 99.10%, promising tongue print as an additional or alternative approach to biometric identification.","","979-8-3315-0537-0","10.1109/ICTEST64710.2025.11042382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042382","Tongue print identification;Generative AI;machine learning;semi-supervised learning;ultrametric distance;synthetic data;identity verification;security systems","Tongue;Generative AI;Semisupervised learning;Feature extraction;Biometric identification;Market research;Numerical models;Security;Random forests;Synthetic data","","","","17","IEEE","24 Jun 2025","","","IEEE","IEEE Conferences"
"Towards LLM-Assisted System Testing for Microservices","M. Almutawa; Q. Ghabrah; M. Canini",KAUST; KAUST; KAUST,2024 IEEE 44th International Conference on Distributed Computing Systems Workshops (ICDCSW),"4 Sep 2024","2024","","","29","34","As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance (QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef, a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents.","2332-5666","979-8-3503-5471-3","10.1109/ICDCSW63686.2024.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10660826","Software Testing;Testing Automation;Large Language Models (LLMs);Communicative Agents","Software testing;System testing;Quality assurance;Large language models;Microservice architectures;Software systems;Reliability engineering","","","","29","IEEE","4 Sep 2024","","","IEEE","IEEE Conferences"
"ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation","D. G. Paul; H. Zhu; I. Bayley","School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK; School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK; School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK",2024 IEEE International Conference on Artificial Intelligence Testing (AITest),"25 Sep 2024","2024","","","55","63","In the scenario-based evaluation of machine learning models, a key problem is how to construct test datasets that represent various scenarios. The methodology proposed in this paper is to construct a benchmark and attach metadata to each test case. Then a test system can be constructed with test morphisms that filter the test cases based on metadata to form a dataset. The paper demonstrates this methodology with large language models for code generation. A benchmark called ScenEval is constructed from problems in textbooks, an online tutorial website and Stack Overflow. Filtering by scenario is demonstrated and the test sets are used to evaluate ChatGPT for Java code generation. Our experiments found that the performance of ChatGPT decreases with the complexity of the coding task. It is weakest for advanced topics like multi-threading, data structure algorithms and recursive methods. The Java code generated by ChatGPT tends to be much shorter than reference solution in terms of number of lines, while it is more likely to be more complex in both cyclomatic and cognitive complexity metrics, if the generated code is correct. However, the generated code is more likely to be less complex than the reference solution if the code is incorrect.","2835-3560","979-8-3503-6505-4","10.1109/AITest62860.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685215","Machine learning;Large language models;ChatGPT;Code Generation;Benchmark;Performance evaluation;Scenario-based testing","Measurement;Java;Codes;Tutorials;Manuals;Metadata;Benchmark testing","","","","28","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine","E. Isaku; C. Laaber; H. Sartaj; S. Ali; T. Schwitalla; J. F. Nygård","Simula Research Laboratory and University of Oslo, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Cancer Registry of Norway, Oslo, Norway; Cancer Registry of Norway and UiT The Arctic University of Norway, Oslo, Norway","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","429","440","The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e., data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989025","Large Language Models;Medical Rules;Automated Software Testing;Test Generation;Differential Testing;Electronic Health Records","Software testing;Web services;Large language models;Robustness;Regulation;Test pattern generators;Stakeholders;Engines;Standards;Cancer","","","","50","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Challenges to Using Large Language Models in Code Generation and Repair","L. Pasquale; A. Sabetta; M. d’Amorim; P. Hegedűs; M. T. Mirakhorli; H. Okhravi; M. Payer; A. Rashid; J. C. S. Santos; J. M. Spring; L. Tan; K. Tuma","University College Dublin, Dublin, Ireland; SAP, Mougins, France; North Carolina State University, Raleigh, NC, USA; Department of Software Engineering, University of Szeged, Szeged, Hungary; University of Hawaii at Manoa, Honolulu, HI, USA; Massachusetts Institute of Technology (MIT) Lincoln Laboratory, Lexington, MA, USA; EPFL, Lausanne, Switzerland; University of Bristol, Bristol, U.K.; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Cybersecurity and Infrastructure Security Agency, VA, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands",IEEE Security & Privacy,"26 Mar 2025","2025","23","2","81","88","Large language models (LLMs) hold great promise in solving many challenges arising from software complexity, including the possibility of automating code generation and repair. Although we cannot deny the groundbreaking nature of LLM-based code repair, we must be realistic in positioning current results.","1558-4046","","10.1109/MSEC.2025.3530488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10942512","","Privacy;Codes;Large language models;Maintenance engineering;Software design;Complexity theory;Software development management;Software reliability;Software development management","","","","15","IEEE","26 Mar 2025","","","IEEE","IEEE Magazines"
"Bare-Metal Firmware Fuzzing: A Survey of Techniques and Approaches","A. Asmita; R. Tsang; S. Ghimire; S. Salehi; H. Homayoun","Electrical and Computer Engineering Department, University of California at Davis, Davis, CA, USA; Electrical and Computer Engineering Department, University of California at Davis, Davis, CA, USA; Electrical and Computer Engineering Department, The University of Arizona, Tucson, AZ, USA; Electrical and Computer Engineering Department, The University of Arizona, Tucson, AZ, USA; Electrical and Computer Engineering Department, University of California at Davis, Davis, CA, USA",IEEE Access,"10 Jun 2025","2025","13","","98253","98277","Firmware attacks are increasingly prevalent, often serving as low-hanging fruit for attackers due to the challenges of firmware security analysis. The complexity of hardware systems, platform diversity, peripheral interactions, and asynchronous events make thorough security analysis of embedded firmware particularly difficult. Despite these challenges, significant research has been dedicated to advancing dynamic analysis techniques, such as fuzzing, to improve firmware security. Existing research approaches these issues with varying methods and emphases. This survey paper examines the implementation of existing firmware fuzzing techniques, providing an overview of their emulation strategies and fuzzing methodologies. It also reviews several existing fuzzers and the application of large language models (LLMs) in fuzzing generic software. Our survey focuses specifically on frameworks for fuzzing embedded bare-metal/monolithic firmware. Our analysis highlights that most existing research has focused primarily on firmware emulation, rehosting, and back-end instrumentation to facilitate fuzzing, often relying on direct integration with existing fuzzers. However, the broader exploration of various fuzzing techniques, such as input generation, mutation, feedback, and scheduling strategies, widely used in generic software remains limited for embedded firmware. Recent efforts have started to address these aspects, with emerging work exploring fuzzing techniques beyond simple fuzzer integration. Furthermore, the application of LLMs presents a promising direction for further investigation. This survey provides a comprehensive overview of the past, present, and future landscape of bare-metal firmware fuzzing.","2169-3536","","10.1109/ACCESS.2025.3575691","Noyce Foundation and NSF(grant numbers:1916741); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11020638","Bare-metal firmware;embedded firmware security;emulator;fuzzing;large language models (LLMs);rehosting;software security","Fuzzing;Microprogramming;Surveys;Security;Software;Hardware;Emulation;Instruments;Embedded systems;Large language models","","","","143","CCBY","2 Jun 2025","","","IEEE","IEEE Journals"
"A Comprehensive Study on Code Completion for Large Language Models","Y. Cai","Guangdong University of Technology, Tianhe District, Guangdong, China",2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE),"9 Jun 2025","2025","","","2506","2510","Large language models have demonstrated remarkable capabilities in code generation, exhibiting a profound understanding of code semantics and functionality. Code completion is a critical task within code generation, playing a key role in enhancing development efficiency and facilitating code comprehension. Although some studies on code completion based on LLMs have emerged, most of these efforts rely on transformer architectures, autoregressive generation models, and graph neural networks, making it difficult to obtain a clear and comprehensive understanding of their overall development storyline. Therefore, this work conducts a comprehensive survey of existing large-model-based code completion approaches from multiple perspectives, systematically analyzing their performance, advantages, and limitations from the viewpoints of task requirements and technical methodologies. To evaluate the real-world effectiveness of current approaches, this study collects representative code snippets and constructs multi-granularity completion samples using multilevel semantic analysis and generation model evaluation methods. These samples cover various programming languages, including Python, Java, and C++. Subsequently, a hierarchical code completion evaluation is conducted to compare the performance of different LLMs across different programming languages. By leveraging these techniques, this paper provides a comprehensive analysis of the practical performance and limitations of different models in real-world development scenarios. Experimental results indicate that current LLM-based code completion methods generally achieve high accuracy and strong contextual understanding in common code completion tasks. However, there is still room for improvement in handling long-range dependencies and complex syntactic structures. The findings suggest that while LLMs perform well in completing simple to moderately complex tasks, their effectiveness declines when dealing with code segments involving long-span dependencies or intricate relationships, particularly in scenarios with complex multi-function calls, nested structures, and asynchronous programming.","","979-8-3315-3508-7","10.1109/ICAACE65325.2025.11020128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11020128","LLM;code generation;C++;code completion;software development","Analytical models;Java;Codes;Accuracy;Large language models;Semantics;C++ languages;Transformers;Python;Context modeling","","","","16","IEEE","9 Jun 2025","","","IEEE","IEEE Conferences"
"CALLM: Enhancing Clinical Interview Analysis Through Data Augmentation With Large Language Models","Y. Wu; K. Mao; Y. Zhang; J. Chen","Electrical & Computer Engineering Department, Faculty of Engineering, University of Alberta, Edmonton, AB, Canada; Electrical & Computer Engineering Department, Faculty of Engineering, University of Alberta, Edmonton, AB, Canada; Department of Psychiatry, Faculty of Medicine and Dentistry, University of Alberta, Edmonton, AB, Canada; Electrical & Computer Engineering Department, Faculty of Engineering, University of Alberta, Edmonton, AB, Canada",IEEE Journal of Biomedical and Health Informatics,"5 Dec 2024","2024","28","12","7531","7542","The global prevalence of mental health disorders is increasing, leading to a significant economic burden estimated in trillions of dollars. In automated mental health diagnosis, the scarcity and imbalance of clinical data pose considerable challenges for researchers, limiting the effectiveness of machine learning algorithms. To cope with this issue, this paper aims to introduce a novel clinical transcript data augmentation framework by leveraging large language models (CALLM). The framework follows a “patient-doctor role-playing” intuition to generate realistic synthetic data. In addition, our study introduces a unique “Textbook-Assignment-Application” (T-A-A) partitioning approach to offer a systematic means of crafting synthetic clinical interview datasets. Concurrently, we have also developed a “Response-Reason” prompt engineering paradigm to generate highly authentic and diagnostically valuable transcripts. By leveraging a fine-tuned DistilBERT model on the E-DAIC PTSD dataset, we achieved a balanced accuracy of 0.77, an F1-score of 0.70, and an AUC of 0.78 during test set evaluations, which showcase robust adaptability in both Zero-Shot Learning (ZSL) and Few-Shot Learning (FSL) scenarios. We further compare the CALLM framework with other data augmentation methods and PTSD diagnostic works and demonstrates consistent improvements. Compared to conventional data collection methods, our synthetic dataset not only demonstrates superior performance but also incurs less than 1% of the associated costs.","2168-2208","","10.1109/JBHI.2024.3435085","China Scholarship Council(grant numbers:202308180002,202000810031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614318","Affective computing;post-traumatic stress disorders;transformer;large language models;data augmentation","Interviews;Mental health;Data augmentation;Data models;Large language models;Medical diagnostic imaging;Natural language processing","Humans;Machine Learning;Natural Language Processing;Algorithms;Mental Disorders;Electronic Health Records;Stress Disorders, Post-Traumatic;Large Language Models","9","","71","IEEE","29 Jul 2024","","","IEEE","IEEE Journals"
"Generative AI Meets Wireless Networking: An Interactive Paradigm for Intent-Driven Communications","X. Qin; M. Sun; J. Dai; P. Ma; Y. Cao; J. Zhang; J. Wang; X. Xu; P. Zhang; D. Niyato","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Psychology, Nanjing Normal University, Nanjing, China; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Cognitive Communications and Networking,"7 Aug 2025","2025","11","4","2056","2077","This paper introduces an innovative paradigm in interactive wireless networking, leveraging large language models (LLMs) and generative artificial intelligence (GenAI) to dynamically align network configurations and transmission strategies with user intents. Central to this paradigm is a “human-in-the-loop” framework that incorporates two pivotal processes: intent alignment with the network and streamlined user interaction. Existing quality of experience (QoE) modeling methodologies often suffer from a weak correlation between user intent and network adjustments. To address this, we review intent consistency in QoE modeling, utilizing reinforcement learning from human feedback (RLHF) and advanced prompt engineering to seamlessly integrate human intent into network configurations. Surrounding our paradigm, we provide a comprehensive survey on a multi-layered integration that spans the application, intent, semantic, network, and transmission layers, seamlessly translating user intent into optimized networking and communication outcomes. We investigate key technologies such as semantic communication and GenAI-enabled transmission and networking for pioneering applications, which allow for the real-time understanding and execution of high-layer intents, enabling the integration of user intents with wireless networks. Additionally, we explore cloud-edge-device collaboration within our paradigm, which distributes computational and semantic tasks across the network infrastructure to enable low-latency, scalable, and intelligent interactions. The paper concludes with an analysis of existing challenges and prospective research directions, highlighting the transformative potential of GenAI in shaping the future of intent-driven wireless networks.","2332-7731","","10.1109/TCCN.2025.3567613","National Key Research and Development Program of China(grant numbers:2023YFB4301901); National Natural Science Foundation of China(grant numbers:62371072,62401074,62371063); Beijing Nova Program(grant numbers:2024102); Beijing Municipal Natural Science Foundation(grant numbers:L242012,L232047); National Research Foundation, Singapore and Infocomm Media Development Authority under its Future Communications Research & Development Programme(grant numbers:FCP-NTU-RG-2022-010,FCPASTAR- TG-2022-003); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG87/22,RG24/24); NTU Centre for Computational Technologies in Finance (NTU-CCTF); RIE2025 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP)(grant numbers:I2301E0026); Alibaba Group and NTU Singapore through Alibaba-NTU Global e-Sustainability CorpLab (ANGEL); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10990238","Interactive wireless networks;generative artificial intelligence (GenAI);large language models (LLMs);human-in-the-loop;reinforcement learning from human feedback (RLHF);experience of quality (QoE);intent alignment;semantic communication (SemCom)","Quality of experience;Wireless networks;Artificial intelligence;Prompt engineering;Diffusion models;Reinforcement learning;Semantic communication;Optimization;Collaboration;Training","","1","","177","IEEE","7 May 2025","","","IEEE","IEEE Journals"
"REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring","T. C. Ho; F. Kharrat; A. Abid; F. Karray; A. Koubaa","Mohamed bin Zayed University of Artificial Intelligence Masdar City, Abu Dhabi, UAE; Mohamed bin Zayed University of Artificial Intelligence Masdar City, Abu Dhabi, UAE; Mohamed bin Zayed University of Artificial Intelligence Masdar City, Abu Dhabi, UAE; Mohamed bin Zayed University of Artificial Intelligence Masdar City, Abu Dhabi, UAE; College of Computer and Information Sciences Prince Sultan University",2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA),"29 Jul 2024","2024","","","1","6","With the widespread adoption of wearable devices in our daily lives, the demand and appeal for remote patient monitoring have significantly increased. Most research in this field has concentrated on collecting sensor data, visualizing it, and analyzing it to detect anomalies in specific diseases such as diabetes, heart disease and depression. However, this domain has a notable gap in the aspect of human-machine interaction. This paper proposes REMONI, an autonomous REmote health MONItoring system that integrates multimodal large language models (MLLMs), the Internet of Things (loT), and wearable devices. The system automatically and continuously collects vital signs, accelerometer data from a special wearable (such as a smartwatch), and visual data in patient video clips collected from cameras. This data is processed by an anomaly detection module, which includes a fall detection model and algorithms to identify and alert caregivers of the patient's emergency conditions. A distinctive feature of our proposed system is the natural lan-guage processing component, developed with MLLMs capable of detecting and recognizing a patient's activity and emotion while responding to healthcare worker's inquiries. Additionally, prompt engineering is employed to integrate all patient information seamlessly. As a result, doctors and nurses can access real-time vital signs and the patient's current state and mood by interacting with an intelligent agent through a user-friendly web application. Our experiments demonstrate that our system is implementable and scalable for real-life scenarios, potentially reducing the workload of medical professionals and healthcare costs. A full-fledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities.","2837-5882","979-8-3503-0799-3","10.1109/MeMeA60663.2024.10596778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596778","Remote Health Monitoring;Wearable Technol-ogy;Multimodal Large Language Models;Healthcare","Human computer interaction;Visualization;Costs;Autonomous systems;Large language models;Data visualization;Medical services","","","","23","IEEE","29 Jul 2024","","","IEEE","IEEE Conferences"
"Fuzzy Observer-Based Consensus Tracking Control for Fractional-Order Multi-Agent Systems Under Cyber-Attacks and Its Application to Electronic Circuits","G. Narayanan; M. S. Ali; Q. Zhu; B. Priya; G. K. Thakur","Center for Computational Modeling, Chennai Institute of Technology, Chennai, India; Department of Mathematics, Thiruvalluvar University, Vellore, India; School of Mathematics and Statistics, Hunan Normal University, Changsha, China; Department of Applied Science, G L Bajaj institute of Technology and Management, Greater Noida, India; ABES Engineering College, Ghaziabad, India",IEEE Transactions on Network Science and Engineering,"23 Feb 2023","2023","10","2","698","708","Consensus control of multi-agent systems (MASs) has applications in various domains. As MASs work in networked environments, their security control becomes critically desirable in response to cyber-attacks. In this paper, the observer-based consensus tracking control problem is investigated for a class of Takagi-Sugeno fuzzy fractional-order multi-agent systems (FOMASs) under cyber-attacks. The malicious cyber attacks can impact the security of topologies of the communication networks of both controllers and observers. To estimate unmeasurable system states, a fuzzy observer is built. It is found that the topology of contact for observer states may be different from that of the feedback signals. A novel mathematical model for T-S fuzzy FOMASs with cyber-attacks is proposed. By using algebraic graph theory, Lyapunov functional, and fractional calculus theory, a distributed feed-back controller is developed for each agent, which guarantee the secure performance of tracking consensus error and observer error. Finally, two numerical examples demonstrate the effectiveness of the suggested control scheme, and the controller design for electronic network circuits shows the applicability of the proposed theoretical results. Simulations results for different differential-orders and coupling strength scenarios are given.","2327-4697","","10.1109/TNSE.2022.3217618","Center for Nonlinear Systems, Chennai Institute of Technology; Chennai Institute of Technology(grant numbers:CIT/CNS/2022/RD/003); National Natural Science Foundation of China(grant numbers:62173139); Science and Technology Innovation Program of Hunan Province(grant numbers:2021RC4030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933020","Cyber-attacks;distributed control;fractional-order;multi-agent systems;takagi-sugeno model","Observers;Security;Topology;Network topology;Multi-agent systems;Takagi-Sugeno model;Robot sensing systems","","56","","46","IEEE","31 Oct 2022","","","IEEE","IEEE Journals"
"Secure Distributed Adaptive Control of Nonlinear Multi-Agent Systems","Y. Shi; E. Nekouei","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Cluny Road, Singapore; Department of Electrical Engineering, City University of Hong Kong, Hong Kong, China",IEEE Transactions on Automation Science and Engineering,"28 Mar 2025","2025","22","","8937","8951","This paper addresses the problem of secure consensus tracking control for nonlinear leader-follower multi-agent systems, with a specific focus on safeguarding followers’ private information from external network eavesdroppers or internal untrusted neighbors. To tackle this problem, we first employ the dynamic linearization approximation technique to transform the nonlinear system models into equivalent linear forms that involve unknown time-varying pseudopartial derivatives. Then, a novel model-free secure distributed adaptive control (MFSDAC) framework is proposed using an encoding-decoding mechanism and Paillier encryption. Within this framework, we develop a secure distributed control scheme using a recursive form and design an adaptive updating law with a modified projection to estimate the time-varying pseudoparial derivatives. To enhance security, we introduce an adjustable parameter and a random integer into the distributed communication protocol, effectively preventing the disclosure of followers’ data during both network transmissions and controller evaluations. Additionally, parameter selection rules for the controller, quantizer, and adaptive updating law are provided, along with convergence analysis and guarantees against quantizer saturation. Finally, numerical simulations confirm that the proposed MFSDAC framework successfully achieves leader-following tracking and secure data transmission, even in the presence of external network eavesdroppers or internal untrusted neighbors. Note to Practitioners—Multi-agent systems (MASs) provide a versatile framework for modeling and understanding various real-world applications, including autonomous systems, traffic management, and distributed sensor networks. Designing effective control strategies for MASs is essential to enhance cooperation and coordination among agents, strengthen system-level resilience and adaptability, and tackle complex tasks that surpass the capabilities of individual agents. A key challenge in this area is ensuring network security and protecting individual privacy, especially in the presence of external eavesdroppers and untrusted internal neighbors. To tackle this challenge, we propose a model-free secure distributed adaptive control framework for nonlinear leader-follower MASs. This framework incorporates a confidential communication protocol that leverages homomorphic encryption to protect sensitive information. The proposed framework has undergone rigorous stability analysis and been validated through numerical simulations, demonstrating its feasibility and effectiveness in achieving secure consensus control of nonlinear MASs.","1558-3783","","10.1109/TASE.2024.3493136","Research Grants Council of Hong Kong(grant numbers:CityU 21208921); Chow Sang Sang Group Research Fund Sponsored by Chow Sang Sang Holdings International Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10750422","Multi-agent systems;secure control;model-free adaptive control;privacy preservation;homomorphic encryption","Consensus control;Quantization (signal);Adaptation models;Homomorphic encryption;Control systems;Adaptive control;Nonlinear dynamical systems;Multi-agent systems;Data privacy;Communication networks","","1","","34","IEEE","12 Nov 2024","","","IEEE","IEEE Journals"
"A Generative AI Framework for Cloud Security: Automated Attack Simulation and Threat Detection","M. M. Helal; M. Abu-Elkheir; M. Mashaly","Computer Science and Engineering The German University in Cairo, Cairo, Egypt; Computer Science and Engineering The German University in Cairo, Cairo, Egypt; Computer Science and Engineering The German University in Cairo, Cairo, Egypt",2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT),"22 Jul 2025","2025","","","1","8","In the rapidly evolving domain of cloud computing and cybersecurity, staying ahead of threats requires proactive and adaptive solutions. This paper introduces an end-to-end framework designed to enhance cloud security through automated attack generation, execution, and defense enhancement. The framework consists of four main components: the Database, the Generator, the Executor, and the Defender. The database comprises extensive data sources, including attack logs and security reports. The Generator utilizes the environment and system specifications to generate both traditional and zero-day attack vectors. The Executor classifies attacks, assigns them to specialized workers for execution, and consolidates the results into a comprehensive report. The Defender then analyzes this report to iteratively enhance the Intrusion Detection System (IDS). This iterative process enhances the system’s resilience and robustness to future, unknown threats. The framework is adaptable, allowing customization to meet the security needs of various organizations making it well-suited for applications such as enterprise cloud security, cybersecurity education, and beyond. Future work will focus on building a simplified version of the system, which will later be expanded into a scalable architecture capable of addressing more intricate attack scenarios.","","979-8-3315-6662-3","10.1109/AIIT63112.2025.11082823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082823","Cloud Security;Generative AI;Cloud Computing;Cyber Security;Retrieval Augmented Generation;Threat Intelligence","Technological innovation;Generative AI;Databases;Cloud computing security;Architecture;Buildings;Computer architecture;Generators;Vectors;Threat assessment","","","","32","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review","D. G. Paul; H. Zhu; I. Bayley","School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK; School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK; School of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, UK",2024 IEEE International Conference on Artificial Intelligence Testing (AITest),"25 Sep 2024","2024","","","87","94","With the rapid development of Large Language Models (LLMs), a large number of machine learning models have been developed to assist programming tasks including the generation of program code from natural language input. However, how to evaluate such LLMs for this task is still an open problem despite of the great amount of research efforts that have been made and reported to evaluate and compare them. This paper provides a critical review of the existing work on the testing and evaluation of these tools with a focus on two key aspects: the benchmarks and the metrics used in the evaluations. Based on the review, further research directions are discussed.","2835-3560","979-8-3503-6505-4","10.1109/AITest62860.2024.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685214","Machine learning;Large language models;Code generation;Performance evaluation;Benchmarks;Metrics","Measurement;Codes;Automation;Reviews;Large language models;Natural languages;Machine learning","","4","","45","IEEE","25 Sep 2024","","","IEEE","IEEE Conferences"
"Unleashing the Second Brain: Enhancing Large Language Models through Chain of Thought with Human Feedback","J. Yu; M. Luo; H. Zhou; Z. Lan","Zhejiang University, China; Westlake Xinchen (Hangzhou) Technology Co., Ltd., China; School of Media and Desgin, Hangzhou Dianzi University, China; Zhejiang University, China",2023 16th International Symposium on Computational Intelligence and Design (ISCID),"15 Apr 2024","2023","","","90","95","The expansion of large language models has led to improved performance and efficiency, with prompt engineering emerging as a key strategy for various LLM tasks. However, while these methods have proven beneficial, they are not without their constraints, particularly when it comes to sustaining a comprehensive, logical flow of ideas throughout the entire reasoning process. Despite the effectiveness of the Chain-of-Thought (CoT) and Tree-of-Thought (ToT) methods, they have limitations due to their end-to-end reasoning process. To address this, we introduce the chain-of-thought with human feedback. This new method incorporates human feedback into the model’s reasoning process, allowing for real-time adjustments and optimization. This approach fosters a more interactive and dynamic model operation, enabling the model to learn from human intuition and expertise, and improve its reasoning process over time. We have validated the effectiveness of our method through experiments in GSM8K and MMLU. Our main contributions include proposing the first human feedback to the chain of thought and the development of an intuitive interface for individuals to utilize large-scale models for problem-solving.","2473-3547","979-8-3503-1745-9","10.1109/ISCID59865.2023.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494308","Chain-of-Thought;Prompting;Large Language Model;Human-AI-interaction","Computational modeling;Benchmark testing;Brain modeling;Cognition;Natural language processing;Real-time systems;Problem-solving","","2","","32","IEEE","15 Apr 2024","","","IEEE","IEEE Conferences"
"A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models","R. E. Zezario; S. M. Siniscalchi; H. -M. Wang; Y. Tsao","Academia Sinica, Taipei, Taiwan; University of Palermo, Palermo, Italy; Academia Sinica, Taipei, Taiwan; Academia Sinica, Taipei, Taiwan","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","This work investigates two strategies for zero-shot non-intrusive speech assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses Whisper as an audio-to-text module and evaluates the text’s naturalness via targeted prompt engineering. We evaluate the assessment metrics predicted by GPT-4o and GPT-Whisper, examining their correlation with human-based quality and intelligibility assessments and the character error rate (CER) of automatic speech recognition. Experimental results show that GPT-4o alone is less effective for audio analysis, while GPT-Whisper achieves higher prediction accuracy, has moderate correlation with speech quality and intelligibility, and has higher correlation with CER. Compared to SpeechLMScore and DNSMOS, GPT-Whisper excels in intelligibility metrics, but performs slightly worse than SpeechLMScore in quality estimation. Furthermore, GPT-Whisper outperforms supervised non-intrusive models MOS-SSL and MTI-Net in Spearman’s rank correlation for Whisper’s CER. These findings validate GPT-Whisper’s potential for zero-shot speech assessment without requiring additional training data.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889809","speech assessment;zero-shot;non-intrusive;whisper;ChatGPT;large language model","Measurement;Correlation;Accuracy;Large language models;Estimation;Training data;Signal processing;Quality assessment;Prompt engineering;Speech processing","","","","35","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Semantic Refined Prompting based Automated Essay Scoring System","V. Senthilnathan; S. V. M; A. R","Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India; Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India; Computer Science and Engineering, St. Joseph’s College of Engineering, Chennai, India",2025 International Conference on Electronics and Renewable Systems (ICEARS),"2 Apr 2025","2025","","","1344","1348","The usage of Large Language Models (LLMs) has made significant contributions in various areas including essay scoring. However, its performance is not comparable to human grades due to the reasons that the auto-generated scores are not done based on semantic understanding. Existing research on this topic focuses on grading with predefined rubrics. Since grading can’t be relied only on the predefined rubrics in this paper, we propose an LLM-based scoring mechanism that considers the predefined and semantic refined rubrics. Additionally, the scores obtained are evaluated for consistency and fairness. This system is evaluated using the ASAP dataset and the outcomes demonstrate the effectiveness of the approach in terms of RMSE and MAP.","","979-8-3315-0967-5","10.1109/ICEARS64219.2025.10940227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940227","Automated Essay Scoring (AES);prompt engineering;Large Language Models (LLM)","Renewable energy sources;Accuracy;Large language models;Semantics;Prompt engineering;Usability;Faces","","","","28","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"Deploying a Local Language Learning Assistant Using a Small Large Language Model","K. A. Aksyonov; L. Sun; I. A. Kalinin; O. P. Aksyonova; E. K. Aksyonova","Engineering School of Information Technologies, Telecommunications, and Control Systems, Ural Federal University Named After the First President of Russia B.N.Yeltsin, Yekaterinburg, Russia; Engineering School of Information Technologies, Telecommunications, and Control Systems, Ural Federal University Named After the First President of Russia B.N.Yeltsin, Yekaterinburg, Russia; Engineering School of Information Technologies, Telecommunications, and Control Systems, Ural Federal University Named After the First President of Russia B.N.Yeltsin LLC ""Uralinnovation"", Yekaterinburg, Russia; Engineering School of Information Technologies, Telecommunications, and Control Systems, Ural Federal University Named After the First President of Russia B.N.Yeltsin, Yekaterinburg, Russia; Engineering School of Information Technologies, Telecommunications, and Control Systems, Ural Federal University Named After the First President of Russia B.N.Yeltsin, Yekaterinburg, Russia","2025 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)","2 Jul 2025","2025","","","372","375","This paper investigates the feasibility of deploying a localized language learning assistant using small-scale large language models (LLMs). The proposed tool aims to enhance language learning efficiency by automating the organization and summarization of teaching notes. We leverage the lightweight and locally deployable nature of models such as Qwen, Ollama, and DeepSeek, combined with Retrieval-Augmented Generation (RAG) and Prompt technologies, to improve performance without additional fine-tuning. The study evaluates the models' inference speed and accuracy on consumer-grade hardware, focusing on tasks such as pronunciation error correction, advanced vocabulary extraction, and sentence structure improvement. In this work, five models were tested that could be deployed on consumer-grade computing devices. Experimental results demonstrate that the DeepSeek R1-8B model achieves high accuracy in note summarization, significantly reducing teachers' workload. However, limitations in semantic understanding and parameter size highlight areas for future optimization. This work provides a bit of data-referenced advice for the development of efficient, localised AI-driven language learning tools.","2769-3635","979-8-3503-9270-8","10.1109/USBEREIT65494.2025.11054131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11054131","Large Language Models;DeepSeek;Qwen;Ollama;RAG;Prompt Engineering;Local Deployment","Performance evaluation;Vocabulary;Accuracy;Large language models;Computational modeling;Semantics;Retrieval augmented generation;Organizations;Prompt engineering;Optimization","","","","11","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Assessment of Conflict Structure Recognition and Bias Impact in Japanese LLMs","K. Inoshita","Faculty of Data Science, Shiga University, Hikone, Japan",2024 5th Technology Innovation Management and Engineering Science International Conference (TIMES-iCON),"15 Aug 2024","2024","","","1","5","In recent years, with the evolution and proliferation of large language models (LLMs), their application in various fields, including security, has been expanding. LLMs, which learn from vast amounts of text data and possess the capability to process diverse information, may incorporate biases during the learning process. This necessitates cautious consideration when using LLMs in the realm of security. This study examines the perceptions and biases that Japanese LLMs hold towards global conflict structures, including the warring nations of Ukraine and Russia. We conducted experiments based on a novel method and evaluation metric utilizing sentiment analysis on tweet data. The results revealed that while LLMs have some understanding of conflict structures, they tend to exhibit a positive bias towards specific countries in relationships where the conflict structure is unclear. Furthermore, continual pre-training with Japanese text data showed a tendency to intensify certain biases. These findings highlight the risks associated with using LLMs in the security sector and propose the creation of bias-aware datasets and suggestions for enhancing robustness to ensure fairness towards nations.","","979-8-3503-6260-2","10.1109/TIMES-iCON61890.2024.10630720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630720","Japanese LLM;Conflict Structure;Sentiment Analysis;Bias Assessment;Security Applications","Training;Measurement;Sentiment analysis;Large language models;Innovation management;Robustness;Data models","","2","","13","IEEE","15 Aug 2024","","","IEEE","IEEE Conferences"
"Resilient Constrained Optimization in Multi-Agent Systems With Improved Guarantee on Approximation Bounds","M. Kaheni; E. Usai; M. Franceschelli","Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy",IEEE Control Systems Letters,"16 May 2022","2022","6","","2659","2664","This letter considers resilient decentralized constrained optimization in multi-agent systems where some agents due to cyberattacks become adversaries. We show that the proposed method is resilient despite the persistent influence of up to F anonymous adversaries in the complete graphs. Our approach provides a better approximation of the optimal solution than the current literature. If the agents’ objectives are 2F redundant, then the algorithm converges to the optimal solution. In addition to current literature, we consider a constrained optimization problem. Finally, we present numerical simulations to corroborate the theoretical analysis.","2475-1456","","10.1109/LCSYS.2022.3173495","Fondazione di Sardegna through “Formal Methods and Technologies for the Future of Energy Systems”(grant numbers:F72F20000350007); Project “ARGOSAT–Microsatellite Cluster for the Observation of Optical Transients in Astronomy,”(grant numbers:CUP: F74I19001070007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770946","Cyber-physical security;distributed optimization;multi-agent systems;resilient optimization","Optimization;Multi-agent systems;Convergence;Approximation algorithms;Optical filters;Linear programming;Cost function","","7","","17","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"Generative AI for Cloud Solutions: Architect modern AI LLMs in secure, scalable, and ethical cloud environments","P. Singh; A. Karuparti; J. Maeda",NA; NA; NA,"Generative AI for Cloud Solutions: Architect modern AI LLMs in secure, scalable, and ethical cloud environments","","2024","","","","","Explore Generative AI, the engine behind ChatGPT, and delve into topics like LLM-infused frameworks, autonomous agents, and responsible innovation, to gain valuable insights into the future of AIKey FeaturesGain foundational GenAI knowledge and understand how to scale GenAI/ChatGPT in the cloudUnderstand advanced techniques for customizing LLMs for organizations via fine-tuning, prompt engineering, and responsible AIPeek into the future to explore emerging trends like multimodal AI and autonomous agentsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative artificial intelligence technologies and services, including ChatGPT, are transforming our work, life, and communication landscapes. To thrive in this new era, harnessing the full potential of these technologies is crucial. Generative AI for Cloud Solutions is a comprehensive guide to understanding and using Generative AI within cloud platforms. This book covers the basics of cloud computing and Generative AI/ChatGPT, addressing scaling strategies and security concerns. With its help, you’ll be able to apply responsible AI practices and other methods such as fine-tuning, RAG, autonomous agents, LLMOps, and Assistants APIs. As you progress, you’ll learn how to design and implement secure and scalable ChatGPT solutions on the cloud, while also gaining insights into the foundations of building conversational AI, such as chatbots. This process will help you customize your AI applications to suit your specific requirements. By the end of this book, you’ll have gained a solid understanding of the capabilities of Generative AI and cloud computing, empowering you to develop efficient and ethical AI solutions for a variety of applications and services.What you will learnGet started with the essentials of generative AI, LLMs, and ChatGPT, and understand how they function togetherUnderstand how we started applying NLP to concepts like transformersGrasp the process of fine-tuning and developing apps based on RAGExplore effective prompt engineering strategiesAcquire insights into the app development frameworks and lifecycles of LLMs, including important aspects of LLMOps, autonomous agents, and Assistants APIsDiscover how to scale and secure GenAI systems, while understanding the principles of responsible AIWho this book is forThis artificial intelligence book is for aspiring cloud architects, data analysts, cloud developers, data scientists, AI researchers, technical business leaders, and technology evangelists looking to understanding the interplay between GenAI and cloud computing. Some chapters provide a broad overview of GenAI, which are suitable for readers with basic to no prior AI experience, aspiring to harness AI's potential. Other chapters delve into technical concepts that require intermediate data and AI skills. A basic understanding of a cloud ecosystem is required to get the most out of this book.","","9781835080160","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10522549.pdf&bkn=10522548&pdfType=book","","","","","","","","8 May 2024","","","Packt Publishing","Packt Publishing eBooks"
"Generative AI in Cyber Security of Cyber Physical Systems: Benefits and Threats","H. S. Mavikumbure; V. Cobilean; C. S. Wickramasinghe; D. Drake; M. Manic","Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA",2024 16th International Conference on Human System Interaction (HSI),"9 Aug 2024","2024","","","1","8","The advancements in Cyber-Physical Systems (CPSs) have also increased their vulnerability to various cyber-attacks. Therefore, it is crucial to develop strong cybersecurity mechanisms, shielding these critical systems from potential cyber intrusions. Among many AI technologies, Generative AI (GenAI) has gained significant attention in the last couple of years. This is due to its distinctive capability to autonomously generate original and diverse content across different domains, offering potential for novel advancements in several applications. Given the massive success of GenAI, it is essential to explore its role in ensuring the cybersecurity of CPSs. Therefore, in this paper, we present: 1) the evolution and current state of GenAI, 2) benefits of GenAI on the cybersecurity of CPS, 3) threats of GenAI on the cybersecurity of CPS, 4) defense strategies against threats and 5) future research opportunities. We hope this systematic survey will help the community prioritize research efforts to address pressing issues in cybersecurity of CPSs.","2158-2254","979-8-3503-6291-6","10.1109/HSI61632.2024.10613562","Commonwealth Cyber Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10613562","Anomaly Detection;Cyber Security;GenAI;Cyber-Physical Systems;Large Language Models;GPT;Chat-GPT","Surveys;Structured Query Language;Privacy;Codes;Systematics;Generative AI;Pressing","","15","","57","IEEE","9 Aug 2024","","","IEEE","IEEE Conferences"
"Generative Artificial Intelligence for Industry: Opportunities, Challenges, and Impact","B. K. Saha","Grid Automation R&D, Hitachi Energy, Bangalore, India",2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),"20 Mar 2024","2024","","","081","086","The recent advances in Generative Artificial In-telligence (GenAI) and Large Language Models (LLMs) have generated significant interest across the world. For a successful adoption of GenAI and LLMs by industry, it is critical to identify their potential benefits, impact, and challenges. Accordingly, in this work, we investigate a few use cases of LLMs, which are relevant across most industry segments. In order to empirically evaluate the impact of GenAI on the code generation use case, we build CodePrompt, a handcrafted dataset of sequential prompts used by a human user to generate code. We approximate efficiency by considering the ratio of the number of tokens of code generated by an LLM to the number of tokens in the user's prompt. Experimental results reveal that a sequential trial of prompts for code generation may lead to an efficiency factor of about 6.33, on average, which means that a user's effort is reduced to about one-sixth.","2831-6983","979-8-3503-4434-9","10.1109/ICAIIC60209.2024.10463245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463245","Artificial Intelligence;Generative AI;Large Language Models;Code Generation;Tokens;Data Pipeline;Bid Engineering","Industries;Codes;Generative AI","","2","","15","IEEE","20 Mar 2024","","","IEEE","IEEE Conferences"
"23 Security Risks in Black-Box Large Language Model Foundation Models","G. McGraw; R. Bonett; H. Figueroa; K. McMahon","Berryville Institute of Machine Learning, Berryville, VA, USA; Berryville Institute of Machine Learning, Berryville, VA, USA; Berryville Institute of Machine Learning, Berryville, VA, USA; Berryville Institute of Machine Learning, Berryville, VA, USA",Computer,"2 Apr 2024","2024","57","4","160","164","We applied our previous generic machine learning risk analysis to the more specific case of large language models (LLMs), identifying an architectural black box with 23 associated risks—a reasonable starting point for the regulation of LLMs.","1558-0814","","10.1109/MC.2024.3363250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488883","","Analytical models;Computational modeling;Closed box;Machine learning;Regulation;Risk analysis;Computer security","","1","","4","IEEE","2 Apr 2024","","","IEEE","IEEE Magazines"
"HPC-Coder-v2: Studying Code LLMs Across Low-Resource Parallel Languages","A. Chaturvedi; D. Nichols; S. Singh; A. Bhatele","Department of Computer Science, University of Maryland, College Park, MD, USA; Department of Computer Science, University of Maryland, College Park, MD, USA; Department of Computer Science, University of Maryland, College Park, MD, USA; Department of Computer Science, University of Maryland, College Park, MD, USA",ISC High Performance 2025 Research Paper Proceedings (40th International Conference),"30 May 2025","2025","","","1","14","Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date.","","978-3-9826336-1-9","","National Science Foundation(grant numbers:2047120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11017585","Large Language Models;Code Generation;HPC","Parallel languages;Codes;Large language models;Computational modeling;Memory management;Training data;Programming;Data models;Synthetic data;Software development management","","","","36","CCBY","30 May 2025","","","Prometeus GmbH","Prometeus GmbH Conferences"
"Foundation Models for Big Data: Enabling AI-Powered Data Insights to Accelerate Business Outcomes and Achieve Sustainable Success","K. Godavarthi; J. Hallur; S. Das","Data Architect, VA; Data Architect, Richmond, VA; Data Architect, Chicago, IL",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","4727","4736","Foundation models are giant AI models able to solve numerous problems across many domains. These models are increasingly important drivers of innovation and business outcomes; however, their performance is inextricably linked to adequate big data management practices across the entire lifecycle at pre-training, with an emphasis on acquisition, preparation, and storage. The paper discusses the main challenges and best practices of handling big data for foundation models. This is done by emphasizing the architectures allowing fine-tuning and prompt-tuning, where scalable and efficient data pipelines find special significance.It also stresses the fact that prompt engineering, like creating and maintaining prompt libraries, has remained one of the operational areas where model accuracy improvements and adaptability are realized. This forms part of the discussion on operational challenges in resource management, data security, and scaling, with possible suggestions to ensure that the technology supports dependable performance in multi-party environments. A number of these considerations are addressed to provide insights into improving the generalization, robustness, and usability of foundation models.It finally gives strategic directions based on the way forward concerning BDA in the foundation model ecosystem by emphasizing harmonization between data management practices and AI advancements. These will be useful in commonly guiding organizations toward sustainable success by capturing the opportunities underlying foundation models, solve complex challenges, and creating a competitive advantage in a data-driven world.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825551","Foundation Models;Big Data Management;Pre-training;Prompt Engineering and Scalability;Artificial Intelligence;Data Processing","Ethics;Adaptation models;Technological innovation;Foundation models;Biological system modeling;Organizations;Big Data;Data models;Best practices;Tuning","","","","15","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"A Study on the Representativeness Heuristics Problem in Large Language Models","J. Ryu; J. Kim; J. Kim","Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea",IEEE Access,"17 Oct 2024","2024","12","","147958","147966","Large language models (LLMs) exhibit remarkable proficiency in text generation. However, their logical reasoning capabilities require enhancement. Major strides have been achieved in reasoning techniques for LLM, such as Few-shot, Zero-shot, and Chain-of-Thought (CoT). Nevertheless, these techniques have shortcomings, particularly in addressing the representativeness heuristic (RH) phenomenon. RH is a cognitive bias that occurs when a person judges the probability of an event or the likelihood that an object belongs to a particular category based on how well it matches the prototype or stereotype of that category. In this study, we investigated the pervasive issue of RH errors in LLMs. This research surpasses the constraints of previous studies by analyzing various RH scenarios that they did not cover and by directly constructing and testing the corresponding datasets. Moreover, a novel prompt called zero-shot-RH is proposed to augment the reasoning ability of LLMs, mitigate RH errors, and thus bolster logical reasoning. This approach seeks to enable LLMs to comprehend the given information better and reduce the biases stemming from RH errors. The prompt zero-shot-RH achieved an average accuracy higher than zero-shot-CoT by 0.145 and 0.277 in the tasks of correct reasoning and correct reasonings by sex, respectively, without relying on RH. The outcomes of this research endeavor are a deeper understanding of RH errors in LLMs and novel strategies to mitigate these biases, thereby advancing the domain of logical reasoning within LLMs.","2169-3536","","10.1109/ACCESS.2024.3474677","Institute of Information and communications Technology Planning and Evaluation (IITP); Korea government (MSIT)(grant numbers:2022-0-00184); Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics) and partly supported by Institute of Information and communications Technology Planning and Evaluation (IITP); Korea government (MSIT)(grant numbers:2021-0-01341); Artificial Intelligence Graduate School Program, Chung-Ang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10706240","Prompt engineering;zero-shot learning;large language model;representativeness heuristic","Cognition;Problem-solving;Gender issues;Large language models;Accuracy;Standards;Planning;Usability;Transformers;Prompt engineering;Zero-shot learning;Heuristic algorithms;Text processing","","1","","20","CCBYNCND","7 Oct 2024","","","IEEE","IEEE Journals"
"Advancing Agricultural Decision-Making with a Multi-Dimensional Evaluation of Large Language Models for Sustainable Pest Management","S. Yang; Z. Yuan; S. Li; R. Peng; K. Liu; P. Yang","Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom",2024 IEEE 22nd International Conference on Industrial Informatics (INDIN),"12 Dec 2024","2024","","","1","6","In the rapidly evolving field of artificial intelligence, large language models (LLMs) have attracted much attention from researchers in various fields due to their unexpected text generation and comprehension capabilities. However, the applications of LLMs for sustainable pest management are under-explored due to the heavy reliance on specialized expert knowledge. In addition, evaluating the quality of LLMs' content is another technological challenge for applying LLMs in sustainable pest management. Therefore, we propose an instruction-based prompting method that integrates pest expert knowledge into the prompt, equipping LLMs with the necessary context to generate more accurate and relevant pest management advice. Furthermore, we propose an LLM-based evaluation framework to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehension, and Exhaustion. Additionally, we integrate an Expert System based on crop threshold data as a baseline to obtain scores for Accuracy on whether pests found in crop fields should take management action. Each model's score is weighted by percentage to get a final score. The results show that GPT-3.5 and GPT-4 outperform the FLAN models in most evaluation dimensions. Furthermore, while using instruction-based prompting containing domain-specific knowledge outperforms other prompting methods with an accuracy of 72%, ongoing refinements and assessments of end-user satisfaction are essential to enhance the LLMs' effectiveness and practical helpfulness in providing pest management advice.","2378-363X","979-8-3315-2747-1","10.1109/INDIN58382.2024.10774285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774285","Large Language Model;Prompt Engineering;Sustainable Pest Management;Model Evaluation","Accuracy;Large language models;Decision making;Crops;Coherence;Informatics;Expert systems","","","","26","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"Source Code Summarization in the Era of Large Language Models","W. Sun; Y. Miao; Y. Li; H. Zhang; C. Fang; Y. Liu; G. Deng; Y. Liu; Z. Chen","College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Engineering, University of New South Wales, Sidney, Australia; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1882","1894","To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of coderelated tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLMbased code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types (e.g., procedural and object-oriented programming languages). Finally, we unexpectedly find that CodeLlamaInstruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code design rationale and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029737","large language model;source code summarization;prompt engineering","Computer languages;Codes;Systematics;Source coding;Large language models;Object oriented modeling;Software;Prompt engineering;Object oriented programming;Software engineering","","","","71","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Comparative Advances in Financial Sentiment Analysis:A Review of BERT,FinBert, and Large Language Models","M. B. Mahendran; A. K. Gokul; P. Lakshmi; S. Pavithra","Dept. of Computer Science and Engineering, Chennai Institute of Technology, Chennai, Tamil Nadu, India; Dept. of Computer Science and Engineering, Chennai Institute of Technology, Chennai, Tamil Nadu, India; Dept. of Computer Science and Engineering, Chennai Institute of Technology, Chennai, Tamil Nadu, India; Dept. of Computer Science and Engineering, Chennai Institute of Technology, Chennai, Tamil Nadu, India",2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),"13 Mar 2025","2025","","","39","45","Capturing market sentiments and supporting well-informed financial decision-making depend on the developing field of Financial Sentiment Analysis (FSA). Natural language processing (NLP) has made significant strides in comprehending and categorizing sentiment in intricate financial texts, especially with the use of Large Language Models (LLMs). The application of various LLMs, such as Bidirectional Encoder Representations form Transformers(BERT) and Financial BERT (FinBERT), as well as distilled models, such as DistilBERT and DistilRoBERTa, on a variety of financial datasets, including Financial Phrase Bank and LexisNexis news articles, was the main focus of this review article. highlighting different approaches such as model fine-tuning, zero-shot and few-shot learning, and prompt engineering. The study focuses on practical model predictions through a case study of sentimental analysis of cryptocurrencies. While FinBERT, a financial variant of BERT, exhibits high accuracy and robustness, other LLMs exhibit varying degrees of success based on the dataset and domain requirements. The analysis concentrates on the difficulties, compromises, and potential paths for improving LLMs for financial sentiment analysis.","","979-8-3315-2754-9","10.1109/IDCIOT64235.2025.10914764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10914764","Fin Bert;LLMs;BERT;Financial Sentimental Analysis;FSA;Financial Datasets;Data preprocessing","Sentiment analysis;Technological innovation;Reviews;Large language models;Decision making;Bidirectional control;Market research;Encoding;Risk management;Indexing","","","","32","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"$\mathrm{H} \infty$ Consensus PID Control of Multi-Agent Systems with Stochastic Communication Protocols Under Network Attacks","S. Jinbo; L. Junning; Z. Shiqiang; C. Mengdi; X. Chao","Artificial Intelligence Energy Research Institute, Northeast Petroleum University, School of Electrical & Information Engineering, Northeast Petroleum University, Daqing, China; Artificial Intelligence Energy Research Institute, Northeast Petroleum University, School of Electrical & Information Engineering, Northeast Petroleum University, Daqing, China; Artificial Intelligence Energy Research Institute, Northeast Petroleum University, School of Electrical & Information Engineering, Northeast Petroleum University, Daqing, China; Artificial Intelligence Energy Research Institute, Northeast Petroleum University, Daqing, China; Artificial Intelligence Energy Research Institute, Northeast Petroleum University, Daqing, China",2025 37th Chinese Control and Decision Conference (CCDC),"5 Aug 2025","2025","","","5898","5905","This paper is concerned with the observer based $H_{\infty}$ consensus control problem for a class of discrete-time multi-agent systems(MAS). The stochastic communication protocol (SCP) obeying Markovian chain is adopted to schedule the information transmission among agents, by which only one neighbor agent is stochastically selected and the information transmitted to the agent at each transmission instant, so that to avoid the problem of channel congestion and enhance communication security. To comprehensively depict the communication scheduling of the MAS, multiple Markov chains are mapped. Furthermore, the cyber-attacks, which include both denial-of-service and deception attacks, are allowed to be randomly occurring as regulated by two sequences of Bernoulli distributed random variables with certain probabilities. Then, a novel observer-based proportional-integralderivative (PID) controller is proposed such that the closedloop system achieves the desired $H_{\infty}$ consensus performance. Sufficient conditions are derived under which the mean-square exponential stability is guaranteed. The PID controller gains are obtained by solving a set of linear matrix inequalities. Finally, the validity of the developed design approach is verified via an illustrative example.","1948-9447","979-8-3315-1056-5","10.1109/CCDC65474.2025.11090460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11090460","$-H_{\infty}$ consensus control;multi-agent systems;cyber attack;mean-square exponential stability;stochastic communication protocol(SCP)","Sufficient conditions;Schedules;Protocols;Stability;Consensus control;Control theory;Security;Cyberattack;Time-varying systems;Multi-agent systems","","","","23","IEEE","5 Aug 2025","","","IEEE","IEEE Conferences"
"A Reference Architecture for Deploying Large Language Model Applications in Industrial Environments","F. Mahr; G. Angeli; T. Sindel; K. Schmidt; J. Franke","Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Institute for Factory Automation and Production Systems (FAPS), Nuremberg, Germany; Siemens AG, Amberg, Germany; Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Institute for Factory Automation and Production Systems (FAPS), Nuremberg, Germany; Siemens AG, Amberg, Germany; Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Institute for Factory Automation and Production Systems (FAPS), Nuremberg, Germany",2024 IEEE 30th International Symposium for Design and Technology in Electronic Packaging (SIITME),"30 Dec 2024","2024","","","19","23","Integrating Large Language Models (LLMs) into industrial environments presents numerous challenges. Firstly, understanding the term LLM, its required components for implementation, and the various types of LLMs demands research effort. Furthermore, the processing of textual data involves enormous data volumes, and executing large language models requires substantial computational power. A significant challenge in model deployment on the shopfloor is the stringent data security measures, which often prohibit direct internet access to the cloud.To address these challenges, a reference architecture was developed to give an overview of the components for integrating LLM applications in industry. This architecture covers all essential steps of LLM applications, including data preparation, model training, model evaluation, model deployment, and prompt engineering. For model deployment, two possible locations were distinguished: the digital workplace, such as the office, and the shopfloor. In addition to model deployment, data preparation and model evaluation are emphasized as critical elements for the success of Large Language Model Operations (LLMOps) applications.This reference architecture enables project teams to better understand the elements involved in the LLM integration process. Furthermore, the universal approach helps in standardizing the associated processes, facilitating a more effective acceptance of LLMs in industrial environments.","2642-7036","979-8-3315-3951-1","10.1109/SIITME63973.2024.10814877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814877","electronics production;machine learning;large language models;large language model operations","Training;Industries;Large language models;Data security;Evaluation models;Employment;Computer architecture;Data models;Prompt engineering;Electronics packaging","","","","11","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Efficient Detection of Toxic Prompts in Large Language Models","Y. Liu; J. Yu; H. Sun; L. Shi; G. Deng; Y. Chen; Y. Liu","Nanyang Technological University, Singapore; ShanghaiTech University, Shanghai, China; ShanghaiTech University, Shanghai, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; ShanghaiTech University, Shanghai, China; Nanyang Technological University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","455","467","Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39% and a low false positive rate of 2.00%, outperforming state-of-the-art methods. Additionally, ToxicDetector’s processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765053","llm;large language model;jailbreak;detection;ai security;llm security;toxic prompt","Accuracy;Scalability;Large language models;Chatbots;Feature extraction;Vectors;Real-time systems;Safety;Software engineering;Glass box","","","","54","","29 Nov 2024","","","IEEE","IEEE Conferences"
"ELICITATION: A Satellite Constellation Simulator Using Multi-Agent Systems, Blockchain and the IoMT","J. E. Silva Filho; N. d. A. Moreira; M. Bourgais; C. Zanni-Merk; L. Vercouter; J. A. Nunes Da Silveira; W. Abrahao Dos Santos","Dept. of Teleinformatics Engineering, Federal University of Ceara, Fortaleza, Brazil; Dept. of Teleinformatics Engineering, Federal University of Ceara, Fortaleza, Brazil; INSA Rouen Normandie, Normandie Univ, LITIS UR 4108, Rouen, France; INSA Rouen Normandie, Normandie Univ, LITIS UR 4108, Rouen, France; INSA Rouen Normandie, Normandie Univ, LITIS UR 4108, Rouen, France; Dept. of Teleinformatics Engineering, Federal University of Ceara, Fortaleza, Brazil; Small Satellites Division, National Institute for Space Research, São José dos Camps, Brazil",2024 IEEE Symposium on Computers and Communications (ISCC),"31 Oct 2024","2024","","","1","6","With the continuous increase in the number of satellites orbiting Earth, the efficient and coordinated management of these assets becomes a critical concern. This paper introduces the innovative project Elicitation, a simulator for satellite constellations based on Multi-Agent Systems (MAS), Blockchain, and the Internet of Mobile Things (IoMT). The current scenario highlights growing challenges in the supervision and coordination of satellites, emphasizing the need for automated solutions. Elicitation addresses this complexity by integrating MAS to simulate dynamic interactions between satellites in Earth climate monitoring tasks, taking advantage of blockchain technology to guarantee the security, transparency and integrity of the data generated. One notable aspect that contributes to Elicitation’s appeal is its ability to take advantage of authentic satellite positioning data and real information relating to natural events. Consequently, the system operates autonomously to identify the ideal satellite capable of capturing data related to natural events at any given time. The average decision time per agent was 0.19 seconds for a collective set of 11 agents, corresponding to 11 satellites in a 120-day dataset with 43 target events.","2642-7389","979-8-3503-5423-2","10.1109/ISCC61673.2024.10733588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10733588","Multi-agent Systems;Blockchain;Permissioned Network;Internet of Mobile Things and Nanosatellites","Earth;Satellite constellations;Satellites;Predictive models;Blockchains;Trajectory;Security;Monitoring;Multi-agent systems;Meteorology","","","","10","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Generative AI for Cybersecurity: LLM-Driven Attack Dataset Augmentation in Web API Detection","I. W. A. J. Pawana; F. Purnama; Linawati; Y. Kim; I. You","Department of Financial Information Security, Kookmin University, Seoul, South Korea; Department of Information Technology, Udayana University, Bali, Indonesia; Department of Electrical Engineering, Udayana University, Bali, Indonesia; Department of Financial Information Security, Kookmin University, Seoul, South Korea; Department of Financial Information Security, Kookmin University, Seoul, South Korea",2025 1st International Conference on Consumer Technology (ICCT-Pacific),"30 May 2025","2025","","","1","4","As Web APIs become integral to modern digital ecosystems, their increasing complexity and widespread adoption have made them appealing targets for cyberattacks. Effective detection and prevention mechanisms are crucial, and machine learning (ML) techniques provide promising solutions for identifying anomalous API traffic and addressing emerging threats. However, the effectiveness of ML-based security models is limited by the scarcity of realistic, up-to-date datasets and the rapid obsolescence of static training data in the face of evolving attack methodologies. To address these challenges, we present an approach that employs Large Language Models (LLMs) to generate synthetic Web API attack datasets. By leveraging LLMs, researchers can access abundant, customizable, and domain-specific synthetic data that closely simulate contemporary traffic patterns. This eliminates privacy and security concerns associated with real-world datasets and ensures a continuously adaptable data source for training and refining ML-based Intrusion Detection Systems. Ultimately, this strategy enables the development of more resilient ML models capable of effectively countering the dynamic and sophisticated nature of modern Web API threats.","","979-8-3315-0412-0","10.1109/ICCT-Pacific63901.2025.11012893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11012893","LLM;Generative AI;Data Augmentation;Web API;Intrusion Detection System;Privacy","Training;Data privacy;Generative AI;Biological system modeling;Training data;Intrusion detection;Machine learning;Aging;Data models;Synthetic data","","","","16","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"ThreatFinderAI: Automated Threat Modeling Applied to LLM System Integration","J. Von der Assen; A. Huertas; J. Sharif; C. Feng; G. Bovet; B. Stiller","Department of Informatics, Communication Systems Group, University of Zurich UZH, Zürich, Switzerland; Department of Informatics, Communication Systems Group, University of Zurich UZH, Zürich, Switzerland; Department of Informatics, Communication Systems Group, University of Zurich UZH, Zürich, Switzerland; Department of Informatics, Communication Systems Group, University of Zurich UZH, Zürich, Switzerland; Cyber-Defence Campus, Armasuisse Science & Technology, Thun, Switzerland; Department of Informatics, Communication Systems Group, University of Zurich UZH, Zürich, Switzerland",2024 20th International Conference on Network and Service Management (CNSM),"31 Dec 2024","2024","","","1","3","Artificial Intelligence (AI) is a rapidly integrated technology, significantly contributing to advancements like 6G. However, its swift adoption raises considerable security concerns. Large Language Models (LLMs) pose risks such as spear phishing, code injections, and remote code execution. Conventional threat modeling, used in secure software development, faces challenges when applied to AI systems, as existing methodologies are designed for traditional software. Furthermore, AI-specific threat modeling research is sparse and lacks approaches providing practical support or automation. Thus, this demo paper presents ThreatFinderAI, an asset-centric threat modeling and risk assessment framework. ThreatFinderAI fulfills seven steps aligned with AI system design and transforms AI threat and control knowledge bases into a queryable knowledge graph for automated asset identification and threat elicitation. It also proposes business impact analysis and expert estimates for AI threat impact quantification. In the demonstration, ThreatFinderAI is illustrated by securing a customer care application relying on LLMs. Through this, it is demonstrated how the proposed framework can be used to identify relevant threats and practical countermeasures and communicate strategic risk.","2165-963X","978-3-903176-66-9","10.23919/CNSM62983.2024.10814632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814632","Threat Modeling;AI Systems;Large Language Models;AI Security","Threat modeling;Codes;Transforms;System integration;Software;Security;Risk management;Artificial intelligence;System analysis and design;Software development management","","","","8","","31 Dec 2024","","","IEEE","IEEE Conferences"
"Multi-Agent for Network Security Monitoring and Warning: A Generative AI Solution","Q. Yuan; Q. Meng; J. Tao; G. Li; J. Fei; B. Lu; Y. Wang","MoE Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, China; NA; MoE Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; Henan Key Laboratory of Network Cryptography Technology, Information Engineering University and Key Laboratory of Cyberspace Security, Ministry of Education, Zhengzhou, China; NA; NA; NA",IEEE Network,"","2025","PP","99","1","1","With the development of internet technology, intelligent and covert threat methods, such as zero-day attacks, Advanced Persistent Threats, and ransomware attacks, have emerged in an endless stream, making the network security landscape increasingly complex. In this context, network security monitoring and warning systems, as a critical defense line to ensure cyberspace security, are of paramount importance. This paper analyzes the history and current status of traditional security monitoring systems, highlighting unresolved research issues such as insufficient capabilities and low efficiency. Thanks to the rapid development of Generative Artificial Intelligence, network security monitoring and warning are transitioning from traditional models to automated and intelligent systems, achieving significant results. To effectively address these challenges, there is an urgent need to introduce more advanced technological approaches, based on multi-agent systems that simulate human security expert collaboration, to build a next-generation network security monitoring system that combines accuracy, efficiency, and intelligence, thereby enhancing network security monitoring and warning capabilities comprehensively. The DeepSeek-V3 model was utilized for language refinement in this paper1.","1558-156X","","10.1109/MNET.2025.3579001","National Key Research and Development Program of China(grant numbers:No.2023YFB2705000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11031464","Generative AI;Autonomous agent;Large language model;Cyber-security","Network security;Monitoring;Electronic mail;Malware;Feature extraction;Security;Data models;Phishing;Cryptography;Learning systems","","2","","","IEEE","12 Jun 2025","","","IEEE","IEEE Early Access Articles"
"OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models","L. S. S. Ray; B. Zhou; S. Suh; P. Lukowicz","German Research Center for Artificial Intelligence (DFKI), Germany; Department of Computer Science, RPTU Kaiserslautern-Landau, Germany; Department of Computer Science, RPTU Kaiserslautern-Landau, Germany; Department of Computer Science, RPTU Kaiserslautern-Landau, Germany","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890689","Human Activity Recognition;Human to Human Interaction;Open-World Video Understanding;Segment anything;Llama","Vocabulary;Visualization;Surveillance;Large language models;Speech recognition;Activity recognition;Signal processing;Real-time systems;Speech processing;Videos","","1","","38","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Siamese Neural Networks Method for Semantic Requirements Similarity Detection","N. A. Alnajem; M. Binkhonain; M. Shamim Hossain","Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia",IEEE Access,"4 Oct 2024","2024","12","","140932","140947","Detecting semantic similarity between textual requirements is a crucial task for various natural language processing (NLP)-based requirements engineering (RE) applications. It is also challenging due to the nature of these requirements, which are written in natural language (NL), include domain knowledge, and often follow pre-defined templates that contain duplicated words. Recently, deep neural networks (DNNs) have shown promising results in measuring semantic similarity between texts. Siamese neural networks (SNNs), a class of DNNs, are widely used for measuring similarity between various data types, demonstrating their capability and independence of language and domain. Nevertheless, SNNs have a limited use in measuring semantic requirements similarity (SRS). In this paper, a novel metric-based learning method is proposed using SNNs that combines a sentence Transformer model (LLM) and long short-term memory (LSTM) networks with a backward network layer to measure semantic similarity between pairs of requirements. The proposed method is evaluated on an annotated SRS dataset that was built based on public datasets (i.e., PROMISE and PURE) and compared with other state-of-the-art methods (i.e., fine-tuning and zero-shot methods) using accuracy, precision, recall, and F1-score classification metrics. The results show that the proposed method achieved an accuracy of 95.42% and an F1-score of 95.71%, outperforming the state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2024.3469636","King Saud University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:RSP2024R32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10697170","Artificial intelligence for requirements engineering;large language models;long short-term memory networks;requirements;requirements engineering;requirements similarity;semantic requirements similarity;Siamese neural networks;similarity;transformer models","Semantics;Transformers;Accuracy;Vectors;Software;Long short term memory;Requirements engineering;Computer architecture;Natural language processing;XML;Artificial intelligence;Neural networks","","1","","66","CCBYNCND","27 Sep 2024","","","IEEE","IEEE Journals"
"Write me this Code: An Analysis of ChatGPT Quality for Producing Source Code","K. Moratis; T. Diamantopoulos; D. -N. Nastos; A. Symeonidis","Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","147","151","Developers nowadays are increasingly turning to large language models (LLMs) like ChatGPT to assist them with coding tasks, inspired by the promise of efficiency and the advanced capabilities they offer. However, this raises important questions about the ease of integration and the safety of incorporating these tools into the development process. To investigate these questions, this paper examines a set of ChatGPT conversations. Upon annotating the conversations according to the intent of the developer, we focus on two critical aspects: firstly, the ease with which developers can produce suitable source code using ChatGPT, and, secondly, the quality aspects of the generated source code, determined by the compliance to standards and best practices. We research both the quality of the generated code itself and its impact on the project of the developer. Our results indicate that ChatGPT can be a useful tool for software development when used with discretion.CCS CONCEPTS• Software and its engineering → Reusability; Open source model; Software defect analysis; Software libraries and repositories.","2574-3864","979-8-4007-0587-8","","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555604","Code Generation;Code Quality;ChatGPT;Large Language Models","Codes;Source coding;Oral communication;Writing;Chatbots;Turning;Software","","2","","9","","18 Jun 2024","","","IEEE","IEEE Conferences"
"VulKiller: Java Web Vulnerability Detection with Code Property Graph and Large Language Models","X. Chen; B. Wang; M. Zhang; Y. Cao; Q. Liu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; MYbank AntGroup, Hangzhou, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","In recent years, web application development has become more efficient, yet vulnerabilities still pose significant risks. Traditional static and dynamic detection techniques are prone to false positives and negatives, making it challenging for small and medium-sized developers with limited security knowledge to accurately assess the results. To address these challenges, we introduced VulKiller, an automated vulnerability detection tool powered by large language models (LLM). VulKiller leverages static analysis to convert application code into Code Property Graphs (CPG) and utilizes Neo4j to identify high-risk method call chains. By designing structured interactions with ChatGPT, these call chains and corresponding code are transformed into Proofs of Concept (PoCs), which are then parsed into attack payloads and evaluated by a vulnerability monitor for effectiveness. In comparison with traditional tools, VulKiller excels in reducing false positives and negatives. Additionally, in zero-day vulnerability detection experiments, VulKiller identified 12 zero-day vulnerabilities. Our results offer significant encouragement for using LLM to enhance vulnerability detection.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890652","National Natural Science Foundation of China; Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890652","code property graph;static analysis;vulnerability detection;large language models","Java;Codes;Speech coding;Large language models;Static analysis;Signal processing;Security;Speech processing;Monitoring;Payloads","","","","29","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis","J. I. Ahad; R. M. Sultan; A. Kaikobad; F. Rahman; M. R. Amin; N. Mohammed; S. Rahman","Apurba-NSU R&D Lab, ECE, North South University, Dhaka, Bangladesh; Apurba-NSU R&D Lab, ECE, North South University, Dhaka, Bangladesh; Apurba-NSU R&D Lab, ECE, North South University, Dhaka, Bangladesh; Apurba Technologies, Sunnyvale, CA, USA; Computer and Information Science, Fordham University, New York, USA; Apurba-NSU R&D Lab, ECE, North South University, Dhaka, Bangladesh; Apurba-NSU R&D Lab, ECE, North South University, Dhaka, Bangladesh",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","1615","1624","This study investigates the automation of metaanalysis in scientific documents using large language models (LLMs). Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding. We know that a metaarticle provides a structured analysis of several articles. However, conducting meta-analysis by hand is labor-intensive, time-consuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process. Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction. We automate and optimize the meta-analysis process by integrating Retrieval Augmented Generation (RAG). Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content. Human evaluation then assesses relevance and provides information on model performance in key metrics. This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts. The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%. These experiments were conducted in a low-resource environment, highlighting the study’s contribution to enhancing the efficiency and reliability of meta-analysis automation.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825310","Meta-analysis;Large contextual data;Human evaluation;Prompt engineering;Large Language Model","Measurement;Training;Automation;Statistical analysis;Large language models;Retrieval augmented generation;Pipelines;Big Data;Reliability;Prompt engineering","","","","49","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Enhancing Developer Productivity: Benchmarking LLM-Powered Tools like GitHub Copilot and TabNine in Real-Time Coding Environments","F. Slama; D. Lemire","Dept. Science and Technology, TÉLUQ University, Montreal, Canada; Dept. Science and Technology, TÉLUQ University, Montreal, Canada",2025 IEEE 11th International Conference on Intelligent Data and Security (IDS),"19 Jun 2025","2025","","","39","45","The adoption of Large Language Models (LLMs) is disrupting software development — including code generation, debugging, and project management. We classify LLM applications into six domains specializing in general usage, templated interactive development environments (IDEs) programming, complex reasoning, cloud programming, large-volume code management, and bilingual project programming. It assesses models including GPT-4, Codex, GitHub Copilot, TabNine, Claude 3 Opus, and Code Llama, with a focus on their impact on productivity, debugging correctness, and workflow efficiency. We offer concrete recommendations for developers to best leverage these tools. And even though the paper includes all kind of LLMs, I would say that the practical experimentation is target on IDE integrations (having in mind provisioning) and with the focus (actually based on another two LLMs for real-time completion) GitHub Copilot and TabNine, both extensions for Visual Studio Code. This benchmarking of their performance and usability in live coding environments provides critical insights into the capabilities and limitations of LLMs, enabling developers to strategically optimize their workflows by selecting and integrating the most intelligent tools available.","","979-8-3315-9661-3","10.1109/IDS66066.2025.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11038790","Large Language Models (LLMs);IDE Integration;Code Optimization;GitHub Copilot;TabNine;Code Generation","Productivity;Visualization;Codes;Large language models;Debugging;Programming;Benchmark testing;Encoding;Real-time systems;Software development management","","","","17","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems","E. Ko; P. Kang","Department of Computer Engineering, Dankook University, Yongin, Republic of Korea; Department of Software Science, Dankook University, Yongin, Republic of Korea",IEEE Access,"28 Mar 2025","2025","13","","52925","52938","Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, but their effectiveness in coding workflows, particularly in machine learning (ML), requires deeper evaluation. This paper investigates the coding proficiency of LLMs such as GPT and Gemini by benchmarking their performance on three ML problems: Titanic, MNIST, and Steel Defect. These problems were chosen to encompass a range of challenges, including handling missing data, feature engineering, deep learning architectures, and multi-label classification. Using systematic prompts, we evaluated the LLMs’ abilities in data preprocessing, hyperparameter tuning, and classifier generation, comparing their outputs with those of human developers and AutoML frameworks. Experimental results indicate that the human developer outperformed untuned LLMs in data preprocessing, maintaining a 3–5% accuracy advantage across datasets. However, GPT’s hyperparameter tuning improved model performance by up to 6.3% in Titanic and 3.33% in Steel Defect, surpassing human-tuned models in some cases. In contrast, Gemini exhibited only marginal tuning improvements (0.19–1.78%) and failed to compensate for preprocessing inefficiencies. These findings show that while LLMs can assist with ML coding tasks, they exhibit varying levels of efficiency depending on task complexity and preprocessing requirements. GPT demonstrated superior hyperparameter tuning capabilities, whereas both LLMs struggled with intuitive data preprocessing, particularly in feature selection and transformation. This study provides practical insights into the strengths and limitations of LLMs in ML workflows, offering guidance for their effective integration into real-world applications.","2169-3536","","10.1109/ACCESS.2025.3553870","National Research Foundation of Korea (NRF) funded by Korean Government [Ministry of Science and ICT (MSIT)](grant numbers:RS-2023-00244605); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937484","Artificial intelligence;machine learning;code generation;large language model (LLM);generative pre-trained transformer (GPT)","Encoding;Codes;Machine learning;Benchmark testing;Tuning;Software development management;Automated machine learning;Steel;Chatbots;Accuracy","","","","42","CCBYNCND","24 Mar 2025","","","IEEE","IEEE Journals"
"At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence","A. Celik; A. M. Eltawil","Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",IEEE Open Journal of the Communications Society,"1 May 2024","2024","5","","2433","2489","As we transition from the 5G epoch, a new horizon beckons with the advent of 6G, seeking a profound fusion with novel communication paradigms and emerging technological trends, bringing once-futuristic visions to life along with added technical intricacies. Although analytical models lay the foundations and offer systematic insights, we have recently witnessed a noticeable surge in research suggesting machine learning (ML) and artificial intelligence (AI) can efficiently deal with complex problems by complementing or replacing model-based approaches. The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal examples such as generative adversarial networks, variational autoencoders, flow-based GMs, diffusion-based GMs, generative transformers, large language models, autoregressive GMs, to name a few. Contrary to the prevailing belief that GenAI is a nascent trend, our exhaustive review of approximately 120 technical papers demonstrates the scope of research across core wireless research areas, including 1) physical layer design; 2) network optimization, organization, and management; 3) network traffic analytics; 4) cross-layer network security; and 5) localization & positioning. Furthermore, we outline the central role of GMs in pioneering areas of 6G network research, including semantic communications, integrated sensing and communications, THz communications, extremely large antenna arrays, near-field communications, digital twins, AI-generated content services, mobile edge computing and edge AI, adversarial ML, and trustworthy AI. Lastly, we shed light on the multifarious challenges ahead, suggesting potential strategies and promising remedies. Given its depth and breadth, we are confident that this tutorial-cum-survey will serve as a pivotal reference for researchers and professionals delving into this dynamic and promising domain.","2644-125X","","10.1109/OJCOMS.2024.3362271","Office of Sponsored Research (OSR) at King Abdullah University of Science and Technology (KAUST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10422716","5G;6G;machine learning (ML);deep learning (DL);artificial intelligence (AI);discriminative AI;generative AI;generative models;generative adversarial networks;variational autoencoders;normalizing flows;diffusion models;generative transformers;generative pre-trained transformers;large language models;autoregressive generative models;semantic communications;integrated sensing and communications;digital twins;trustworthy AI;explainable AI;adversarial ML;mmWave;mMIMO;terahertz;near-field communication;extremely large antenna arrays;holographic beamforming;open RAN;zero-touch service management;AI-generated content;network function virtualization;software defined networks;trustworthy AI","Wireless communication;6G mobile communication;Generative adversarial networks;Surveys;Artificial intelligence;Data models;Communication system security","","40","","303","CCBY","5 Feb 2024","","","IEEE","IEEE Journals"
"Troll and Abuser Identification in Social Media for Code-Mixed English, Bangla, and Banglish Text Analysis Using LLMs","M. A. Nezami; M. N. Huda","Masters of Science in CSE, United International University, Dhaka, Bangladesh; Dept. of CSE & Head of Dept., United International University, Bangladesh, Dhaka, Bangladesh",2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN),"14 Feb 2025","2024","","","1","5","Social networking platforms have become our center for daily communication. These networks removed all the barriers that used to exist by providing open platform for sharing thoughts, keeping in touch with friends, and even voicing for what we believe in. Unfortunately, this has also opened the door for trolls and online bullies to run rampant. Along with democratizing communication, these platforms have created a distinctive vocabulary, such as the widespread usage of emojis, which are particularly well-liked by younger audiences. “Banglish” a hybrid language that combines Bengali and English is widely used by Bengalis around the world. It creates serious difficulties for traditional content filtering systems. This study examines how to identify trolls and abusive content in Banglish text using sophisticated large language models (LLMs), such as OpenAI GPT-4, Google Gemini, and Meta LLaMA. Our goal is to use these models to improve the security and reduce the toxicity of social media environments, ensuring a more secure online experience for all users.","","979-8-3315-2922-2","10.1109/ICNGN63705.2024.10871363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871363","AI;LLMs;Machine learning;Sentiment Analysis;Troll detection;Social media;Code-mixed language;Banglish;Bangla","Vocabulary;Toxicology;Text analysis;Filtering;Large language models;Cyberbullying;Internet;Security;Next generation networking;Emojis","","","","11","IEEE","14 Feb 2025","","","IEEE","IEEE Conferences"
"Sliding Mode Interval Observer-Based Controller Design for Multi-Agent Systems Under DoS Attack","X. Shen; J. Gao; P. X. Liu","School of Information Science and Engineering, Zhejiang Sci-Tech University, Hangzhou, China; School of Information Science and Engineering, Zhejiang Sci-Tech University, Hangzhou, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Systems Journal,"12 Dec 2023","2023","17","4","6656","6664","The consensus problem of the multi-agent systems with uncertainty and denial-of-service attack is studied. The interval observer control method is utilized to reduced the influence of unknown external disturbances. By introducing sliding mode term, the observation error of the system is released and the robustness is improved. Using the coordination theory, the control algorithm based on the sliding mode interval observer drive the uncertain system to achieve the coordinated consensus. In order to make the multi-agent systems achieve asymptotically consensus under denial-of-service attack, an attack time security protocol based on persistent dwell time scheme is proposed. Finally, the simulation results are given to demonstrate the validity and superiority of the proposed method.","1937-9234","","10.1109/JSYST.2023.3323549","National Natural Science Foundation of China(grant numbers:62073296); Zhejiang Province Natural Science Foundation of China(grant numbers:LZ23F030010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292502","Denial-of-service (DoS) attack;multi-agent systems (MASs);sliding mode interval observer;uncertain networked system","Observers;Uncertainty;Topology;Multi-agent systems;Stability analysis;Security;Robustness","","4","","27","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"From Syntax To Semantics: AI assisted Computational Linguistics In The Era Of Large computational Language Models","Z. Sattorova; R. R. Hossain; W. Suryasa; Y. U. Vokhidjon Ugli; F. Rahman","Tashkent State University of Oriental Studies, Uzbekistan; Department of Computers Techniques Engineering, College of Technical Engineering, The Islamic University, Najaf, Iraq; ITB STIKOM Bali, Denpasar, Indonesia; Faculty of Linguistics, Turan International University, Namangan, Uzbekistan; Department of CS & IT, Kalinga University, Raipur, India",2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES),"17 Jun 2025","2025","","","1","6","Computational linguistics has evolved from rule-based syntax analysis to deep semantic understanding, driven by advancements in artificial intelligence. The emergence of Large Language Models (LLMs) has revolutionized this field, offering new opportunities for AI-assisted linguistic processing. However, existing methods often struggle with ambiguity, contextual understanding, and resource-intensive training, limiting their ability to achieve human-like comprehension. To address these challenges, this study leverages LLMs to enhance syntactic parsing, semantic disambiguation, and context-aware language processing. Techniques such as prompt engineering, fine-tuning, and retrieval-augmented generation (RAG) are employed to improve linguistic accuracy and efficiency. The proposed framework enables more precise translation, sentiment analysis, and information retrieval while reducing computational overhead. By integrating AI-driven computational linguistics with LLMs, this approach enhances natural language understanding and generation across diverse applications. The findings suggest that LLM-assisted models outperform traditional methods in terms of contextual coherence, accuracy, and adaptability, marking a significant advancement in the field.","","979-8-3315-3669-5","10.1109/ICCIES63851.2025.11032453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11032453","Computational Linguistics;Large Language Models (LLMs);Syntax;Semantics;AI-assisted Linguistic Processing;Semantic Disambiguation;Natural Language Understanding (NLU);Context-aware Language Processing;Machine Learning;Prompt Engineering","Sentiment analysis;Accuracy;Translation;Computational modeling;Large language models;Semantics;Syntactics;Computational linguistics;Prompt engineering;Context modeling","","","","17","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"An Empirical Comparison of Code Generation Approaches for Ansible","B. Darnell; H. Chopra; A. Councilman; D. Grove; Y. -X. Wang; V. Adve","University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; IBM Research Yorktown Heights, New York, USA; University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; University of Illinois at Urbana-Champaign, Urbana, Illinois, USA","2024 IEEE/ACM 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE)","10 Sep 2024","2024","","","1","6","The rapid proliferation of LLM-based programming assistants has enabled fast and accurate automatic code generation for general purpose programming languages. Domain-specific languages like Ansible, a DSL for IT Automation, have seen a lack of support despite being critical to many fields, due to limited public-domain code for training models and a lack of interest from tool developers. To address this issue, we collect a novel dataset of permissively licensed Ansible code, and use it to create Warp, an LLM for code fine-tuned to produce Ansible tasks from a natural language prompt. We evaluate state-of-the-art tools for LLM-based code generation models, comparing multiple common strategies, including fine-tuning base models on Ansible code and retrieval-augmented-generation using documentation, in order to understand challenges with existing methodology and identify future research directions to enable better code generation for DSLs.CCS CONCEPTS• Software and its engineering → Domain specific languages; Source code generation.","","979-8-4007-0564-9","","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669830","Large language models;Code generation;Domain specific languages;Ansible","Training;Codes;Source coding;Natural languages;Programming;Software;Robustness","","","","23","CCBY","10 Sep 2024","","","IEEE","IEEE Conferences"
"Large Language Models and Security","M. Bezzi","SAP Security Research, Mougins, France",IEEE Security & Privacy,"2 Apr 2024","2024","22","2","60","68","We analyze the security implications of large language models (LLMs) from their use as security tools for both attackers and defenders and the security of LLMs. We discuss how LLMs increase the scale of traditional threats such as social engineering and add new ones such as prompt injections.","1558-4046","","10.1109/MSEC.2023.3345568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413528","","Security;Phishing;Malware;Fake news;Electronic mail;Costs;Computational modeling;Large language models","","1","","14","IEEE","24 Jan 2024","","","IEEE","IEEE Magazines"
"Interdisciplinary Research Project ‘AI Shield’","T. Van Ede; T. Mulder; E. Moyakine","Semantics, Cybersecurity and Services (SCS) group, the University of Twente, Enschede, the Netherlands; Legal Aspects of Entrepreneurship, the Hanze University of Applied Sciences, Groningen, the Netherlands; Transboundary Legal Studies (TLS), the University of Groningen, Groningen, the Netherlands",2024 8th Cyber Security in Networking Conference (CSNet),"28 Jan 2025","2024","","","268","271","In the modern day and age, cybersecurity faces numerous challenges. Computer systems and networks become more and more sophisticated and interconnected, and the attack surface constantly increases. In addition, cyber-attacks keep growing in complexity and scale. In order to address these challenges, security professionals started to employ generative AI (GenAI) to quickly respond to attacks. However, this introduces challenges in terms of how GenAI can be adapted to the security environment and where the legal and ethical responsibilities lie. The Universities of Twente and Groningen and the Hanze University of Applied Sciences have initiated an interdisciplinary research project to investigate the legal and technical aspects of these LLMs in the cybersecurity domain and develop an advanced AI-powered tool. This project is currently being developed and will form the basis of the grant application to be submitted in the near future at the Dutch Research Council (‘Nederlandse Organisatie voor Wetenschappelijk Onderzoek’ or NWO).","2768-0029","979-8-3315-3410-3","10.1109/CSNet64211.2024.10851746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851746","generative AI;large language models;cybersecurity;ethical and legal compliance","Ethics;Law;Generative AI;Instruments;Europe;Regulation;Standards;Faces;Guidelines;Information systems","","","","8","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation","S. Fakhoury; A. Naik; G. Sakkas; S. Chakraborty; S. K. Lahiri","Microsoft Research, Redmond, WA, USA; University of Pennsylvania, Philadelphia, PA, USA; University of California, San Diego, CA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA",IEEE Transactions on Software Engineering,"18 Sep 2024","2024","50","9","2254","2268","Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.","1939-3520","","10.1109/TSE.2024.3428972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606356","Intent disambiguation;code generation;LLMs;human factors;cognitive load;test generation","Codes;Accuracy;Natural languages;Artificial intelligence;Python;Task analysis;Benchmark testing","","19","","48","IEEE","22 Jul 2024","","","IEEE","IEEE Journals"
"Towards Evaluation and Understanding of Large Language Models for Cyber Operation Automation","M. Sultana; A. Taylor; L. Li; S. Majumdar","Defence Research and Development Canada, Ottawa, Canada; Defence Research and Development Canada, Ottawa, Canada; Defence Research and Development Canada, Ottawa, Canada; Concordia University, Montreal, Canada",2023 IEEE Conference on Communications and Network Security (CNS),"27 Oct 2023","2023","","","1","6","Can foundational language models be useful in automating cybersecurity tasks? To address this open question, systematic and comprehensive evaluation of large language models (LLMs) across diverse cyber operational tasks (e.g., incident response, threat identification, forensic analysis, etc.), as well as understanding their risks and limitations, are crucial. A significant challenge lies in the absence of a standard benchmark dataset encompassing real-life cyber operational tasks that can be processed by LLMs. This paper tackles this challenge by conducting a preliminary study towards evaluation and understanding of LLMs for cyber operation automation. To that end, we first identify a list of defensive cyber operational tasks with increasing complexities and suggests the creation of new datasets to accomplish these tasks. Second, we review recent works leveraging LLMs for downstream cyber operational tasks to identify research gaps and open problems. Third, we propose a framework to understand and benchmark the cyber operational tasks to report potential solutions and research directions for the reliable evaluation of LLMs. Finally, this paper serves as an open call to the cybersecurity researchers and professionals to contribute to the development of an open-source evaluation framework paving the way for the trustworthy use of foundation models in cyber domain.","","979-8-3503-3945-1","10.1109/CNS59707.2023.10288677","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288677","natural language processing;cyber security;cyber operational tasks;benchmark dataset;autonomous cyber defence","Automation;Systematics;Standards organizations;Natural languages;Organizations;Benchmark testing;Network security","","16","","49","Crown","27 Oct 2023","","","IEEE","IEEE Conferences"
"Zero-Shot Prediction of Conversational Derailment With Large Language Models","K. Nonaka; M. Yoshida","Degree Programs in Systems and Information Engineering, University of Tsukuba, Tsukuba, Ibaraki, Japan; Institute of Business Sciences, University of Tsukuba, Tokyo, Japan",IEEE Access,"2 Apr 2025","2025","13","","55081","55093","Online discussion platforms often show a tendency for conversations to stray from the topic and devolve into personal attacks. Previous studies have trained machine learning algorithms to detect conversational derailment using supervised methods. However, creating the datasets required for supervised training is very costly. To address this challenge, we focus on the zero-shot performance of large language models (LLMs), which have advanced rapidly in recent years. This study aims to evaluate the zero-shot prediction performance of conversational derailment using LLMs. First, we measured the performance of the most commonly used LLMs in predicting conversational derailments and found that the zero-shot prediction performance is comparable to that of traditional fine-tuning approaches. Secondly, we explored the effect of inserting prior knowledge into the prompts on the behavior of the LLMs. We discovered that this practice does not necessarily improve the performance of LLMs, resulting in unexpected changes in prediction timing. This study’s contributions are as follows: We demonstrate that zero-shot inference with LLMs is an effective method for predicting conversational derailment in the absence of training data. Additionally, we found that altering the prompt to include prior knowledge leads to unintended results in the conversational derailment prediction task. This observation emphasizes the need to evaluate the impact of prompt changes from various perspectives, going beyond simple performance metrics.","2169-3536","","10.1109/ACCESS.2025.3554548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938545","Large language model;zero-shot learning;prompt engineering;online conversation;conversation derailment;personal attack;anti-social behavior","Oral communication;Railway accidents;Social networking (online);Online services;Internet;Encyclopedias;Training;Predictive models;Large language models;Training data","","","","38","CCBY","25 Mar 2025","","","IEEE","IEEE Journals"
"A Blockchain-Enhanced Secure and Reliable Data Transaction Scheme in MAS via HTLC","B. Yu; Y. Guan; S. Geng; L. Miao; Y. Zhang; Y. Gong","Beijing Information Science & Technology University, Beijing, P. R. China; Beijing Information Science & Technology University, Beijing, P. R. China; Beijing Information Science & Technology University, Beijing, P. R. China; Beijing Information Science & Technology University, Beijing, P. R. China; Beijing Information Science & Technology University, Beijing, P. R. China; Beijing Information Science & Technology University, Beijing, P. R. China",2024 3rd Conference on Fully Actuated System Theory and Applications (FASTA),"23 Jul 2024","2024","","","494","499","Data transactions within distributed networks are increasingly critical in modern technological infrastructures. However, such transactions encounter significant challenges, including security vulnerabilities and a lack of participant trust. Traditional centralized approaches often fail to address these issues due to their lack of transparency and flexibility, resulting in inefficiencies and potential risks. This paper presents a novel transaction mechanism that enhances security and transparency in Multi-Agent Systems (MAS) by integrating Hashed Timelock Contracts (HTLC) with blockchain technology, effectively addressing the concerns of data transaction reliability and trust. The efficacy of this mechanism in enhancing transaction security and efficiency was validated through simulation experiments. Results demonstrate that the system operates reliably under various network conditions, boasting high success rates and robust security. Overall, this research successfully explores and validates an enhanced blockchain-based solution for secure and reliable data transactions in distributed networks, offering a new strategy for addressing the challenges of data exchanges.","","979-8-3503-7369-1","10.1109/FASTA61401.2024.10595264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10595264","Blockchain;Data Transaction Security;Hashed Timelock Contract;Multi-Agent Systems","Adaptation models;Scalability;Blockchains;Security;Reliability;Contracts;Resilience","","","","15","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring","D. Wu; F. Mu; L. Shi; Z. Guo; K. Liu; W. Zhuang; Y. Zhong; L. Zhang","Beihang University, Beijing, China; Institute of Software Chinese Academy of Sciences, Beijing, China; Beihang University, Beijing, China; Software Engineering Application Technology Lab, Huawei, China; Software Engineering Application Technology Lab, Huawei, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1345","1357","Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17% on code smell detection, outperforming LLMs baselines by an increase of 35.05% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.","2643-1572","979-8-4007-1248-7","","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764947","","Measurement;Learning systems;Codes;Source coding;Scalability;Large language models;Feature extraction;Vectors;Software;Software engineering","","1","","64","","29 Nov 2024","","","IEEE","IEEE Conferences"
"CVALLM: A Cloud Platform Security Assessment Framework Based on Large Language Models","W. Jing; C. Zhang; B. Zhang; L. Wei","School of Cyberspace Science and Technology, University of Science and Technology of China, Hefei, China; School of Cyberspace Science and Technology, University of Science and Technology of China, Hefei, China; School of Cyberspace Science and Technology, University of Science and Technology of China, Hefei, China; School of Cyberspace Science and Technology, University of Science and Technology of China, Hefei, China","2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","4 Apr 2025","2024","","","2039","2044","Cloud computing has revolutionized computing and data storage by providing flexible resource management and on-demand services. However, the centralized nature of cloud computing results in significant security issues that pose significant threats to cloud services. The security threat can be alleviated to a certain extent through the cloud platform security assessment. However, the existing cloud platform security assessment framework mainly measures security from the enterprise perspective and lacks methods from the user perspective. For users, much information is internal to the enterprise and cannot be obtained. To address these challenges, we propose a framework called CVALLM to measure cloud platform security from the user’s perspective, which uses a large language model to automatically complete a cloud platform security assessment. First, we utilize a large language model to complete cloud platform information extraction and vulnerability collection from SLA agreements and product description documents that the user can access. Second, to solve the vulnerability assessment problem, we propose a vulnerability severity prediction method based on deep learning. This method considers both the textual description characteristics and the source code characteristics of the vulnerability. Finally, we propose an overall security assessment method for the cloud platform based on a large language model, design quantitative indicators to measure the security of the cloud platform, and automatically generate security reports to facilitate users’ ability to quickly compare and choose the right cloud service.","2324-9013","979-8-3315-0620-9","10.1109/TrustCom63139.2024.00283","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10945174","Cloud Security;Vulnerability Assessment;Large Language Model","Deep learning;Cloud computing;Privacy;Large language models;Source coding;Memory;Prediction methods;Information retrieval;Security;Resource management","","","","10","IEEE","4 Apr 2025","","","IEEE","IEEE Conferences"
"Chatgpt-Based Test Generation for Refactoring Engines Enhanced by Feature Analysis on Examples","C. Dong; Y. Jiang; Y. Zhang; Y. Zhang; H. Liu","Beijing Institute of Technology, Beijing, China; Peking University, Beijing, China; Beijing Institute of Technology, Beijing, China; Hebei University of Science and Technology, Shijiazhuang, Hebei, China; Beijing Institute of Technology, Beijing, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2714","2725","Software refactoring is widely employed to improve software quality. However, conducting refactorings manually is tedious, time-consuming, and error-prone. Consequently, automated and semi-automated tool support is highly desirable for software refactoring in the industry, and most of the main-stream IDEs provide powerful tool support for refactoring. However, complex refactoring engines are prone to errors, which in turn may result in imperfect and incorrect refactorings. To this end, in this paper, we propose a ChatGPT-based approach to testing refactoring engines. We first manually analyze bug reports and test cases associated with refactoring engines, and construct a feature library containing fine-grained features that may trigger defects in refactoring engines. The approach automatically generates prompts according to both predefined prompt templates and features randomly selected from the feature library, requesting ChatGPT to generate test programs with the requested features. Test programs generated by ChatGPT are then forwarded to multiple refactoring engines for differential testing. To the best of our knowledge, it is the first approach in testing refactoring engines that guides test program generation with features derived from existing bugs. It is also the first approach in this line that exploits LLMs in the generation of test programs. Our initial evaluation of four main-stream refactoring engines suggests that the proposed approach is effective. It identified a total of 115 previously unknown bugs besides 28 inconsistent refactoring behaviors among different engines. Among the 115 bugs, 78 have been manually confirmed by the original developers of the tested engines, i.e., IntelliJ IDEA, Eclipse, VScode-Java, and NetBeans.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00210","National Natural Science Foundation of China(grant numbers:62232003,62172037); China Postdoctoral Science Foundation(grant numbers:2023M740078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029808","refactoring;ChatGPT;refactoring engines;differential testing","Industries;Computer bugs;Software quality;Chatbots;Libraries;Test pattern generators;Engines;Testing;Software engineering","","","","40","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"LLM-Empowered Autonomous Edge AI for Anomaly Detection and Resolution in Distributed Systems","A. P. A; A. R. Sreeram; P. K. Auradkar","Dept. of Computer Science, PES University, Bengaluru, India; Dept. of Computer Science, PES University, Bengaluru, India; Dept. of Computer Science, PES University, Bengaluru, India",2025 10th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),"20 Jun 2025","2025","","","698","703","There is a growing need for strong security measures to combat cyberattacks with increasing data consumption and reliance on interconnected networks. This is especially required at the network edge, where the data are present and first acquired. Cyber attacks such as distributed denial-of-service (DDoS) attacks and others pose risk and various other inconveniences to all stakeholder involved. The dynamic nature of these attacks and the lack of processing power at the edge require new methods to tackle cyber-attacks. This paper proposes a novel approach for network anomaly detection and mitigation that uses edge AI and the contextual understanding power of Large Language Models (LLMs). The system utilizes deep learning methods to identify cyberattacks where the models are deployed on edge devices and servers enabling quick identification of anomalies. The paper then explores the use of LLMs to automate the resolution and mitigation of these attacks in real-time, leading to lower impact by reducing downtimes and increasing availability. This research aims to contribute to the field of network security by proposing an edge AI driven solution for real-time local DDoS attack detection and the utilization of LLMs for network anomaly resolution and system adaptation.","2832-3734","979-8-3315-3080-8","10.1109/ICCCBDA64898.2025.11030483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11030483","Distributed Denial of Service;Edge AI;Large Language Models;GPT;Deep Learning;Anomaly Detection;Network Security","Deep learning;Prevention and mitigation;Large language models;Image edge detection;Edge AI;Network security;Denial-of-service attack;Real-time systems;Object recognition;Anomaly detection","","","","19","IEEE","20 Jun 2025","","","IEEE","IEEE Conferences"
"Case study: using AI-assisted code generation in mobile teams","M. -S. Vasiliniuc; A. Groza","Department of Computer Science, Technical University Of Cluj-Napoca and Nexttech International, Cluj-Napoca, Romania; Department of Computer Science, Technical University of Cluj-Napoca, Cluj-Napoca, Romania",2023 IEEE 19th International Conference on Intelligent Computer Communication and Processing (ICCP),"23 Jan 2024","2023","","","339","346","We evaluate the performance of AI-assisted programming in mobile development teams that are focused on native mobile languages like Kotlin and Swift. The case study involves 16 participants and 2 technical reviewers, from a software development department and it is designed to understand the impact of using Large Language Models trained for code generation in particular phases of the team, more specifically, technical onboarding and technical stack switch. The study uses technical problems dedicated to each phase and requests solutions from the participants with and without using AI-Code generators. It measures time, correctness, and ‘technical integration’ using a new proposed metric ReviewerScore, extracted from industry standards, the code reviewer of merge/pull requests.The output is converted and analyzed together with feedback from the participants in an attempt to determine if using AI-assisted programming tools will have an impact on getting developers onboard in a project or helping them with a smooth transition between the two native development environments of mobile development, Android and iOS. The study was performed between May and June 2023 with members of the mobile department of a software development company based in Cluj-Napoca, with Romanian ownership and management.","2766-8495","979-8-3503-7035-5","10.1109/ICCP60212.2023.10398656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398656","BigCode;Machine Learning (ML);Large Language Models (LLM);Mobile Development;Swift;Kotlin;Software Development Industry;Code Generation;Text-to-Code","Codes;Switches;Writing;Programming;Generators;Software;Encoding","","2","","11","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"Leveraging Human Expertise and AI for Engineering Regulatory Data Expansion: A Case Study With ChatGPT","E. Al Nama; M. Mahmud; H. Al Madhoob","College of Business Administration, University of Bahrain, Manam, Kingdom of Bahrain; School of Computing, Ulster University, Belfast, Northem Ireland, UK; College of Engineering, University of Bahrain, Manam, Kingdom of Bahrain",2024 International Conference on Decision Aid Sciences and Applications (DASA),"17 Jan 2025","2024","","","1","6","In recent years, the integration of Large Language Models (LLMs) like ChatGPT into various tasks has revolutionized the process of generating training data for machine learning models. This paper presents a novel approach that leverages both human expertise and AI collaboration to expand small datasets, particularly in cases where data scarcity limits the performance of models requiring fine-tuning. The study documents methodologies used to scale data generation efforts by combining human input with advanced AI, focuses on prompt engineering to optimize outputs. The objective is to generate comprehensive datasets from a limited number of regulatory articles related to regulating the practice of engineering professions, ensuring accuracy and contextual relevance. The methodology involved processing articles individually, transitioning to batch processing, and iterating with continuous feedback. The results underscore the importance of human-AI synergy in achieving high-quality outputs, where the human element ensures accuracy, and the AI accelerates the data generation process. The findings demonstrate that prompt engineering plays a critical role in guiding AI to generate reliable data. Finally, the research emphasizes the potential of this approach to improve the efficiency and scalability of data generation for model fine-tuning, offering insights into the effective use of human-AI collaboration in broader contexts.","","979-8-3503-6910-6","10.1109/DASA63652.2024.10836337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10836337","Human-AI Collaboration;Fine-Tuning Models;Small Data Expansion;Large Language Models (LLMs);Prompt Engineering;Architecture;Engineering & Construction (AEC) Industry;Building Regulations;Kingdom of Bahrain","Feedback loop;Accuracy;Scalability;Collaboration;Machine learning;Data collection;Chatbots;Data models;Prompt engineering;Artificial intelligence","","","","29","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Secure Blockchain-Integrated Deep Learning Framework for Generative AI Model","M. N; V. G; L. G; B. S; G. G; P. D","Department of Computing Technologies, SRMIST KTR Campus, School of Computing, Chengalpattu, India; Department of CSE, CEG Anna University, Chennai, India; VIT, School of CSE, Chennai, India; Department of CSE, CEG Anna University, Chennai, India; Department of CSE, CEG Anna University, Chennai, India; Department of CSE, Anna University Regional Campus, Coimbatore, India","2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)","25 Jun 2025","2025","","","1","6","Generative AI is an advanced AI subset designed to create new data resembling existing data, widely applied in text generation, image synthesis, and predictive modeling to enhance automation and creativity across industries. Deep learning, particularly Long Short-Term Memory (LSTM) networks, plays a crucial role in generative AI, as LSTM effectively processes sequential data, making it essential for tasks like natural language processing and time-series forecasting. Data security is critical in generative AI, especially when handling sensitive information. Encryption techniques such as Advanced Encryption Standard (AES) and Rivest-Shamir-Adleman (RSA) ensure data security before processing, with AES providing fast encryption and RSA enabling secure key exchanges. Homomorphic encryption allows AI models to process encrypted data without decryption, maintaining confidentiality. Blockchain technology further enhances data security through a decentralized and immutable ledger for verification, employing smart contracts and cryptographic hashing to ensure data integrity. Blockchain-based identity management systems authenticate inputs while preventing unauthorized access, preserving both privacy and security. Implementing encryption and blockchain mechanisms does not compromise generative AI model accuracy, achieving 92.75% accuracy through rigorous testing, validated using benchmarking datasets and performance evaluation metrics. The integration of encryption and blockchain ensures robust data security while maintaining high accuracy.","","979-8-3315-9848-8","10.1109/RMKMATE64874.2025.11042434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042434","Generative AI;Deep Learning;Encryption;Decryption;Long Short Term Memory;Blockchain","Deep learning;Industries;Accuracy;Generative AI;Computational modeling;Data protection;Data models;Encryption;Blockchains;Long short term memory","","","","20","IEEE","25 Jun 2025","","","IEEE","IEEE Conferences"
"Examining Zero-Shot Vulnerability Repair with Large Language Models","H. Pearce; B. Tan; B. Ahmad; R. Karri; B. Dolan-Gavitt",New York University; University of Calgary; New York University; New York University; New York University,2023 IEEE Symposium on Security and Privacy (SP),"21 Jul 2023","2023","","","2339","2356","Human developers can produce code with cybersecurity bugs. Can emerging ‘smart’ code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI’s Codex and AI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information— both semantically and syntactically—with natural languages. We perform a large scale study of five commercially available, black-box, ""off-the-shelf"" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model’s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179420","Cybersecurity;AI;code generation;CWE","Privacy;Codes;Computer bugs;Natural languages;Closed box;Maintenance engineering;Computer crime","","126","","52","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Examining Zero-Shot Vulnerability Repair with Large Language Models","H. Pearce; B. Tan; B. Ahmad; R. Karri; B. Dolan-Gavitt",New York University; University of Calgary; New York University; New York University; New York University,2023 IEEE Symposium on Security and Privacy (SP),"21 Jul 2023","2023","","","2339","2356","Human developers can produce code with cybersecurity bugs. Can emerging ‘smart’ code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI’s Codex and AI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information— both semantically and syntactically—with natural languages. We perform a large scale study of five commercially available, black-box, ""off-the-shelf"" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model’s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179324","Cybersecurity;AI;code generation;CWE","Privacy;Codes;Computer bugs;Natural languages;Closed box;Maintenance engineering;Computer crime","","52","","52","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"AI and Security, From an Information Security and Risk Manager Standpoint","P. Shetty","Cisco Systems Inc., San Jose, CA, USA",IEEE Access,"5 Jun 2024","2024","12","","77468","77474","Fields such as machine learning and artificial intelligence are proven business enablers, we have several use cases in the field of technology from basic implementations such as automated scanners, and weak forms such as Alexa and Siri that have paved the way for further development. Search for more avenues of investment in this field continues till date. It’s also evident that researchers and organizations have barely scratched the surface in terms of exploring and using AI (Artificial intelligence) as a technology. Theoretical forms of AI have not yet been implemented. This paper aims to provide some context around the various forms of AI currently used and can be explored in the near future. It’s important to be mindful that, with the use of any technology, even AI, there are challenges, regulations, and security risks that tag along. Firms need to pay close attention to the macro factors that could impact the adoption and AI in their respective businesses. This study aims to provide a brief overview of the aforementioned road blocks that could impact AI adoption, however, there are some solution strategies or approaches that could help firms along the way to overcome those road blocks. These strategies might take time to build and take effect but they will be beneficial in the long run and thus help with not only sustainable implementation, but also complete and successful adoption of AI as a technology enabler.","2169-3536","","10.1109/ACCESS.2024.3408144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542982","Artificial intelligence;generative AI;security;risk;risk management;AI for security;security for AI;AI security","Artificial intelligence;Security;Virtual assistants;Data models;Business;Regulation;Deep learning;Generative AI;Risk management","","4","","28","CCBY","31 May 2024","","","IEEE","IEEE Journals"
"“Can You be my Mum?”: Manipulating Social Robots in the Large Language Models Era","G. A. Abbo; G. Desideri; T. Belpaeme; M. Spitale","IDLab-AIRO, Ghent University – imec, Ghent, Belgium; DEIB, Politecnico di Milano, Milano, Italy; IDLab-AIRO, Ghent University – imec, Ghent, Belgium; DEIB, Politecnico di Milano, Milano, Italy",2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"30 Apr 2025","2025","","","1181","1185","Recent advancements in robots powered by large language models have enhanced their conversational abilities, enabling interactions closely resembling human dialogue. However, these models introduce safety and security concerns in HRI, as they are vulnerable to manipulation that can bypass built-in safety measures. Imagining a social robot deployed in a home, this work aims to understand how everyday users try to exploit a language model to violate ethical principles, such as by prompting the robot to act like a life partner. We conducted a pilot study involving 21 university students who interacted with a Misty robot, attempting to circumvent its safety mechanisms across three scenarios based on specific HRI ethical principles: attachment, freedom, and empathy. Our results reveal that participants employed five techniques, including insulting and appealing to pity using emotional language. We hope this work can inform future research in designing strong safeguards to ensure ethical and secure human-robot interactions.","","979-8-3503-7893-1","10.1109/HRI61500.2025.10973919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973919","red-teaming;manipulation;large language model;LLM;ethical principles;social norms;social robots","Ethics;Large language models;Social robots;Safety;Security","","","","21","IEEE","30 Apr 2025","","","IEEE","IEEE Conferences"
"Research on Teaching Video Monitoring Platform Based on Large Language Model Prompt Engineering","Y. Cao; X. Xiong; X. Shao; R. Chen; Y. Hou; B. Li; P. Zhao; K. Guo","Beijing Hengchuang Institute of Additive Manufacturing Technology Co., Ltd., University of Science and Technology, Beijing; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Technology Center of Software Engineering, Institute of Software Chinese Academy of Sciences, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China; Department of Information, Beijing City University, Beijing, China","2025 6th International Conference on Computer Science, Engineering, and Education (CSEE)","23 Jun 2025","2025","","","118","125","With the deepening of educational reforms and the rapid development of online education, the analysis of classroom teaching videos has gradually become an important means to enhance teaching quality and effectiveness. The information contained in teaching videos, such as teacher-student interactions and teaching behaviors, holds significant value for teaching quality assessment and strategy optimization. However, existing research often lacks effective integration capabilities when dealing with multimodal data, with insufficient semantic understanding and contextual awareness. Moreover, there is still room for improvement in the accuracy and interpretability of identifying inappropriate behaviors and assessing teaching quality. To address these issues, this study has developed a software system based on large language model prompt engineering and retrieval-augmented generation technology. By deeply analyzing text, audio, and image information in videos, combined with speech recognition and facial recognition technologies, the system can accurately analyze classroom teaching content and behaviors, providing objective and precise teaching feedback. The main contribution of this paper lies in proposing an analytical framework that significantly enhances the integration of multimodal data and semantic understanding capabilities, and in constructing a multimodal teaching video monitoring platform capable of automatically identifying inappropriate behaviors and providing interpretable analysis results. Experimental validation shows that the system excels in image-sensitive content recognition, contextual awareness, and video analysis, addressing the shortcomings of existing models and demonstrating significant practical application value.","","979-8-3315-0516-5","10.1109/CSEE64583.2025.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11037850","Supervision of Instructional Videos;Multimodal Large Language Models;Educational Assistance;RAG Technology;Prompt engineering","Analytical models;Accuracy;Large language models;Education;Semantics;Speech recognition;Prompt engineering;Optimization;Monitoring;Videos","","","","25","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Distributed Finite-Time Consensus Tracking Control for Nonlinear Multi-Agent Systems With FDI Attacks and Application to Single-Link Robots","Y. Jiang; B. Niu; X. Wang; X. Zhao; H. Wang; B. Yan","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; School of Mathematics and Physics, Bohai University, Jinzhou, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China",IEEE Transactions on Circuits and Systems II: Express Briefs,"27 Mar 2023","2023","70","4","1505","1509","This brief considers the distributed finite-time consensus tracking control problem for nonlinear multi-agent systems with false data injection (FDI) attacks. Firstly, since instability is inevitable when false datas are injected into the researched system, a security control algorithm combining the modified coordinate transformation and the dynamic surface control (DSC) method is developed to eliminate the negative impact of FDI attacks and reduce the burden caused by iterative computations of virtual controls. Besides, there are unknown nonlinearities in the dynamics of each agent, which can not be directly handled by the traditional control design. Thus, the neural-network-based approximation technique is introduced to compensate the unknown nonlinearities. In summary, a novel controller is constructed for each agent, which guarantees the consensus of the outputs of all agents can be achieved and that all the signals of the studied system are bounded within a finite-time period. To prove the correctness of the designed security control algorithm, a simulation example of the multi-agent system composed of multiple single-link robots is presented.","1558-3791","","10.1109/TCSII.2022.3220359","National Natural Science Foundation of China(grant numbers:61873151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941483","Consensus tracking control;finite-time control;false data injection attacks;nonlinear multi-agent systems;single-link robots","Security;Multi-agent systems;Circuit stability;Stability criteria;Robot kinematics;Nonlinear dynamical systems;Lyapunov methods","","13","","17","IEEE","8 Nov 2022","","","IEEE","IEEE Journals"
"Novel approach for detecting Bacterial spot combining Transfer Learning and Large Language Models","Y. Tace; M. Tabaa; S. Elfilali; H. Bensag; C. Leghris","Laboratory of Information Technologies and Modelling, Faculty of Sciences Ben M’sik, Hassan II University, Casablanca, Morocco; Pluridisciplinary Laboratory of Research & Innovation (LPRI), EMSI, Casablanca, Morocco; Laboratory of Information Technologies and Modelling, Faculty of Sciences Ben M’sik, Hassan II University, Casablanca, Morocco; Pluridisciplinary Laboratory of Research & Innovation (LPRI), EMSI, Casablanca, Morocco; Computer Science Department, RTM Team, FST Mohammedia, HIIU, Casablanca, Morocco","2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)","4 Jul 2024","2024","","","1","6","Bacterial spot, a pervasive disease affecting Solanaceae crops such as tomatoes and peppers, poses a significant threat to global food security and agricultural productivity. This paper presents a novel approach that integrates transfer learning techniques with Large Language Models (LLMs) to detect bacterial spot with high accuracy and efficiency. We employed state-of-the-art convolutional neural network (CNN) architectures, including Inception-V3 and MobileNet-V2, which were pre-trained on extensive image datasets and fine-tuned on a curated dataset comprising both PlantVillage images and in-house generated photographs from diverse agricultural settings. These images were further enhanced for model training through age and color classification, addressing the phenotypic variability inherent in real-world agricultural environments.The performance of the CNN models was rigorously evaluated, with Inception-V3 achieving a remarkable 97% accuracy in identifying bacterial spot, while the integration with LLMs provided critical insights into disease management strategies, effectively creating a multifaceted decision support system. The LLMs facilitated the interpretation of complex disease symptoms and treatment efficacy reports, enhancing the overall precision of the diagnostic process.Our research underscores the potential of combining transfer learning and LLMs to revolutionize plant disease detection and agricultural practices. This integrative approach not only advances the field of precision agriculture but also opens up new pathways for scalable, data-driven solutions to combat plant diseases and bolster crop health monitoring worldwide.","2832-8337","979-8-3503-8526-7","10.1109/ISIVC61350.2024.10577820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577820","Bacterial spot;LLMs;Transfer Learning;Plant diseases","Training;Productivity;Plant diseases;Microorganisms;Accuracy;Image color analysis;Transfer learning","","","","14","IEEE","4 Jul 2024","","","IEEE","IEEE Conferences"
"Generating PLC Code with Universal Large Language Models","K. Tran; J. Zhang; J. Pfeiffer; A. Wortmann; B. Wiesmayr","University of Stuttgart, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany; LIT CPS Lab, Johannes Kepler University Linz, Linz, Austria",2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA),"16 Oct 2024","2024","","","1","8","Control software for production systems is typically developed by domain experts, despite its high complexity. The increasingly available Large Language Models (LLMs) can assist developers with code generation and debugging. However, their suitability for generating control software for production systems is still unexplored. Therefore, this study explores the generation of Structured Text (ST) according to IEC-61131-3 by different LLMs. We selected 21 coding examples that are representative of PLC programming and developed an approach for comparing the outputs of different LLMs using metrics for testing generated code (CodeBERTScore, pass@k, generation time). The strategies for prompt optimization that were developed as part of this work can be directly used for improved ST generation. Our results show that, at the time of the study, ChatGPT-4 had the highest reliability in generating syntactically correct ST code that expresses the desired functionality.","1946-0759","979-8-3503-6123-0","10.1109/ETFA61755.2024.10711113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711113","Code Generation;Structured Text;IEC 61131;Large Language Model","Production systems;Codes;Large language models;Programming;Software;Time factors;Reliability;Prompt engineering;Optimization;Testing","","1","","27","IEEE","16 Oct 2024","","","IEEE","IEEE Conferences"
"Penetration Testing Overview-Opportunities and Ethical Considerations: Literature Notes","M. Asassfeh; G. Samara; A. A. Zaid; D. A. Laila; S. Al-Anzi; A. Alqammaz; A. Al Smadi; A. Al-Shaikh; M. R. Al-Mousa","Department of Cybersecurity, Zarqa University, Zarqa, Jordan; Department of Computer Science, Zarqa University, Zarqa, Jordan; Department of Cybersecurity, Zarqa University, Zarqa, Jordan; Department of Cyber Security, Faculty of Information Technology, Zarqa Technical Intermediate College, Zarqa, Jordan; Department of Cybersecurity, Zarqa University, Zarqa, Jordan; Department of Cybersecurity, Zarqa University, Zarqa, Jordan; Department of Data Science and AI, Zarqa University, Zarqa, Jordan; Department of Cybersecurity, Zarqa University, Zarqa, Jordan; Department of Cybersecurity, Zarqa University, Zarqa, Jordan",2024 International Jordanian Cybersecurity Conference (IJCC),"21 Jan 2025","2024","","","131","135","The paper relates to how Large Language Models and penetration testing practices interlink, providing a critical outlook on the potential of LLMs, along with their ethical ramifications. It performs an overview of the state-of-art in LLM-driven penetration testing through the analysis of three different studies. The first introduces PENTESTGPT, an LLM specifically designed for the purpose of penetration testing. This is indeed an LLM that proves to be quite efficient in performing tasks and giving logical reasoning. This study, however, also underlines several challenges on the narrow scope and generalizability of this model and questions its greater applicability. The second involves research into LLMs as AI sparring partners for the simulation of real-world cyberattacks that target vulnerabilities in systems. In the interest of finding flaws in security, the study exposes some strong ethical concerns-particularly on adversarial uses of AI in that direction. This third study shall explore the role of penetration testing in cloud security frameworks and help indicate how such practice is indispensable in maintaining security in cloud infrastructures. The paper further outlines the shared responsibility that exists in the maintenance of security between the cloud service providers and users besides highlighting what LLMs can do in this area. Aggregately, these studies help to outline the transformative power of LLMs in improving pen-testing. Yet, they also at the same time require great attention to ethical considerations and further research to make sure that deployment of LLMs really is responsible within the evolving cybersecurity landscape.","","979-8-3315-1846-2","10.1109/IJCC64742.2024.10847295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10847295","Penetration Testing;Large Language Models (LLMs);PENTESTGPT;AI Sparring Partner;Cloud Security;Ethical Considerations","Ethics;Cloud computing security;Large language models;Cognition;Maintenance;Computer crime;Standards;Penetration testing","","2","","30","IEEE","21 Jan 2025","","","IEEE","IEEE Conferences"
"Leveraging LLMs for Legacy Code Modernization: Evaluation of LLM-Generated Documentation","C. Diggs; M. Doyle; A. Madan; E. O. Scott; E. Escamilla; J. Zimmer; N. Nekoo; P. Ursino; M. Bartholf; Z. Robin; A. Patel; C. Glasz; W. Macke; P. Kirk; J. Phillips; A. Sridharan; D. Wendt; S. Rosen; N. Naik; J. F. Brunelle; S. Thaker","The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA; The MITRE Corporation, McLean, VA",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","177","184","Legacy software systems, written in outdated languages like MUMPS and mainframe assembly, pose challenges in efficiency, maintenance, staffing, and security. While LLMs offer promise for modernizing these systems, their ability to understand legacy languages is largely unknown. This paper investigates the utilization of LLMs to generate documentation for legacy code using two datasets: an electronic health records (EHR) system in MUMPS and an open-source application in IBM mainframe Assembly Language Code (ALC). We propose a prompting strategy for generating line-wise code comments and a rubric to evaluate their completeness, readability, usefulness, and hallucination. Our study assesses the correlation between human evaluations and automated metrics, such as code complexity and reference-based metrics. We find that LLM-generated comments for MUMPS and ALC are generally hallucination-free, complete, readable, and useful compared to ground-truth comments, though ALC poses challenges. However, no automated metrics strongly correlate with comment quality to predict or measure LLM performance. Our findings highlight the limitations of current automated measures and the need for better evaluation metrics for LLM-generated documentation in legacy systems.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028228","artificial intelligence;large language models;legacy software;software modernization;software maintenance","Measurement;Codes;Correlation;Automation;Large language models;Current measurement;Documentation;Complexity theory;Assembly;Mainframes","","","","40","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"DriveRP: RAG and Prompt Engineering Embodied Parallel Driving in Cyber-Physical-Social Spaces","J. Huang; H. Ma; T. Zhang; F. Lin; S. Ma; X. Wang; F. -Y. Wang","Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; School of Artificial Intelligence, Anhui University, Hefei, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; School of Artificial Intelligence, Anhui University, Hefei, China; State Key Laboratory for Management and Control of Complex Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China",2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI),"12 Dec 2024","2024","","","547","553","In recent years, numerous technological advancements in Artificial Generative Intelligences (AGIs) have demonstrated significant potential to transform the intelligence acquisition mechanisms in connected autonomous vehicles (CAVs). Integrating technologies like ChatGPT into CAVs can enhance human-machine interactions. However, the emergence of such new traffic entities may introduce unforeseen hallucinations and complex risks that surpass our current understanding. To address these challenges, Retrieval-Augmented Generation (RAG) and prompt engineering technologies are being explored to enhance the reliability and safety of autonomous driving systems. RAG retrieves relevant contextual information, such as driving experiences and real-time road network status, from external databases to ensure that foundation models have access to accurate and timely data for informed decision-making. Prompt engineering optimizes the performance of large language models in autonomous driving systems by designing and refining prompts that guide the models’ responses, thereby improving their relevance and accuracy in various driving scenarios. Together, these technologies enhance the robustness and trustworthiness of autonomous driving systems. This paper proposes DriveRP, a framework that integrates RAG and prompt engineering within the Descriptive-Predictive-Prescriptive Intelligence framework of Parallel Driving theory. DriveRP aims to enhance the safety and interpretability of autonomous vehicle trajectory planning, decision-making, and motion control, ultimately achieving the ""6S"" goals. Grounded in Digital Twins and Metaverse-embodied parallel driving theory, DriveRP provides the infrastructure and foundational intelligence for parallel driving with Multi-modal Large Lange Models(MLLMs). Additionally, the paper discusses future trends and potential research directions, focusing on the ""6S"" goals of parallel driving: Smart, Safe, Secure, Sensitive, Sustainable, and Serviceable.","","979-8-3503-4925-2","10.1109/DTPI61353.2024.10778684","National Natural Science Foundation of China; Science and Technology Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778684","Intelligent Transportation Systems;Autonomous Vehicles;Large Language Models;Metaverse;Digital Twin;Parallel Driving;Retrieval-Augmented Generation","Accuracy;Trajectory planning;Decision making;Transportation;Transforms;Safety;Digital twins;Prompt engineering;Motion control;Autonomous vehicles","","","","39","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"Ensuring security and data integrity in Multi Micro-Agent System Middleware with Blockchain Technology","A. N. Cherif; Y. Achir; M. Youssfi; M. Elgarej; O. Bouattane","2IACS Laboratory ENSET, Hassan II University, Mohammedia, Morocco; 2IACS Laboratory ENSET, Hassan II University, Mohammedia, Morocco; 2IACS Laboratory ENSET, Hassan II University, Mohammedia, Morocco; 2IACS Laboratory ENSET, Hassan II University, Mohammedia, Morocco; EESS Laboratory ENSET, Hassan II University, Mohammedia, Morocco","2023 3rd International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)","21 Jun 2023","2023","","","1","6","Multi-agent systems offer the advantage of performing tasks in a distributed and decentralized manner, thereby increasing efficiency and effectiveness. However, building these systems also presents challenges in terms of communication, security, and data integrity. Blockchain technology has the potential to address these challenges and to revolutionize the way that data is stored and shared, by providing a tamper-evident log of events in event-driven distributed multi-agent systems. In this paper, we propose a blockchain-based approach for event-sourcing in such systems, which allows for the reliable and transparent recording of events and state changes. Our approach leverages the decentralized nature of blockchains to provide a tamper-resistant event log, enabling agents to verify the integrity of the data they rely on.","","979-8-3503-9836-6","10.1109/IRASET57153.2023.10152950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152950","Data integrity;Event-driven;CQRS;multi-agent systems;Blockchain;Distributed systems","Supply chain management;Data integrity;Scalability;Recording;Consensus protocol;Security;Reliability","","","","14","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"Early Design of a Conversational AI Development Platform for Middle Schoolers","A. Kumar; X. Tian; M. Celepkolu; M. Israel; K. E. Boyer","University of Florida, Gainesville, United States; University of Florida, Gainesville, United States; University of Florida, Gainesville, United States; University of Florida, Gainesville, United States; University of Florida, Gainesville, United States",2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC),"17 Aug 2022","2022","","","1","3","More young people are interacting with smart conversational agents such as Alexa and Google Assistant. These platforms are extensible, providing, in principle, a compelling opportunity for young users to create and tinker with their own conversational agents. However, to date the interfaces for conversational app development are adult-focused. This paper presents the early design process for AMBY (AI Made by You), which we are building to empower young learners to create their own conversational agents. We first conducted a contextual inquiry with 14 middle school students (aged 11-13) in an AI summer camp, followed by two other usability studies. The system design has been refined after each study. Key features of AMBY include a visual dialogue management panel, testing panel with a diverse avatar, and a voice input modality. AMBY is designed to serve as a pedagogically-robust resource for K-12 AI education and as an engaging and creative way for middle schoolers to explore AI.","1943-6106","978-1-6654-4214-5","10.1109/VL/HCC53370.2022.9833129","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833129","K-12;AI education;conversational AI;conversational agent;AI literacy;interface design;middle school","Visualization;Virtual assistants;Education;Buildings;Internet;Iterative methods;Artificial intelligence","","2","","9","IEEE","17 Aug 2022","","","IEEE","IEEE Conferences"
"Front-running Attack Detection in Blockchain using Conditional Packing Generative AI","S. Henna; M. Amjath","Department of Computing, Atlantic Technological University, Donegal, Ireland; Department of Computing, Atlantic Technological University, Donegal, Ireland",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5585","5591","Detecting front-running attacks in Ethereum blockchain transactions is crucial for maintaining security and integrity within decentralized ecosystems. However, existing models struggle to accurately model the complex distributions inherent in tabular data, particularly in the presence of class imbalance and mode collapse. This paper leverages the potentials of Conditional Tabular Generative Adversarial Networks and PacGAN, called a Conditional Packing GAN (cPacGAN), to address these challenges. cPacGAN effectively generates synthetic data that closely mimics the distribution of real transactions, thereby augmenting the dataset and improving the performance of front-running attack detection. PacGAN mitigates mode collapse by incorporating packed samples in the discriminator, improving the diversity of generated samples and improving the stability of the training process. Through experimental evaluations of a real-world Ethereum transactions dataset, cPacGAN demonstrates improved performance across all selected machine learning classifiers, particularly augmenting the effectiveness of Tabular Neural Networks (TabNet).","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825410","Generative AI for Blockchain;Front-running Attack Detection;Security in Cryptocurrency","Training;Generative AI;Biological system modeling;Neural networks;Generative adversarial networks;Data models;Stability analysis;Blockchains;Security;Synthetic data","","1","","16","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Extending the Frontier of ChatGPT: Code Generation and Debugging","F. A. Sakib; S. H. Khan; A. H. M. R. Karim","Department of Computer Science, George Mason University, Fairfax, USA; Department of Computer Science, George Mason University, Fairfax, USA; Department of Computer Science, George Mason University, Fairfax, USA","2024 International Conference on Electrical, Computer and Energy Technologies (ICECET","8 Oct 2024","2024","","","1","6","Large language models (LLMs), trained on vast corpora, have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents. Among these LLMs, ChatGPT has pioneered a new phase in AI, adeptly handling varied tasks from writing essays and biographies to solving complex mathematical problems. However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness. We delve into the efficacy of ChatGPT (GPT-4) in generating correct code for programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity. A custom dataset containing problems of various topics and difficulties from Leetcode has been used. The research reveals an overall success rate of 71.875%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode. It exhibits strength in structured problems and shows a linear correlation between its success rate and problem acceptance rates. However, it struggles to improve incorrect solutions based on feedback, pointing to potential shortcomings in debugging tasks. These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.","","979-8-3503-9591-4","10.1109/ICECET61485.2024.10698405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698405","ChatGPT;Code Generation;Programming Problems;Debugging","Technological innovation;Codes;Correlation;Runtime;Large language models;Memory management;Debugging;Programming;Chatbots;Encoding","","14","","40","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System","M. A. Arefeen; B. Debnath; M. Y. Sarwar Uddin; S. Chakradhar",NEC Laboratories America; NEC Laboratories America; University of Missouri-Kansas City; NEC Laboratories America,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"27 Sep 2024","2024","","","2266","2274","Retrieval-augmented generation (RAG) is used in natural language processing (NLP) to provide query-relevant information in enterprise documents to large language models (LLMs). Such enterprise context enables the LLMs to generate more informed and accurate responses. When enterprise data is primarily videos, AI models like vision language models (VLMs) are necessary to convert information in videos into text. While essential, this conversion is a bottleneck, especially for large corpus of videos. It delays the timely use of enterprise videos to generate useful responses.We propose ViTA, a novel method that leverages two unique characteristics of VLMs to expedite the conversion process. As VLMs output more text tokens, they incur higher latency. In addition, large (heavyweight) VLMs can extract intricate details from images and videos, but they incur much higher latency per output token when compared to smaller (lightweight) VLMs that may miss details. To expedite conversion, ViTA first employs a lightweight VLM to quickly understand the gist or overview of an image or a video clip, and directs a heavyweight VLM (through prompt engineering) to extract additional details by using only a few (preset number of) output tokens. Our experimental results show that ViTA expedites the conversion time by as much as 43%, without compromising the accuracy of responses when compared to a baseline system that only uses a heavyweight VLM.","2160-7516","979-8-3503-6547-4","10.1109/CVPRW63382.2024.00232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678192","Video Analytics;Retrieval Augmented Generation (RAG);Natural Language Processing;Vision Language Models (VLMs);Large Language Models (LLMs)","Computer vision;Accuracy;Large language models;Conferences;Natural language processing;Data models;Pattern recognition","","3","","32","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"Cyberattacks on Large Language Models - Attack Detection and Architecture Adaptability","S. Alla; A. S. Sichani","Electrical Engineering and Computer Science Department, University of Missouri, Columbia, MO; Electrical Engineering and Computer Science Department, University of Missouri, Columbia, MO",SoutheastCon 2025,"25 Apr 2025","2025","","","143","148","Large Language Models (LLMs) like GPT and $PaLM$ have transformed natural language processing, enabling advancements in text generation, language translation, and conversational AI. However, their increasing adoption has exposed critical vulnerabilities, making them susceptible to cyberattacks such as prompt injection, data poisoning, and Distributed Denial of Service (DDoS) attacks. These threats compromise the security, reliability, and integrity of LLM applications in real-world scenarios. To mitigate these challenges, advanced defense mechanisms such as attack detectors and attack libraries play a crucial role. Attack detectors analyze input patterns and monitor model responses to identify anomalies and potential security breaches. These systems rely on attack libraries that act as structured repositories of predefined attack patterns, known exploits, and evolving threats. The attack library functions as a dynamic lookup table, enabling real-time vulnerability detection and rapid response to existing and novel threats. By integrating systematic vulnerability detection techniques and structured attack analysis, these mechanisms strengthen LLM security. Attack detectors and libraries provide a comprehensive approach to identifying, classifying, and mitigating security risks. This framework ensures the safe deployment of LLMs in various industries, strengthening their resilience and minimizing exposure to emerging cyber threats in sensitive applications.","1558-058X","979-8-3315-0484-7","10.1109/SoutheastCon56624.2025.10971722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971722","Cyberattacks;LLM;Security;Threats;GPT","Translation;Systematics;Large language models;Detectors;Libraries;Real-time systems;Security;Reliability;Computer crime;Resilience","","","","19","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Advancing Android Malware Detection with Deep Learning & LLMs in Big Data Ecosystem: A Forward Vision","J. Rodriguez-Cardenas; N. Sakib; S. Sneha","Department of Information Technology, Kennesaw State University, Marietta, United States; Department of Information Technology, Kennesaw State University, Marietta, United States; Department of Information Systems, Kennesaw State University, Kennesaw, United States",2025 IEEE International Conference on Big Data and Smart Computing (BigComp),"31 Mar 2025","2025","","","221","227","As the Android operating system dominates the mobile ecosystem, its open-source nature has made it increasingly vulnerable to sophisticated malware attacks. Traditional security techniques, such as static and dynamic code analysis, often fail to detect modern malware due to issues like code obfuscation and limited scalability. This paper explores the limitations of these methods and suggests incorporating advanced Deep Neural Networks as more effective solutions for malware detection in Android environments. Additionally, the integration of Large Language Models (LLMs) offers new possibilities for understanding complex malware behaviors and patterns. We also discuss Android’s security architecture, highlighting key vulnerabilities and attack surfaces. Through this study, we suggest a forward-thinking approach to enhancing Android malware detection using machine learning models, envisioning the future of security in the Android ecosystem as big data and artificial intelligence technologies continue to evolve.","2375-9356","979-8-3315-2902-4","10.1109/BigComp64353.2025.00051","Kennesaw State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10936914","Android Malware Detection;Deep Learning;Large Language Models;Big Data Security;Cybersecurity","Deep learning;Operating systems;Large language models;Biological system modeling;Scalability;Ecosystems;Big Data;Malware;Data models;Security","","","","24","IEEE","31 Mar 2025","","","IEEE","IEEE Conferences"
"Pre-made Empowering Artificial Intelligence and ChatGPT: The Growing Importance of Human AI-Experts","M. Virvou; G. A. Tsihrintzis","Department of Informatics, University of Piraeus, Piraeus, Greece; Department of Informatics, University of Piraeus, Piraeus, Greece","2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)","15 Dec 2023","2023","","","1","8","This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts","","979-8-3503-1806-7","10.1109/IISA59645.2023.10345880","European Regional Development Fund of the European Union(grant numbers:T2EDK-02654); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10345880","Responsible Artificial Intelligence;Human-centered AI;AI-Empowered Software Engineering;ChatGPT;Large Language Models (LLMs);Generative AI;e-learning Requirements Engineering in AI;Artificial Intelligence;Intelligent Information Systems","Industries;Ethics;Medical services;User interfaces;Chatbots;Software;Requirements engineering","","12","","43","IEEE","15 Dec 2023","","","IEEE","IEEE Conferences"
"The Current State of Generative AI Prompt Framework Design for Enhancing Utility in Organizational Decision-Making","H. Sansanee; S. Kiattisin","Faculty of Engineering, Technology of Information System Management Division, Mahidol University, Nakonpathom, Thailand; Faculty of Engineering, Technology of Information System Management Division, Mahidol University, Nakonpathom, Thailand",2024 5th Technology Innovation Management and Engineering Science International Conference (TIMES-iCON),"15 Aug 2024","2024","","","1","6","This paper proposes a novel framework that combines deep learning algorithms with reinforcement learning techniques to optimize decision-making processes in dynamic and complex environments. The framework aims to improve the accuracy, efficiency, and adaptability of AI systems in supporting strategic decision-making for organizations across various industries. Key features of generative AI prompt frameworks include the development of prompt engineering techniques, addressing technological challenges, and applying prompt engineering across various domains like entrepreneurship, art, and science. The AI PROMPT framework provides guidelines for text-to-text prompt engineering. This paper provides prompt engineering techniques and indicators to improve the overall quality and effectiveness of utility in organizational decision-making.","","979-8-3503-6260-2","10.1109/TIMES-iCON61890.2024.10630713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630713","Prompt;AI;Generative AI;Decision making;Open AI;ChatGPT","Industries;Deep learning;Heuristic algorithms;Decision making;Entrepreneurship;Reinforcement learning;Organizations","","2","","15","IEEE","15 Aug 2024","","","IEEE","IEEE Conferences"
"LLMSmartSec: Smart Contract Security Auditing with LLM and Annotated Control Flow Graph","V. Mothukuri; R. M. Parizi; J. L. Massa","Decentralized Science Lab, College of Computing and Software Engineering, Kennesaw State University, Marietta, GA, USA; Decentralized Science Lab, College of Computing and Software Engineering, Kennesaw State University, Marietta, GA, USA; Decentralized Science Lab, College of Computing and Software Engineering, Kennesaw State University, Marietta, GA, USA",2024 IEEE International Conference on Blockchain (Blockchain),"18 Sep 2024","2024","","","434","441","Historically, the complexity of identifying vulnerabilities in smart contracts required human-intensive audits to supplement imprecise automated code scans. The growing smart contract market highlights the urgency for effective automated security auditing. Furthermore, new AI-powered coding assistants can introduce vulnerabilities that evade engineers, and autonomous AI can now write 100% code changes without human oversight. This research introduces LLMSmartSec, a new approach that accurately identifies and fixes smart contract vulnerabilities, leveraging the strengths of machine intelligence to operate at high speed, scale, and without fatigue. To develop LLMSmartSec, we first fine-tuned Open AI GPT-4 to understand Solidity, the programming language of Ethereum blockchain, and to micro-analyze smart contracts holistically from the viewpoints of the developer (LLMDev), auditor (LLMAudit), and ethical hacker (LLMeHack) and to identify vulnerabilities and generate code fixes for them. We used GPT-4 again to store the smart contract code in a Control Flow Graph (CFG) annotated with the code vulnerabilities and their fixes. Finally, we trained an LLMGraphAgent with open-source LLMs on the annotated CFG so that it can run locally, identifying vulnerabilities and fixes without costly calls to GPT-4. This research enhances the security of blockchain projects in Web3 by supplementing a costly human bottleneck with a low-cost automated security tool based upon the innovative creation of an annotated CFG with GPT-4.","2834-9946","979-8-3503-5159-0","10.1109/Blockchain62396.2024.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664261","Blockchain;Smart Contracts;Security Auditing;AI;Large Language Models (LLMs);OpenAI Assistants;Control Flow Graphs (CFGs)","Ethics;Codes;Smart contracts;Fatigue;Blockchains;Security;Flow graphs","","2","","59","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"SPADE 3: Supporting the New Generation of Multi-Agent Systems","J. Palanca; A. Terrasa; V. Julian; C. Carrascosa","Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Politècnica de València, València, Spain; Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Politècnica de València, València, Spain; Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Politècnica de València, València, Spain; Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Politècnica de València, València, Spain",IEEE Access,"13 Oct 2020","2020","8","","182537","182549","Although intelligent agent-based systems have existed for several years, the progression in terms of real applications or their integration in the industry have not yet reached the expected levels. During the last two decades, many agent platforms have appeared with the aim of simplifying the development of multi-agent systems. Some of these platforms have been designed for general purposes, while others have been oriented towards specific domains. However, the lack of standards and the complexity associated with supporting such systems, among other difficulties, have hampered their generalised use. This article looks in depth at the current situation of existing agent platforms, trying to analyse their current shortcomings and their expected needs in the near future. The goal of the paper is to identify possible lines of work and some of the most crucial aspects to be considered in order to popularize the application of agent technology as a dynamic and flexible solution to current problems. Moreover, the paper presents SPADE 3, a new version of the SPADE middleware, which has been totally redesigned in order to conform to the identified challenges. Finally, a case study is proposed to illustrate how SPADE 3 is able to fulfill these challenges.","2169-3536","","10.1109/ACCESS.2020.3027357","Spanish Government(grant numbers:RTI2018-095390-B-C31-AR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207929","Multi-agent systems;intelligent agents;middleware","Multi-agent systems;Proposals;Middleware;Tools;Internet of Things;Buildings;Solid modeling","","61","","40","CCBY","28 Sep 2020","","","IEEE","IEEE Journals"
"How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations","E. A. AlOmar; A. Venkatakrishnan; M. W. Mkaouer; C. D. Newman; A. Ouni","Stevens Institute of Technology, Hoboken, New Jersey, USA; Rochester Institute of Technology, Rochester, New York, USA; University of Michigan-Flint, Flint, Michigan, USA; Rochester Institute of Technology, Rochester, New York, USA; ETS Montreal, University of Quebec, Montreal, Quebec, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","202","206","Large Language Models (LLMs), like ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with ChatGPT. In this paper, our goal is to explore conversations between developers and ChatGPT related to refactoring to better understand how developers identify areas for improvement in code and how ChatGPT addresses developers’ needs. Our approach relies on text mining refactoring-related conversations from 17,913 ChatGPT prompts and responses, and investigating developers’ explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while ChatGPT typically includes the refactoring intention; and (3) various learning settings when prompting ChatGPT in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models.CCS CONCEPTS• Software Engineering → Software Quality; Refactoring.","2574-3864","979-8-4007-0587-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555806","Refactoring documentation;ChatGPT;mining software repositories","Text mining;Codes;Reviews;Oral communication;Software quality;Documentation;Chatbots","","5","","45","","18 Jun 2024","","","IEEE","IEEE Conferences"
"MAS: Mobile-Apps Assessment and Analysis System","C. -W. Tien; T. -Y. Huang; T. -C. Huang; W. -H. Chung; S. -Y. Kuo","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan R.O.C.; CyberTrust Technology Institute, Institute for Information Industry, Taipei, Taiwan, R.O.C.; CyberTrust Technology Institute, Institute for Information Industry, Taipei, Taiwan, R.O.C.; CyberTrust Technology Institute, Institute for Information Industry, Taipei, Taiwan, R.O.C.; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan R.O.C.",2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W),"31 Aug 2017","2017","","","145","148","Mobile apps are widely adopted in daily life, and contain increasing security flaws. Many regulatory agencies and organizations have announced security guidelines for app development. However, most security guidelines involving technicality and compliance with this requirement is not easily feasible. Thus, we propose Mobile Apps Assessment and Analysis System (MAS), an automatic security validation system to improve guideline compliance. MAS combines static and dynamic analysis techniques, which can be used to verify whether android apps meet the security guideline requirements. We implemented MAS in practice and verified 143 real-world apps produced by the Taiwan government. Besides, we also validated 15,000 popular apps collected from Google Play Store produced in three countries. We found that most apps contain at least three security issues. Finally, we summarize the results and list the most common security flaws for consideration in further app development.","2325-6664","978-1-5386-2272-8","10.1109/DSN-W.2017.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023720","Android app;Mobile Security;Static Analysis;Dynamic Analysis;Security Validation;Trusted computing","Mobile communication;Guidelines;Google;Government;Encryption;Mobile applications","","3","","17","IEEE","31 Aug 2017","","","IEEE","IEEE Conferences"
"Hex2Sign: Automatic IDS Signature Generation from Hexadecimal Data using LLMs","P. Balasubramanian; T. Ali; M. Salmani; D. KhoshKholgh; P. Kostakos","Faculty of Information Technology and Electrical Engineering Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering Center for Ubiquitous Computing, University of Oulu, Oulu, Finland; Faculty of Information Technology and Electrical Engineering Center for Ubiquitous Computing, University of Oulu, Oulu, Finland",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","4524","4532","Despite the growing utilization of large language models (LLMs) in cyber defense operations, their integration within intrusion detection systems (IDS) remains substantially underexplored. This paper proposes a novel approach to generating human-readable IDS signatures by fine-tuning LLMs on hexadecimal data. In our experimental framework, we deploy honeypots to capture malicious network traffic in real-world conditions, generating packet capture (PCAP) files accompanied by text-based alerts and Suricata signatures. The collected hexadecimal data, derived from actual attack vectors, serves as the training corpus for multiple generative and classification models, which are fine-tuned for optimal performance in generating human-readable IDS alerts. According to the results, generative model GPT-3-Davinci-002 excelled across metrics with BERTscore over 96%, while RoBERTa base achieved high accuracy of 96% among classifiers. These findings enhance our understanding that foundational models can improve hexadecimal data processing for cybersecurity. Our conclusions emphasize the potential of advanced generative-AI models in automating dynamic Suricata rule generation, thus enhancing IDS efficiency and accuracy. Moreover, this paper proposes an AI-powered IDS system for securing network environments that can significantly mitigate the risks associated with diverse and widespread devices. By integrating LLMs into security frameworks, this system offers a robust defense mechanism that dynamically adapts to emerging threats, thus enhancing IDS efficiency and accuracy in handling big data challenges.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825710","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825710","LLMs;GPT-3;IDS;Suricata;Cybersecurity;Hexadecimal;Generative AI;Classifiers","Training;Measurement;Accuracy;Large language models;Intrusion detection;Telecommunication traffic;Big Data;Data processing;Data models;Vectors","","","","27","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Utilizing Process Models in the Requirements Engineering Process Through Model2Text Transformation","N. Klievtsova; J. Mangler; T. Kampik; S. Rinderle-Ma","Technical University of Munich, TUM School of Computation, Information and Technology, Garching, Germany; Technical University of Munich, TUM School of Computation, Information and Technology, Garching, Germany; SAP Signavio, Berlin, Germany; Technical University of Munich, TUM School of Computation, Information and Technology, Garching, Germany",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","205","217","With the advent of large language models (LLMs), requirements engineers have gained a powerful natural language processing tool to analyze, query, and validate a wide variety of textual artifacts, thus potentially supporting the whole re-quirements engineering process from requirements elicitation to management. However, the input for the requirements engineering process often encompasses a variety of potential information sources in various formats, especially graphical models such as process models. Hence, this work aims to contribute to the state of the art by assessing the feasibility of utilizing graphical process models and their textual representations in the requirements engineering process. In particular, we focus on the extraction of textual process descriptions from process models as i) input for the requirements engineering process and ii) documentation as the result of process-oriented requirements engineering. To this end, we explore, quantify, and compare traditional deterministic and LLM-based extraction methods where the latter includes GPT3, GPT3.5, GPT4, and LLAMA. The evaluation assesses output quality and information loss based on one data set. The results indicate that LLMs produce human-like process descriptions based on the predefined patterns, but apparently lack true comprehension of the process models.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628509","AI4RE;Process Models;Process Descriptions;Large Language Models","Graphical models;Large language models;Documentation;Natural language processing;Requirements engineering","","","","47","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Enhancing Database Encryption: Adaptive Measures for Digital Assets Against LLMs-Based Reverse Engineering","K. Zhou; J. Qiu; Y. Wang; X. Ye","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Academy of Military Science, Beijing, China; Tsinghua University, Beijing, China",2024 Annual Computer Security Applications Conference (ACSAC),"18 Mar 2025","2024","","","1","14","With the development of large language models (LLMs) technology, generative AI (GAI) has significantly lowered the barriers to software reverse engineering attacks. Traditional database system security controls, face new challenges when confronted with the powerful analytical capabilities of GAI. This paper provides a detailed demonstration of the reverse engineering of database cryptographic functions using GAI tools. It finds that existing encryption mechanisms in embedded DBSs need new approaches to prevent internal data attacks. The main contributions are in two aspects. First, by emulating human reverse engineering and encrypted data theft behaviors, it demonstrates how to perform software reverse engineering using GAI tools. It reveals that LLMs can accelerate analysts in obtaining the keys needed for decryption or in identifying cache extraction points for decrypted data, enabling attackers to access sensitive information. Second, to counteract the strong code generation and analytical capabilities of LLMs, the paper proposes a solution based on random data blocks and timestamp-based decryption mechanisms. This approach aims to increase the cost for attackers using GAI tools to perform software reverse engineering and exploit potential vulnerabilities in database systems.","2576-9103","979-8-3315-2088-5","10.1109/ACSAC63791.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917597","Software Vulnerability Analysis;Large Language Models;Software Reverse Engineering;Database Encryption","Costs;Generative AI;Large language models;Reverse engineering;Software;Database systems;Encryption;Software measurement;Data mining;Faces","","","","27","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Large Language Model in Financial Regulatory Interpretation","Z. Cao; Z. Feinstein","School of Business, Stevens Institute of Technology, Hoboken, New Jersey, USA; School of Business, Stevens Institute of Technology, Hoboken, New Jersey, USA",2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr),"10 Dec 2024","2024","","","1","7","This study explores the innovative use of Large Language Models (LLMs) as analytical tools for interpreting complex financial regulations. The primary objective is to design effective prompts that guide LLMs in distilling verbose and intricate regulatory texts, such as the Basel III capital requirement regulations, into a concise mathematical framework that can be subsequently translated into actionable code. This novel approach aims to streamline the implementation of regulatory mandates within the financial reporting and risk management systems of global banking institutions. A case study was conducted to assess the performance of various LLMs, demonstrating that GPT-4 outperforms other models in processing and collecting necessary information, as well as executing mathematical calculations. The case study utilized numerical simulations with asset holdings - including fixed income, equities, currency pairs, and commodities - to demonstrate how LLMs can effectively implement the Basel III capital adequacy requirements.","2640-7701","979-8-3503-5483-6","10.1109/CIFEr62890.2024.10772991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772991","Large Language Models;Prompt Engineering;LLMs in Finance;Basel III;Minimum Capital Requirements;LLM Ethics","Text analysis;Systematics;Large language models;Numerical simulation;Regulation;Robustness;Risk management;Stress;Testing;Synthetic data","","3","","21","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Securing AI Inference in the Cloud: Is CPU-GPU Confidential Computing Ready?","A. Mohan; M. Ye; H. Franke; M. Srivatsa; Z. Liu; N. M. Gonzalez","IBM T.J. Watson Research Center, Yorktown Heights, New York, USA; IBM T.J. Watson Research Center, Yorktown Heights, New York, USA; IBM T.J. Watson Research Center, Yorktown Heights, New York, USA; IBM T.J. Watson Research Center, Yorktown Heights, New York, USA; IBM T.J. Watson Research Center, Yorktown Heights, New York, USA; IBM T.J. Watson Research Center, Yorktown Heights, New York, USA",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","164","175","Many applications have been offloaded onto cloud environments to achieve higher agility, access to more powerful computational resources, and obtain better infrastructure management. Although cloud environments provide solid security solutions, users with highly sensitive data or regulatory compliance requirements, such as HIPAA (Health Insurance Portability and Accountability Act) and GDPR (General Data Protection Regulation), still hesitate to move such application domains to the cloud. To address these concerns, cloud service providers have started to offer solutions to protect data confidentiality and integrity through trusted execution environments (TEEs). While so far these were limited to CPU TEEs only, NVIDIA's Hopper architecture has shifted the landscape by enabling confidential computing features essential to protecting confidentiality and integrity for real-world applications offloaded to GPUs, such as large language models (LLMs). However, there lacks a sufficient study on how much performance overhead confidential computing introduces in a TEE comprised of a CPU-GPU configuration. In this paper we evaluate a confidential computing environment comprised of an Intel TDX system and NVIDIA H100 GPUs through various micro benchmarks and real workloads including BERT, LLaMA, and Granite large language models and provide discussions on the overhead incurred by confidential computing when GPUs are utilized. We show that while LLMs are sensitive to the model types and batch sizes, when larger models with pipelined processing are deployed, the performance of LLM inference in CPU-GPU TEEs can be close to par with their non-confidential setups.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643934","confidential computing;cloud security;cloud computing;foundation models;large language models;high performance computing","Cloud computing;Computational modeling;Large language models;Switches;Benchmark testing;Programming;Solids","","3","","66","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"A Survey of Machine Unlearning in Generative AI Models: Methods, Applications, Security, and Challenges","A. Huang; Z. Cai; Z. Xiong","Department of Computer Science, University of Nevada Las Vegas, Las Vegas, NV, USA; Department of Computer Science, Georgia State University, Atlanta, GA, USA; Department of Computer Science, University of Nevada Las Vegas, Las Vegas, NV, USA",IEEE Internet of Things Journal,"8 Aug 2025","2025","12","16","32563","32580","Generative AI has flourished over the past decade, with generative models advancing in both the industrial and academic sectors. Given various applications, some scenarios have seen the misuse of generative AI, particularly in the integration with the Internet of Things (IoT). IoT devices often handle personal and sensitive data, raising serious concerns about privacy leakage and security breaches when generating data. As a promising countermeasure, machine unlearning has emerged to solve the problems posed by these generative models by effectively removing specific concepts or sensitive information from trained models. In this survey, anchored in generative models, machine unlearning approaches are reviewed, categorized, and discussed comprehensively and systematically. Existing unlearning approaches are classified into gradient-based techniques, task vectors, knowledge distillation, data sharding, and reliable unlearning methods. Apart from previous works, this survey extends the review of attack methods that aim to exploit the vulnerability in generative models and assess the robustness of these unlearning methods. In addition, popular metrics and datasets in machine unlearning research are summarized and evaluated based on effectiveness, efficiency, and security. Finally, we shed light on the future directions of this emerging research topic by discussing applications, highlighting challenges, and exploring research frontiers for the current machine unlearning community and the new investigators to come.","2327-4662","","10.1109/JIOT.2025.3570989","National Science Foundation(grant numbers:2429960,2434899,2416872,2315596,2244219,2146497); Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant funded by the South Korea Government (MSIT)(grant numbers:RS-2024-00431388); Global Research Support Program in the Digital Field Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006878","Generative AI;machine unlearning;security and privacy;the Internet of Things (IoT)","Data models;Surveys;Security;Internet of Things;Generative AI;Law;Computational modeling;Training;Taxonomy;Generators","","","","152","IEEE","19 May 2025","","","IEEE","IEEE Journals"
"A Tool for Safe and Accurate IoT Automation Rule Generation Using Large Language Models","M. S. Islam; M. Kantarcioglu","Data Security Technologies, Richardson, Texas; Data Security Technologies and Virginia Tech, Blacksburg, VA",2025 IEEE Security and Privacy Workshops (SPW),"3 Jul 2025","2025","","","191","198","The proliferation of Internet of Things (IoT) devices has led to an increasing reliance on trigger-action rules for home automation. However, creating these rules in structured formats remains challenging for users who prefer natural language expressions. We present Ruleengine, a tool leveraging Large Language Models (LLMs) to translate natural language descriptionsinto valid IoT automation rules with platform specific format. Our approach introduces three key innovations: (1) a structured prompt engineering framework combining explicit output structuring and semantic constraint definition, (2) a RAG- based architecture enhanced with semantic search and contextual re-ranking for improved rule generation, and (3) a context-aware safety scoring mechanism that evaluates rules across trigger conditions and action implications. Through empirical evaluation across multiple LLMs using real-world datasets, we demonstrate substantial improvements in both syntactic and semantic validity of generated rules, while successfully identifying safety violations in generated rules. We conduct a comprehensive study on performance analysis across different LLMs, prompt engineering strategies, embedding techniques, and RAG configurations, revealing interesting trade-offs between accuracy, latency, and computational efficiency in automated rule generation.","2770-8411","979-8-3315-6643-2","10.1109/SPW67851.2025.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050808","IoT Automation Platform;Natural Language Processing;Security and Privacy","Privacy;Automation;Accuracy;Translation;Large language models;Computer architecture;Safety;Internet of Things;Security;Prompt engineering","","","","43","IEEE","3 Jul 2025","","","IEEE","IEEE Conferences"
"A Survey on Hybrid-CNN and LLMs for Intrusion Detection Systems: Recent IoT Datasets","S. Elouardi; A. Motii; M. Jouhari; A. Nasser Hassane Amadou; M. Hedabou","College of Computing, University Mohammed VI Polytechnic, Ben Guerir, Morocco; College of Computing, University Mohammed VI Polytechnic, Ben Guerir, Morocco; LSIA Laboratory, Moroccan School of Engineering Sciences (EMSI), Tangier, Morocco; College of Computing, University Mohammed VI Polytechnic, Ben Guerir, Morocco; College of Computing, University Mohammed VI Polytechnic, Ben Guerir, Morocco",IEEE Access,"5 Dec 2024","2024","12","","180009","180033","Recently, the growing popularity of the Internet of Things (IoT) presents a promising opportunity not only for the expansion of various home automation systems but also for diverse industrial applications. By leveraging these benefits, automation is being implemented in industries, leading to the Industrial Internet of Things (IIoT). Although IoT simplifies daily activities that benefit human operations, it poses significant security challenges that warrant attention. Consequently, implementing an Intrusion Detection System (IDS) is a vital and effective solution. IDS aims to address the security and privacy challenges by detecting various IoT attacks. Various IDS methodologies, including those using Machine Learning (ML), Deep Learning (DL) and Large Language Models (LLMs), are employed to identify intrusions within the data; however, improvements to the detection systems are still needed. A literature survey on IDS in the IoT domain is provided, focusing primarily on the recent approaches used in the field. The survey aims to evaluate the literature, identify current trends, retest these approaches on recent data, and highlight open problems and future directions.","2169-3536","","10.1109/ACCESS.2024.3506604","National Science Foundation(grant numbers:123456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767686","Internet of Things;intrusion detection;deep learning;large language models;machine learning;security;network","Internet of Things;Intrusion detection;Deep learning;Surveys;Security;Artificial intelligence;Market research;Reviews;Large language models;Focusing","","5","","84","CCBY","26 Nov 2024","","","IEEE","IEEE Journals"
"Can LLMs Understand Computer Networks? Towards a Virtual System Administrator","D. Donadel; F. Marchiori; L. Pajola; M. Conti","Department of Mathematics, University of Padova, Padua, Italy; Department of Mathematics, University of Padova, Padua, Italy; Department of Mathematics, University of Padova, Padua, Italy; Department of Mathematics, University of Padova, Padua, Italy",2024 IEEE 49th Conference on Local Computer Networks (LCN),"9 Sep 2024","2024","","","1","10","Recent advancements in Artificial Intelligence, and particularly Large Language Models (LLMs), offer promising prospects for aiding system administrators in managing the complexity of modern networks. However, despite this potential, a significant gap exists in the literature regarding the extent to which LLMs can understand computer networks. Without empirical evidence, system administrators might rely on these models without assurance of their efficacy in performing network-related tasks accurately.In this paper, we are the first to conduct an exhaustive study on LLMs’ comprehension of computer networks. We formulate several research questions to determine whether LLMs can provide correct answers when supplied with a network topology and questions on it. To assess them, we developed a thorough framework for evaluating LLMs’ capabilities in various network-related tasks. We evaluate our framework on multiple computer networks employing proprietary (e.g., GPT4) and open-source (e.g., Llama2) models. Our findings in general purpose LLMs using a zero-shot scenario demonstrate promising results, with the best model achieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy results in small and medium networks, while challenges persist in comprehending complex network topologies, particularly for open-source models. Moreover, we provide insight into how prompt engineering can enhance the accuracy of some tasks.","2832-1421","979-8-3503-8800-8","10.1109/LCN60385.2024.10639641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10639641","Large Language Models;Computer Networks;System Administrators","Accuracy;Costs;Computational modeling;Large language models;Computer networks;Topology;Prompt engineering","","7","","59","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Privacy-Preserving Healthcare Data Security Using Large Language Models and Adaptive Access Control","S. Yarram; N. Dasari; S. B. Seshagani; P. Ganguly","Independent Researcher, India; Navy Federal Credit Union, USA; Independent Researcher, USA; Widener University, USA",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0854","0860","Protecting privacy-sensitive data in the digital healthcare sector is imperative due to the escalating threat of data breaches, unauthorized access, and cyberattacks. Traditional security systems, including rule-based and signature-based approaches, may struggle to adapt to evolving circumstances and manage high-risk situations. This study presents a security solution that uses adaptive access control systems and a Large Language Model (LLM) to protect sensitive patient information and electronic health records (EHRs). This method ensures HIPAA and GDPR compliance through context-aware risk assessment, anomaly detection, and query sanitization procedures, enhancing data security. Using a dynamic threat modeling technique, the proposed system identifies zero-day vulnerabilities and malicious behavior through real-time healthcare data transmissions. Comparative analyses show that the proposed LLM-based methodology outperforms traditional security techniques regarding accuracy, recall, precision, and F1 score. This improves threat detection rates and reduces false positives. The results demonstrate the effectiveness of LLM-based security solutions in securing medical records and ensuring patient privacy, thus providing robust and flexible protection.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105296","Healthcare;LLM;Access Control;Data Security;Anomaly Detection","Access control;Threat modeling;Privacy;Data security;Large language models;Real-time systems;Risk management;Protection;Electronic medical records;Anomaly detection","","","","15","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"IDE Native, Foundation Model Based Agents for Software Refactoring","A. Bellur; F. Batole","University of Colorado, Boulder, Boulder, CO, USA; Tulane University, New Orleans, LA, USA",2025 IEEE/ACM Second IDE Workshop (IDE),"1 Jul 2025","2025","","","42","45","Foundation Models (FMs) have been trained on a massive amount of coding data and, at their best, are capable of looking at code like an expert software developer. This has led researchers to explore the possibility of FM-based agents for various software engineering tasks. These agents are capable of planning and executing complex tasks, such as bug repair, in the manner that an expert developer who is familiar with the codebase would. However, FMs often produce puzzling responses that are capable of resulting in buggy or vulnerable code. Inspired by recent work in agents for software engineering tasks, we discuss the idea of a FM-based refactoring agent, which is capable of scanning the entire codebase to suggest changes that improve the quality of the software system. Additionally, we posit that the IDEs (equipped with a massive number of static-analysis based checks), are the ideal place for these agents to live. In this paper, we discuss the challenges and issues related to building FM-based refactoring agents that live within the IDE.","","979-8-3315-0188-4","10.1109/IDE66625.2025.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052708","artificial-intelligence ide ai-agent refactoring software-engineering","Productivity;Codes;Frequency modulation;Foundation models;Shape;Maintenance engineering;Software systems;Encoding;Planning;Software engineering","","","","18","CCBY","1 Jul 2025","","","IEEE","IEEE Conferences"
"Adaptive Formation Control With Limited Information for Uncertain Nonlinear Multi-Agent Systems","X. Xu; Y. Liu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",IEEE Transactions on Automation Science and Engineering,"23 May 2025","2025","22","","15461","15472","The paper studies the distributed adaptive formation control in the setting of limited information transmitted between agents. The limited information, due to nonzero-kernel communication weight matrices between agents, has benefits in information security and privacy preserving, although as incomplete information, it challenges the formation control. Particularly, the multi-agent systems allow heterogeneous nonlinearities and serious uncertainties, making the formation control more difficult to achieve. In the paper, a fully-distributed adaptive control scheme is presented under appropriate conditions on the communication network, by incorporating two dynamic compensators and an auxiliary dynamic variable. The two compensators are employed to separately counteract the serious uncertainties and unavailable global information of the network resulting from unmatched states and weights. The auxiliary dynamic variable is introduced in the coordinate transformation to ensure the boundedness of the control, which cannot be derived by the traditional coordinate transformation. Two numerical examples are provided to show the effectiveness of the developed approach. Note to Practitioners—In real-world applications such as smart grids and smart transportation, communication networks play an essential role in cooperative operation, by which information is transmitted between agents. However, information transmitted in the communication network might be vulnerable to malicious eavesdropping due to the openness of networks. This could result in the leakage of sensitive information of agents, potentially disrupting the cooperative behavior and even the overall system security. In this paper, we consider adaptive formation control with limited information, instead of complete one. The limited information stems from nonzero-kernel weight matrices. In this case, even if the transmitted limited information is obtained by eavesdroppers, they still cannot reconstruct the full information of agents owning to the irreversibility of matrix weights, thus achieving information security and privacy preserving. In addition, the proposed distributed adaptive control strategy also can effectively counteract heterogeneous nonlinearities and serious uncertainties, which extends its application to a wide range of physical situations, such as cooperative engagement of multiple unmanned aerial and ground vehicles.","1558-3783","","10.1109/TASE.2025.3569646","National Natural Science Foundation of China(grant numbers:62033007); Fundamental Research Program of Shandong Province(grant numbers:ZR2023ZD37); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11003146","Limited information;heterogeneous nonlinear multi-agent systems;serious uncertainties;adaptive formation control;fully distributed protocol","Formation control;Protocols;Uncertainty;Vectors;Kernel;Communication networks;Adaptive control;Multi-agent systems;Information security;Vehicle dynamics","","1","","47","IEEE","13 May 2025","","","IEEE","IEEE Journals"
"Evaluating Multi-Modal LLMs for Automatically Recognizing Semantic Elements in UML Use Case Diagram Images","J. Hassine","Information and Computer Science Department, KFUPM, Dhahran, Saudi Arabia Interdisciplinary Research Center for Intelligent Secure Systems, KFUPM, Dhahran, Saudi Arabia","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","861","866","Requirements engineering commonly employs UML Use Case Diagrams (UCD) to visually capture system interactions and functionality, facilitating clear communication between stakeholders. Recognizing and extracting semantic information from UCDs is essential for applications such as automated requirements extraction and system design validation, which improves software analysis accuracy, and streamlines model understanding for both developers and stakeholders. Recent advancements in large language models (LLMs) with visual processing capabilities enable interpreting intricate diagrammatic content. This paper evaluates multi-modal LLMs, specifically GPT-4o and GPT-4o-mini, in accurately identifying semantic elements within UCDs. We conducted experiments on a new dataset of UCDs and other diagrams collected from online sources. Experimental results show that both models struggled to accurately identify and interpret key UCD elements, often misclassifying or overlooking essential ones.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992325","Requirements engineering;UML Use Case Diagram;semantic information;Multi-Modal LLMs","Visualization;Image recognition;Large language models;Unified modeling language;Semantics;Software;Stakeholders;Requirements engineering;Data mining;System analysis and design","","","","13","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Can RAG Calibrate LLMs for Information Extraction Tasks?","J. He; D. Kumar; P. Rasamsetty","Infrrd.ai, San Jose, USA; Infrrd.ai, Bengaluru, India; Infrrd.ai, San Jose, USA",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),"5 Mar 2025","2025","","","00414","00420","Information extraction (IE) is a crucial process in natural language processing (NLP) that involves automatically retrieving structured information from semi-structured and un-structured text sources. Text sources are semi-structured and unstructured documents, for example, W-2 forms, paystubs, mortgage documents, etc. Due to the recent bloom of LLMs, the accuracy of many NLP-based information extraction systems has improved. Another key attribute of a good information extraction system is certainty. The user must know how certain the system is regarding its extraction. Using LLM suffers from the calibration problem as most LLMs use a decoder-only architecture that produces logits conditioned on previous tokens. The confidence score computed by applying softmax on these logits is mostly off from being calibration. In this paper, we provide a generalizable framework to properly calibrate an LLM-based information extraction system with novel context rejection training tasks to teach LLM to ignore less useful examples. It can be easily extended to other tasks other than information extraction. Our method results in a 17.25% improvement in F1-score and a 46.49% improvement in certainty score over the baseline. It also outperforms the state-of-the-art ChatGPT model by a large margin.","","979-8-3315-0769-5","10.1109/CCWC62904.2025.10903733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903733","Constrastive Learning;Information Extraction;Large language models;Knowledge based systems;Prompt engineering;Retrieval-Augmented Generation","Training;Measurement;Loans and mortgages;Accuracy;Retrieval augmented generation;Predictive models;Information retrieval;Robustness;Calibration;Context modeling","","","","19","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"Overview of the Comprehensive Evaluation of Large Language Models","L. Lin; D. Zhu; J. Shang","Software Quality Engineering Research Center, CEPREI, Guangzhou, China; Software Quality Engineering Research Center, CEPREI, Guangzhou, China; Software Quality Engineering Research Center, CEPREI, Guangzhou, China",2024 IEEE Smart World Congress (SWC),"24 Mar 2025","2024","","","1504","1512","In recent years, the rapid development of large language models (LLMs) has attracted increasing interest from artificial intelligence researchers, and many large language models have emerged at home and abroad. These models have demonstrated amazing abilities in specific fields and have become important tools in the field of natural language processing. With the widespread promotion of large language models in research and daily application, it is increasingly important to conduct comprehensive testing on large language models. However, there is currently no clear methodology or unified technical standards for the comprehensive evaluation of large language models. In view of this, this paper will focus on the comprehensive evaluation of large language models, review the existing mainstream evaluation methods and standards. And based on this, the paper will explore the comprehensive assessment of large language models in various aspects, including intelligence, reliability, security, robustness, resource consumption, and social norms. This paper aims to provide a valuable reference for large language model researchers and developers and to promote the development and application of large language models.","2993-396X","979-8-3315-2086-1","10.1109/SWC62898.2024.00231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925029","large language model;artificial intelligence;model evaluation;natural language processing","Reviews;Large language models;Natural language processing;Robustness;Security;Standards;Testing","","","","81","IEEE","24 Mar 2025","","","IEEE","IEEE Conferences"
"Using Prompt Engineering to Enhance STEM Education","M. Z. Li",The Pingry School,2024 IEEE Integrated STEM Education Conference (ISEC),"17 Sep 2024","2024","","","1","2","With the advent of large language models (LLMs), such as ChatGPT, Gemini and LLaMA, there is no doubt that AI will forever change how education works. However, there is a gap between K-12 students and the LLM. The prompts given to LLM need to be well designed to be effectively utilized for K-12 education. To use LLMs more appropriately for K-12 STEM educational purposes, the author developed a prototype tool with prompt engineering to fully utilize the educational potential of LLMs and reduce usage for academic dishonesty. The tool would have a student register by giving the grade that they're in, and then ask as the topic the student would like to learn more about. Using prompt engineering techniques, the tool can prompt LLMs to produce educational content such as a descriptions, question and answer, AI-generated quizzes, and reviews, as well as asking the LLM to simplifying complex topics further to aid in understanding. The AI-enabled tool, effectively a virtual and personal mentor, could help propel STEM education further and make STEM more interesting to students as it could help explain complex topics in a way that students can understand easily. The tool and AI can help students understand a topic through interactive practice instead of just memorizing facts and putting them on a sheet of paper. The tool present in this paper will enhance the STEP education by AI.","2473-7623","979-8-3503-5280-1","10.1109/ISEC61299.2024.10664775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664775","","Reviews;Large language models;Education;Prototypes;Propulsion;Chatbots;Registers","","3","","8","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Secure Encoding Strategy for Consensus of Multi-Agent Systems in the Presence of Eavesdropper","B. Zhao; Y. Zhang","School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China",IEEE Transactions on Circuits and Systems II: Express Briefs,"29 Jul 2022","2022","69","8","3420","3424","This brief studies consensus problem of single-integrator multi-agent network in the presence of eavesdropper. In the network, agents communicate with each other through digital communication channels, and the eavesdropper overhears the messages transmitted by the agents over wiretap channels. A secure encoding strategy based on constructing auxiliary systems is designed to ensure that the multi-agent network achieves consensus, while the eavesdropper cannot precisely estimate the state of any agent in the network. To defend against eavesdropping, each agent in the network transmits the state of its constructed auxiliary system instead of its state value to its neighbors. A sufficient condition of consensus and security is given by studying the relationship between topological entropy and the maximum transmission rates. A simulation example is given to illustrate the effectiveness of the encoding strategy.","1558-3791","","10.1109/TCSII.2022.3140589","National Natural Science Foundation (NNSF) of China(grant numbers:61973082); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); Six Talent Peaks Project in Jiangsu Province(grant numbers:XYDXX-005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672183","Multi-agent system;consensus;security;eavesdropper;topological entropy;channel capacity","Encoding;Security;Multi-agent systems;Entropy;Eavesdropping;Eigenvalues and eigenfunctions;Steady-state","","1","","21","IEEE","6 Jan 2022","","","IEEE","IEEE Journals"
"Using ChatGPT on Improving Program Performance with pprof and Benchmark","W. -C. Lei; L. -Y. Jian; Y. -W. Chen; L. -D. Chou","Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan",2023 5th International Conference on Computer Communication and the Internet (ICCCI),"14 Aug 2023","2023","","","256","260","In the context of limited computing resources, optimizing program architecture is crucial. Therefore, this paper proposes to apply the powerful analytical capabilities of large language models (LLM) to the field of systematic performance optimization. The output of pprof and benchmark is fed into ChatGPT, and the program's performance is improved based on feedback. In the case study, the number of memory allocations for the objective function was successfully reduced from 99 to 1, resulting in a reduction of the test execution time from 8.6 microseconds to 0.36 microseconds. At the same time, the memory allocation was also reduced from 53.5KB to approximately 1KB.","2833-2350","979-8-3503-2695-6","10.1109/ICCCI59363.2023.10210148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210148","Performance analysis;System optimization;Large Language Models;Prompt Engineering","Analytical models;Systematics;System performance;Computational modeling;Memory management;Benchmark testing;Chatbots","","","","15","IEEE","14 Aug 2023","","","IEEE","IEEE Conferences"
"Data Redundancy Elimination and Noise Processing via Large Language Model Prompt Engineering","J. Guang; J. Chao; Z. Tianqi; C. Siya; C. Chaoyuan; F. Jun","College of Systems Engineering, National University of Defense Technology, Changsha, China; the Institute of Intelligent Machines, Hefei Institutes of Physical Sciences Chinese Academy of Sciences, Hefei, China; the Institute of Intelligent Machines, Hefei Institutes of Physical Sciences Chinese Academy of Sciences, Hefei, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; the Institute of Intelligent Machines, Hefei Institutes of Physical Sciences Chinese Academy of Sciences, Hefei, China; College of Systems Engineering, National University of Defense Technology, Changsha, China",2024 10th International Conference on Big Data and Information Analytics (BigDIA),"27 Dec 2024","2024","","","818","825","With the rapid advancement of artificial intelligence (AI), intelligent systems have been widely applied across various practical domains. During their experimental processes, data collection often occurs in complex environments, leading to challenges in ensuring data quality and integrity. Two significant issues are data redundancy and data noise, which can degrade model performance and impact the generalization ability and prediction accuracy of AI models. Traditional methods for handling these issues are either rule-based, relying heavily on domain knowledge, or machine learning-based, which require large volumes of high-quality training data and significant computational resources. In this paper, we propose a novel approach leveraging Large Language Models (LLMs) combined with the Sequential Chain optimization algorithm to address data redundancy elimination and noise processing. By designing automated prompt templates tailored to experimental data characteristics and utilizing LLMs' extensive knowledge and reasoning capabilities, our approach improves the effectiveness of preprocessing tasks. Additionally, the Sequential Chain technique enhances LLM processing per-formance, reducing the hallucination phenomenon and ensuring higher accuracy in data preprocessing. Our experimental results demonstrate that LLM-based methods can achieve results comparable to traditional techniques while offering greater scalability and adaptability. Future work could focus on developing more efficient LLMs with lower computational requirements and refining prompt engineering techniques to reduce the time investment needed, making these advanced methods more accessible and practical for a broader range of applications.","2771-6902","979-8-3503-5462-1","10.1109/BigDIA63733.2024.10808217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10808217","Large Language Models;data redundancy elim-ination;Sequential Chain technique","Large language models;Computational modeling;Redundancy;Noise;Refining;Predictive models;Data models;Prompt engineering;Optimization;Investment","","","","13","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Evaluating Generative Language Models with Prompt Engineering for Categorizing User Stories to its Sector Domains","B. Alawaji; M. Hakami; B. Alshemaimri","Department of Computer, Qassim University, Qassim, Saudi Arabia; Department of Software Engineering, King Saud University, Riyadh, Saudi Arabia; Department of Software Engineering, King Saud University, Riyadh, Saudi Arabia",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),"10 Jun 2024","2024","","","1","8","In Agile software development, user stories capture requirements through a concise, user-centric approach. Manual categorization of these stories is both labor-intensive and error-prone. This study addresses the gap in existing research by exploring the application of generative language models, specifically GPT-style models, with prompt engineering for user story categorization.Our research introduces experiments utilizing two generative language models, gpt-3.5-turbo and Llama-2-chat-hf, emphasizing innovative prompt engineering. The evaluation of gpt-3.5-turbo and Llama-2-chat-hf demonstrates commendable performance in zero-shot experiments. Notably, gpt-3.5-turbo outperforms in few-shot prompting with a higher F-score of 67.93%. The gpt-3.5-turbo showcased superior compliance with instructions compared to Llama-2-chat-hf. The study underscores the promising capability of generative language models, with prompt engineering, to automate user story categorization effectively.This work contributes to the potential of automating user story categorization in low-resource settings, showcasing how leveraging LLMs can enhance accuracy and streamline the categorization process in software development, ultimately leading to more efficient outcomes.","","979-8-3503-9447-4","10.1109/I2CT61223.2024.10544242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544242","requirements engineering;user stories;prompt engineering;generative language models;text classification;software development","Smart homes;Manuals;Syntactics;Software;Robustness;Data models;Requirements engineering","","2","","25","IEEE","10 Jun 2024","","","IEEE","IEEE Conferences"
"Reconfiguration and Cyber-Attack Tolerant Control for Nonlinear Multi-Agent Systems","E. Hassine; A. Thabet; N. Gasmi; G. B. H. Frej; H. Thabet","MACS Research Laboratory, University of Gabes, Gabes, Tunisia; MACS Research Laboratory, University of Gabes, Gabes, Tunisia; Laboratoire D’Informatique and Télécommunications, ECAM Rennes - Louis de Broglie, Campus de Ker Lann - Bruz, Rennes, France; IMS Laboratory, CNRS UMR 5218, University of Bordeaux, Bordeaux, France; MACS Laboratory Research, University of Gabes, Gabes, Tunisia",2023 IEEE International Workshop on Mechatronic Systems Supervision (IW_MSS),"29 Dec 2023","2023","","","1","6","This work deals with the development of an observer-based control/monitoring against Cyber-Attacks (CAs) for interconnected cyber-physical systems. The latter can disconnect an entire node in a network as a result of attacks on control and/or observer chains. The Cyber-Attack Tolerant Control (CATC) of nonlinear Multi-Agent System (MAS) is treated with the consideration of observer-based control gains as well as the optimization of the coupling parameters for each for each scenario using Linear Matrix Inequality (LMI). Indeed, each state of the global nonlinear system achieves a Laplacian matrix describing its topology of interconnections between the various nodes (master and followers). An algorithm has been proposed to guarantee the cyber-security of MAS with the consideration of two issues related to stabilization and the tracking control problems. The theoretical results are validated by simulations on a nonlinear MAS consisting of a master (reference) and 5 followers.","","979-8-3503-2756-4","10.1109/IW_MSS59200.2023.10369717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369717","Multi-Agent Systems;Cyber-Attacks(CAs);Stabilization;Tracking Control;Nonlinear Observer;Cyber-Attack Tolerant Control","Couplings;Laplace equations;Observers;Topology;Linear matrix inequalities;Nonlinear systems;Task analysis","","1","","17","IEEE","29 Dec 2023","","","IEEE","IEEE Conferences"
"Revisiting the Non-Determinism of Code Generation by the GPT-3.5 Large Language Model","S. Sawadogo; A. Sabane; R. Kafando; A. K. Kabore; T. F. Bissyande","Centre d'Excellence en IA (CITADEL), Université Joseph Ki-Zerbo, Ouagadougou, Burkina Faso; Centre d'Excellence en IA (CITADEL), Université Joseph Ki-Zerbo, Ouagadougou, Burkina Faso; Centre d'Excellence en IA (CITADEL), Université Virtuelle du Burkina Faso, Ouagadougou, Burkina Faso; Centre d'Excellence en IA (CITADEL), Université du Luxembourg, Ouagadougou, Burkina Faso; Université du Luxembourg & Université Joseph Ki-Zerbo, Luxembourg, Luxembourg","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","36","44","Despite recent advancements in Large Language Models (LLMs) for code generation, their inherent non-determinism remains a significant obstacle for reliable and reproducible software engineering research. Prior work has highlighted the high degree of variability in LLM-generated code, even when prompted with identical inputs. This non-deterministic behavior can undermine the validity of scientific conclusions drawn from LLM-based experiments. This paper showcases the Tree of Thoughts (ToT) prompting strategy as a promising alternative for improving the predictability and quality of code generation results. By guiding the LLM through a structured Thoughts process, ToT aims to reduce the randomness inherent in the generation process and improve the consistency of the output. Our experiments on GPT-3.5 Turbo model using 829 code generation problems from benchmarks such as CodeContests, APPS (Automated Programming Progress Standard) and HumanEval demonstrate a substantial reduction in non-determinism compared to previous findings. Specifically, we observed a significant decrease in the number of coding tasks that produced inconsistent outputs across multiple requests. Nevertheless, we show that the reduction in semantic variability was less pronounced for HumanEval (69%), indicating unique challenges present in this dataset that are not fully mitigated by ToT.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992476","Code Generation;Tree of Thoughts;LLMs;Non-Determinism","Codes;Translation;Large language models;Semantics;Transforms;Programming;Software;Software reliability;Standards;Software engineering","","","","22","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"Leveraging LLMs for Dynamic IoT Systems Generation Through Mixed-Initiative Interaction","B. Adnan; S. Miryala; A. Sambu; K. Vaidhyanathan; M. De Sanctis; R. Spalazzese","SERC, IIIT Hyderabad, India; SERC, IIIT Hyderabad, India; SERC, IIIT Hyderabad, India; SERC, IIIT Hyderabad, India; GSSI, L'Aquila, Italy; Dept. of Computer Science and Media Technology, Sustainable Digitalisation Research Centre, Malmö University, Sweden",2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C),"30 May 2025","2025","","","488","497","IoT systems face significant challenges adapting to user needs, often under-specified and evolving with changing environmental contexts. To address these complexities, users should be able to explore possibilities. At the same time, IoT systems must learn and support users in providing proper services, e.g., to serve novel experiences. The IoT-Together paradigm aims to meet this demand through the Mixed-Initiative Interaction (MII) paradigm that facilitates a collaborative synergy between users and IoT systems, enabling the co-creation of intelligent and adaptive solutions that are precisely aligned with user-defined goals. This work is a realization of IoT-Together by integrating Large Language Models (LLMs) into its architecture. The presented work enables intelligent goal interpretation through a three-pass dialogue framework and dynamic IoT systems generation according to user needs. To demonstrate the efficacy of our methodology, we design and implement the system in the context of a smart city tourism case study. We evaluated the system performance using agent-based simulation and user studies. Results indicate efficient and accurate service identification and high adaptation quality. The empirical evidence indicates that integrating Large Language Models (LLMs) into IoT architectures can significantly enhance the architectural adaptability of the system while ensuring real-world usability.","2768-4288","979-8-3315-3336-6","10.1109/ICSA-C65153.2025.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11015078","LLMs;Dynamic IoT System Generation;IoT-Together Paradigm;Mixed-Initiative Interaction;Self-Adaptation;Software Architecture;Software Engineering","Software architecture;Smart cities;Large language models;System performance;Design methodology;Collaboration;Computer architecture;Complexity theory;Usability;Faces","","","","17","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"10 Large Language Models as Data Compression Engines","A. Banafa",NA,Artificial Intelligence in Action: Real-World Applications and Innovations,"","2025","","","61","68","This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.","","9788770046190","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948940.pdf&bkn=10948919&pdfType=chapter","","","","","","","","3 Apr 2025","","","River Publishers","River eBook Chapters"
"CodeSift: An LLM-Based Reference-Less Framework for Automatic Code Validation","P. Aggarwal; O. Chatterjee; T. Dai; P. Mohapatra; B. Paulovicks; B. Blancett; A. De Magalhaes","IBM Research, India; IBM Research, India; IBM Research, US; IBM Research, India; IBM Research, US; IBM, US; IBM, US",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","404","410","The advent of large language models (LLMs) has greatly facilitated code generation, but ensuring the functional correctness of generated code remains a challenge. Traditional validation methods are often time-consuming, error-prone, and impractical for large volumes of code. We introduce CodeSift, a novel framework that leverages LLMs as the first-line filter of code validation without the need for execution, reference code, or human feedback, thereby reducing the validation effort. We assess the effectiveness of our method across three diverse datasets encompassing two programming languages. Our results indicate that CodeSift outperforms state-of-the-art code evaluation methods. Internal testing conducted with subject matter experts reveals that the output generated by CodeSift is in line with human preference, reinforcing its effectiveness as a dependable automated code validation tool.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643933","code generation;validation;large language models;generative AI","Cloud computing;Computer languages;Codes;Subject matter experts;Large language models;Computational modeling;Testing","","","","32","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"AI-Enhanced Static Analysis: Reducing False Alarms Using Large Language Models","G. D. Apostolidis; I. Kalouptsoglou; M. Siavvas; D. Kehagias; D. Tzovaras","Centre for Research and Technology Hellas/lnformation Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/lnformation Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/lnformation Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/lnformation Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas/lnformation Technologies Institute, Thessaloniki, Greece",2025 IEEE International Conference on Smart Computing (SMARTCOMP),"3 Jul 2025","2025","","","288","293","In modern software systems, early and accurate vulnerability detection is crucial. Traditional Static Analysis Tools (SATs) highlight potential security issues, providing fine-grained information including lines of code and vulnerability categories; however, they are hindered by a large number of false alarms. On the other hand, Artificial Intelligence (AI)-based Vulnerability Prediction (VP) has emerged as a promising alternative for vulnerability identification in software products. Nevertheless, current VP methods face important limitations, such as the granularity level of the predictions, since VP is commonly conducted at the file or function level. In this study, we examine whether the utilization of AI-based vulnerability prediction as a filtering mechanism for static analysis alerts could reduce the number of false alarms, leading to more practical Static Application Security Testing (SAST). The results of the analysis show that this approach improves the practicality of static analysis, reducing false positives, with the impact on the detection accuracy being small.","2693-8340","979-8-3315-8646-1","10.1109/SMARTCOMP65954.2025.00088","European Union's Horizon Europe Research and Innovation(grant numbers:101120270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11058686","Vulnerability Detection;Static Analysis;Large Language Models;Actionable Alerts","Hands;Accuracy;Codes;Filtering;Large language models;Static analysis;Software systems;Security;Faces;Testing","","","","49","IEEE","3 Jul 2025","","","IEEE","IEEE Conferences"
"Can GPT-4 Aid in Detecting Ambiguities, Inconsistencies, and Incompleteness in Requirements Analysis? A Comprehensive Case Study","T. Mahbub; D. Dghaym; A. Shankarnarayanan; T. Syed; S. Shapsough; I. Zualkernan","Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates",IEEE Access,"22 Nov 2024","2024","12","","171972","171992","Effective software projects hinge on robust requirements, yet flawed requirements often lead to costly delays and revisions. While tools have been developed to identify defects in Software Requirements Specifications (SRS), the advent of Large Language Models (LLMs) like GPT-4 presents new opportunities for enhancing requirements quality. However, the potential of LLMs in this realm remains largely unexplored, particularly in the context of large-scale industrial documents. To bridge this gap, we investigate the efficacy of zero-shot GPT-4 in various requirements analysis tasks using an industrial software specification document. Our study evaluates LLM performance in detecting defects, such as ambiguities, inconsistencies, and incompleteness, while also analyzing GPT-4’s ability to identify issues across version iterations and support technical experts in requirements analysis. Qualitatively, we identify key limitations of LLMs in defect detection, notably their inability to cross-reference throughout the document and their constrained understanding of specialized contexts. Quantitatively, we find that while LLMs excel in identifying incomplete requirements (precision 0.61), their performance is less impressive in detecting inconsistencies (precision 0.43) and ambiguities (precision 0.39). Although GPT-4 demonstrates promise in automating early defect detection across versions and providing accurate technical answers, our results underscore that they cannot entirely replace human analysts due to their lack of nuanced domain knowledge in a zero-shot setting. Nevertheless, avenues like few-shot learning and complex prompt design offer the potential to enhance LLM precision in defect detection.","2169-3536","","10.1109/ACCESS.2024.3464242","Open Access Program from American University of Sharjah; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684184","Ambiguity;completeness;GPT;inconsistency;large language models (LLMs);requirements engineering;software engineering;software requirements specifications (SRS)","Stakeholders;Large language models;Accuracy;Requirements engineering;Defect detection;Software development management;Software engineering","","1","","61","CCBYNCND","19 Sep 2024","","","IEEE","IEEE Journals"
"Risks for Conversational AI Security","V. Bhardwaj; S. S. Khan; G. Singh; S. Patil; D. Kuril; S. Nahar","School of Computer Science and Engineering, Manipal University Jaipur, Jaipur, India; Department of Computer Science and Engineering, Medi&#x2010;Caps University, Indore, India; Department of Computer Science and Engineering, Lovely Professional University, Jalandhar, Punjab, India; Department of Computer Science and Engineering, Vedica Institute of Technology, RKDF University Bhopal, Bhopal, India; Department of Information Technology, Shri Vaishnav Vidyapeeth Vishwavidyalaya, Indore, India; Department of Computer Science Engineering, Shri Vaishnav Vidyapeeth Vishwavidyalaya Indore, Indore, India",Conversational Artificial Intelligence,"","2024","","","557","587","Summary <p>Conversational artificial intelligence (AI) systems have become increasingly popular, and their integration into various industries has grown significantly. With the advent of advanced technologies like machine learning and natural language processing, conversational AI has become capable of performing complex tasks, including processing customer requests, providing recommendations, and facilitating transactions. However, these systems are also vulnerable to various security risks that can expose sensitive data and compromise the privacy of users. This paper provides a comprehensive review of the risks associated with conversational AI security, including attacks such as phishing, malware injection, and voice cloning. The paper also discusses the potential impact of these attacks on businesses and end‐users and proposes strategies to mitigate these risks [1].</p> <p>Conversational AI is a rapidly growing field, with a wide range of applications such as customer service, personal assistants, and healthcare. However, with the increasing use of conversational AI, there are also growing concerns about the security risks associated with these systems. This research paper aims to explore the risks associated with conversational AI security, including privacy breaches, data leakage, and manipulation of conversational agents. This paper also discusses the current state of research in this area and proposes recommendations for improving conversational AI security. Conversational AI has gained significant popularity in recent years due to its ability to enable natural and seamless interactions between humans and machines. However, with the increasing use of conversational AI, the risks for security have also increased. The risks for conversational AI security include both technical and non‐technical threats that can compromise the confidentiality, integrity, and availability of sensitive information. This paper provides an overview of the risks associated with conversational AI security, including threats such as data breaches, malicious attacks, impersonation attacks, and social engineering attacks. It also discusses the measures that can be taken to mitigate these risks, including implementing secure authentication mechanisms, using encryption, monitoring user behavior, and adopting a risk management approach. Ultimately, understanding the risks for conversational AI security is crucial for organizations to develop effective security strategies and protect their sensitive information from potential threats.</p>","","9781394200795","10.1002/9781394200801.ch32","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10951661.pdf&bkn=10950236&pdfType=chapter","","Conversational artificial intelligence;Security;Privacy;Chatbots;Surveys;Authentication;Computer science;Personal voice assistants;History;Reviews","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Observer-Based Sampled-Data Adaptive Tracking Control for Heterogeneous Nonlinear Multi-Agent Systems Under Denial-of-Service Attacks","N. Zhao; H. Zhang; P. Shi","College of Control Science and Engineering, Bohai University, Jinzhou, China; National Research Base of Intelligent Manufacturing Service, Chongqing Technology and Business University, Chongqing, China; School of Electrical and Mechanical Engineering, The University of Adelaide, Adelaide, SA, Australia",IEEE Transactions on Automation Science and Engineering,"28 Feb 2025","2025","22","","4771","4779","This paper tackles the security cooperative tracking control problem for uncertain heterogeneous nonlinear multi-agent systems (MASs) subject to denial-of-service (DoS) attacks. Since the communication between agents can only transmit digital signals, a periodic sampling mechanism is used to reduce bandwidth pressure. Based on these digital signals, a novel distributed switching observer is designed to estimate the state of the reference system for each follower in the case of DoS attacks on aperiodic intermittently blocked communication channels. Then, an attack-parameter-dependent Lyapunov functional is established to analyze the exponential convergence of the error signals and a design scheme of observer gains is proposed. Further, with the help of the sampling observer outputs, follower’s states and adaptive signals, a sampled-data neural network controller is designed to guarantee the boundedness of the tracking errors by backstepping method and impulsive system analysis theory. Finally, a simulation example is performed to showcase the effectiveness of the developed strategy. Note to Practitioners—In industrial systems, since it is difficult for a single plant to complete complex control tasks, the cooperative control of MASs has gradually become a hot research issue. For example, in industrial automated production, a single robotic arm cannot achieve mass production of products, while multiple robotic arms can achieve this task and improve production efficiency. In this context, the study on cooperative control of MASs is of great scientific and economic value. The wireless communication network is the medium of information transfer between the agents and the controllers, while giving attacker an opportunity to attack. When network interaction information is affected by attacks, the performance and stability of the system are easily damaged. Moreover, under the condition of ensuring the performance of the system, the number of packet transmission is as small as possible, which will be conducive to the applications with limited bandwidth or congested network. Taking these problems into consideration, this paper studies sampled-data tracking control for uncertain heterogeneous nonlinear MASs under DoS attacks. This result provides a reference for the design of digital security controller, and can also be applied to intelligent engineering systems.","1558-3783","","10.1109/TASE.2024.3411074","National Natural Science Foundation of China(grant numbers:62303069); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJZDK202300807); Australian Research Council(grant numbers:DP240101140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556772","Multi-agent systems;switched distributed observer;sampled-data adaptive control;denial-of-service attacks","Observers;Adaptive systems;Denial-of-service attack;Switches;Neural networks;Vectors;Task analysis","","25","","37","IEEE","13 Jun 2024","","","IEEE","IEEE Journals"
"Adaptive Prescribed-Time Consensus Tracking Control Scheme of Nonlinear Multi-Agent Systems Under Deception Attacks","B. Niu; Y. Gao; G. Zhang; X. Zhao; H. Wang; D. Wang; C. Liu","Key Laboratory of Intelligent Control and Optimization for Industrial Equipment of Ministry of Education and the School of Control Science and Engineering, Dalian University of Technology, Dalian, Liaoning, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Chemical Engineering, Sichuan University, Chengdu, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; College of Mathematical Sciences, Bohai University, Jinzhou, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China",IEEE Transactions on Automation Science and Engineering,"21 Feb 2025","2025","22","","4196","4205","This article investigates the adaptive prescribed-time consensus tracking control problem for nonlinear multi-agent systems (MASs), where the states of systems are unmeasured and the actuators suffer from the deception attacks. Firstly, a novel coordinate transformation technology is developed by introducing a time-varying constraint function, such that the prescribed-time tracking control problem of nonlinear MASs is converted into the constraint problem of the error variables. Then, a new attack compensator is proposed to address the unknown time-varying attack gains caused by the actuator deception attacks. Further, the state observers are designed to estimate the unavailable state variables and fuzzy-logic systems (FLSs) are employed to handle the unknown functions that exist within the systems. In addition, the attack compensator-based controller ensures the boundedness of all signals, while the error variables converge to the predefined region in a specified time. The upper bound of the whole tracking errors in the mean square sense can be decreased by selecting the appropriate design parameters. At last, the simulation example illustrates the availability of the developed control method. Note to Practitioners—In the industry, consensus tracking control of nonlinear MASs exists in many different systems, such as mobile robot networks, intelligent transportation management, surveillance and monitoring. Since the above systems operate in a network environment, the security problems of the systems cannot be ignored. Hence, considering the unmeasured states, the unknown functions, and the unknown time-varying attack gains existing simultaneously in the studied systems, it is a challenging and meaningful task to achieve the desired security control objectives. On the other hand, based on a time-varying constraint function, this article presents an adaptive prescribed-time consensus tracking control scheme for the nonlinear MASs under the deception attacks. It provides a viable strategy for industrial applications.","1558-3783","","10.1109/TASE.2024.3408453","National Natural Science Foundations of China(grant numbers:62376170,62173046,62222301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551713","Nonlinear multi-agent systems;prescribed-time control;deception attacks;state observers;fuzzy-logic systems","Actuators;Security;Electronic mail;Observers;Consensus control;Multi-agent systems;Convergence","","9","","49","IEEE","7 Jun 2024","","","IEEE","IEEE Journals"
"♪ With a Little Help from My (LLM) Friends: Enhancing Static Analysis with LLMs to Detect Software Vulnerabilities","A. Munson; J. Gomez; Á. A. Cárdenas","Computer Science and Engineering, University of California, San Diego; Computer Science and Engineering, University of California, Santa Cruz; Computer Science and Engineering, University of California, Santa Cruz",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","25","32","This paper explores the integration of Large Language Models (LLMs) with static analysis tools, specifically Semgrep, to enhance vulnerability detection in Java applications. Through a series of experiments, we evaluate the performance of various LLMs in triaging security weaknesses identified by Semgrep. We also study how LLMs perform across different types of vulnerabilities and assess the impact of various prompt engineering strategies. Our results reveal that while some LLM models reduce the accuracy of baseline results with static analysis, they show a consistent improvement with each new model released. In particular, o1-mini significantly outperformed others in our experiments in terms of their accuracy and false positive reduction. Although LLMs might not be ready for prime time in vulnerability detection yet, this study highlights their growing potential to complement existing tools and paves the way for future research to further optimize LLM-based vulnerability detection systems.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00008","Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028575","static analysis;Large Language Models;vulnerability detection","Analytical models;Java;Accuracy;Codes;Large language models;Conferences;Static analysis;Software;Security;Prompt engineering","","","","25","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Computational Analysis of Quran Text Using Machine Learning and Large Language Models","U. Shahid; M. Z. Hussain; W. Sayers","School of Business, Computing and Social Sciences University of Gloucestershire, Cheltenham, United Kingdom; Department of Computer Science, Bahria University, Lahore, Pakistan; School of Business, Computing and Social Sciences University of Gloucestershire, Cheltenham, United Kingdom",2025 8th International Conference on Data Science and Machine Learning Applications (CDMA),"7 Mar 2025","2025","","","18","24","The Quran verses are foundational for Muslims worldwide. Significant research has been dedicated to information retrieval (IR) from Quran; however, multiple studies have focused on descriptive analysis and topic modelling of the Quran in Arabic and translated versions. This study presents a comprehensive framework for analysing large textual data using an English translation of the Quran. Initially, it conducts a descriptive analysis of the verses to uncover various features, including readability, word clouds, significant n-grams, and network graphs illustrating word associations. The framework then applies machine learning techniques, specifically clustering models based on numerical vectors from text-embedding-3-large, to identify effective groupings of verses. Additionally, GPT-4-turbo is used for topic modelling within each cluster through prompt engineering, aiming to enhance the understanding of these clusters. The results include statistical information graphs and concise knowledge summaries that are beneficial to both domain experts and wider populace.","","979-8-3315-3969-6","10.1109/CDMA61895.2025.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908764","quran;data science;machine learning;large language models;natural language processing;text mining","Translation;Semantic search;Machine learning;Tag clouds;Vectors;Natural language processing;Numerical models;Data mining;Prompt engineering;Multiaccess communication","","","","24","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Unknown-Input-Proportional-Differential Observer-Based Event-Triggered Intrusion-Tolerant Control for Human-in-the-Loop Multi-Agent Systems Against Unconstrained Actuator and Sensor FDIAs","B. -Q. Wang; X. -G. Guo; J. -L. Wang; D. Coutinho; J. H. Park","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Autonomous Intelligent Systems Department, Hangzhou Innovation Institute of Beihang University, Hangzhou, China; Department of Automation and Systems, Universidade Federal de Santa Catarina, Florianópolis, Brazil; Department of Electrical Engineering, Yeungnam University, Gyeongsan, South Korea",IEEE Transactions on Automation Science and Engineering,"30 May 2025","2025","22","","15701","15712","This paper investigates an intrusion-tolerant control problem for human-in-the-loop multi-agent systems (HILMASs) subjected to external disturbances and unconstrained actuator and sensor false data injection attacks (FDIAs) under directed graph. It is critical to emphasize that once a hacker gets into the control loop of the HILMAS, he can do whatever damage he wants, which means that the attack signal should be free of any constraints. Two main unconstrained attacks, i.e., unbounded FDIAs and variable-frequency FDIAs, are hard to accurately estimate and defend against since the widely adopted constraints on the existing FDIAs such as the bounded or/and the bounded first-order derivative have been removed. To tackle this challenging obstacle, a novel unknown-input-proportional-differential observer (UIPDO) is developed to not only reconstruct the follower agents’ states as well as unconstrained actuator and sensor FDIAs simultaneously, but also avoid the decrease of estimation accuracy caused by measurement deviation. It should be noted that this measurement deviation may be extremely large as it is caused by unconstrained sensor FDIAs, which renders the traditional observer ineffective in providing reliable and accurate estimates of the system states and unconstrained actuator and sensor FDIAs. Then, a novel UIPDO-based intrusion-tolerant control strategy without requiring boundedness of the first-order derivatives of the FDIAs as in existing literature is proposed. Furthermore, an adaptive Zeno-free event-triggered mechanism (ETM) solely relying local state information is developed to reduce the communication burden. Finally, the numerical simulation is provided to verify the merits and effectiveness of the developed methodology. Note to Practitioners—In HILMASs, security is of paramount importance. However, these systems face increasing vulnerability to external disturbances and unconstrained FDIAs under directed graph. Unconstrained attacks, such as unbounded and variable-frequency FDIAs, pose significant challenges due to their lack of traditional signal constraints, complicating their estimation and defense. This paper introduces a novel unknown-input-proportional-differential observer (UIPDO) based on an augmented descriptor system to reconstruct agent states and estimate these attacks, maintaining accuracy even with large measurement deviations. Furthermore, we propose an UIPDO-based intrusion-tolerant control strategy that does not assume boundedness of FDIA first-order derivatives, allowing tolerance of rapid and significant attack changes. Additionally, an adaptive Zeno-free ETM using local real-time state information optimizes communication resources. Our research provides a robust solution to enhance the security, resilience, and practicality of HILMASs against unconstrained FDIAs and external disturbances.","1558-3783","","10.1109/TASE.2025.3570756","National Natural Science Foundation of China(grant numbers:62173028,62233015,62173024); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2024A1515011493); Science, Technology and Innovation Project of Xiongan New Area(grant numbers:2023XAGG0062); Beijing Natural Science Foundation(grant numbers:4232060); International Scientists Project, Beijing Natural Science Foundation(grant numbers:IS23065); Portuguese, Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)(grant numbers:303289/2022-8/PQ); National Research Foundation of Korea (NRF) Grant funded by Korea Government (MSIT)(grant numbers:RS-2019-NR040065); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006037","Human-in-the-loop multi-agent systems (HILMASs);unconstrained false data injection attacks (FDIAs);intrusion-tolerant technique;event-triggered mechanism (ETM);unknown-input-proportional-differential observer (UIPDO)","Actuators;Observers;Accuracy;Multi-agent systems;Human in the loop;Event detection;Automation;Vectors;Symmetric matrices;Directed graphs","","","","31","IEEE","16 May 2025","","","IEEE","IEEE Journals"
"Generative AI for Cyber Security: Analyzing the Potential of ChatGPT, DALL-E, and Other Models for Enhancing the Security Space","S. Sai; U. Yashvardhan; V. Chamola; B. Sikdar","Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science, Pilani (BITS Pilani), Pilani Campus, Pilani, India; Department of Computer Science and Information Systems, Birla Institute of Technology and Science, Pilani (BITS Pilani), Pilani Campus, Pilani, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science, Pilani (BITS Pilani), Pilani Campus, Pilani, India; Department of Electrical and Computer Engineering, National University of Singapore, Queenstown, Singapore",IEEE Access,"18 Apr 2024","2024","12","","53497","53516","This research paper intends to provide real-life applications of Generative AI (GAI) in the cybersecurity domain. The frequency, sophistication and impact of cyber threats have continued to rise in today’s world. This ever-evolving threat landscape poses challenges for organizations and security professionals who continue looking for better solutions to tackle these threats. GAI technology provides an effective way for them to address these issues in an automated manner with increasing efficiency. It enables them to work on more critical security aspects which require human intervention, while GAI systems deal with general threat situations. Further, GAI systems can better detect novel malware and threatening situations than humans. This feature of GAI, when leveraged, can lead to higher robustness of the security system. Many tech giants like Google, Microsoft etc., are motivated by this idea and are incorporating elements of GAI in their cybersecurity systems to make them more efficient in dealing with ever-evolving threats. Many cybersecurity tools like Google Cloud Security AI Workbench, Microsoft Security Copilot, SentinelOne Purple AI etc., have come into the picture, which leverage GAI to develop more straightforward and robust ways to deal with emerging cybersecurity perils. With the advent of GAI in the cybersecurity domain, one also needs to take into account the limitations and drawbacks that such systems have. This paper also provides some of the limitations of GAI, like periodically giving wrong results, costly training, the potential of GAI being used by malicious actors for illicit activities etc.","2169-3536","","10.1109/ACCESS.2024.3385107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491270","Security;Artificial Intelligence;machine learning;natural language processing;learning systems","Security;Passwords;Malware;Computer security;Chatbots;Training;Industries;Generative adversarial networks;Artificial intelligence","","49","","138","CCBYNCND","4 Apr 2024","","","IEEE","IEEE Journals"
"Symbolic Execution with Test Cases Generated by Large Language Models","J. Xu; J. Xu; T. Chen; X. Ma","State Key Lab of Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Lab of Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computing and Mathematical Sciences, Birkbeck University of London, UK; State Key Lab of Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","26 Sep 2024","2024","","","228","237","Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10% increase for line coverage.","2693-9177","979-8-3503-6563-4","10.1109/QRS62785.2024.00031","National Natural Science Foundation of China; State Key Laboratory of Novel Software Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684633","software testing;symbolic execution;large language model","Codes;Large language models;Pipelines;Manuals;Software quality;Reliability engineering;Software reliability","","1","","35","IEEE","26 Sep 2024","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Dynamic Scenario Building targeting Enhanced Cyber-threat Detection and Security Training","C. Marantos; S. Evangelatos; E. Veroni; G. Lalas; K. Chasapas; I. T. Christou; P. Lappas","Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg; Research & Innovation Development Department, Netcompany-Intrasoft S.A., Luxembourg, Luxembourg",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2779","2788","As cybercrime is becoming increasingly sophisticated, effective cybersecurity is crucial to safeguard digital assets and protect critical infrastructures from emerging threats. Several security applications exploit recent advances in (Big) data analysis and Artificial Intelligence (AI) to prevent and respond to malicious activities. Towards this direction, supervised and unsupervised Machine Learning (ML) methods are used to detect anomalies or reveal patterns that may indicate potential threats. However, the successful implementation of these technologies requires security practitioners to undergo specialized training to fully understand and use AI-driven tools and data analytics. On the other hand, AI models themselves are vulnerable to a variety of cyber threats, which can compromise their training data and learning processes. To ensure the safe operation of these systems, especially when deployed in adversarial environments, it is crucial to create novel AI adversarial algorithms and models that are resilient against diverse security threats. This work presents a conceptual framework based on Large Language Models (LLMs) supported by a Multi-Agent layer for training of security practitioners in various advanced technologies and enhance ML models ability to detect and respond to emerging cyber threats effectively.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825681","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825681","Scenario Building;LLM;Adversarial Learning;Data Augmentation;Explainable AI","Training;Explainable AI;Large language models;Buildings;Training data;Transforms;Big Data;Data models;Security;Multi-agent systems","","1","","30","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Continuity in Security: Leveraging LLM for Translating Security Properties Across Hardware Designs","B. Ahmed; S. K. Saha; J. Zhou; S. Aftabjahani; M. Tehranipoor; F. Farahmandi","Electrical and Computer Engineering, University of Florida; Electrical and Computer Engineering, University of Florida; Electrical and Computer Engineering, University of Florida; Intel Corporation; Electrical and Computer Engineering, University of Florida; Electrical and Computer Engineering, University of Florida",2024 IFIP/IEEE 32nd International Conference on Very Large Scale Integration (VLSI-SoC),"3 Dec 2024","2024","","","1","6","Systems on Chips (SoCs) are integral to modern devices, from consumer electronics to critical applications in healthcare, finance, and defense, housing various vital assets. Ensuring comprehensive security verification is crucial to protect these assets from diverse vulnerabilities. However, traditional security verification is time-consuming, and the rapid pace of market-driven design cycles demands new versions within tight time-to-market windows. Conducting exhaustive security verification from scratch for each new design iteration is both challenging and impractical. This paper introduces a novel framework leveraging large language models (LLMs) to translate security properties from legacy designs to new versions at the Register Transfer Level (RTL). By reusing existing verification efforts, this approach significantly reduces verification time while maintaining security continuity. Our methodology not only trans-lates but also extends and expands security properties to detect new vulnerabilities. Experimental results demonstrate substantial improvements in security continuity and vulnerability detection, advancing hardware security verification for evolving SoCs.","2324-8440","979-8-3315-3967-2","10.1109/VLSI-SoC62099.2024.10767831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767831","Security Property;Property-based Verification;Security Verification Reuse;Security Continuity;Formal Verifi-cation","Systematics;Large language models;Hardware security;Refining;Finance;Medical services;Very large scale integration;System-on-chip;Security;Design optimization","","","","14","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Improving Spam Detection with a Multi-Agent Debate Framework","R. Huang","University of Science and Technology of China, Hefei, China","2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","11 Feb 2025","2024","4","","1345","1349","Spam detection is a critical area within information security, where machine learning (ML) and deep learning (DL) have significantly advanced the state-of-the-art. This paper presents an advanced multi-agent debate framework designed to enhance the accuracy and practicality of spam detection within the realm of information security. By integrating multiple Large Language Models (LLMs), the framework employs innovative discussion strategies and tailored algorithms to simulate complex human evaluation processes. The multi-agent system assigns distinct roles to individual agents, each contributing uniquely to the discussion dynamics, which facilitates a multifaceted evaluation akin to human judgment. The framework’s collaborative approach not only bolsters the authenticity of responses but also strengthens the system’s capacity to handle complex tasks. Through rigorous experimentation, we demonstrate that our multi-agent debate framework significantly outperforms traditional single-agent methods in terms of precision and robustness. The framework’s ability to adapt to the evolving tactics of spammers and its potential to be optimized for various datasets and tasks make it a promising tool for enhancing automated text evaluations.","","979-8-3503-6360-9","10.1109/ICIBA62489.2024.10868417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10868417","Spam Detection;Multi-Agent System;Debate Framework;Machine Learning;Deep Learning;Large Language Models;Information Security","Deep learning;Accuracy;Large language models;Heuristic algorithms;Decision making;Information security;Performance gain;Robustness;Information technology;Multi-agent systems","","","","13","IEEE","11 Feb 2025","","","IEEE","IEEE Conferences"
"Coding with ChatGPT and Other LLMs: Navigate LLMs for effective coding, debugging, and AI-driven development","D. V. A. Hall",NA,"Coding with ChatGPT and Other LLMs: Navigate LLMs for effective coding, debugging, and AI-driven development","","2024","","","","","Leverage LLM (large language models) for developing unmatched coding skills, solving complex problems faster, and implementing AI responsiblyKey FeaturesUnderstand the strengths and weaknesses of LLM-powered software for enhancing performance while minimizing potential issuesGrasp the ethical considerations, biases, and legal aspects of LLM-generated code for responsible AI usageBoost your coding speed and improve quality with IDE integrationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionKeeping up with the AI revolution and its application in coding can be challenging, but with guidance from AI and ML expert Dr. Vincent Hall—who holds a PhD in machine learning and has extensive experience in licensed software development—this book helps both new and experienced coders to quickly adopt best practices and stay relevant in the field. You’ll learn how to use LLMs such as ChatGPT and Gemini to produce efficient, explainable, and shareable code and discover techniques to maximize the potential of LLMs. The book focuses on integrated development environments (IDEs) and provides tips to avoid pitfalls, such as bias and unexplainable code, to accelerate your coding speed. You’ll master advanced coding applications with LLMs, including refactoring, debugging, and optimization, while examining ethical considerations, biases, and legal implications. You’ll also use cutting-edge tools for code generation, architecting, description, and testing to avoid legal hassles while advancing your career. By the end of this book, you’ll be well-prepared for future innovations in AI-driven software development, with the ability to anticipate emerging LLM technologies and generate ideas that shape the future of development.What you will learnUtilize LLMs for advanced coding tasks, such as refactoring and optimizationUnderstand how IDEs and LLM tools help coding productivityMaster advanced debugging to resolve complex coding issuesIdentify and avoid common pitfalls in LLM-generated codeExplore advanced strategies for code generation, testing, and descriptionDevelop practical skills to advance your coding career with LLMsWho this book is forThis book is for experienced coders and new developers aiming to master LLMs, data scientists and machine learning engineers looking for advanced techniques for coding with LLMs, and AI enthusiasts exploring ethical and legal implications. Tech professionals will find practical insights for innovation and career growth in this book, while AI consultants and tech hobbyists will discover new methods for training and personal projects.","","9781805127963","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10803973.pdf&bkn=10803972&pdfType=book","","","","","","","","16 Dec 2024","","","Packt Publishing","Packt Publishing eBooks"
"Utilizing Large Language Models for DDoS Attack Detection","M. Mahmoodi; S. M. Jameii","Department of Computer Engineering, Shahr-e-Qods Branch, Islamic Azad University, Tehran, Iran; Department of Computer Engineering, Shahr-e-Qods Branch, Islamic Azad University, Tehran, Iran",2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0,"30 Sep 2024","2024","","","1","6","Large Language Models (LLMs) have shown many advantages for various tasks. These large language models, pre-trained on big textual datasets, allow us to work in different scenarios, especially for classification purposes, even with incomplete information. While LLMs’ ability is limited to text inputs, converting network packets to text form can enable LLMs to find malignant patterns, such as DDoS (Distributed Denial of Service) attacks. In this study, we use Llama 2 to detect DDoS attacks. We introduce a new method for real-time identification of DDoS attacks through network packet scanning and converting them to text data, utilizing Llama 2 LLM, and finetuned it using the CIC-IDS2017 dataset to classify network packets into malignant and benign. We compare the proposed model to previously introduced deep learning architectures, and the final results prove the effectiveness of the proposed model in DDoS attack detection. Our proposed approach utilizes QloRA from parameter-efficient finetuning and the proximal policy optimization from the TRL library, presenting promising results for detecting DDoS attacks in real-world scenarios.","","979-8-3503-7378-3","10.1109/OTCON60325.2024.10688345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10688345","cyber security;cyberattack;DDoS attack;LLM;Llama2;PEFT;PPO","Technological innovation;Large language models;Deep architecture;Denial-of-service attack;Real-time systems;Libraries;Fourth Industrial Revolution","","2","","40","IEEE","30 Sep 2024","","","IEEE","IEEE Conferences"
"APT-KG2QA: An Intelligent Fine-tuning Strategy for Large Language Models Utilizing the APT Knowledge Graph","B. Ma; Y. Zhou; S. Wu; Z. Wang; Y. Xiao; Y. Cui; Y. Liu; Z. Tian","Cyberspace Institute of Advanced Technology, China; Cyberspace Institute of Advanced Technology, China; China Aerospace Science and Technology Corporation (CASC), China; Cyberspace Institute of Advanced Technology, China; PINGXING Lab (Nsfocus Technology Group Company), Guangzhou, China; Harbin Institute of Technology, Harbin, China; Cyberspace Institute of Advanced Technology, China; Huangpu Research School of Guangzhou University, Guangzhou, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","The proliferation of Internet of Things (IoT) devices, now numbering in the tens of billions, has exposed new attack surfaces due to their heterogeneous network architectures and vast numbers of distributed endpoints. The offensive-defensive dynamics of Advanced Persistent Threats (APTs) in IoT environments exhibit unique complexities, including enhanced stealth capabilities and prolonged attack lifecycles. This paper introduces APT-KG2QA, a knowledge graph-driven framework for generating specialized question-answering datasets. This methodology addresses two critical challenges in deploying large language models (LLMs) for cybersecurity applications: mitigating inherent biases in attack behavior recognition and overcoming logical reasoning deficits stemming from limited access to high-quality, domain-specific training data. The methodology utilizes a systematic conversion mechanism to translate the APT KG data into a hierarchical instruction template framework, which incorporates a hybrid prompt template engine with adversarial augmentation modules to produce domain-adaptive fine-tuning data. Low-Rank Adaptation (LoRA) technique facilitates parameter-efficient fine-tuning for four basic models. Experimental results indicate that the models enriched with domain knowledge achieve average increases of 577.7% in BLEU-4 and 623.7% in ROUGE-L measures, showing significant enhancements noted in output correctness, relevance, and comprehensibility. This study explores cross-modal integration pathways between KGs and LLMs, confirming the effectiveness of structured knowledge infusion in improving cybersecurity analysis capabilities, thus offering methodological guidance for developing intelligent security defense systems.","2327-4662","","10.1109/JIOT.2025.3586658","Guangdong S&T Program under Grant(grant numbers:2024B0101010002); Project of Guangdong Key Laboratory of Industrial Control System Security(grant numbers:2024B1212020010); National Natural ScienceFoundation of China(grant numbers:62372129,U2436208); Special Funds for the Cultivation of Guangdong College Students' Scientific and Technological Innovation. ??Climbing Program? Special Funds.?(grant numbers:pdjh2025bk172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11072442","Cybersecurity;APT Knowledge Graphs;Instruction dataset curation;LLMs","Computer security;Semantics;Cognition;Adaptation models;Knowledge graphs;Internet of Things;Training;Brain modeling;Systematics;Robustness","","","","","IEEE","7 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Generative AI Beyond LLMs: System Implications of Multi-Modal Generation","A. Golden; S. Hsia; F. Sun; B. Acun; B. Hosmer; Y. Lee; Z. DeVito; J. Johnson; G. -Y. Wei; D. Brooks; C. -J. Wu","FAIR, Meta; FAIR, Meta; Meta; FAIR, Meta; FAIR, Meta; FAIR, Meta; FAIR, Meta; FAIR, Meta; Harvard University; Harvard University; FAIR, Meta",2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),"16 Jul 2024","2024","","","257","267","As the development of large-scale Generative AI models evolve beyond text (1D) generation to include image (2D) and video (3D) generation, processing spatial and temporal information presents unique challenges to quality, performance, and efficiency. We present the first work towards understanding this new system design space for multi-modal text-to-image (TTI) and text-to-video (TTV) generation models. Current model architecture designs are bifurcated into 2 categories: Diffusion-and Transformer-based models. Our systematic performance characterization on a suite of eight representative TTI/TTV models shows that after state-of-the-art optimization techniques such as Flash Attention are applied, Convolution accounts for up to 44% of execution time for Diffusion-based TTI models, while Linear layers consume up to 49 % of execution time for Transformer-based models. We additionally observe that Diffusion-based TTI models resemble the Prefill stage of LLM inference, and benefit from 1.1-2.5x greater speedup from Flash Attention than Transformer-based TTI models that resemble the Decode phase. Since optimizations designed for LLMs do not map directly onto TTI/TTV models, we must conduct a thorough characterization of these workloads to gain insights for new optimization opportunities. In doing so, we define sequence length in the context of TTI/TTV models and observe sequence length can vary up to 4x in Diffusion model inference. We additionally observe temporal aspects of TTV workloads pose unique system bottlenecks, with Temporal Attention accounting for over 60 % of total Attention time. Overall, our in-depth system performance characterization is a critical first step towards designing efficient and deployable systems for emerging TTI/TTV workloads.","2766-0486","979-8-3503-7638-8","10.1109/ISPASS61541.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590047","Generative AI;Multi-Modal;Diffusion Model;Transformer;Sequence Length;Attention","Three-dimensional displays;Systematics;Generative AI;Convolution;System performance;Text to image;Transformers","","8","","43","IEEE","16 Jul 2024","","","IEEE","IEEE Conferences"
"“No Free Lunch” when using Large Language Models to Verify Self-Generated Programs","S. Zilberman; H. C. Betty Cheng","Computer Science & Engineering, Michigan State University; Computer Science & Engineering, Michigan State University","2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","17 Sep 2024","2024","","","29","36","Large Language Models (LLMs) have shown great success in a wide range of text-generation tasks including the synthesis of code from natural language descriptions. As LLMbased techniques continue to grow in popularity, especially amongst entry-level developers, LLM-generated code has the potential to be deployed in a diverse set of application domains. While LLMs can generate syntactically correct code output, recent work has shown the presence of nonsensical and faulty reasoning in LLM-generated text. As such, overreliance on LLMs for software generation may potentially result in the deployment of faulty software leading to critical system failures. This study explores the capabilities of a single LLM to generate both software and corresponding test suites from the same initial program descriptions, which can be considered analogous to an individual developer coding and unit testing for a given piece of software. We present an empirical framework and evaluation methodology to assess the usefulness of LLM-generated test cases for verifying programs generated by the same LLM. Our findings indicate that LLMs frequently generate irrelevant tests that suffer from numerous quality concerns.","2159-4848","979-8-3503-4479-0","10.1109/ICSTW60967.2024.00018","Michigan State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10675844","automated software testing;software engineering;deep learning","Software testing;Codes;Automation;Reviews;Large language models;Conferences;Natural languages","","1","","36","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot","Z. Ságodi; I. Siket; R. Ferenc","Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary",IEEE Access,"27 May 2024","2024","12","","72303","72316","Large Language Models (LLMs) have grown in popularity in recent years and are now employed in a variety of software engineering domains thanks to their Natural Language Processing (NLP) capabilities, which include source code generation, understanding, and documentation. Selecting the appropriate model for source code generation presents a problem to developers as more and more powerful LLMs become available. While some studies have evaluated Copilot or ChatGPT, there is a lack of research on how developers can choose from available LLMs, which is a key factor in the growing set of available models and services. It is crucial to know if a model is capable of generating useful source code that meets the quality requirements and if the developers will be able to use the generated code. Regarding these factors, one has to decide which model to utilize during everyday tasks. This paper shows a methodology to compare such models by demonstrating an actual comparison of two models. Subsequently, we investigated the functional and non-functional qualities of the code synthesized by the models on a program synthesis benchmark containing 25 tasks. On average, the functional testing shows that ChatGPT generated 17 perfect solutions, while Copilot could only solve 13. The non-functional analysis reflected that both models generated good quality code, however, both have characteristic code smells. Our evaluation shows that ChatGPT performs better using this methodology, which is supported by human reviewers who evaluated the generated code by hand.","2169-3536","","10.1109/ACCESS.2024.3403858","European Union Project within the Framework of the Artificial Intelligence National Laboratory(grant numbers:RRF-2.3.1-21-2022-00004); Ministry of Culture and Innovation of Hungary from the National Research, Development and Innovation Fund, financed under the TKP2021-NVA Funding Scheme(grant numbers:TKP2021-NVA-09); European Union under the Horizon Europe Programme (AI4CYBER)(grant numbers:101070450); University of Szeged Open Access Fund(grant numbers:6537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535504","Artificial intelligence;copilot;ChatGPT;code-synthesis;code quality;large language models;model selection","Codes;Source coding;Chatbots;Task analysis;Static analysis;C++ languages;Analytical models;Artificial intelligence;Large language models","","10","","34","CCBYNCND","21 May 2024","","","IEEE","IEEE Journals"
"QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling","B. Guda; G. Z. Ashungafac; L. Francis; C. Joe-Wong","Carnegie Mellon University Africa, Kigali, Rwanda; Carnegie Mellon University Africa, Kigali, Rwanda; Carnegie Mellon University Africa, Kigali, Rwanda; Carnegie Mellon University, Pittsburgh, USA",2024 IEEE Globecom Workshops (GC Wkshps),"12 Aug 2025","2024","","","1","6","Large Language models (LLMs) have brought about substantial advancements in the field of Question Answering (QA) systems. These models do remarkably well in addressing intricate inquiries in a variety of disciplines. However, because of domain-specific vocabulary, complex technological concepts, and the requirement for exact responses, applying LLMs to specialized sectors like telecommunications presents additional obstacles. GPT-3.5 has been used in recent work, to obtain noteworthy accuracy for telecom-related questions in a Retrieval Augmented Generation (RAG) framework. Notwithstanding these developments, the practical use of models such as GPT-3.5 is restricted by their proprietary nature and high computing demands. This paper introduces QMOS, an innovative approach which uses a Question-Masked loss and Option Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice Questions in the telecommunications domain. Our focus was on using open-source, smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework. Our multi-faceted approach involves several enhancements to the whole LLM-RAG pipeline of finetuning, retrieval, prompt engineering and inference. Our approaches significantly outperform existing results, achieving accuracy improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07% to 84.65% with Phi-2.","2166-0077","979-8-3315-0567-7","10.1109/GCWkshp64532.2024.11100592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11100592","Large Language Models;Telecommunication;RAG;Question Masked Loss;Option Batch-Shuffle Trick","Vocabulary;Accuracy;Computational modeling;Large language models;Conferences;Retrieval augmented generation;Pipelines;Question answering (information retrieval);Telecommunications;Prompt engineering","","","","28","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"Digital Rubber Duck: Leveraging Large Language Models for Extreme Programming","T. Elvira; T. T. Procko; J. O. Couder; O. Ochoa","Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University, Daytona Beach, Florida, United States of America","2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)","9 Apr 2024","2023","","","295","304","The recent prevalence of Large Language models (LLMs), e.g., GPT-3.5 and GPT-4, has brought about a new age of man-computer symbiosis, where LLMs are employed for a litany of creative, constructive, scientific, or otherwise content-generative tasks, e.g., as general chatbot assistants, writing editors, digital subject matter experts, programming consultants, and so on. Of interest to software engineers is the concept of “rubber duck debugging”, which is the act of expressing code, line-by-line, in natural language, to an inanimate object, e.g., a rubber duck, for the purpose of elucidating potential issues that can then be corrected. In this paper, we detail a workflow process that leverages the concept of rubber duck debugging, replacing the duck with a capable LLM, e.g., GPT-4. We call it Digital Rubber Duck Programming. Furthermore, the Extreme Programming (XP) method, an implementation of the Agile paradigm, is considered as easily integrated with the proposed workflow, as XP is performed in pairs (much like the modern software engineer works in pairwise fashion with an LLM) and because XP places emphasis on performing extensive code reviews and unit testing all code, which capable LLMs like GPT-4 can facilitate.","","979-8-3503-2759-5","10.1109/CSCE60160.2023.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487209","Software Engineering;Code Refactoring;Walkthroughs;Extreme Programmaing;GPT-4;ChatGPT Paper Type - “Regular research Paper”","Codes;Natural languages;Process control;Debugging;Programming;Chatbots;Software","","","","47","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Security Consensus of Multi-agent Systems under Cyber-attack: An Iterative Learning Control Strategy","Z. Luo; H. Yang; J. Jiang","School of Science, Civil Aviation Flight, University of China, Chengdu, China; School of Science, Civil Aviation Flight, University of China, Chengdu, China; School of Science, Civil Aviation Flight, University of China, Chengdu, China",2024 6th International Conference on Data-driven Optimization of Complex Systems (DOCS),"10 Oct 2024","2024","","","524","529","This paper focuses on the security consensus problem of multi-agent systems under cyber-attack. Two types of cyber-attack are considered in our research, this situation is more common in real applications. These two types of network attacks (deception and DoS attacks) may occur alternately, resulting in the infeasibility to obtain actual output measurements. To reach the system security, a novel iterative learning control strategy is designed to analyze the issue. Then, sufficient conditions are proposed from the view of the trajectory tracking, and our achieved convergence conditions are relatively simpler in comparison to the existing literature. Furthermore, the theoretical result in this paper is an extension of existing research. Finally, a numerical simulation is presented for illustration.","","979-8-3503-7784-2","10.1109/DOCS63458.2024.10704508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10704508","Security consensus;Multi-agent system;ILC;Convergence analysis","Sufficient conditions;Trajectory tracking;Numerical simulation;Security;Complex systems;Cyberattack;Optimization;Iterative learning control;Multi-agent systems;Convergence","","","","20","IEEE","10 Oct 2024","","","IEEE","IEEE Conferences"
"Assessing GPT's Potential for Word Sense Disambiguation: A Quantitative Evaluation on Prompt Engineering Techniques","D. Sumanathilaka; N. Micallef; J. Hough","Department of Computer Science, Swansea University, Swansea, Wales, United Kingdom; Department of Computer Science, Swansea University, Swansea, Wales, United Kingdom; Department of Computer Science, Swansea University, Swansea, Wales, United Kingdom",2024 IEEE 15th Control and System Graduate Research Colloquium (ICSGRC),"1 Oct 2024","2024","","","204","209","Modern digital communications (including social media content) often contain ambiguous words due to their potential for multiple related interpretations (polysemy). This ambiguity poses challenges for traditional Word Sense Disambiguation (WSD) methods, which struggle with limited data and lack of contextual understanding. These limitations hinder efficient translation, information retrieval, and question-answering systems, thereby restricting the benefits of computational linguistics techniques when applied to digital communication technologies. Our research investigates the use of Large Language Models (LLMs) to improve WSD using various prompt engineering techniques. We propose and evaluate a novel method that combines a knowledge graph, together with Part-of-Speech (POS) tagging and few-shot prompting to guide LLMs. By utilizing prompt augmentation with human-in-loop on few-shot prompt approaches, this work demonstrates a substantial improvement in WSD. This research advances accurate word interpretation in digital communications, leading to important implications for improved translation systems, better search results, and more intelligent question-answering technology.","2833-1028","979-8-3503-8655-4","10.1109/ICSGRC62081.2024.10691283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691283","Large Language Models;Word Sense Disambiguation;FEWS sense tags;Few Shot Prompting;Knowledge Graph","Accuracy;Social networking (online);Large language models;Knowledge graphs;Tagging;Digital communication;Control systems;Computational linguistics;Prompt engineering","","2","","37","Crown","1 Oct 2024","","","IEEE","IEEE Conferences"
"Assessing the effectiveness of AI generated code in Improving Software Engineering Processes using GPT-Engineer with OpenAI GPT models","T. Y. Wikedzi; M. Gladness","NOTTECH Company Limited, Arusha, Tanzania; NOTTECH Company Limited, Arusha, Tanzania",2025 IEEE/ACM Symposium on Software Engineering in the Global South (SEiGS),"11 Jun 2025","2025","","","27","34","This paper evaluates GPT-Engineer, an open-source AI-powered tool leveraging OpenAI’s models, other Large Language Models (LLMs), and flexible integrations to enhance software engineering workflows. We applied GPT-Engineer to document an API codebase using Swagger Documentation, showcasing the tool’s potential to streamline engineering processes, optimize resources, and address challenges such as workforce shortages. This study focused on assessing GPT-Engineer’s effectiveness in generating precise, reliable API documentation and its understanding of the underlying codebase by targeting specific Laravel controllers and models, without requiring manual annotations. The evaluation explored adaptability, prompt effectiveness, and cost-efficiency using the ""improve (-i)"" module within GPT-Engineer across various GPT models. Findings indicate that AI-driven tools like GPT-Engineer can mitigate resource constraints, enabling organizations in low-resource environments to produce high-quality software. Key results highlight the importance of prompt engineering strategies, effective cost management, and scalable integrations for maximizing the utility of AI solutions in resource-limited settings.","","979-8-3315-1428-0","10.1109/SEiGS66664.2025.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11026896","GPT-Engineer;API Documentation;Large Language Models (LLMs);Swagger;Laravel;Prompt Engineering;AI Integration;Cost-efficiency","Productivity;Adaptation models;Costs;Codes;Large language models;Documentation;Software reliability;Prompt engineering;Best practices;Software engineering","","","","17","IEEE","11 Jun 2025","","","IEEE","IEEE Conferences"
"Decentralized Multi-Agent System for Protection and the Power Restoration Process in Microgrids","H. F. Habib; O. Mohammed","Department of Information Engineering, University of Padova (UNIPD-DEI); Florida International University, Miami, FL, US",2017 Ninth Annual IEEE Green Technologies Conference (GreenTech),"11 May 2017","2017","","","358","364","This paper proposes a fault localization, isolation and restoration method for microgrids based on multiagent system (MAS) utilizing a communication network. The agents are located in the middle and the two ends of each section. The fault is detected through phase angle comparison of current signals at both sides of the distribution line and sends the trip signal to circuit breakers. This technique does not require voltage transformers or relays and does not transfer the data for long distances between agents to decrease the delay time for isolating the fault. We also performed power restoration process following fault clearance considering voltage, frequency and power flow constraints in the microgrid. Simulation studies were performed to validate the proposed protection scheme.","2166-5478","978-1-5090-4535-8","10.1109/GreenTech.2017.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7923982","Protection;Microgrid;Multi Agent Systems (MAS);Restoration Process","Circuit breakers;Circuit faults;Microgrids;Current transformers;Relays;Generators;Current measurement","","23","","12","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"State of Practice: LLMs in Software Engineering and Software Architecture","J. Jahić; A. Sami","University of Cambridge, Cambridge, UK; Edinburgh Napier University, Edinburgh, UK",2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C),"21 Aug 2024","2024","","","311","318","Large Language Models (LLMs) are finding their way into Software Engineering by assisting with tasks such as code generation. Furthermore, LLMs might have a potential to perform even more complex tasks, such as suggesting architectural design. However, there is a lack of empirical surveys on how software engineering companies use (and plan to use) LLMs and if LLMs truly can provide benefits to software architects. To understand the state of practice considering adoption of LLMs in software engineering, existing challenges, and future trends, we have surveyed 15 different software engineering companies. To understand the ability of LLMs to perform more complex tasks, we report on our experiments with LLM-assisted architectural design. We applied ChatGPT on 5 software projects and in total performed 50 different experiments. Our results capture the state of the practice of LLMs in software engineering and demonstrate how LLMs perform when assisting with (more complex task such as) architectural design. Engineers, architects, and project managers should profit from these results to guide their decision towards targeted adoption of LLMs in their business and engineering domains.","2768-4288","979-8-3503-6625-9","10.1109/ICSA-C63560.2024.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628428","Architecture;AI;Design Space Exploration;ChatGPT","Surveys;Codes;Software architecture;Large language models;Companies;Market research;Chatbots","","7","","19","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Evaluation of Generative AI Models in Python Code Generation: A Comparative Study","D. Palla; A. Slaby","Faculty of Informatics and Management, University of Hradec Kralove, Hradec Kralove, Czech Republic; Faculty of Informatics and Management, University of Hradec Kralove, Hradec Kralove, Czech Republic",IEEE Access,"18 Apr 2025","2025","13","","65334","65347","This study evaluates leading generative AI models for Python code generation. Evaluation criteria include syntax accuracy, response time, completeness, reliability, and cost. The models tested comprise OpenAI’s GPT series (GPT-4 Turbo, GPT-4o, GPT-4o Mini, GPT-3.5 Turbo), Google’s Gemini (1.0 Pro, 1.5 Flash, 1.5 Pro), Meta’s LLaMA (3.0 8B, 3.1 8B), and Anthropic’s Claude models (3.5 Sonnet, 3 Opus, 3 Sonnet, 3 Haiku). Ten coding tasks of varying complexity were tested across three iterations per model to measure performance and consistency. Claude models, especially Claude 3.5 Sonnet, achieved the highest accuracy and reliability. They outperformed all other models in both simple and complex tasks. Gemini models showed limitations in handling complex code. Cost-effective options like Claude 3 Haiku and Gemini 1.5 Flash were budget-friendly and maintained good accuracy on simpler problems. Unlike earlier single-metric studies, this work introduces a multi-dimensional evaluation framework that considers accuracy, reliability, cost, and exception handling. Future work will explore other programming languages and include metrics such as code optimization and security robustness.","2169-3536","","10.1109/ACCESS.2025.3560244","Article Processing Charge (APC) through the Specific Research Support Project 2025 (SPEV 2025) Run with the Faculty of Informatics and Management, University of Hradec Kralove; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10963975","Automatization;generative AI;LLM;python;software development","Codes;Generative AI;Costs;Encoding;Accuracy;Python;Artificial intelligence;Internet;Software development management;Reliability","","1","","60","CCBY","11 Apr 2025","","","IEEE","IEEE Journals"
"A Systematic Study on the Potentials and Limitations of LLM-assisted Software Development","C. Michelutti; J. Eckert; M. Monecke; J. Klein; S. Glesner","Technische Universität Berlin, Berlin, Germany; Technische Universität Berlin, Berlin, Germany; Technische Universität Berlin, Berlin, Germany; Technische Universität Berlin, Berlin, Germany; Technische Universität Berlin, Berlin, Germany",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","330","338","In the field of software engineering, Large Language Models like GPT have gained enormous interest in recent times. With its expanding area of application, ChatGPT has become an essential tool for code generation. Several studies have shown that the quality of generated code depends on the underlying dataset and the quality of the provided prompts. However, its precise capabilities and limitations remain uncertain, as does the extent of assistance required for effective code generation. We present the results of our systematic study in which we investigate the potential of ChatGPT, based on GPT-4, in solving assignments of an introductory-level programming class. We examine the impact of programming language choice, different prompting strategies, and the results of the model compared to those of real students. Our results show that ChatGPT cannot solve the assignments independently, but outperforms the average student with human assistance.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852455","Large Language Models;Software Development;ChatGPT;Code Generation;Haskell;Java;Functional Programming;Object Oriented Programming;Prompt Engineering","Computer languages;Java;Sequential analysis;Codes;Systematics;Large language models;Chatbots;Testing;Software engineering;Software development management","","","","28","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks","S. Ullah; M. Han; S. Pujar; H. Pearce; A. Coskun; G. Stringhini",Boston University; Boston University; IBM Research; UNSW Sydney; Boston University; Boston University,2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","862","880","Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like ‘PaLM2’ and ‘GPT-4’: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646663","Machine Learning;Computer Security","Privacy;Source coding;Computer bugs;Benchmark testing;Maintenance engineering;Cognition;Libraries","","32","","50","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"An Automated Scenario Generation Model for Anti-phishing using Generative AI","J. Y. Park; T. -S. Kim","Dept. of Big Data, Chungbuk National University, Cheong-ju, South Korea; Dept of Management Information System, Chungbuk National University, Cheong-ju, South Korea",2025 IEEE International Conference on Big Data and Smart Computing (BigComp),"31 Mar 2025","2025","","","368","370","Generative AI could be used not only for classification or prediction, but also to generate images, music, etc. based on the input data. Generative AI was abused to generate deepfake and spear-phishing to cause social and economic problems. To prevent these attacks, simulation training is necessary. Simulation training was necessary because it allowed us to effectively respond to attacks and reduce damage. We proposed an ensemble model that automated scenarios generation by utilizing multiple generative AIs. This study was expected to develop an automated training scenario generation model using generative AI that could be used in cybersecurity training, anti-phishing training, and security consulting to automatically generate realistic and customized training scenarios.","2375-9356","979-8-3315-2902-4","10.1109/BigComp64353.2025.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10936851","Cybersecurity;Anti-phishing;Training scenario;Automated generation model;Generative AI","Training;Measurement;Economics;Generative AI;Databases;Phishing;Computational modeling;Organizations;Vectors;Scenario generation","","","","9","IEEE","31 Mar 2025","","","IEEE","IEEE Conferences"
"Integrating Artificial Open Generative Artificial Intelligence into Software Supply Chain Security","V. Alevizos; G. A. Papakostas; A. Simasiku; D. Malliarou; A. Messinis; S. Edralin; C. Xu; Z. Yue","Karolinska Institutet, Solna, Sweden; MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, Greece; Zambia University, Ndola, Zambia; IntelliSolutions, Athens, Greece; HEDNO SA, Athens, Greece; University of Illinois Urbana-Champaign, Illinois, USA; Mayo Clinic Artificial Intelligence & Discovery, Minnesota, USA; Auburn University Harrison College of Pharmacy, Alabama, USA",2024 5th International Conference on Data Analytics for Business and Industry (ICDABI),"20 Dec 2024","2024","","","200","206","While new technologies emerge, human errors always looming. Software supply chain is increasingly complex and intertwined, the security of a service has become paramount to ensuring the integrity of products, safeguarding data privacy, and maintaining operational continuity. In this work, we conducted experiments on the promising open Large Language Models (LLMs) into two main software security challenges: source code language errors and deprecated code, with a focus on their potential to replace conventional static and dynamic security scanners that rely on predefined rules and patterns. Our findings suggest that while LLMs present some unexpected results, they also encounter significant limitations, particularly in memory complexity and the management of new and unfamiliar data patterns. Despite these challenges, the proactive application of LLMs, coupled with extensive security databases and continuous updates, holds the potential to fortify Software Supply Chain (SSC) processes against emerging threats.","","979-8-3503-6871-0","10.1109/ICDABI63787.2024.10800301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800301","Large Language Models;Software Supply Chain Security;Vulnerabilities","Industries;Data privacy;Generative AI;Databases;Large language models;Source coding;Supply chains;Memory management;Software;Security","","","","34","IEEE","20 Dec 2024","","","IEEE","IEEE Conferences"
"A Superalignment Framework in Autonomous Driving with Large Language Models","X. Kong; T. Braunl; M. Fahmi; Y. Wang","Department of Electrical, Electronic and Computer Engineering, University of Western Australia, Crawley, WA, Australia; Department of Electrical, Electronic and Computer Engineering, University of Western Australia, Crawley, WA, Australia; Department of Transport and Main Roads, Queensland Government, Brisbane, QLD, Australia; Department of Transport and Main Roads, Queensland Government, Brisbane, QLD, Australia",2024 IEEE Intelligent Vehicles Symposium (IV),"15 Jul 2024","2024","","","1715","1720","Over the last year, significant advancements have been made in the realms of large language models (LLMs) and multi-modal large language models (MLLMs), particularly in their application to autonomous driving. These models have showcased remarkable abilities in processing and interacting with complex information. In autonomous driving, LLMs and MLLMs are extensively used, requiring access to sensitive vehicle data such as precise locations, images, and road conditions. This data is transmitted to an LLM-based inference cloud for advanced analysis. However, concerns arise regarding data security, as the protection against data and privacy breaches primarily depends on the LLM’s inherent security measures, without additional scrutiny or evaluation of the LLM’s inference outputs. Despite its importance, the security aspect of LLMs in autonomous driving remains underexplored. Addressing this gap, our research introduces a novel security framework for autonomous vehicles, utilizing a multi-agent LLM approach. This framework is designed to safeguard sensitive information associated with autonomous vehicles from potential leaks, while also ensuring that LLM outputs adhere to driving regulations and align with human values. It includes mechanisms to filter out irrelevant queries and verify the safety and reliability of LLM outputs. Utilizing this framework, we evaluated the security, privacy, and cost aspects of eleven large language model-driven autonomous driving cues. Additionally, we performed QA tests on these driving prompts, which successfully demonstrated the framework’s efficacy.","2642-7214","979-8-3503-4881-1","10.1109/IV55156.2024.10588403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10588403","","Large language models;Roads;Privacy breach;Information filters;Regulation;Safety;Reliability","","5","","37","IEEE","15 Jul 2024","","","IEEE","IEEE Conferences"
"A Data-Driven Distributed Adaptive Control Approach for Nonlinear Multi-Agent Systems","X. Yu; S. Jin; G. Liu; T. Lei; Y. Ren","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; College of Electrical and Information Engineering, Zhengzhou University of Light Industry, Zhengzhou, China; School of Electrical and Control Engineering, North China University of Technology, Beijing, China",IEEE Access,"24 Nov 2020","2020","8","","207884","207893","In this paper the distributed leader-follower consensus tracking problem is investigated for unknown nonlinear non-affine discrete-time multi-agent systems. Via a dynamic linearization method both for the agent system and the local ideal distributed controller, a distributed adaptive control scheme is proposed in this paper using the Newton-type optimization method. The proposed approach is data-driven since only the local measurement information among neighboring agents is utilized in the control system design. The consensus tracking stabilities of the proposed approach are rigorously guaranteed in the cases of fixed and switching communication topologies. The simulations are conducted to verify the effectiveness of the proposed approach.","2169-3536","","10.1109/ACCESS.2020.3038629","National Natural Science Foundation of China (NSFC)(grant numbers:61433002,61833001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261580","Dynamic linearization;data-driven control;adaptive control;multi-agent systems;consensus tracking","Adaptive control;Decentralized control;Heuristic algorithms;Data models;Multi-agent systems;Adaptation models;Topology","","7","","36","CCBYNCND","17 Nov 2020","","","IEEE","IEEE Journals"
"AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs","M. Gao; J. Zhao; Z. Lin; W. Ding; X. Hou; Y. Feng; C. Li; M. Guo",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Sun Yat-sen University; Fudan University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE 42nd International Conference on Computer Design (ICCD),"2 Jan 2025","2024","","","162","169","Recently, the use of large language models (LLMs) for software code generation, e.g., C/C++ and Python, has proven a great success. However, LLMs still suffer from low syntactic and functional correctness when it comes to the generation of register-transfer level (RTL) code, such as Verilog. To address this issue, in this paper, we develop AutoVCoder, a systematic open-source framework that significantly improves the LLMs' correctness of generating Verilog code and enhances the quality of its output at the same time. Our framework integrates three novel techniques, including a high-quality hardware dataset generation approach, a two-round LLM fine-tuning method and a domain-specific retrieval-augmented generation (RAG) mechanism. Experimental results demonstrate that AutoVCoder outperforms both industrial and academic LLMs in Verilog code generation. Code and models are available at https://github.com/sjtu-zhao-lab/AutoVCoder.","2576-6996","979-8-3503-8040-8","10.1109/ICCD63220.2024.00033","National Natural Science Foundation of China(grant numbers:62472273,62232015); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2024A1515013155); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10817982","","Codes;Systematics;Retrieval augmented generation;Natural languages;Syntactics;Register transfer level;Hardware;Software;Hardware design languages;Python","","3","","24","IEEE","2 Jan 2025","","","IEEE","IEEE Conferences"
"Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs","Auffarth",NA,"Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs","","2023","","","","","2024 Edition – Get to grips with the LangChain framework to develop production-ready applications, including agents and personal assistants. The 2024 edition features updated code examples and an improved GitHub repository. Purchase of the print or Kindle book includes a free PDF eBook. Key FeaturesLearn how to leverage LangChain to work around LLMs’ inherent weaknessesDelve into LLMs with LangChain and explore their fundamentals, ethical dimensions, and application challengesGet better at using ChatGPT and GPT models, from heuristics and training to scalable deployment, empowering you to transform ideas into realityBook DescriptionChatGPT and the GPT models by OpenAI have brought about a revolution not only in how we write and research but also in how we can process information. This book discusses the functioning, capabilities, and limitations of LLMs underlying chat systems, including ChatGPT and Gemini. It demonstrates, in a series of practical examples, how to use the LangChain framework to build production-ready and responsive LLM applications for tasks ranging from customer support to software development assistance and data analysis – illustrating the expansive utility of LLMs in real-world applications. Unlock the full potential of LLMs within your projects as you navigate through guidance on fine-tuning, prompt engineering, and best practices for deployment and monitoring in production environments. Whether you're building creative writing tools, developing sophisticated chatbots, or crafting cutting-edge software development aids, this book will be your roadmap to mastering the transformative power of generative AI with confidence and creativity.What you will learnCreate LLM apps with LangChain, like question-answering systems and chatbotsUnderstand transformer models and attention mechanismsAutomate data analysis and visualization using pandas and PythonGrasp prompt engineering to improve performanceFine-tune LLMs and get to know the tools to unleash their powerDeploy LLMs as a service with LangChain and apply evaluation strategiesPrivately interact with documents using open-source LLMs to prevent data leaksWho this book is forThe book is for developers, researchers, and anyone interested in learning more about LangChain. Whether you are a beginner or an experienced developer, this book will serve as a valuable resource if you want to get the most out of LLMs using LangChain. Basic knowledge of Python is a prerequisite, while prior exposure to machine learning will help you follow along more easily.","","9781835088364","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10718333.pdf&bkn=10718332&pdfType=book","","","","","","","","15 Oct 2024","","","Packt Publishing","Packt Publishing eBooks"
"Rule-Augmented Artificial Intelligence-empowered Systems for Medical Diagnosis using Large Language Models","D. P. Panagoulias; F. A. Palamidas; M. Virvou; G. A. Tsihrintzis","Department of Informatics, University of Piraeus, Piraeus, Greece; Department of Medicine, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, University of Piraeus, Piraeus, Greece; Department of Informatics, University of Piraeus, Piraeus, Greece",2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI),"20 Dec 2023","2023","","","70","77","In this paper, we investigate the enhancement of Artificial Intelligence (AI) technologies in healthcare and the better understanding of medical literature with the use of Large Language Models (LLMs) and Natural Language Processing (NLP). Specifically, we introduce a rule-augmented AI-empowered system which incorporates a rule-based decision system, the ChatGPT application programming interface (API), and other external machine learning and analytical APIs to offer diagnostic suggestions to patients. The complexities of patient healthcare experiences, including doctor-patient interactions, understanding levels, treatment procedures, and preventive care, are considered. We illustrate how a diagnostic process typically integrates various strategies depending on various factors. To digitize the greatest portion of the process, we propose and illustrate the use of LLMs for humanizing the communication process and investigating ways to reduce burdens and costs in primary healthcare. We also outline a theoretical decision model for evaluating the use of technological components from external sources versus building them from scratch. The paper is structured into sections detailing background theories and context, our proposed and implemented rule-augmented AI-empowered system, as well as a system test in a corresponding use case. Finally, the paper key findings are presented, which contribute valuable insights for future work in this field.","2375-0197","979-8-3503-4273-4","10.1109/ICTAI59109.2023.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356438","AI-empowered software engineering;explainability;ChatGPT;LLM;NLP;prompt-engineering","Costs;Buildings;Machine learning;Chatbots;Software;Complexity theory;Medical diagnosis","","11","","33","IEEE","20 Dec 2023","","","IEEE","IEEE Conferences"
"Engineering Safety Requirements for Autonomous Driving with Large Language Models","A. Nouri; B. Cabrero-Daniel; F. Törner; H. Sivencrona; C. Berger","Volvo Cars, Gothenburg, Sweden; Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden; Volvo Cars, Gothenburg, Sweden; Zenseact, Gothenburg, Sweden; Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden",2024 IEEE 32nd International Requirements Engineering Conference (RE),"21 Aug 2024","2024","","","218","228","Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","2332-6441","979-8-3503-9511-2","10.1109/RE59067.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628478","Requirement Engineering;Hazard Analysis Risk Assessment;Autonomous Vehicles;DevOps;Safety;Large Language Model;Prompt Engineering;LLM;ChatGPT","Reviews;Large language models;Pipelines;Refining;Prototypes;Companies;Natural language processing","","7","","20","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Secure Consensus Control for Time-Varying Multi-Agent Systems with Mixed Types Attacks","X. -M. Li; W. Xiao; Q. Zhou; H. Li","Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China","2019 6th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)","12 Jun 2020","2019","","","132","137","Secure control plays a important role in consensus control of multi-agent systems, which significance has made the security performance a main concern. This paper considers the secure consensus control problem for time-varying multi-agent systems with randomly occurred mixed types attacks. Different existing studies have concerned with secure control by assuming specific type attacks, the general of mixed types attacks model is considered. Specifically, DoS and FDI attacks are considered for measurement and control channels, and they are assumed to obey Bernoulli process. To analyze the secure consensus of the resulting closed-loop MASs, the stochastic analysis technique is introduced, based on which sufficient criteria for the H∞ controller are obtained, ensuring the consensus and the prescribed H∞ performanc. The proposed control approach is demonstrate by the controller utilized in MASs subject to mixed types attacks.","2639-4235","978-1-7281-5857-0","10.1109/ICCSS48103.2019.9115461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115461","","","","2","","12","IEEE","12 Jun 2020","","","IEEE","IEEE Conferences"
"Will Generative AI Fill the Automation Gap in Software Architecting?","J. Ivers; I. Ozkaya","Software Engineering Institute Carnegie Mellon University, Pittsburgh, USA; Software Engineering Institute Carnegie Mellon University, Pittsburgh, USA",2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C),"30 May 2025","2025","","","41","45","Researchers are aware that software architects lack effective automation to support much of their work. Generative AI (GenAI) is sparking research interest regarding its potential role in filling this gap, inspired by promising applications of GenAI to other software engineering activities. In this paper, we aim to reflect and sharpen this conversation from the vague “how can GenAI be applied to architecture” to “which architecture activities are most amenable to application of GenAI.” We stress the importance of considering contributions in the context of workflows and reflect on the alignment (or lack thereof) of GenAI with the nature of common architecture tasks through the discussion of five common architecture activities. We offer guiding criteria to assist architecture researchers in focusing on activities that are both amenable to automation and likely to obtain significant utility from GenAI.","2768-4288","979-8-3315-3336-6","10.1109/ICSA-C65153.2025.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11015085","software architecture;generative AI;LLM;architecture decision making;software architecture automation;architecting workflows","Automation;Software architecture;Generative AI;Decision making;Focusing;Computer architecture;Oral communication;Software;Filling;Stress","","","","17","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"Security of Multi-Agent Cyber-Physical Systems: A Survey","R. Owoputi; S. Ray","Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",IEEE Access,"24 Nov 2022","2022","10","","121465","121479","Multi-agent systems are becoming increasingly popular due to their successful implementation in several sectors. However, there are a variety of threats that might undermine the agent’s security and imperil system security. As a result, security concerns should be addressed during the design of multi-agent systems. This survey reviews different models for securing multi-agent systems, which were developed based on the concepts regarding the agent’s role and communications. This paper presents and categorizes the most common attacks on MASs. Then, we study and analyze numerous security strategies in the literature, classifying them as prevention, detection, and resiliency approaches based on reputation and trust. Finally, we recommend which security approach is the best countermeasure for specific types of attacks based on recent advances in the research field.","2169-3536","","10.1109/ACCESS.2022.3223362","National Science Foundation(grant numbers:CNS-1908549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955543","Cyber physical system;multi agent systems;resiliency;security","Security;Taxonomy;Multi-agent systems;Topology;Cyber-physical systems;Task analysis;Ontologies","","12","","134","CCBY","18 Nov 2022","","","IEEE","IEEE Journals"
"Real-Time Environment Monitoring and Response Through IoT and Retrieval-Augmented Generation","S. Oh; S. Kum; J. Moon","Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea; Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea; Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea",2024 15th International Conference on Information and Communication Technology Convergence (ICTC),"14 Jan 2025","2024","","","1658","1659","This paper introduces an architecture that combines IoT-driven data collection with Retrieval-Augmented Generation (RAG) for real-time environmental monitoring, analysis, and prediction. Using edge computing for local processing and centralized LLMs for complex analysis, the system delivers scalable and timely insights into environmental issues like climate change and disaster prevention. Data from IoT sensors is preprocessed at edge devices and sent to a central server, where embeddings are generated, stored in a vector database, and analyzed by the LLM for fast query handling. This system provides real-time feedback through dashboards, focusing on overcoming challenges such as network delays, scalability, and security to deliver reliable environmental monitoring for non-expert users.","2162-1241","979-8-3503-6463-7","10.1109/ICTC62082.2024.10827595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827595","IoT;Edge Computing;Retrieval-Augmented Generation (RAG);Large Language Models (LLMs);Environment Monitoring;Sustainable Energy Management","Accuracy;Scalability;Retrieval augmented generation;Computer architecture;Data collection;Real-time systems;Vectors;Environmental monitoring;Servers;Edge computing","","1","","3","IEEE","14 Jan 2025","","","IEEE","IEEE Conferences"
"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models","H. Strobelt; A. Webson; V. Sanh; B. Hoover; J. Beyer; H. Pfister; A. M. Rush","IBM Research, China; Brown University, USA; Huggingface, USA; IBM Research, China; Harvard SEAS, USA; Harvard SEAS, USA; Huggingface, USA",IEEE Transactions on Visualization and Computer Graphics,"16 Dec 2022","2023","29","1","1146","1156","State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.","1941-0506","","10.1109/TVCG.2022.3209479","NSF(grant numbers:IIS-1901030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9908590","Natural language processing;language modeling;zero-shot models","Task analysis;Visualization;Analytical models;Training;Natural language processing;Transformers;Computational modeling","","66","","46","IEEE","3 Oct 2022","","","IEEE","IEEE Journals"
"Requirements Conflicts Detection: Advancing with Conversational AI","G. Kisso; F. Fotrousi","Capgemini AB Sweden, Västerås, Sweden; Chalmers and University of Gothenburg, Gothenburg, Sweden",2024 IEEE 32nd International Requirements Engineering Conference Workshops (REW),"21 Aug 2024","2024","","","101","107","Conversational AI, which includes technologies like chatbots and virtual assistants facilitate Crowd-Based Requirements Engineering (CrowdRE) by streamlining the process of gathering and analyzing requirements from a large and diverse group of stakeholders through conversations. These technologies offer significant advantages, but they also present unique challenges, especially when dealing with conflicting requirements of crowds with different goals and needs. Such conflicts can lead to inconsistencies in system design, causing project delays, increased costs, and potential failures. This study introduces a novel solution to manage requirement conflicts by leveraging a Conversational AI developed with the open-source Rasa framework. The proposed system is designed to detect conflicts in real-time during stakeholder conversations. The study conducted an experiment to evaluate Conversational AI performance compared with requirements engineers using four different datasets. The preliminary evaluation of the Conversational AI shows its efficacy in real-time conflict detection. The analysis implies no significant difference in mean performance between the Conversational AI and the requirements engineers in detecting requirements conflicts. The initial results are promising for a user-friendly and efficient method for instantaneous conflict detection, although further training and evaluation in operational settings are needed. This opens avenues for conflict detection of crowd-based requirements and conflict resolution using Conversational AI, to enhance the quality and success rates of software projects.","2770-6834","979-8-3503-9551-8","10.1109/REW61692.2024.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628668","Requirements conflicts;Conversational AI;CrowdRE;Software requirements analysis","Training;Statistical analysis;Virtual assistants;Focusing;Oral communication;Real-time systems;Stakeholders","","1","","22","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Hybrid Phishing Detection System: Integrating URL Feature Analysis with Local Large Language Models for Enhanced Security","K. Sandya; T. S. K. Sharma; G. V. Reddy; M. H. Kumar","Department of Computer Science and Engineering (AI & ML), Vignan Institute of Technology and Science, Nalgonda, Telangana; Department of Computer Science and Engineering (AI & ML), Vignan Institute of Technology and Science, Nalgonda, Telangana; Department of Computer Science and Engineering (AI & ML), Vignan Institute of Technology and Science, Nalgonda, Telangana; Department of Computer Science and Engineering (AI & ML), Vignan Institute of Technology and Science, Nalgonda, Telangana",2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),"21 Jul 2025","2025","","","1782","1790","In the past few years, phishing attacks were known to be the greatest threat to cybersecurity, hitting individuals with an illegal website that has the ability to breach their sensitive data. Phishing URLs are designed to replicate a real web address in an attempt to deceive victims into revealing personal details like login details, financial data, or other confidential data. Classic phishing detection methods, i.e., blacklisting and rule-based filtering, have not been capable of keeping up with changing attack techniques. Blacklist mechanisms use current databases and look to see if the URL is present, forgetting zero-day phishing URLs; rule-based methods are guilty of excessive false positives. This paper attempts to solve these problems by introducing a more sophisticated machine learning framework with URL-based feature extraction and a local large language model for phishing detection robustness. The static analyses the URL to extract some prominent features consisting of domain characteristics, URL format, and security markers to the local large language model in the to enhance the contextual knowledge for detection precision. The hybrid model can identify not just classic phishing characteristics but also sophisticated evasion strategies that try to bypass classic detection. The hybrid model is lightweight and has the added advantage of being able to operate offline, thereby making it ideal for secure environments where detection must occur offline. The system under consideration is established to possess an accuracy level and resistance to dynamic phishing attacks, enhancing the web security defence from a reactive to a proactive approach.","","979-8-3315-3884-2","10.1109/ICSSAS66150.2025.11081063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081063","Phishing detection;URL analysis;machine learning;local language model;large language model;cybersecurity;zero-day attacks;feature extraction;web security;adversarial detection;data privacy","Uniform resource locators;Resistance;Filtering;Phishing;Large language models;Machine learning;Static analysis;Feature extraction;Robustness;Blocklists","","","","15","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models and Social Media for Power Outage Detection","E. Stefanowska; M. Jawad; P. Bomba; K. Chmielowiec","Hitachi Energy Research, AGH University of Krakow, Krakow, Poland; Hitachi Energy Research, Krakow, Poland; Hitachi Energy Research, Krakow, Poland; Hitachi Energy Research, Krakow, Poland",IEEE EUROCON 2025 - 21st International Conference on Smart Technologies,"15 Jul 2025","2025","","","1","6","Social media is increasingly used in various emergency response scenarios, such as natural disasters, public health crises, and security incidents. This study explored the potential to overcome the limitations of traditional Outage Management Systems (OMSs) data collection by leveraging the widespread use of social media and advancements in Machine Learning (ML). The objective of this study is to propose and validate a multi-step processing and interaction pipeline that could filter, analyze, and extract relevant information from social media posts, ultimately integrating the obtained information into OMSs for more efficient outage management. Our validation process revealed that large language model Mixtral- $8 \times 7 \mathrm{~B}$-Instruct-v 0.1 can accurately flag posts related to ongoing power outages with a 91% success rate. By integrating ML-driven insights from social media, utility companies could improve their response capabilities, foster community engagement, and build a more resilient power infrastructure.","2837-7990","979-8-3315-0878-4","10.1109/EUROCON64445.2025.11073383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11073383","power outage management;social media;large language models;machine learning;data analysis","Social networking (online);Large language models;Disasters;Pipelines;Machine learning;Information filters;Emergency services;Power system reliability;Security;Public healthcare","","","","11","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Knowledge Graph-Enhanced Semantic Cache for Low-Latency and Cost-Effective Inference in Large Language Models","N. Dominic; B. Pardamean","Bioinformatics & Data Science Research Center, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, BINUS Graduate Program - Master of Computer Science, Bina Nusantara University, Jakarta, Indonesia",2024 International Conference on Information Management and Technology (ICIMTech),"16 Dec 2024","2024","","","340","344","In organizational knowledge management, Large Language Model (LLM) caches act as a semantic repository gathered from previous LLM responses. Due to intensive calls from multiple users, LLM may suffer from high inference latency. While there are many prior available approaches to solve this problem, most of them are inherently complex. This paper introduced a Knowledge Graph-enhanced Semantic Cache mechanism as an alternative, lightweight technique to boost retrieval for similar prompts. The latest state-of-the-art open-source LLM, named Google's Gemma-2B-it, was used to generate sample prompts and responses as a draft, while a knowledge graph (KG) was built from Wikipedia sentences. To create embeddings of prompts and KG, all-MiniLM-L6-v2 from SentenceTransformer was used. This new cache system resulted in up to 28% improvement over a standard model. In particular, reinforcement with KG cache embeddings yielded more than 85% semantic cache accuracy. To map the next trajectory of this pilot study, an overview of the extended framework for LLM knowledge management was also presented in this paper. The framework includes the new KG- enhanced cache system equipped with scalable security and fallback mechanisms that can promote green technology through substantial improvements in latency, throughput, and overall LLM costs.","2837-2778","979-8-3503-9002-5","10.1109/ICIMTech63123.2024.10780864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10780864","semantic cache;knowledge graph;symmetric similarity search;large language models;green technology","Large language models;Semantics;Throughput;Knowledge management;Internet;Trajectory;Security;Online services;Low latency communication;Standards","","","","25","IEEE","16 Dec 2024","","","IEEE","IEEE Conferences"
"Integrating Generative AI in Cybersecurity Curricula","B. AlOmar; Z. Trabelsi","College of Information Technology, United Arab Emirates University, AlAin, UAE; College of Information Technology, United Arab Emirates University, AlAin, UAE",2025 IEEE Global Engineering Education Conference (EDUCON),"3 Jun 2025","2025","","","1","9","Artificial intelligence technologies with generative capabilities have accelerated fundamental transformations in security architectures, necessitating reconceptualization of security frameworks and threat assessment protocols. This study presents a pedagogical framework for integrating generative AI (GenAI) into university-level cybersecurity curricula. The methodology establishes foundational knowledge in generative models and language processing architectures, followed by applications across defensive security measures. The framework includes automated cyber threat intelligence, malicious code and malware detection, log anomaly detection, digital image forensics, and AI-assisted penetration testing. The framework acknowledges the dual-use nature of GenAI in security domains, incorporating prompt injection attacks that manipulate model behaviors and compromise system integrity. Laboratory modules presented will provide students with hands-on experience on advanced tools including large language models, diffusion models, and cognitive architectures for automated security assessment. This study seeks to prepare cybersecurity professionals with critical competencies necessary for effective operation within an environment increasingly shaped by artificial intelligence systems.","2165-9567","979-8-3315-3949-8","10.1109/EDUCON62633.2025.11016426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016426","Generative AI;Cybersecurity;LLMs;Diffusion Models;Penetration Testing","Protocols;Generative AI;Large language models;Forensics;Diffusion models;Malware;Threat assessment;Regulation;Engineering education;Penetration testing","","","","59","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Adaptive Memory Event-Triggered Observer-Based Control for Nonlinear Multi-Agent Systems Under DoS Attacks","X. Guo; D. Zhang; J. Wang; C. K. Ahn","School of Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Autonomous Intelligent Systems Department, Hangzhou Innovation Institute of Beihang University, Hangzhou, China; School of Electrical Engineering, Korea University, Seoul, Korea",IEEE/CAA Journal of Automatica Sinica,"27 Jul 2021","2021","8","10","1644","1656","This paper investigates the event-triggered security consensus problem for nonlinear multi-agent systems (MASs) under denial-of-service (DoS) attacks over an undirected graph. A novel adaptive memory observer-based anti-disturbance control scheme is presented to improve the observer accuracy by adding a buffer for the system output measurements. Meanwhile, this control scheme can also provide more reasonable control signals when DoS attacks occur. To save network resources, an adaptive memory event-triggered mechanism (AMETM) is also proposed and Zeno behavior is excluded. It is worth mentioning that the AMETM's updates do not require global information. Then, the observer and controller gains are obtained by using the linear matrix inequality (LMI) technique. Finally, simulation examples show the effectiveness of the proposed control scheme.","2329-9274","","10.1109/JAS.2021.1004132","National Natural Science Foundation of China(grant numbers:61773056); National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497874","Adaptive memory event-triggered mechanism (AMETM);compensation mechanism;denial-of-service (DoS) attacks;nonlinear multi-agent systems (MASs);observer-based anti-disturbance control","Fault tolerance;Uncertainty;Design methodology;Simulation;Fault tolerant systems;Observers;Denial-of-service attack","","91","","44","","27 Jul 2021","","","IEEE","IEEE Journals"
"Fault Estimation and Fault-Tolerant Tracking Control for Multi-Agent Systems With Lipschitz Nonlinearities Using Double Periodic Event-Triggered Mechanism","S. Li; Y. Chen; P. X. Liu","College of Artificial Intelligence and Automation, Beijing University of Technology, Beijing, China; College of Artificial Intelligence and Automation, Beijing University of Technology, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Transactions on Signal and Information Processing over Networks,"20 Apr 2023","2023","9","","229","241","This paper is concerned with the problem of fault estimation and fault-tolerant consensus tracking control for Lipschitz nonlinear multi-agent systems subject to external disturbance. In order to improve the communication efficiency of main network channels, two periodic event-triggered mechanisms are developed between sensor to observer and observer to controller, respectively. An event-triggered fault observer is developed to estimate existing faults and the estimated result is used for the design of a fault-tolerant controller to ensure system security. According to Lyapunov-Krasovskii theorem and the free-weighting matrix technique, the model gains of the observer and the controller can be obtained by solving a series of bilinear matrix inequalities (BMIs). To address the difficulty associated with BMIs, two iterative algorithms based on linear matrix inequalities (LMIs) are developed. Finally, a simulation example of satellite vehicles is given to illustrate the effectiveness of the obtained theoretical results.","2373-776X","","10.1109/TSIPN.2023.3264992","Beijing Municipal Natural Science Foundation(grant numbers:4232041); National Natural Science Foundation of China(grant numbers:62273014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093990","Fault estimation;fault-tolerant tracking control;multi-agent systems;periodic event-triggered mechanism","Fault tolerant systems;Fault tolerance;Observers;Security;Iron;Estimation;Multi-agent systems","","13","","38","IEEE","6 Apr 2023","","","IEEE","IEEE Journals"
"Fully Distributed Secure State Estimation for Nonlinear Multi-Agent Systems Against DoS Attacks: An Edge-Pinning-Based Method","D. Liu; D. Ye; X. -G. Zhao","College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; College of Information Science and Engineering and the State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, Liaoning, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, CAS, Shenyang, Liaoning, China",IEEE Transactions on Automation Science and Engineering,"8 Aug 2024","2024","21","3","4237","4246","This paper investigates the fully distributed secure state estimation problem of nonlinear multi-agent systems (MASs) against denial-of-service (DoS) attacks. The design of state observer depends on the relative measurement outputs between neighboring agents rather than the absolute ones of agents themselves. To cooperatively estimate system states with the influence of DoS attacks in a fully distributed scenario, an edge-pinning-based synchronous update strategy for coupling weights is proposed. The purpose of adopting edge pinning method is to adjust partial coupling weights between neighboring agents instead of all ones, which can effectively reduce estimation complexity and save computing resources, especially for large-scale MASs. Further, an edge-pinning-based asynchronous update strategy for coupling weights is designed to meet different actual demands. Then, the security state estimation schemes under synchronous and asynchronous update strategies are proposed without utilizing any global information, such as the Laplacian matrix of communication graph or its smallest nonzero eigenvalue, and so on. Finally, a simulation example is provided to verify the theoretical results. Note to Practitioners—This paper addresses the distributed secure state estimation problem for nonlinear MASs, which plays an important role in unmanned aerial vehicle (UAV) formation, intelligent transport systems, smart grids, and so on. The absolute measurement outputs of agents themselves are usually unavailable in practical situations, so the relative measurement outputs between neighboring agents are used to design observers. Moreover, to overcome the difficulty that global information is unavailable in industrial applications with DoS attacks, the edge-pining-based synchronous and asynchronous update strategies for coupling weights are proposed in a fully distributed scenario.","1558-3783","","10.1109/TASE.2023.3293844","National Natural Science Foundation of China(grant numbers:62173071,U22A2067); Liaoning Distinguished Young Funds(grant numbers:2023020359-JH6/1005); Fundamental Research Funds for the Central Universities(grant numbers:N2204008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186081","Multi-agent systems (MASs);fully distributed scenario;state estimation;denial-of-service (DoS) attacks;edge-pinning-based method","Observers;Denial-of-service attack;Couplings;Security;Laplace equations;Ultrasonic variables measurement;Topology","","11","","42","IEEE","18 Jul 2023","","","IEEE","IEEE Journals"
"Distributed Prescribed-Time Consensus Tracking for Heterogeneous Nonlinear Multi-Agent Systems Under Deception Attacks and Actuator Faults","Y. Gao; W. Zhou; B. Niu; Y. Kao; H. Wang; N. Sun","School of Information Science and Engineering, Shandong Normal University, Jinan, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Department of Mathematics, Harbin Institute of Technology at Weihai, Weihai, China; College of Mathematical Sciences, Bohai University, Jinzhou, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China",IEEE Transactions on Automation Science and Engineering,"15 Oct 2024","2024","21","4","6920","6929","In this article, a novel distributed prescribed-time consensus tracking control strategy is presented for heterogeneous nonlinear multi-agent systems (MASs) under deception attacks and actuator faults. Since the original states of the studied system are unavailable under deception attacks, we present a new coordinate transformation technique considering the compromised states and the outputs of command filters to design the corresponding controller. Then, to handle the unknown control gains caused by actuator faults, the rational adaptive laws are cleverly designed for the upper bounds involving the unknown control gains. Besides, by introducing a time-varying constraint function in the backstepping design process, a new adaptive prescribed-time controller is constructed in this paper, such that the prescribed-time tracking control problem of the heterogeneous nonlinear MASs is converted into the constraint problem of the tracking errors. Through the Lyapunov stability analysis, the final results prove that the whole signals in the closed-loop MASs remain bounded and the tracking errors can converge to the pre-set region in the predefined time. Finally, the simulation results demonstrate the availability of the developed control method. Note to Practitioners—This paper investigates the distributed prescribed-time consensus tracking control problem for heterogeneous nonlinear MASs, whose models can be extended to more complex industrial applications, such as flexible manipulators, series elastic actuators and electric systems. In certain practical scenarios, security and fault problems are inevitable. Hence, it is challenging to design a control strategy that achieves distributed consensus tracking control in an insecure network environment. Furthermore, the proposed approach based on the time-varying constraint function ensures the system stability within the predefined time, which provides a viable strategy for engineering applications.","1558-3783","","10.1109/TASE.2023.3334613","National Natural Science Foundation of China(grant numbers:62376170); Taishan Scholar Project of Shandong Province of China(grant numbers:tsqn201909078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328895","Prescribed-time control;heterogeneous nonlinear multi-agent systems (MASs);distributed consensus tracking control;deception attacks;actuator faults","Actuators;Security;Consensus control;Multi-agent systems;Convergence;Control systems;Backstepping;Nonlinear systems","","10","","49","IEEE","27 Nov 2023","","","IEEE","IEEE Journals"
"Fault-Tolerant Consensus of Multi-Agent Systems Subject to Multiple Faults and Random Attacks","C. Liu; W. Wang; B. Jiang; R. J. Patton","School of Mechatronic Engineering and Automation and the School of Future Technology (Institute of Artificial Intelligence), Shanghai University, Shanghai, China; School of Future Technology (Institute of Artificial Intelligence), Shanghai University, Shanghai, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Engineering, University of Hull, Hull, U.K.",IEEE Transactions on Circuits and Systems I: Regular Papers,"","2024","PP","99","1","11","This paper explores the consensus control problem of nonlinear multi-agent systems (MASs) under complex cyber-physical threats (CPTs), which encompass sensor/actuator faults, input/output channel noises, and random cyber-attacks. The multiple sensor/actuator faults are uniformly modeled as an exponential type, while random cyber-attacks are characterized by a Markov chain. To enhance the safety and security of MASs under CPTs, the distributed normalized observers are first developed, enabling precise estimations of unknown state and fault information. Subsequently, the distributed fault-tolerant consensus control (FTCC) scheme with a positive reconstruction mechanism is proposed to maintain resilience against attacks, compensation for faults, and robustness to noises in MASs under adverse CPTs. The two notable innovations can be outlined as follows: i) The achievement of FTCC objectives under complex CPTs, demonstrating strong algorithmic transferability in both non-attack and random attack scenarios. ii) The adoption of a double-layer distributed framework in the estimation layer and control layer, balancing computational complexity and efficiency improvements compared to a combination of decentralized and distributed approaches. Simulation results finally confirm the efficacy and feasibility of the proposed FTCC algorithm.","1558-0806","","10.1109/TCSI.2024.3351214","National Natural Science Foundation of China(grant numbers:62103250,62333011,62020106003); Shanghai Sailing Program(grant numbers:21YF1414000); Project of Science and Technology Commission of Shanghai Municipality(grant numbers:22JC1401401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416799","Multiple faults;fault-tolerant consensus control;distributed observers;random attacks;multi-agent systems","Circuit faults;Actuators;Cyberattack;Topology;Fault tolerant systems;Fault tolerance;Security","","1","","","IEEE","30 Jan 2024","","","IEEE","IEEE Early Access Articles"
"Mean-Square Exponential Consensus of Nonlinear Multi-Agent Systems via Distributed Random Impulsive Control","Y. Yu; W. He; Y. Yang","Key Laboratory of Smart Manufacturing in Energy Chemical Processes, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Processes, Ministry of Education, East China University of Science and Technology, Shanghai, China; College of Information Science and Technology and Engineering Research Center of Digitized, Donghua University, Shanghai, China",IECON 2023- 49th Annual Conference of the IEEE Industrial Electronics Society,"16 Nov 2023","2023","","","1","6","This paper aims to study mean-square exponential consensus of nonlinear leader-following multi-agent systems (MASs) in the presence of noise via distributed random impulsive control. The noise is driven by a second-order moment process with bounded mean power. A distributed random impulsive controller whose impulses occur at random moments is designed to ensure the desired consensus performance while enhancing security of the system. Sufficient conditions in terms of the network topology and impulsive parameter are derived, using the Lyapunov-based approach and properties of stochastic process. Finally, a numerical example is presented to demonstrate the effectiveness of our theoretical results.","2577-1647","979-8-3503-3182-0","10.1109/IECON51785.2023.10311871","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10311871","Leader-following multi-agent systems;mean-square exponential consensus;distributed random impulsive control","Industrial electronics;Sufficient conditions;Protocols;Laplace equations;Network topology;Stochastic processes;Control systems","","1","","22","IEEE","16 Nov 2023","","","IEEE","IEEE Conferences"
"Cyber-Physical Defense for Heterogeneous Multi-Agent Systems Against Exponentially Unbounded Attacks on Signed Digraphs","Y. Wang; M. Rajabinezhad; Y. Zhang; S. Zuo","Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical and Computer Engineering, University of Connecticut, Storrs, CT, USA",IEEE Transactions on Network Science and Engineering,"24 Apr 2025","2025","12","3","2165","2179","Cyber-physical systems (CPSs) are subjected to attacks on both cyber and physical spaces. In reality, attackers could launch any time-varying signals. Existing literature generally addresses bounded attack signals and/or bounded-first-order-derivative attack signals. In contrast, this paper proposes a privacy-preserving fully-distributed attack-resilient bilayer defense framework to address the bipartite output containment problem for heterogeneous multi-agent systems (MASs) on signed digraphs, in the presence of exponentially unbounded false data injection (EU-FDI) attacks on both the cyber-physical layer (CPL) and observer layer (OL). First, we design attack-resilient dynamic compensators that utilize data communicated on the OL to estimate the convex combinations of the states and negative states of the leaders. To enhance the security of transmitted data, a privacy-preserving mechanism is incorporated into the observer design. The privacy-preserving attack-resilient observers address the EU-FDI attacks on the OL and guarantee the uniformly ultimately bounded (UUB) estimation of the leaders' states in the presence of the eavesdroppers. Then, by using the observers' states, fully-distributed attack-resilient controllers are designed on the CPL to further address the EU-FDI attacks on the actuators. The theoretical soundness of the proposed bilayer resilient defense framework is proved by Lyapunov stability analysis. Finally, a comparative case study for heterogeneous MASs and the application in DC microgrids as a specific case study validate the enhanced resilience of the proposed defense strategies.","2327-4697","","10.1109/TNSE.2025.3545280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10902133","Cyber-physical defense;heterogeneous multi-agent systems;resilient control;signed digraph;exponentially-unbounded attacks;privacy preserving","Observers;Privacy;Vectors;Topology;Regulation;Protocols;Multi-agent systems;Cryptography;Vehicle dynamics;Training","","","","51","IEEE","24 Feb 2025","","","IEEE","IEEE Journals"
"Security Enhancement of CSI-Based Wireless Sensing via Generative AI","J. Wang; C. Feng; G. Sun; H. Du; D. Niyato; T. Q. S. Quek; V. C. M. Leung","College of Computing and Data Science, Nanyang Technological University, Singapore; Eurecom, France; College of Computer Science and Technology, Jilin University, China; Department of Electrical and Electronic Engineering, University of Hong Kong, China; College of Computing and Data Science, Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; Department of Electrical and Computer Engineering, The University of British Columbia, Canada",2025 IEEE International Workshop on Radio Frequency and Antenna Technologies (iWRF&AT),"11 Aug 2025","2025","","","83","88","Integrated Sensing and Communication (ISAC) is becoming a key technology in 6 G networks, where sensing based on channel state information (CSI) plays an essential role. Present research primarily focuses on enhancing sensing performance, yet often overlooks security issue, especially the threat of unauthorized sensing that tends to receive little attention. In response to the above threat, this paper proposes to use generative AI to enhance the security of CSI-based sensing systems. Specifically, we design the guarding signal according to the characteristics of CSI fluctuations caused by user activities and build the corresponding database based on the measurements collected by software-defined radio. Utilizing the constructed dataset, we train the conditional generative diffusion model, which can produce guarding signals that are similar yet distinct from the original training samples. Then, these guarding signals are modulated onto pilot signals, effectively masking the user-induced fluctuations, thereby preventing unauthorized devices from performing illegitimate sensing. Taking the user activity recognition as the example, experimental evaluations illustrate that the proposed method reduces the recognition accuracy of unauthorized devices by about 75 %, significantly enhancing user privacy protection against unauthorized sensing.","","979-8-3315-4422-5","10.1109/iWRFAT65352.2025.11103416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11103416","Channel state information;wireless sensing security;generative AI","Wireless communication;Training;Wireless sensor networks;Fluctuations;Generative AI;Diffusion models;Sensors;Security;Communication system security;Protection","","","","10","IEEE","11 Aug 2025","","","IEEE","IEEE Conferences"
"Promise: Prompt-Driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models","H. Li; H. Liu; D. Hu; J. Wang; I. Oguz",Vanderbilt University; Vanderbilt University; Vanderbilt University; Vanderbilt University; Vanderbilt University,2024 IEEE International Symposium on Biomedical Imaging (ISBI),"22 Aug 2024","2024","","","1","5","To address prevalent issues in medical imaging, such as data acquisition challenges and label availability, transfer learning from natural to medical image domains serves as a viable strategy to produce reliable segmentation results. However, several existing barriers between domains need to be broken down, including addressing contrast discrepancies, managing anatomical variability, and adapting 2D pretrained models for 3D segmentation tasks. In this paper, we propose ProMISe, a prompt-driven 3D medical image segmentation model using only a single point prompt to leverage knowledge from a pretrained 2D image foundation model. In particular, we use the pretrained vision transformer from the Segment Anything Model (SAM) and integrate lightweight adapters to extract depth-related (3D) spatial context without updating the pretrained weights. For robust results, a hybrid network with complementary encoders is designed, and a boundary-aware loss is proposed to achieve precise boundaries. We evaluate our model on two public datasets for colon and pancreas tumor segmentations, respectively. Compared to the state-of- the-art segmentation methods with and without prompt engineering, our proposed method achieves superior performance. The code is publicly available at https://github.com/MedICL-VU/ProMISe","1945-8452","979-8-3503-1333-8","10.1109/ISBI56570.2024.10635207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10635207","Medical image segmentation;lightweight adapter;transfer learning;prompt engineering;pretrained Segment Anything Model (SAM)","Image segmentation;Adaptation models;Solid modeling;Three-dimensional displays;Biological system modeling;Transfer learning;Transformers","","13","","20","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"System Design for Automation in Multi-Agent-Based Manufacturing Systems","S. Noh; J. Park","Autonomous IoT Research Laboratory, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Smart ICT Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, South Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","986","990","This paper proposes a system design for automation in multi-agent-based manufacturing systems to conduct a given complex task automatically by controlling multiple robotic manipulators in a systematic manner. To this end, the proposed system is designed with three-module configurations: environmental perception, task planning, and motion planning. The environmental perception module utilizes a vision sensor to recognize all objects placed on the workspace and extract their unique ID, size, and pose. The task planning module divides a given task into primitive skill levels and distributes each primitive skill to the associated robotic manipulator with the relevant object information in a systematic manner for robotic manipulators not to collide with each other. The motion planning module determines the motion of a robotic arm and a robotic hand by solving inverse kinematics for the robotic arm and by opening or closing two fingers. The proposed system has been tested and verified in real robot environments through a complex task ""peg in hole"" that requires at least two robotic manipulators.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268357","Electronics and Telecommunications Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268357","Manufacturing automation;multi-agent systems;peg in hole;robotic manipulator;ROS;visual servoing","Robots;Manipulators;Task analysis;Planning;Service robots;Systematics;Springs","","7","","13","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Large Language Models Empowered Online Log Anomaly Detection in AIOps","S. Zhang; D. Fan; L. He","Zhejiang Sci-Tech University, Hangzhou, China; Zhejiang Sci-Tech University, Hangzhou, China; Zhejiang Sci-Tech University, Hangzhou, China",2024 31st Asia-Pacific Software Engineering Conference (APSEC),"25 Apr 2025","2024","","","402","411","AIOps has become increasingly crucial in managing modern IT infrastructures, leveraging AI techniques to enhance operational efficiency and reliability in log anomaly detection. However, existing approaches, such as Deeplog, face two significant challenges in log anomaly detection: frequent changes in data patterns due to software and hardware upgrades, and the demand for high efficiency in online scenarios. To address these issues, we propose LogX, a novel method based on large language models and optimized prompting strategies, particularly interactive modes, to promptly correct previously unseen errors. By integrating input-label pairs directly into the prompt, LogX eliminates the need for iterative training processes and additional resource costs, ensuring high adaptability in online scenarios. Furthermore, to maintain control over sensitive data while ensuring privacy and security, we utilize open-source tools and on-premise infrastructure for AIOps system. It seamlessly integrates with LogX's online log diagnostic capabilities, providing a robust solution for companies aiming to manage their software maintenance processes internally.","2640-0715","979-8-3315-3401-1","10.1109/APSEC65559.2024.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967310","large language model;prompt engineering;anomaly detection;AIOps","Training;Software maintenance;Costs;Large language models;Hardware;Software reliability;Prompt engineering;Iterative methods;Anomaly detection;Software engineering","","","","41","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT","Z. Liu; Y. Tang; X. Luo; Y. Zhou; L. F. Zhang","ShanghaiTech University, Shanghai, China; University of Glasgow, Glasgow, U.K.; Department of Computing, Hong Kong Polytechnic University, Hong Kong SAR, China; Nanjing University, Nanjing, China; ShanghaiTech University, Shanghai, China",IEEE Transactions on Software Engineering,"14 Jun 2024","2024","50","6","1548","1584","Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability, chatting between users and ChatGPT for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) ChatGPT is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with $48.14\%$48.14% advantage in Accepted rate on judgment platform, but ChatGPT's ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with ChatGPT  generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by ChatGPT  has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than $89\%$89% of vulnerabilities successfully addressed; and (4) code generation may be affected by ChatGPT's non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.","1939-3520","","10.1109/TSE.2024.3392499","National Natural Science Foundation of China(grant numbers:62172205,62202306); HKPolyU(grant numbers:ZGGG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507163","Large language model;ChatGPT;code generation","Codes;Chatbots;Task analysis;Complexity theory;Security;Transformers;Electronic mail","","56","","89","IEEE","23 Apr 2024","","","IEEE","IEEE Journals"
"PwnPilot: Reflections on Trusting Trust in the Age of Large Language Models and AI Code Assistants","D. Horne","Department of Computer Science, Baylor University, Waco, TX, USA","2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)","9 Apr 2024","2023","","","2457","2464","At the dawn of a new era in software engineering, one defined by large language models (LLMs) and AI code assistants like GitHub Copilot, new meaning can be found from a historic Turing Award Lecture that concluded one cannot trust source code they “did not totally create” themselves. In this paper, a targeted, systematic survey of the latest research results from 2019 to early 2023 highlights the possible risks of using AI code assistants that produce substantial source code contributions, and the potential for an AI Copilot to unknowingly become PwnPilot, a malevolent digital actor that introduces vulnerabilities and compromises trust. During a period of explosive growth for generative AI, renewed reflections on trusting trust point to conclusions similar to the original assertions of Ken Thompson in 1984. But despite a recent theoretical roadblock from proof of ability to plant undetectable backdoors in machine learning models, the potential for enhanced productivity from AI code assistants may still be realizable with an acceptable level of risk, perhaps even for safety critical and sensitive security relevant contexts. In support of that goal, a number of near-term risk management options and longer term research paths are identified as enablers for practitioners and inputs to potential research roadmaps toward more secure and trusted AI code generation.","","979-8-3503-2759-5","10.1109/CSCE60160.2023.00396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487488","Trusting trust;artificial intelligence (AI) and machine learning (ML) security;AI code assistant;GitHub Copilot;ChatGPT;PwnPilot;AI Driven N-Version Programming (AID-NVP)","Surveys;Codes;Systematics;Source coding;Machine learning;Reflection;Security","","4","","58","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Dockerfile Flakiness: Characterization and Repair","T. Shabani; N. Nashid; P. Alian; A. Mesbah","University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1793","1805","Dockerfile flakiness-unpredictable temporal build failures caused by external dependencies and evolving environments-undermines deployment reliability and increases debugging overhead. Unlike traditional Dockerfile issues, flakiness occurs without modifications to the Dockerfile itself, complicating its resolution. In this work, we present the first comprehensive study of Dockerfile flakiness, featuring a nine-month analysis of 8,132 Dockerized projects, revealing that around 10% exhibit flaky behavior. We propose a taxonomy categorizing common flakiness causes, including dependency errors and server connectivity issues. Existing tools fail to effectively address these challenges due to their reliance on pre-defined rules and limited generalizability. To overcome these limitations, we introduce FLAKIDOCK, a novel repair framework combining static and dynamic analysis, similarity retrieval, and an iterative feedback loop powered by Large Language Models (LLMs). Our evaluation demonstrates that FLAKIDOCK achieves a repair accuracy of 73.55%, significantly surpassing state-of-the-art tools and baselines.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029932","Docker;Flakiness;Large Language Models;Automated Program Repair","Feedback loop;Accuracy;Large language models;Taxonomy;Retrieval augmented generation;Maintenance engineering;Reliability;Iterative methods;Servers;Software engineering","","","","53","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Organizing and Augmenting Cybersecurity Knowledge Using Generative AI","A. Piemonti; V. Cianchini; M. Danousis; C. Skianis","Martel Innovate, Chiasso, Switzerland; Martel Innovate, Chiasso, Switzerland; Eight Bells Ltd, Nicosia, Cyprus; Eight Bells Ltd, Nicosia, Cyprus",2025 IEEE 11th International Conference on Network Softwarization (NetSoft),"21 Jul 2025","2025","","","585","590","With the rapid advancements in Artificial Intelligence (AI), cybersecurity threats are becoming increasingly sophisticated. Static, human-curated approaches such as MITRE ATT&CK and CAPEC are often insufficient for companies to implement effective countermeasures, and security experts frequently face challenges in integrating these frameworks into their specific architectures. In this paper, we explore the application of generative AI to organize and enhance cybersecurity knowledge bases. We leverage multiple state-of-the-art Large Language Models (LLMs) to augment an initial dataset of attackmitigation pairs, generating new countermeasures along with their prioritized execution order in a reasonable time frame. To evaluate the generated responses, we employ the LLM-as-aJudge technique. Evaluations with Claude 3.5, DeepSeek R1, and human oversight show that top-performing models on general benchmarks also perform well in this specialized task, with more than 70% of responses rated 4 and 5 out of 5 for correctness, and the weaker models tend to perform the worst. Additionally, we discuss the challenges of running LLMs with a larger number of parameters on less powerful hardware, where the performance of such models can degrade significantly, even performing worse than their smaller counterparts. In such cases, advanced prompt engineering becomes necessary to improve results. Our code and dataset are publicly available at “https://github.com/martel-innovate/HORSE-GenAI-CKB”","2693-9789","979-8-3315-4345-7","10.1109/NetSoft64993.2025.11080630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11080630","Generative AI;LLM;Security;Cybersecurity;MITRE ATT&CK;Knowledge Base","Knowledge engineering;Codes;Large language models;Knowledge based systems;Companies;Benchmark testing;Hardware;Prompt engineering;Computer security;Faces","","","","19","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Enhancing Transdisciplinary Tutoring in Analog Electronics and Embedded Systems Using Generative AI, Game-Based Learning (GBL) and Moodle","F. J. Zamora Navarro","Engineering Faculty - Electronic Engineering, LIDER Research Group, Universidad Distrital Francisco Jose de Caldas, Bogota, Colombia",2025 IEEE Global Engineering Education Conference (EDUCON),"3 Jun 2025","2025","","","1","5","This paper presents a novel method for enhancing active learning and outcomes in electronic engineering by combining Generative AI for Game-Based Learning (GBL), and transdisciplinary tutoring in Moodle LMS. The approach bridges Analog Electronics (3rd semester) and Digital Design with Microcontrollers (6th semester, embedded systems) engineering courses, fostering collaboration through game-based projects focused on real-world analog-digital integration. Generative AI supports this by generating adaptable game scenarios, problems, and feedback, tailored to each student's learning progress. By incorporating popular game mechanics like Monopoly (for technological evolution), Trivial Pursuit (for component knowledge), and Settlers of Catan (for system design), the approach engages students in key topics like diodetransistor circuits, microcontroller programming, and analogdigital system design. Games are supervised in Moodle using workshop and other activities for peer review, allowing students to submit solutions, review peers' work, and track progress through quizzes, forums, journals, and peer feedback. Analog Electronics students learn Arduino microcontroller applications for lab instrumentation, while Digital Design students explore analog components' roles in embedded systems, deepening understanding of analog-digital interactions. A pilot case study shows increased engagement, collaboration, and practical application of concepts, highlighting the potential for AI, GBL, and Moodle-based transdisciplinary tutoring to personalize learning and deepen comprehension in complex engineering topics. Future research will refine GBL AI-generated content and assess scalability across other STEM, career, and engineering fields.","2165-9567","979-8-3315-3949-8","10.1109/EDUCON62633.2025.11016465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016465","Transdisciplinary Student-Centered Tutoring (TSCT);Generative AI (GenAI);Game Based Learning (GBL);PjBL;Moodle;Workshop;Quiz;Forum;Journal;Learning Object (LO);Analog Electronics;Embedded Systems;Arduino;CEXduino;LTSPICE;PROTEUS VSM;MPLABX IDE;Modern Electronics Game Board (MEGaBoard)","Embedded systems;Generative AI;Microcontrollers;Reviews;Conferences;Scalability;Collaboration;Games;Analog-digital conversion;STEM","","","","10","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models for Automatic Smart Contract Generation","E. A. Napoli; F. Barbàra; V. Gatteschi; C. Schifanella","Department of Control and Computer Engineering, Politecnico di Torino, Turin, ITALY; Department of Computer Science, University of Turin, Turin, ITALY; Department of Control and Computer Engineering, Politecnico di Torino, Turin, ITALY; Department of Computer Science, University of Turin, Turin, ITALY","2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)","26 Aug 2024","2024","","","701","710","In the rapidly evolving landscape of blockchain technology, smart contracts stand as pivotal instrument for automating and enforcing digital agreements. However, their creation often necessitates specialized programming skills, hindering broader adoption and accessibility. This paper proposes a pipeline that leverages the capabilities of Large Language Models (LLMs) to automate the generation of smart contracts. By harnessing the natural language understanding and generation capabilities of LLMs, our approach aims to make accessible smart contract development to people that are not familiar with this task. The proposed pipeline employs the CO-STAR methodology to optimize prompt creation for high-quality outputs. Moreover, in order to assess the correctness and reliability of the generated smart contracts, we leverage on Slither, one of the most cutting-edge vulnerability detection tools. Furthermore, we propose a benchmarking suite based on metrics such as compilability, vulnerabilities, and presence of comments, among the others, in order to evaluate the effectiveness of the pipeline in terms of consistency of generated smart contracts, LLM's temperature effect, and prompt selection. The results show that our pipeline is able to produce 98.1% of compilable smart contracts, the temperature value has negligible effect on the generated smart contracts, and the CO-STAR methodology produces valuable and consistent outputs with low-impact vulnerabilities.","2836-3795","979-8-3503-7696-8","10.1109/COMPSAC61105.2024.00100","Ministry of Education, University and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633392","automatic code generation;blockchain;LLM;NLP;smart contract;text-to-code","Measurement;Large language models;Instruments;Smart contracts;Pipelines;Programming;Software","","6","","41","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"Cross-Analysis of Social Media Opinions in a High-Security Challenge Context","Z. P. Gregoire Wenceslas; T. Yaya; P. Justin Kouraogo","Department of Computer Science, Joseph KI-ZERBO University, Ouagadougou, Burkina Faso; Department of Computer Science, Joseph KI-ZERBO University, Ouagadougou, Burkina Faso; Department of Computer Science, Joseph KI-ZERBO University, Ouagadougou, Burkina Faso","2024 11th International Conference on Social Networks Analysis, Management and Security (SNAMS)","20 Feb 2025","2024","","","251","257","This study aims to develop an approach for cross-analyzing public opinions on security issues in Burkina Faso, as expressed on social media platforms. We implemented a methodology combining web scraping, API calls, and large language models (LLMs) to collect and analyze data from Fasonet [1], Burkina24 [2] and YouTube [3]. The experiment involved processing 5,189 opinions using GPT-4 and Gemini 1.0 Pro models for sentiment analysis and theme identification. The results showed that GPT-4 achieved superior performance with 99.16% accuracy in opinion classification, outperforming previous methods. The study are revealed significant variations in public perceptions of security throughout 2023 and highlighted YouTube as a dominant platform for public discourse. This multidimensional approach offers a comprehensive understanding of social media opinions, though limitations in data collection and model dependence were identified, suggesting avenues for future research.","2831-7343","979-8-3315-1834-9","10.1109/SNAMS64316.2024.10883778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10883778","Social media analysis;Opinion mining;Sentiment analysis;Large Language Models (LLMs);Cross-platform analysis","Analytical models;Sentiment analysis;Video on demand;Social networking (online);Computational modeling;Data collection;Data models;Real-time systems;Security;Web sites","","","","24","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Generative AI Solution for CNC Machines and Robotics Code Generation","S. Jayanthy; S. M; S. K. S; M. S. A; S. K. M; S. A. T","Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, India",2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES),"17 Jun 2025","2025","","","1","8","The advent of Industry 4.0 has revolutionized the manufacturing landscape, driving significant advancements in automation and intelligence. This study introduces an innovative approach to automated code generation for CNC and robotic systems, leveraging Generative Adversarial Networks (GANs) and GPT(Generative Pre-trained Transformer) Transformers. These AI models enable precise and optimized code creation, minimizing manual errors. Adaptive process control, achieved through Reinforcement Learning (RL), allows real-time adjustments to operational parameters, enhancing performance in dynamic environments. The incorporation of natural language processing through Transformer models facilitates intuitive operator interactions via user-friendly interfaces. Immersive Virtual Reality (VR) technologies provide high-fidelity simulation and training platforms for realistic testing and control. Additionally, collaborative learning mechanisms, achieved through Federated Learning and Edge-cloud computing, support continuous improvement and scalable deployment. Impressive outcomes were attained by the system, including 90.5% training efficiency, 98.7% coding accuracy, 95.2% adaptability, and 93.4% operator satisfaction. Experimental results validate the system’s superior accuracy, adaptability, and user-centric design, showcasing its potential to revolutionize manufacturing processes. This research sets a new benchmark for intelligent, efficient, and scalable automation in the Industry 4.0 era, paving the way for transformative innovations in smart manufacturing.","","979-8-3315-3669-5","10.1109/ICCIES63851.2025.11033032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11033032","Automated Code Generation;CNC Machines;GANs;GPT Transformers;Reinforcement Learning;Virtual Reality;Federated Learning;Edge Computing;Industry 4.0","Training;Technological innovation;Solid modeling;Codes;Accuracy;Federated learning;Service robots;Virtual reality;Transformers;Fourth Industrial Revolution","","","","10","IEEE","17 Jun 2025","","","IEEE","IEEE Conferences"
"Towards an AI-centric Requirements Engineering Framework for Trustworthy AI","K. Ronanki","Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden",2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"12 Jul 2023","2023","","","278","280","Ethical guidelines are an asset for artificial intel-ligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering(RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level.","2574-1934","979-8-3503-2263-7","10.1109/ICSE-Companion58688.2023.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172807","Trustworthy AI;EU AI Act;Requirements Engineering;Frameworks;AI co-worker;Ethical AI;Guidelines","Productivity;Ethics;Terminology;Organizations;Requirements engineering;Artificial intelligence;Task analysis","","","","19","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Applying Aspect-Oriented Design Methodology to Manage Time-Validity of Information in Internet-of-Things Systems","V. O'Neill; B. Soh","Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia",2022 IEEE World AI IoT Congress (AIIoT),"13 Jul 2022","2022","","","749","752","The Internet of Things (IoT) is becoming increasingly ubiquitous, typified in software by large-scale multi-agent systems of heterogeneous agents. IoT devices are constrained in terms of memory and processing power, limiting their capacity to hold large sets of information upon which decision-making logic must execute. IoT devices are also frequently deployed as distributed sensors constrained in terms of time, location and communications bandwidth. These constraints demand a level of multi-agent communication and co-ordination to provide accurate, up-to-date information on-demand to different intelligent agents in the system. In this paper, we propose a novel method by which Aspect-Oriented Software Design can be applied to managing the validity of time-constrained data in IoT systems while decoupling the application code from this concern.","","978-1-6654-8453-4","10.1109/AIIoT54504.2022.9817345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817345","internet of things;multi-agent systems;software architecture;distributed computing;intelligent agents","Codes;Software design;Software architecture;Computational modeling;Data models;Software;Complexity theory","","1","","21","IEEE","13 Jul 2022","","","IEEE","IEEE Conferences"
"Investigating Generative AI in Higher Education: A Case Study Co-Designed with ChatGPT","R. V. Selvakumaran; M. Suhyoon Lee; K. Alkan; P. Lee","Faculty of Science & Technology, University of Canberra, Australian Capital Territory, Australia; Faculty of Science & Technology, University of Canberra, Australian Capital Territory, Australia; Faculty of Science & Technology, University of Canberra, Australian Capital Territory, Australia; Faculty of Science & Technology, University of Canberra, Australian Capital Territory, Australia",2024 IEEE 12th Region 10 Humanitarian Technology Conference (R10-HTC),"11 Dec 2024","2024","","","1","6","The objective of this study is to explore the potential of Generative AI, specifically ChatGPT, in the co-design of higher education curricula. Employing a case study approach, the study scrutinizes the interplay between traditional pedagogical techniques and prompt engineering, a method used to fine-tune the AI model. The study focuses on a case of a Research Methodology unit and utilizes the AI tool GPT-4 to create a curriculum. The outcomes of this study carry significant implications, resulting in the formulation of an innovative ""GPT-4 Co-Designed Curriculum Framework."" Additionally, the study introduces the ""PAKARAMIS Framework"" for prompt engineering. These contributions hold the potential for broader application in future curriculum design across various educational units, offering valuable perspectives on the capabilities and limitations of Generative AI in higher education. This research not only contributes to the ongoing discourse surrounding the integration of AI in higher education but also furnishes practical insights that can benefit educators and policymakers as they navigate the evolving landscape of educational technology.","2572-7621","979-8-3503-4020-4","10.1109/R10-HTC59322.2024.10778811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778811","Generative AI;ChatGPT (GPT-4);Prompt Engineering;Curriculum Design;Indigenization;Research Methodology","Ethics;Navigation;Instruments;Curriculum development;Educational technology;Chatbots;Prompt engineering;IEEE Regions;Guidelines","","","","20","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics","N. v. Stein; T. Bäck","Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands",IEEE Transactions on Evolutionary Computation,"1 Apr 2025","2025","29","2","331","345","Large language models (LLMs), such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This article introduces a novel LLM evolutionary algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates, and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel closed box metaheuristic optimization algorithms for box-constrained, continuous optimization problems automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (covariance matrix adaptation evolution strategy and differential evolution) on the 5-D closed box optimization benchmark (BBOB). The algorithms also show competitive performance on the 10- and 20-D instances of the test functions, although they have not seen such instances during the automated generation process. The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.","1941-0026","","10.1109/TEVC.2024.3497793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10752628","Automated code generation;evolutionary computation (EC);large language models (LLMs);metaheuristics;optimization","Benchmark testing;Evolutionary computation;Metaheuristics;Codes;Large language models;Closed box;Heuristic algorithms;Mathematical models;Vectors;Systematics","","6","","58","CCBY","13 Nov 2024","","","IEEE","IEEE Journals"
"AnaSizeCoder: Code Generator for Analog Integrated Circuit Sizing Automation via Large Language Model","W. Sun; Y. Han; B. Lan; Q. Peng; J. Wan","School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; Suzhou Foohu Technology Co., Ltd, Shanghai, China; Suzhou Foohu Technology Co., Ltd, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China",2025 International Symposium of Electronics Design Automation (ISEDA),"8 Aug 2025","2025","","","817","822","This study proposes a task-driven approach AnaSizeCoder that utilizes the large language model (LLM) Qwen2.5 to automatically generate optimization code for transistor sizing in analog integrated circuits (IC). Traditional optimization techniques often require manual code adjustments when faced with different optimization objectives and constraints, limiting their flexibility and efficiency in complex design tasks. To address this issue, we fine-tuned the Qwen2.5 model using LoRA (Low-Rank Adaptation) technology, enabling it to generate high-quality optimization code based on user-provided natural language requirements. We constructed a dataset comprising user requirements, summarized information, and corresponding optimization code, and consolidated all data for fine-tuning. The model was trained on 3 analog circuits, covering different optimization goals and constraints. Experimental results show that the fine-tuned model significantly improves code generation accuracy, with the best performance observed at the 4th training epoch (Epoch 4). In terms of generating summarized information, the accuracy reached 100%, while for code generation, the accuracy was above 97% across 3 different circuits. Additionally, by implementing the LDO (Low Dropout Regulator) circuit, we validated the code generated by AnaSizeCoder can successfully run and achieve the optimal Pareto frontier. Our approach enhances the automation and efficiency of analog IC design, reduces human intervention, and accelerates the design process.","","979-8-3315-3696-1","10.1109/ISEDA65950.2025.11101179","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11101179","analog ic;large language models;sizing automation;optimization code generation","Adaptation models;Codes;Automation;Accuracy;Large language models;Natural languages;Manuals;Integrated circuit modeling;Optimization;Analog integrated circuits","","","","7","IEEE","8 Aug 2025","","","IEEE","IEEE Conferences"
"Assured LLM-Based Software Engineering","N. Alshahwan; M. Harman; I. Harper; A. Marginean; S. Sengupta; E. Wang","Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA","2024 IEEE/ACM 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE)","10 Sep 2024","2024","","","7","12","In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.","","979-8-4007-0564-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669832","Large Language Models (LLMs);Genetic Improvement (GI);Search Based Software Engineering (SBSE);Llama;CodeLlama;Automated Code Generation","Codes;Filters;Large language models;Conferences;Semantics;Benchmark testing;Genetics","","","","41","","10 Sep 2024","","","IEEE","IEEE Conferences"
"Large Language Models powered Malicious Traffic Detection: Architecture, Opportunities and Case Study","X. Zhang; H. Meng; Q. Li; Y. Tan; L. Zhang","Peking University, China; China Unicom, China; Peking University, China; Peking University, China; Zhongguancun Lab, China",IEEE Network,"","2025","PP","99","1","1","Malicious traffic detection is a pivotal technology for network security to identify abnormal network traffic and detect network attacks. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network attacks detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there remains a lack of comprehensive analysis on harnessing LLMs for traffic detection, as well as the opportunities and challenges. In this paper, we focus on unleashing the full potential of Large Language Models (LLMs) in malicious traffic detection. We present a holistic view of the architecture of LLM-powered malicious traffic detection, including the procedures of Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in traffic classification: Classifier, Encoder, and Predictor. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs’ capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement compared to existing systems.","1558-156X","","10.1109/MNET.2025.3583088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050380","network intrusion detection;malicious traffic detection;traffic classification;Large Language Model;DDos","Telecommunication traffic;Training;Data models;Feature extraction;Large language models;Knowledge engineering;Computer crime;Computer architecture;Vectors;Transformers","","","","","IEEE","25 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Assessing Student Perceptions of Video Quality and Effectiveness From AI-Enhanced Digital Resources","M. M. Chan; H. R. Amado-Salvatierra; R. Hernandez-Rizzardini; M. Rosales","GES Department, Universidad Galileo, Guatemala, Guatemala; GES Department, Universidad Galileo, Guatemala, Guatemala; GES Department, Universidad Galileo, Guatemala, Guatemala; GES Department, Universidad Galileo, Guatemala, Guatemala",2024 IEEE Digital Education and MOOCS Conference (DEMOcon),"13 Nov 2024","2024","","","1","5","In this article, the authors explore the practical application of artificial intelligence-based tools in streamlining the course development process, with a particular emphasis on video production and scriptwriting. The work explores the production process for a MOOC lesson, encompassing activities such as video scripting, graphic design, podcast recording, slide presentation preparation, concept mapping, and the creation of interactive learning activities. The article presents the instrument used to evaluate students’ perceptions of the AI-generated video resources, offering insights into the accessibility of creating avatars without the need for specialized equipment. With basic recording devices and adherence to guidelines like maintaining eye contact and minimizing background noise, users can achieve professional results. Furthermore, the work underscores the potential benefits of integrating AI-based tools in the development of MOOCs and large-scale course production. It is observed that the utilization of such technologies may lead to a more streamlined course creation process, with the first exploration indicating a possible reduction of preparation time by approximately 60%. This notable decrease in development time could offer substantial improvements in the scalability and potential personalization of online education.","","979-8-3503-5314-3","10.1109/DEMOcon63027.2024.10747930","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10747930","Artificial Intelligence;LLMs;Generative AI;Prompt Engineering","Computer aided instruction;Electronic learning;Negative feedback;Avatars;Scalability;Education;Production;Streaming media;Quality assessment;Video recording","","1","","10","IEEE","13 Nov 2024","","","IEEE","IEEE Conferences"
"Accelerating Product Innovation: Impact of Serverless Computing and Generative AI on Application Development","A. Donvir; S. Panyam; G. Paliwal; P. Gujar","Application Development, Wayne, NJ, USA; Cloud Computing and SaaS Platforms, Sunnyvale, United States; Product Marketing & Development, Seattle, USA; Enterprise Data Products in Digital Advertising, Saratoga, USA","2024 IEEE 15th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","20 Nov 2024","2024","","","140","147","This research explores the integration of serverless computing and Generative AI in modern application development, highlighting the potential benefits of this convergence. Serverless computing offers scalability and cost efficiency by abstracting infrastructure management, while Generative AI, exemplified by tools like GPT-4, enhances productivity by assisting in ideation, prototyping, and code generation. The study compares two applications of similar complexity—one developed traditionally and the other using these technologies. The results indicate a significant reduction in development time, improved efficiency, and automatic scalability in the AI-assisted application. Challenges such as low-fidelity wireframes and occasional buggy code generation were identified, underscoring the need for further AI advancements. Overall, the combination of serverless computing and Generative AI shows great promise in transforming application development, fostering innovation, and optimizing resource use.","","979-8-3315-4090-6","10.1109/UEMCON62879.2024.10754724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10754724","Serverless computing;Generative AI;Application Development;Product Development SaaS;Software Engineering;Cloud Computing;Scalability;Cost Efficiency;Prototyping;Code Generation;Resource Optimization","Technological innovation;Codes;Costs;Generative AI;Scalability;Serverless computing;Computer architecture;Artificial intelligence;Optimization;Convergence","","","","29","IEEE","20 Nov 2024","","","IEEE","IEEE Conferences"
"Generative AI-Driven Approach to Converting Numerical Code into Mathematical Functions","E. R. Adwaith Krishna; A. Sha; K. Anvesh; N. A. Reddy; B. S. Raj; K. S. Nisha","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India","2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)","26 Jan 2024","2023","","","661","666","In the era of Artificial Intelligence (AI), the role of Generative AI models has witnessed a paradigm shift in addressing programming challenges. This research delves into the evolving landscape of Generative AI-driven code generation for programming problems, with a focus on the convergence behavior, algorithmic efficiency, learning dynamics, and code-length-performance dynamics of AI models, exemplified by ChatGPT and BARD. Through meticulous experimentation and analysis, this study elucidates key findings that illuminate the strengths and nuances of these models in code generation contexts. ChatGPT demonstrates rapid initial convergence, making it advantageous in time-sensitive scenarios, while BARD exhibits potential for precision-critical tasks. Algorithmic efficiency comparisons reveal subtle differences, emphasizing the need for further investigation into runtime influences. Learning curve analysis underscores the significance of early iterations in training, while code length-performance dynamics present intriguing correlations that warrant in-depth exploration. These insights provide a foundation for informed model selection and training strategies, while highlighting the ethical considerations and future avenues in AI-driven code generation.","","979-8-3503-4023-5","10.1109/ICACRS58579.2023.10405148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405148","Generative AI;Code Generation;Programming Problems;Convergence Behavior;ChatGPT;BARD","Training;Codes;Runtime;Generative AI;Programming;Chatbots;Convergence","","8","","16","IEEE","26 Jan 2024","","","IEEE","IEEE Conferences"
"On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization","G. Crupi; R. Tufano; A. Velasco; A. Mastropaolo; D. Poshyvanyk; G. Bavota","SEART @ Software Institute, Università della Svizzera italiana, Switzerland; SEART @ Software Institute, Università della Svizzera italiana, Switzerland; W&M; W&M; W&M; SEART @ Software Institute, Università della Svizzera italiana, Switzerland",IEEE Transactions on Software Engineering,"","2025","PP","99","1","17","Large Language Models (LLMs) have been recently exploited as judges for complex natural language processing tasks, such as Q&A (Question & Answer). The basic idea is to delegate to an LLM the assessment of the “quality” of the output provided by an automated technique (often another LLM) for tasks for which: (i) quantitative metrics would only tell part of the story, and; (ii) a large-scale human-based evaluation would be too expensive. LLMs-as-a-judge, if proven effective for a specific task, can also unlock new possibilities for automation, with several LLMs proposing a solution for a given instance of the task (e.g., an answer to a question) and others judging and deciding what is the best output to show the user. We study the effectiveness of LLMs-as-a-judge for two code-related tasks, namely code generation and code summarization. The rationale for choosing these tasks is two-fold. First, quantitative metrics are usually not enough for the assessment of code summarizers/generators. For example, it is well documented that metrics such as BLEU are quite weak proxies for the quality of the generated summaries. Second, even state-of-the-art techniques still struggle with handling complex instances of these tasks (e.g., summarizing a quite long / complex function), making them good candidates for benefiting from more advanced solutions envisioning collaboration among LLMs. For code generation, we check whether eight LLMs are able to judge the correctness of 1,405 Java methods and 1,281 Python functions generated by the same LLMs or implemented by humans. For code summarization, we compare the judgment of five LLMs to those provided by nine humans for ç1.2k summaries, related to both Java and Python functions. Our findings show that GPT-4-turbo is the best LLM in terms of judging capabilities for both tasks, with “smaller” LLMs featuring tens of billions parameters not being able to cope with judging tasks. However, even the best-performing LLM frequently misjudges the correctness of the code and summary quality.","1939-3520","","10.1109/TSE.2025.3586082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11071936","LLM-as-a-judge;AI4SE;Empirical Study;Large Language Models for Code","Codes;Measurement;Java;Electronic mail;Software;Benchmark testing;Training;Reliability;Chatbots;Reviews","","","","","IEEE","4 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Taxonomy Based Question Generation Using Prompt Engineering for Student Assessment","K. R. Nair; S. Anand; U. Arjun; N. Prasanth; S. S. Kumar; S. N. Rao","Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Amritapuri, India; Center for Wireless Networks & Applications (WNA), Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Amritapuri, India; Center for Wireless Networks & Applications (WNA), Amrita Vishwa Vidyapeetham, Amritapuri, India",2025 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE),"22 Jul 2025","2025","","","1","7","The recent wide popularity of ChatGPT has resulted in immense research interest in Large Language Models in many application areas such as education, business, healthcare, and tourism. Its popularity is due to its human-like conversation and comprehension capability. In this paper, we focus on the usage of generative AI in the education system to address the challenges of generating questions to address the different levels of learning taxonomy and help educators design questions based on the different categories of students. In this work, we are trying to build an AI system that is capable of generating questions based on Bloom’s Taxonomy using prompting techniques. The system is also capable of generating personalised quizzes for students who face difficulty answering learning taxonomy-based questions. We have used the LLM API and four types of prompting techniques to generate questions in our personalised evaluation system. The paper presents the system architecture of the personalised evaluation system. The paper also presents a comparison of various prompting techniques, and the result obtained.","","979-8-3315-0103-7","10.1109/AMATHE65477.2025.11081179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081179","LLM;Generative AI;Prompt Engineering;Education;Learning Taxonomy;Evaluation System","Large language models;Taxonomy;Education;Systems architecture;Oral communication;Medical services;Chatbots;Question generation;Prompt engineering;Faces","","","","26","IEEE","22 Jul 2025","","","IEEE","IEEE Conferences"
"Leveraging Generative AI to Simulate Stakeholder Involvement in the Engineering Design Process: A Case Study of MSc Team-Based Projects","K. J. Offor","School of Electrical and Electronic Engineering University of Sheffield, Sheffield, United Kingdom",2025 IEEE Global Engineering Education Conference (EDUCON),"3 Jun 2025","2025","","","1","3","In engineering education, stakeholder engagement plays a critical role in developing solutions that meet both technical and user-centred requirements. During the 2023/24 academic year, GenAl, specifically ChatGPT, was integrated into an MSc team-based project module to simulate stakeholder involvement. This approach allowed students to explore stakeholder dynamics by using AI to identify relevant stakeholders, generate questions for consultation, and simulate interactions. These AI-driven simulations enhanced students' understanding of stakeholder needs and decision-making processes. However, students' engagement was initially hampered by a lack of prompt engineering skills, which limited their ability to effectively communicate with the AI and extract meaningful responses. This underscores the importance of incorporating GenAl prompt engineering in training engineers of the future. To address this challenge, students were given predefined prompts to guide their GenAl interactions, helping them refine questions for better stakeholder insights. Despite these initial challenges, the use of GenAl helped students appreciate the complexities of real-world engineering problems and fostered a deeper understanding of how to balance diverse stakeholder needs. Preliminary results suggest the need for prompt engineering training and that incorporating GenAl into engineering education can significantly enhance students' ability to critically evaluate and integrate feedback into their designs. Future research will explore incorporating human stakeholder engagement methods, alongside prompt engineering training, to further enhance learning outcomes and compare the impact of AI versus human involvement. This work contributes to ongoing discussions on the role of GenAl in education.","2165-9567","979-8-3315-3949-8","10.1109/EDUCON62633.2025.11016557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016557","Generative AI;GenAI;engineering design process;stakeholder engagement;engineering education (key words)","Training;Decision making;Chatbots;Complexity theory;Stakeholders;Prompt engineering;Engineering education","","","","14","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Structure-Guided and Semantics-Enhanced Collaborative Code Generation","D. Yang; X. Zhang; Y. Cao","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","24 Jun 2025","2025","","","556","562","To improve the quality of pseudocode-to-C++ code generation, this paper proposes a Structure-guided and Semantics-enhanced Collaborative Generation method (SSCGen). By integrating a lightweight translation model and a large language model, SSCGen enhances both the structural diversity and semantic correctness of generated programs. Specifically, it first uses a Mixture-of-Experts (MoE) module within an encoderdecoder architecture to perform line-by-line translation and generate well-formed code fragments. These fragments are then assembled into complete programs via best-first search. The resulting candidates serve as structural guidance for a large model, which further performs semantics-enhanced generation. Experiments on the SPoC dataset demonstrate that SSCGen significantly outperforms existing methods, achieving Pass@100 scores of 77.7% and 85.9% on two test subsets.","","979-8-3315-1091-6","10.1109/AEMCSE65292.2025.11042693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042693","Pseudocode;Code Generation;Large Language Models;Structure Guidance;Semantic Enhancement","Computers;Codes;Translation;Large language models;Semantics;Collaboration;C++ languages;Search problems;Optimization;Software engineering","","","","23","IEEE","24 Jun 2025","","","IEEE","IEEE Conferences"
"Retrieval of Network Packets Information Using a Generated Latent Space Representation for Network Analysis in Cyber Security","O. Manor; O. Lavi; T. Schwartz; M. Sekiya; J. Suga; K. Hikichi; Y. Unno; A. F. Murillo","Fujitsu Research of Europe, UK; Fujitsu Research of Europe, Tel Aviv, Israel; Fujitsu Research of Europe, Tel Aviv, Israel; Fujitsu Research of Europe, UK; Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Fujitsu Limited, Kawasaki, Japan; Fujitsu Research of Europe, UK",2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops),"18 Mar 2025","2024","","","224","231","Network anomaly classification is a fundamental task in cyber security. With new capabilities demonstrated by the advent of Large Language Models (LLMs), their future role in cyber security is presently being explored. Unfortunately, LLMs are not the best suited tool for analysing the network packets for network attack classification. In this paper, we present a retrieval augmented generation (RAG)-like mechanism that can accept network packets as input and return, in a human-readable format, contextual information that could be used by LLMs to analyse the queried packet data. The mechanism is based on the latent-space representation of network data, this representation is generated using autoencoders. Our experiment results show that is feasible to use latent space representation to create a RAG-like mechanism for attack classification using network packets as input.","","979-8-3315-3281-9","10.1109/ACSACW65225.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917535","cyber security;large language models;retrieval augmented generation;attack classification;intrusion detection;autoencoders","Databases;Conferences;Large language models;Autoencoders;Retrieval augmented generation;Network analyzers;Transformers;Data processing;Vectors;Computer crime","","","","20","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Automated Microservice Pattern Instance Detection Using Infrastructure-as-Code Artifacts and Large Language Models","C. E. Duarte","Faculdade de Engenharia da Universidade do Porto, Instituto de Engenharia de Sistemas e Computadores Tecnologia e Ciência, Porto, Portugal",2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C),"30 May 2025","2025","","","161","166","Documenting software architecture is essential to preserve architecture knowledge, even though it is frequently costly. Architecture pattern instances, including microservice pattern instances, provide important structural software information. Practitioners should document this information to prevent knowledge vaporization. However, architecture patterns may not be detectable by analyzing source code artifacts, requiring the analysis of other types of artifacts. Moreover, many existing pattern detection instance approaches are complex to extend. This article presents our ongoing PhD research, early experiments, and a prototype for a tool we call MicroPAD for automating the detection of microservice pattern instances. The prototype uses Large Language Models (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid detection, aiming to keep costs low and maximize the scope of detectable patterns. Early experiments ran the prototype thrice in 22 GitHub projects. We verified that 83% of the patterns that the prototype identified were in the project. The costs of detecting the pattern instances were minimal. These results indicate that the approach is likely viable and, by lowering the entry barrier to automating pattern instance detection, could help democratize developer access to this category of architecture knowledge. Finally, we present our overall research methodology, planned future work, and an overview of MicroPAD's potential industrial impact.","2768-4288","979-8-3315-3336-6","10.1109/ICSA-C65153.2025.00030","European Union; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11014940","software architecture;architecture patterns;architecture documentation;pattern identification;knowledge retrieval;microservices;microservice patterns","Costs;Software architecture;Large language models;Source coding;Microservice architectures;Prototypes;Computer architecture;Software;Knowledge management;Software development management","","","","32","IEEE","30 May 2025","","","IEEE","IEEE Conferences"
"Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding","Y. Di; T. Zhang","Purdue University, West Lafayette, IN, USA; Purdue University, West Lafayette, IN, USA",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1359","1371","Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00165","NSF(grant numbers:ITE-2333736,CCF-2340408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029958","LLM;Code Generation;Code Refinement","Productivity;Codes;Accuracy;Grounding;Collaboration;Programming;Benchmark testing;Usability;Software engineering;Software development management","","","","62","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges","Q. Liu; W. Mo; T. Tong; J. Xu; F. Wang; C. Xiao; M. Chen","University of California, Davis; University of California, Davis; University of California, Davis; NVIDIA; University of Southern California; University of Wisconsin, Madison; University of California, Davis","2024 60th Annual Allerton Conference on Communication, Control, and Computing","4 Nov 2024","2024","","","1","8","The advancement of Large Language Models (LLMs) has significantly impacted various domains, including Web search, healthcare, and software development. However, as these models scale, they become more vulnerable to cybersecurity risks, particularly backdoor attacks. By exploiting the potent memorization capacity of LLMs, adversaries can easily inject backdoors into LLMs by manipulating a small portion of training data, leading to malicious behaviors in downstream applications whenever the hidden backdoor is activated by the pre-defined triggers. Moreover, emerging learning paradigms like instruction tuning and reinforcement learning from human feedback (RLHF) exacerbate these risks as they rely heavily on crowdsourced data and human feedback, which are not fully controlled. In this paper, we present a comprehensive survey of emerging backdoor threats to LLMs that appear during LLM development or inference, and cover recent advancement in both defense and detection strategies for mitigating backdoor threats to LLMs. We also outline key challenges in addressing these threats, highlighting areas for future research.","2836-4503","979-8-3315-4103-3","10.1109/Allerton63246.2024.10735305","DARPA(grant numbers:HR00112490370); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735305","AI Security;Backdoor Attack and Defense","Surveys;Training;Reviews;Large language models;Training data;Reinforcement learning;Safety;Web search;Tuning;Software development management","","","","127","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Comprehensive Performance Modeling and System Design Insights for Foundation Models","S. Subramanian; E. Rrapaj; P. Harrington; S. Chheda; S. Farrell; B. Austin; S. Williams; N. Wright; W. Bhimji","Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Dept. of CS Stony, Stony Brook University, Brook, NY, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA","SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","8 Jan 2025","2024","","","1380","1397","Generative AI, in particular large transformer models, are increasingly driving HPC system design in science and industry. We analyze performance characteristics of such transformer models and discuss their sensitivity to the transformer type, parallelization strategy, and HPC system features (accelerators and interconnects). We utilize a performance model that allows us to explore this complex design space and highlight its key components. We find that different transformer types demand different parallelism and system characteristics at different training regimes. Large Language Models are performant with 3D parallelism and amplify network needs only at pretraining scales with reduced dependence on accelerator capacity and bandwidth. On the other hand, long-sequence transformers, representative of scientific foundation models, place a more uniform dependence on network and capacity with necessary 4D parallelism. Our analysis emphasizes the need for closer performance modeling of different transformer types keeping system features in mind and demonstrates a path towards this.","","979-8-3503-5554-3","10.1109/SCW63240.2024.00179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10820806","performance modeling;transformers;parallelism","Training;Analytical models;Three-dimensional displays;Sensitivity;Foundation models;Computational modeling;Large language models;Parallel processing;Transformers;System analysis and design","","","","57","IEEE","8 Jan 2025","","","IEEE","IEEE Conferences"
"ALANCA: Active Learning Guided Adversarial Attacks for Code Comprehension on Diverse Pre-trained and Large Language Models","D. Liu; S. Zhang","Key Laboratory of High Confidence Software Technologies, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Beijing, China","2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","16 Jul 2024","2024","","","602","613","Neural code models have demonstrated their efficacy across a range of code comprehension tasks, including vulnerability detection, code classification, automatic code summarization, completion, clone detection, etc. Yet, a substantial gap exists in our understanding of the robustness of models in the realm of code comprehension and its associated applications. To probe and illuminate the robustness of code, recent efforts have sought to employ NLP-like techniques to craft adversarial code instances, primarily by perturbing variable and token names. It's worth noting that the semantics of source code predominantly surface through its structural elements, such as abstract syntax trees and control flow graphs, which fundamentally differ from natural languages. The question remains open: Can we perturb the structural aspects of code while preserving its semantics, thereby generating more disruptive adversarial examples that elude current structural-unaware approaches? Moreover, orchestrating adaptive adversarial attacks on diverse neural code models with varying architectures poses formidable challenges, especially in real-world scenarios characterized by constraints on target model access and querying. In this paper, we introduce ALANCA, an active-learning guided adversarial attack framework tailored for neural code models. Leveraging semantic-preserving translations, combined with an adaptive adversarial discriminator and token selector, ALANCA excels in executing adversarial attacks with high success rates, exceptional generation quality, and adaptability across different target models. We substantiate ALANCA's efficacy through comprehensive evaluations across four distinct code comprehension tasks, demonstrating its ability to effectively confound a range of neural models, including pre-trained models and LLMs used in software engineering.","2640-7574","979-8-3503-3066-3","10.1109/SANER60148.2024.00067","National Key Research and Development Program of China(grant numbers:2021YFB3101802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589851","AI for Software Engineering;Software testing and debugging;AI Security","Adaptation models;Analytical models;Codes;Source coding;Semantics;Syntactics;Robustness","","2","","56","IEEE","16 Jul 2024","","","IEEE","IEEE Conferences"
"Privacy and security challenges for autonomous agents : A study of two social humanoid service robots","D. Biström; M. Westerlund; B. Duncan; M. G. Jaatun","Arcada University of Applied Sciences, Finland; Arcada University of Applied Sciences, Finland; University of Aberdeen, Scotland; University of Stavanger, Norway",2022 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),"9 Jan 2023","2022","","","230","237","The development of autonomous agents have gained renewed interest, largely due to the recent successes of machine learning. Social robots can be considered a special class of autonomous agents that are often intended to be integrated into sensitive environments. We present experiences from our work with two specific humanoid social service robots, and highlight how eschewing privacy and security by design principles leads to implementations with serious privacy and security flaws. The paper introduces the robots as platforms and their associated features, ecosystems and cloud platforms that are required for certain use cases or tasks. The paper encourages design aims for privacy and security, and then in this light studies the implementation from two different manufacturers. The results show a worrisome lack of design focus in handling privacy and security. The paper aims not to cover all the security flaws and possible mitigations, but does look closer into the use of the WebSocket protocol and it’s challenges when used for operational control. The conclusions of the paper provide insights on how manufacturers can rectify the discovered security flaws and presents key policies like accountability when it comes to implementing technical features of autonomous agents.","2330-2186","978-1-6654-6367-6","10.1109/CloudCom55334.2022.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005434","Cloud computing;humanoid robot;accountability;security;privacy;DLT;agent;autonomy","Privacy;Cloud computing;Protocols;Social robots;Machine learning;Robot sensing systems;Autonomous agents","","2","","17","IEEE","9 Jan 2023","","","IEEE","IEEE Conferences"
"Fine-Tuning Large Language Models for Text-to-SQL Tasks in Agricultural Census Anomaly Detection","G. P. Nugraha; L. H. Suadaa; S. Pramana","Department of Statistical Computing, Politeknik Statistika STIS, Jakarta, Indonesia; Department of Statistical Computing, Politeknik Statistika STIS, Jakarta, Indonesia; Department of Statistical Computing, Politeknik Statistika STIS, Jakarta, Indonesia",2024 International Conference on Electrical Engineering and Informatics (ICELTICs),"10 Dec 2024","2024","","","136","140","Detecting anomalies from collected data is essential in producing high-quality data for further analysis, including agricultural census data. This research aimed to improve the efficiency of data anomaly checking by utilizing the CodeLlama- 7B Large Language Model (LLM) to translate human questions into SQL queries. Spider and WikiSQL datasets are used in the first fine-tuning phase for translating ability to general queries. The agricultural datasets are constructed using questions and queries in agricultural census database schemas and then used in the second fine-tuning phase. The LoRA method is implemented in the fine-tuning process. This fine-tuning enhanced the model's performance on the SQL-Eval framework, achieving an Average Correct Rate of 69.6%. Combining this approach with Retrieval-Augmented Generation (RAG) and prompt engineering with Chain of Thought (CoT) improvedthe Average Correct Rate to 77.8%.","","979-8-3503-6682-2","10.1109/ICELTICs62730.2024.10776457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776457","LLM;Text-to-SQL;LoRA;SQL-Eval;CodeL-lama","Measurement;Training;Electrical engineering;Structured Query Language;Databases;Large language models;Prompt engineering;Informatics;Anomaly detection","","1","","14","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Poster: Benchmarking of Code Generative LLMs","M. M. Rahman; A. Kundu; E. Bertino","Cisco Research, California, USA; Cisco Research, California, USA; Purdue University, Indiana, USA",2024 IEEE 44th International Conference on Distributed Computing Systems (ICDCS),"22 Aug 2024","2024","","","1448","1449","Generative LLMs have proven to be valuable code generators, thus enabling code copilots and meeting several requirements in software engineering. However, several questions arise: How good an LLM is as a software engineer? How secure is the code generated or fixed by an LLM? These are complex questions to address; however, addressing them is critical for enabling trustworthy software development ecosystems. Addressing those questions requires a designing rigorous benchmark to evaluate: (i) the code that is generated/completed, and (ii) the generative LLMs themselves. In this paper, we propose an automated benchmarking system covering the different aspects of the generated code and the LLMs that generate the code. We also propose the concept of a benchmark dependency graph, coupled with an automated benchmark scoring process that provides a vector of scores on how “good” or how “bad” an LLM is, or the code generated/modified by it is, additionally the artifacts generated or modified around the code.","2575-8411","979-8-3503-8605-9","10.1109/ICDCS60910.2024.00145","NSF(grant numbers:2112471); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630922","Language models;LLM;Code generation;Benchmark;Automation;NLP","Codes;Ecosystems;Benchmark testing;Vectors;Software;Generators;Distributed computing","","","","4","IEEE","22 Aug 2024","","","IEEE","IEEE Conferences"
"RSChat:Intelligent Question Answering Model for Railway Safety Knowledge","J. Li; C. Li; S. Niu; B. Dai","School of Computer and Information Engineering, Shanghai Polytechnic University, Shanghai, China; Shanghai Development Center of Computer Software Technology, Shanghai, China; School of Computer and Information Engineering, Shanghai Polytechnic University, Shanghai, China; Shanghai Development Center of Computer Software Technology, Shanghai, China",2024 IEEE/ACIS 24th International Conference on Computer and Information Science (ICIS),"12 Dec 2024","2024","","","55","60","The release of ChatGPT heralds a new era in artificial intelligence, as large language models (LLMs) empower myriad industries. This paper introduces RSChat, a model based on supervised fine-tuning of multiple large language models (LLMs), exploring a novel paradigm for applying knowledge in railway safety. Initially, we perform prompt engineering using collected domain data, including key information extraction and knowledge expansion. Subsequently, leveraging the constructed fine-tuning datasets, we conduct supervised fine-tuning using multiple large language models (LLMs) as base models. Finally, experimental evaluations demonstrate that RSChat can generate more professional responses in the railway safety domain, exhibiting robust potential for application.","2768-3990","979-8-3503-7664-7","10.1109/ICIS61260.2024.10778327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778327","Railway safety knowledge;Large language models;Supervised fine-tuning","Knowledge engineering;Industries;Information science;Large language models;Computational modeling;Railway safety;Information retrieval;Chatbots;Question answering (information retrieval);Prompt engineering","","","","12","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models","C. Lemieux; J. P. Inala; S. K. Lahiri; S. Sen","University of British Columbia, Canada; Microsoft Research, USA; Microsoft Research, USA; Microsoft Research, USA",2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE),"14 Jul 2023","2023","","","919","931","Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.","1558-1225","978-1-6654-5701-9","10.1109/ICSE48619.2023.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172800","search based software testing;codex;test suite generation;python;large language model;automated testing","Software testing;Codes;Benchmark testing;Software;Space exploration;Test pattern generators;Software engineering","","159","","62","IEEE","14 Jul 2023","","","IEEE","IEEE Conferences"
"Cyber Security Issues and Challenges Related to Generative AI and ChatGPT","R. Pasupuleti; R. Vadapalli; C. Mader","Frost Institute for Data Science and Computing (IDSC), University of Miami, Coral Gables, FL, USA; IDSC Advanced Computing Systems, Electrical and Computer Engineering (ECE), University of Miami, Coral Gables, FL, USA; Systems and Data Engineering, Frost Institute for Data Science and Computing (IDSC), University of Miami, Coral Gables, FL, USA","2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS)","2 Jan 2024","2023","","","1","5","In recent years, Generative Artificial Intelligence (AI) and ChatGPT (Generative Pre-trained Transformer) models that are capable of generating realistic human-mimicked languages have gained progressive popularity. With the evolution of technology, there has been a significant increase in the availability and usage of artificial intelligence tools, such as ChatGPT and Generative AI, that will assist in shaping the future. However, this increasing popularity poses a potential risk if used inappropriately. Threats from AI pose special challenges for government, the private sector, and national security. In this paper, we address some of key concerns of significant cyber security issues and challenges related to Generative AI and ChatGPT. With careful consideration to application usage, organizations can implement appropriate security measures to mitigate these risks. We also incorporate recommendations about ChatGPT usage and its impact on society. It is important that researchers, developers, and policymakers (CIOs, CSOs) work together to mitigate these risks and to ensure that these models are used in a responsible and ethical manner.","2831-7343","979-8-3503-1890-6","10.1109/SNAMS60348.2023.10375472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375472","Cybersecurity Issues;Challenges;Generative AI;ChatGPT;LLM Models;Risks","Ethics;Analytical models;Social networking (online);Government;Chatbots;Transformers;Artificial intelligence","","10","","15","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"A Multi-Agent Approach for Hybrid Intrusion Detection in Industrial Networks: Design and Implementation","C. V. Martinez; M. Sollfrank; B. Vogel-Heuser","Bosch Rexroth AG, Lohr am Main, Germany; Institute of Automation and Information Systems, Technical University of Munich, Germany; Institute of Automation and Information Systems, Technical University of Munich, Germany",2019 IEEE 17th International Conference on Industrial Informatics (INDIN),"30 Jan 2020","2019","1","","351","357","The integration of Network Intrusion Detection Systems (Network IDS) in industrial networks has improved the security of these systems due to their ability to analyze network traffic in order to detect potential system intrusions. Unfortunately, their detection scope is often limited to strategical network locations and may therefore not be capable to detect intrusions occurring at other system locations (e.g., specific devices). Hence, it is necessary to increase their detection scope by further analyzing additional information pertaining to other system components. The introduction of these new information sources adds more complexity to the intrusion detection problem, as it is not only necessary to identify them, but it is also required to define how their authentication, capture and analysis is to be carried out. Multi-Agent Systems are an architectural paradigm that can deal with such complexity. This paper presents a multi-agent approach for hybrid intrusion detection that takes into consideration the aforementioned challenges. This approach is comprised of a multi-agent hybrid intrusion detection architecture designed according to a set of properties. These properties consider IDS-specific requirements. It also takes into consideration current trends in the field of Multi-Agent Systems to provide security, scalability and adaptability across multiple systems. The feasibility of this approach is validated through a prototypical implementation.","2378-363X","978-1-7281-2927-3","10.1109/INDIN41052.2019.8972055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972055","Agent Architecture;Industrial Network Security;Intrusion Detection;Multi-Agent Systems (MAS)","Scalability;Network intrusion detection;Authentication;Telecommunication traffic;Network security;Market research;Complexity theory;Informatics;Multi-agent systems","","5","","42","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Too Noisy To Learn: Enhancing Data Quality for Code Review Comment Generation","C. Liu; H. Y. Lin; P. Thongtanunam",The University of Melbourne; The University of Melbourne; The University of Melbourne,2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR),"13 Jun 2025","2025","","","236","248","Code review is an important practice in software development, yet it is time-consuming and requires substantial effort. While open-source datasets have been used to train neural models for automating code review tasks, including review comment generation, these datasets contain a significant amount of noisy comments (e.g., vague or non-actionable feedback) that persist despite cleaning methods using heuristics and machine learning approaches. Such remaining noise may lead models to generate low-quality review comments, yet removing them requires a complex semantic understanding of both code changes and natural language comments. In this paper, we investigate the impact of such noise on review comment generation and propose a novel approach using large language models (LLMs) to further clean these datasets. Based on an empirical study on a large-scale code review dataset, our LLM-based approach achieves $66-85 \%$ precision in detecting valid comments. Using the predicted valid comments to fine-tune the state-of-the-art code review models (cleaned models) can generate review comments that are $13.0 \%-12.4 \%$ more similar to valid human-written comments than the original models. We also find that the cleaned models can generate more informative and relevant comments than the original models. Our findings underscore the critical impact of dataset quality on the performance of review comment generation. We advocate for further research into cleaning training data to enhance the practical utility and quality of automated code review.","2574-3864","979-8-3315-0183-9","10.1109/MSR66628.2025.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025607","Automated Code Review;Review Comment Generation;Dataset Quality","Training;Codes;Reviews;Data integrity;Large language models;Noise;Training data;Predictive models;Cleaning;Noise measurement","","","","54","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"LASSI: An LLM-Based Automated Self-Correcting Pipeline for Translating Parallel Scientific Codes","M. T. Dearing; Y. Tao; X. Wu; Z. Lan; V. Taylor","University of Illinois, Chicago, USA; University of Illinois, Chicago, USA; Argonne National Laboratory, USA; University of Illinois, Chicago, USA; Argonne National Laboratory, USA",2024 IEEE International Conference on Cluster Computing Workshops (CLUSTER Workshops),"6 Nov 2024","2024","","","136","143","This paper addresses the problem of providing a novel approach to sourcing significant training data for LLMs focused on science and engineering. In particular, a crucial challenge is sourcing parallel scientific codes in the ranges of millions to billions of codes. To tackle this problem, we propose an automated pipeline framework called LASSI, designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates autonomous enhancement through self-correcting loops where errors encountered during the compilation and execution of generated code are fed back to the LLM through guided prompting for debugging and refactoring. We highlight the bidirectional translation of existing GPU benchmarks between OpenMP target offload and CUDA to validate LASSI. The results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.","","979-8-3503-8345-4","10.1109/CLUSTERWorkshops61563.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740822","Large Language Models (LLMs);Code Generation;Code Translation;Parallel Scientific Codes;Self-Correcting","Codes;Runtime;Parallel programming;Conferences;Large language models;Pipelines;Graphics processing units;Training data;Debugging;Benchmark testing","","1","","28","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings","P. R.S.; H. Deep; D. Yadav","Department of CTECH, Chennai, Tamil Nadu; Department of CTECH, Chennai, Tamil Nadu; Department of CTECH, Chennai, Tamil Nadu",2025 International Conference on Data Science and Business Systems (ICDSBS),"20 Jun 2025","2025","","","1","5","Mental illness has become an issue of concern all over the world, with millions experiencing anxiety, depression, and stress. Chatbots powered by artificial intelligence have proved to be a scalable solution to delivering initial assistance through accessible and instant solutions. This work introduces a chatbot for mental wellness that utilizes generative AI and embedding-based relevance detection to provide contextually relevant and empathic answers. In contrast to rule-based systems, the chatbot utilizes few-shot learning, which enables the system to dynamically adjust according to user queries with a low level of training data. The system uses Sentence Transformer embeddings to determine query relevance and the Google Gemini API to create human-like responses. A series of experiments involving classification accuracy tests, response quality evaluation, and user interaction studies proved the high efficacy of the chatbot in identifying relevant queries while sustaining an empathetic tone of conversation. However, issues like spurious positives to vague inputs and crisis response bottlenecks are still areas that need to be addressed. Areas of future improvement include dataset increase, reinforcement learning, multilinguality, and improving crisis intervention mechanisms to improve the effectiveness of the chatbot in offering mental health support. The chatbot was tested using a curated dataset of mental health dialogues in English, with future work aimed at multilingual expansion.","","979-8-3315-8560-0","10.1109/ICDSBS63635.2025.11031881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11031881","Mental health chatbot;generative AI;few-shot learning;SentenceTransformer;relevance detection;Google Gemini API;natural language processing;embedding-based filtering;conversational AI;crisis intervention;sentiment analysis;machine learning;user interaction study;mental health support;AI ethics;privacy and security;reinforcement learning;multilingual chatbot;context-aware responses;cognitive behavioral therapy","Generative AI;Training data;Mental health;Reinforcement learning;Chatbots;Transformers;Vectors;Multilingual;Internet;Few shot learning","","","","20","IEEE","20 Jun 2025","","","IEEE","IEEE Conferences"
"SoK: Prompt Hacking of Large Language Models","B. Rababah; S. T. Wu; M. Kwiatkowski; C. K. Leung; C. G. Akcora","Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada; AI Initiative, University of Central Florida, Orlando, FL, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5392","5401","The safety and robustness of large language models (LLMs) based applications remain critical challenges in artificial intelligence. Among the key threats to these applications are prompt hacking attacks, which can significantly undermine the security and reliability of LLM-based systems. In this work, we offer a comprehensive and systematic overview of three distinct types of prompt hacking: jailbreaking, leaking, and injection, addressing the nuances that differentiate them despite their overlapping characteristics. To enhance the evaluation of LLM-based applications, we propose a novel framework that categorizes LLM responses into five distinct classes, moving beyond the traditional binary classification. This approach provides more granular insights into the AI’s behavior, improving diagnostic precision and enabling more targeted enhancements to the system’s safety and robustness.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825103","LLMs","Hands;Systematics;Large language models;Refining;Big Data;Robustness;Safety;Security;Computer crime;Usability","","","","42","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Assisting Hearing Impaired Children by Personalized Gamification of Auditory Verbal Therapy","S. Fernando; C. Leo; C. Tharushika; R. Mawaththa; J. Perera; S. Vidhanaarachchi; D. Kasthurirathna","Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka",2024 6th International Conference on Advancements in Computing (ICAC),"27 Jan 2025","2024","","","414","419","Children with hearing impairments often need to adapt to their hearing aids or cochlear implants through various techniques, with Auditory Verbal Therapy (AVT) being a prominent long-term method. AVT, led by auditory and speech clinical experts, involves four key stages: awareness, discrimination, identification, and comprehension, as outlined in the auditory skills checklist. To enhance the effectiveness of AVT and maintain the children's engagement, this study integrates personalized gamification into the therapy process. Focusing primarily on the comprehension stage, which aligns with daily human interactions, the study utilizes patient profiles and clinical suggestions to generate quiz-like activities. Prompt engineering and Large Language Model (LLM) evaluation were conducted to ensure gamification efficacy. To develop the most effective prompts to generate assistive activities for AVT, prompt engineering techniques were utilized. Additionally, the study introduced three evaluation metrics: Tri-Aspect Benchmark (TAB), Token-Based Cost Effectiveness Metric (TBCEM), and Fernando's Cost-Performance Index (FCPI); to assess the performance and cost-effectiveness of the corresponding LLMs, ensuring producing content is therapeutically valuable and cost-efficient. GPT -4o Mini emerged as the optimal model, balancing therapeutic value and cost-efficiency for generating AVT content.","2837-5424","979-8-3315-1787-8","10.1109/ICAC64487.2024.10850970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850970","auditory verbal therapy;auditory hierarchy;generative AI;prompt engineering;large language models","Measurement;Pediatrics;Computational modeling;Large language models;Medical treatment;Focusing;Auditory system;Hearing aids;Prompt engineering;Indexes","","","","27","IEEE","27 Jan 2025","","","IEEE","IEEE Conferences"
"Leveraging Generative AI for Proactive Cybersecurity Threat Detection in Cloud Environments","A. Patel; R. C. Sachan; H. Ragothaman; A. Sheth; P. Pandey; S. K. Udayakumar","NA; NA; Advanced Micro Devices (AMD); NA; NA; Salesforce, Inc.",2025 8th International Conference on Information and Computer Technologies (ICICT),"3 Jul 2025","2025","","","80","85","This paper proposes a proactive approach to cloud cybersecurity threat detection that generative AI can complement. To this end, the proposed framework fuses generative models, including GANs and VAEs, with cloud-native security services to model attacks and detect vulnerabilities in advance. The study explores how generative AI can prevent, model and mitigate cyber threats before being used to exploit vulnerabilities to counter conventional security measures. The study focuses on a timely and contextually proactive approach rather than reactive, addresses the issue of resources and covers an essential aspect of concerns with the proposed solution for the future prospect of cybersecurity concerns","2769-4542","979-8-3315-0518-9","10.1109/ICICT64582.2025.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11058483","Generative AI;Cybersecurity;Cloud Environments;Threat Detection;GANs;Anomaly Detection","Ethics;Generative AI;Fuses;Prevention and mitigation;Cloud computing security;Transforms;Threat assessment;Computer security;Anomaly detection","","","","10","IEEE","3 Jul 2025","","","IEEE","IEEE Conferences"
"CogniLearn: Integrating AI-Powered Insights for Class 10 Syllabus","B. K. Bhoomika; L. Brunda; K. H. Chandan; P. S. Anure; S. R. Upadhyaya","Department of CSE, PES University, Bengaluru, India; Department of CSE, PES University, Bengaluru, India; Department of CSE, PES University, Bengaluru, India; Department of CSE, PES University, Bengaluru, India; Department of CSE, PES University, Bengaluru, India",2025 11th International Conference on Computing and Artificial Intelligence (ICCAI),"11 Aug 2025","2025","","","465","472","Cognilearn is an AI-powered educational platform designed to provide high-quality and examination-focused answers for class 10 CBSE exams. The project aims to fine-tune and train AI models to deliver responses that align with board examination standards for Science, Social Studies, and English courses. The study uses two base Language Models namely, Llama2 7B and Qwen $\mathbf{0. 5 B}$. These pre-trained language models are further fine tuned with custom datasets for the question-answering task. Other fine tuning techniques such as LoRA and QLoRa are employed to enhance the capability of the Language Models. The project incorporated Retrieval-Augmented Generation which enables feeding of textbooks and question papers to improve the answer accuracy and relevance. Finally, Prompt Engineering ensures that the retrieved answers are presented appropriately, adhering to standard guidelines. The entire approach ensures that responses are contextually enriched to meet the specific needs of the board examination preparation. The study includes experiments that analyze how effectively these Language Models are used for question-answering tasks in the context of CBSE Class 10 Examination preparation.","","979-8-3315-2491-3","10.1109/ICCAI66501.2025.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11106064","AI in education;CBSE Class 10;large language models (LLMs);LLaMA 2;Qwen 0.5B;finetuning;Retrieval-Augmented Generation (RAG)","Hands;Analytical models;Accuracy;Large language models;Retrieval augmented generation;Education;Prompt engineering;Standards;Tuning;Guidelines","","","","12","IEEE","11 Aug 2025","","","IEEE","IEEE Conferences"
"Enhancing Code Language Models for Program Repair by Curricular Fine-tuning Framework","S. Hao; X. Shi; H. Liu; Y. Shu","Faculty of Computing, Harbin Institute of Technology, China; Faculty of Computing, Harbin Institute of Technology, China; Faculty of Computing, Harbin Institute of Technology, China; Faculty of Computing, Harbin Institute of Technology, China",2023 IEEE International Conference on Software Maintenance and Evolution (ICSME),"11 Dec 2023","2023","","","136","146","Automated program repair (APR) is a key technique for enhancing software maintenance productivity by fixing buggy code automatically. Recently, large code language models (CLMs) have exhibited impressive capabilities in code generation. However, for complex programming tasks, especially program repair, the success rate of CLMs is still low. One of the reasons is that CLMs are typically developed for general purpose and their potential for APR applications has yet to be fully explored. In this paper, we propose APRFiT, a general curricular fine-tuning framework that improves the success rate of CLMs for APR. Firstly, APRFiT generates syntactically diverse but semantically equivalent bug-fixing programs via code augmentation operators to enrich the diversity of bug-fixing dataset automatically. Secondly, APRFiT designs a curriculum learning-based mechanism to help CLMs develop deep understanding of program semantics from these augmented bug-fixing code variants and improve the effectiveness of fine-tuning for APR tasks. We implement APRFiT on different CLMs and evaluate them on Bugs2Fix small and medium datasets. The extensive experiments demonstrate that, the existing CLMs implemented with APRFiT substantially outperform original models and generate 2.5 to 14.5 percent more correct patches than baselines both effectively and efficiently.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00024","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336339","Program Repair;Large Language Models of Code;Curriculum Learning","Training;Productivity;Software maintenance;Codes;Semantics;Computer bugs;Computer architecture","","4","","54","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
