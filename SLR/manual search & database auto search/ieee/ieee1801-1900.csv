"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"The role of open big data within the public sector, case study: Jordan","N. K. T. El-Omari; M. H. Alzaghal","WISE University, Faculty of Information Technology, Amman, Jordan; Excellence in Business Continuity and Crisis Management (EBCCM), Istanbul, Turkey",2017 8th International Conference on Information Technology (ICIT),"23 Oct 2017","2017","","","182","186","In the digital era, the Big Data concept started to play a tremendous role in the public sector. Jordan started an E-Government program to provide citizens with electronic services. In this paper; the role of Open Big Data in the Jordanian public sector is discussed. We will evaluate the E-Government program in Jordan which is the driver behind collecting Big Data and making it available for its citizens. A model is introduced to enhance the usage of Big Data with the National Center for Security and Crises Management (NCSCM) playing a leading role.","","978-1-5090-6332-1","10.1109/ICITECH.2017.8079997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079997","Open Data;Big Data;E-Government;Crowdsourcing;Internet of Things (IoT);Modeling And Simulation (MAS);National Center for Security and Crises Management (NCSCM)","Big Data;Electronic government;Security;Data models;Crowdsourcing;Tools","","","","22","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Static Analysis and LLM for Comprehensive Java Unit Test Generation","W. Wei","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","24 Jun 2025","2025","","","87","92","Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a $10 \%$ improvement in code coverage and a $13 \%$ enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of $89.7 \%$, proving its effectiveness in producing both high-quality and reliable test cases.","","979-8-3315-1091-6","10.1109/AEMCSE65292.2025.11042526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042526","software testing;Java unit testing;static analysis;LLM;test case generation","Software testing;Java;Large language models;Static analysis;Manuals;Syntactics;Software;Software reliability;Test pattern generators;Software engineering","","","","19","IEEE","24 Jun 2025","","","IEEE","IEEE Conferences"
"Generative AI for OCL Constraint Generation: Dataset Collection and LLM Fine-tuning","F. Pan; V. Zolfaghari; L. Wen; N. Petrovic; J. Lin; A. Knoll","Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany; Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany; Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany; Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany; Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany; Robotics, Artificial Intelligence and Real-Time Systems, School of Computation, Information and Technology, Technical University of Munich, Munich, Germany",2024 IEEE International Symposium on Systems Engineering (ISSE),"6 Nov 2024","2024","","","1","8","The Object Constraint Language (OCL) is a formal specification language in model-based systems and software engineering. It defines complex rules and constraints for model-based system design and verification. Constructing an OCL constraint requires expertise not only in OCL syntax but also in meta-model information, which can hinder its application in the practical industrial scenario despite its broad usage. Recently, generative artificial intelligence has demonstrated remarkable performance in code and text generation. This work discusses the generation of OCL constraints from natural language specifications using large language models (LLMs). Given that the automotive and aviation industries are major consumers of model-based engineering, the use of commercial LLMs raises concerns about data privacy. Therefore, we propose to employ open-source and locally deployed LLMs for OCL generation tasks. In this work, we collected a set of meta-models and OCL constraints, which were syntactically validated to ensure the quality of the OCL dataset. Synthetic natural language specifications were generated and used in the dataset for model fine-tuning. Additionally, we designed a retrieval-augmented approach to incorporate meta-model information during LLM fine-tuning and OCL generation. The proposed fine-tuning and OCL generation approach has been experimented with the state-of-the-art open-source LLM, Llama 3 8B. The locally fine-tuned and deployed language model achieved comparable syntactic accuracy and a higher semantic similarity score for OCL generation compared to the cutting-edge commercial models, GPT-4 Turbo and Gemini 1.5 Pro. The usability of the fine-tuned model has been demonstrated for OCL generation in the context of automotive resource allocation.","2687-8828","979-8-3503-5372-3","10.1109/ISSE63315.2024.10741141","Bundesministerium für Bildung und Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10741141","","Visualization;Generative AI;Semantics;Natural languages;Syntactics;Modeling;Usability;System analysis and design;Automotive engineering;Context modeling","","3","","26","IEEE","6 Nov 2024","","","IEEE","IEEE Conferences"
"Self-Evolving Programs: A Novel Approach Leveraging LLMs and Quine Programs","A. M. Saghiri; N. Wang","Department of Computer Science, William Paterson, Wayne, USA; Department of Computer Science, William Paterson, Wayne, USA","2024 International Conference on Computing, Internet of Things and Microwave Systems (ICCIMS)","1 Oct 2024","2024","","","1","4","In an era where software systems undergo rapid evolution, the demand for self-evolving programs has received much attention. This paper introduces a groundbreaking methodology that amalgamates the predictive power of Large Language Model-Based Methods with the self-replicating nature of Quine Programs to organize self-evolving software. This innovative integration not only facilitates dynamic code adaptation to fluctuating runtime conditions but also pioneers a shift from traditional static coding paradigms. Through this approach, we aim to revolutionize software development, particularly enhancing security and performance with a focus on countering selfish mining attacks in Bitcoin. It should be noted that the suggested approach has not yet been considered in the field of software engineering, and its applications are not limited to software security. It has high potential to be used in every domain that requires adaptive programs. Since the suggested approach is novel, the proposed solution in Bitcoin is also novel. The results of simulation show the efficiency of the proposed solution.","","979-8-3503-5173-6","10.1109/ICCIMS61672.2024.10690672","National Science Foundation(grant numbers:2028011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690672","Self-Evolving Programs;Language Model-based Methods;Quine Programs;Dynamic Code Optimization;Selfish Mining Defense","Microwave integrated circuits;Codes;Runtime;Bitcoin;Predictive models;Software systems;Microwave theory and techniques;Security;Microwave FET integrated circuits;Software development management","","","","10","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"VindSec-Llama — Fine-Tuned Meta’s Llama-3 LLM, Federated Learning, Blockchain and PBOM-enabled Data Security Architecture for Wind Energy Data Platforms","E. Bandara; S. H. Bouk; S. Shetty; R. Gore; S. Kompella; R. Mukkamala; A. Rahman; P. Foytik; X. Liang; N. W. Keong; K. De Zoysa","Old Dominion University, Norfolk, VA, USA; Old Dominion University, Norfolk, VA, USA; Old Dominion University, Norfolk, VA, USA; Old Dominion University, Norfolk, VA, USA; U.S. Naval Research Laboratory; Old Dominion University, Norfolk, VA, USA; Deloitte & Touche LLP; Old Dominion University, Norfolk, VA, USA; Florida International University, USA; Nanyang Technological University, Singapore; University of Colombo School of Computing, Sri Lanka",2025 International Wireless Communications and Mobile Computing (IWCMC),"2 Jul 2025","2025","","","120","126","Current wind energy data platforms face significant challenges in securing and managing extensive data from both offshore and onshore wind farms. These challenges include vulnerabilities to cyber-attacks, data tampering, breaches, complex data-sharing issues due to privacy concerns and regulatory compliance, and a lack of scalability and flexibility in analytical tools for real-time data processing. This paper proposes a novel multilayered data security architecture, termed ""VindSec-Llama,"" to address these challenges. It integrates Generative AI, blockchain, federated learning, and Pipeline Bill of Materials (PBOM) to enhance data analytics, model development, and security across several layers, including Infrastructure, Data Lake, Federated Learning, MLOps, Data Provenance, and LLM. Each layer is designed to meet specific functional requirements, such as handling large datasets, facilitating secure federated learning, automating risk management, and ensuring data provenance and traceability. The platform, deployable in server environments (cloud or on-premises), complies with the Risk Management Framework (RMF) guidelines and security standards. It features a blockchain-enabled, coordinator-less federated learning system to enhance data privacy and security by enabling the development of privacy-preserving machine learning models with data from different wind farms. Automation plays a pivotal role throughout VindSec-Llama, with Meta’s custom-trained Llama-3 LLM used for generating remediation scripts in the Infrastructure Layer and for producing PPBOM in the MLOps Layer. The Llama-3 LLM has been quantized and fine-tuned using Qlora to ensure optimal performance on consumer-grade hardware. The MLOps pipeline setup, a critical functionality of VindSec-Llama, ensures seamless integration and deployment of machine learning models, embodying best practices in continuous integration and delivery. This setup is geared towards maximizing security, compliance, and operational efficiency. A prototype of the platform has been implemented within a wind-energy testbed with the collaboration of Department of Energy US, illustrating its practical applications and benefits.","2376-6506","979-8-3315-0887-6","10.1109/IWCMC65282.2025.11059510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11059510","DevSecOps;Generative-AI;LLM;Llama-3;Blockchain;PBOM;Wind-Energy","Wireless communication;Data privacy;Federated learning;Wind energy;Data security;Pipelines;Wind farms;Data models;Blockchains;Risk management","","","","20","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services","M. Xu; H. Du; D. Niyato; J. Kang; Z. Xiong; S. Mao; Z. Han; A. Jamalipour; D. I. Kim; X. Shen; V. C. M. Leung; H. V. Poor","School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Nanyang, Singapore; School of Automation, the Key Laboratory of Intelligent Information Processing and System Integration of IoT, Ministry of Education, and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Chinatown, Singapore; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA",IEEE Communications Surveys & Tutorials,"22 May 2024","2024","26","2","1127","1170","Artificial Intelligence-Generated Content (AIGC) is an automated method for generating, manipulating, and modifying valuable and diverse data using AI algorithms creatively. This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile AIGC networks, that provide personalized and customized AIGC services in real time while maintaining user privacy. We begin by introducing the background and fundamentals of generative models and the lifecycle of AIGC services at mobile AIGC networks, which includes data collection, training, fine-tuning, inference, and product management. We then discuss the collaborative cloud-edge-mobile infrastructure and technologies required to support AIGC services and enable users to access AIGC at mobile edge networks. Furthermore, we explore AIGC-driven creative applications and use cases for mobile AIGC networks. Additionally, we discuss the implementation, security, and privacy challenges of deploying mobile AIGC networks. Finally, we highlight some future research directions and open issues for the full realization of mobile AIGC networks.","1553-877X","","10.1109/COMST.2024.3353265","NSFC(grant numbers:62102099,U22A2054); Guangzhou Basic Research Program(grant numbers:SL2022A04J01471); Guangdong Provincial Pearl River Talents Program(grant numbers:2021QN02S643); National Research Foundation, Singapore; Infocomm Media Development Authority under its Future Communications Research and Development Programme; DSO National Laboratories through the AI Singapore Programme (AISG)(grant numbers:Award AISG2-RP-2020-019,FCP-ASTAR-TG-2022-003); Energy Research Test-Bed and Industry Partnership Funding Initiative, Energy Grid (EG) 2.0 Programme; DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) Programme; MOE Tier 1(grant numbers:RG87/22); NSF(grant numbers:CNS-2148382); Infocomm Media Development Authority under the Future Communications Research Development Programme (FCP); Singapore University of Technology and Design(grant numbers:SRG-ISTD-2021-165); Ministry of Education, Singapore, under its SMU-SUTD Joint Grant(grant numbers:22-SIS-SMU-048); Ministry of Science and ICT (MSIT), South Korea, through the ICT Creative Consilience Program supervised by the Institute for Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2020-0-01821); U.S. National Science Foundation(grant numbers:CNS-2128448,ECCS-2335876); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398474","AIGC;generative AI;mobile edge networks;communication and networking;AI training and inference;Internet technology","Computational modeling;Servers;Biological system modeling;Artificial intelligence;Generative AI;Surveys;Mobile handsets","","185","","304","IEEE","12 Jan 2024","","","IEEE","IEEE Journals"
"Generative AI for Advanced UAV Networking","G. Sun; W. Xie; D. Niyato; H. Du; J. Kang; J. Wu; S. Sun; P. Zhang","College of Computer Science and Technology and the Key Laboratory of Symbolic Computation and Knowledge Engineering, Ministry of Education, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China; School of Automation, Guangdong University of Technology, Guangzhou, China; College of Computer Science and Technology, Jilin University, Changchun, China; Institute for Infocomm Research, Agency for Science, Technology and Research, Fusionopolis, Singapore; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",IEEE Network,"14 Jul 2025","2025","39","4","244","253","With the impressive achievements of chatGPT and Sora, generative artificial intelligence (GAI) has received increasing attention. Not limited to the field of content generation, GAI is also widely used to solve the problems in wireless communication scenarios due to its powerful learning and generalization capabilities. Therefore, we discuss key applications of GAI in improving unmanned aerial vehicle (UAV) communication and networking performance in this article. Specifically, we first review the key technologies of GAI and the important roles of UAV networking. Then, we show how GAI can improve the communication, networking, and security performances of UAV systems. Subsequently, we propose a novel framework of GAI for advanced UAV networking, and then present a case study of UAV-enabled spectrum map estimation and transmission rate optimization based on the proposed framework to verify the effectiveness of GAI-enabled UAV systems. Finally, we discuss some important open directions.","1558-156X","","10.1109/MNET.2024.3494862","National Natural Science Foundation of China(grant numbers:62272194); Science and Technology Development Program (Key R&D) Project of Jilin Province(grant numbers:20230201087GX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749978","Generative AI;UAV communications and networking;optimization;UAV spectrum estimation;diffusion model","Autonomous aerial vehicles;Artificial intelligence;Optimization;Generative adversarial networks;Training;Base stations;Relays;Transformers;Generative AI;Radio spectrum management","","14","","15","IEEE","11 Nov 2024","","","IEEE","IEEE Magazines"
"Energy-Efficient Resource Allocation in Generative AI-Aided Secure Semantic Mobile Networks","J. Zheng; B. Du; H. Du; J. Kang; D. Niyato; H. Zhang","State-Province Joint Engineering and Research Center of Advanced Networking and Intelligent Information Services, School of Information Science and Technology, Northwest University, Xi’an, China; School of Electrical and Information Engineering, JiLin Agricultural Science and Technology University, Jilin, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Automation of School, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China",IEEE Transactions on Mobile Computing,"6 Nov 2024","2024","23","12","11422","11435","The integration of semantic communication with Internet of Things (IoT) technologies has advanced the development of Semantic IoT (SIoT), with edge mobile networks playing an increasingly vital role. This paper presents a framework for SIoT-based image retrieval services, focusing on the application in automotive market analysis. Here, semantic information in the form of textual representations is transmitted to users, such as automotive companies, and stored as knowledge graphs, instead of raw imagery. This approach reduces the amount of data transmitted, thereby lowering communication resource usage, and ensures user privacy. We explore potential adversarial attacks that could disrupt image transmission in SIoT and propose a defense mechanism utilizing Generative Artificial Intelligence (GAI), specifically the Generative Diffusion Models (GDMs). Unlike methods that necessitate adversarial training with specifically crafted adversarial example samples, GDMs adopt a strategy of adding and removing noise to negate adversarial perturbations embedded in images, offering a more universally applicable defense strategy. The GDM-based defense aims to protect image transmission in SIoT. Furthermore, considering mobile devices’ resource constraints, we employ GDM to devise resource allocation strategies, optimizing energy use and balancing between image transmission and defense-related energy consumption. Our numerical analysis reveals the efficacy of GDM in reducing energy consumption during adversarial attacks. For instance, in a scenario, GDM-based defense lowers energy consumption by 5.64%, decreasing the number of image retransmissions from 18 to 6, thus underscoring GDM's role in bolstering network security.","1558-0660","","10.1109/TMC.2024.3396860","National Natural Science Foundation of China(grant numbers:62072362,62072373); National Research Foundation Singapore; Info-communications Media Development Authority; Future Communications Research and Development Programme; Defence Science Organisation (DSO) National Laboratories; AI Singapore Programme AISG(grant numbers:AISG2-RP-2020-019,FCP-ASTAR-TG-2022-003); Singapore Ministry of Education Tier 1(grant numbers:RG87/22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10520929","Generative AI;resource allocation;semantic communication;energy efficiency","Semantics;Task analysis;Training;Image edge detection;Computational modeling;Data models;Social Internet of Things","","9","","78","IEEE","6 May 2024","","","IEEE","IEEE Journals"
"Generative AI for Context-Aware 3D Object Creation Using Vision-Language Models in Augmented Reality","M. Behravan; K. Matković; D. Gračanin","Department of Computer Science, Virginia Tech, Virginia, USA; VRVis Research, Vienna, Austria; Department of Computer Science, Virginia Tech, Virginia, USA",2025 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR),"26 Feb 2025","2025","","","73","81","We present a novel Artificial Intelligence (AI) system that functions as a designer assistant in augmented reality (AR) environments. Leveraging Vision Language Models (VLMs) like LLaVA and advanced text-to-3D generative models, users can capture images of their surroundings with an Augmented Reality (AR) headset. The system analyzes these images to recommend contextually relevant objects that enhance both functionality and visual appeal. The recommended objects are generated as 3D models and seamlessly integrated into the AR environment for interactive use. Our system utilizes open-source AI models running on local systems to enhance data security and reduce operational costs. Key features include context-aware object suggestions, optimal placement guidance, aesthetic matching, and an intuitive user interface for real-time interaction. Evaluations using the COCO 2017 dataset and real-world AR testing demonstrated high accuracy in object detection and contextual fit rating of 4.1 out of 5. By addressing the challenge of providing context-aware object recommendations in AR, our system expands the capabilities of AI applications in this domain. It enables users to create personalized digital spaces efficiently, leveraging AI for contextually relevant suggestions.","2771-7453","979-8-3315-2157-8","10.1109/AIxVR63409.2025.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10896061","Augmented reality;generative AI;vision language models;3D object generation","Solid modeling;Visualization;Three-dimensional displays;Object detection;User interfaces;Real-time systems;Artificial intelligence;Augmented reality;Context modeling;Testing","","2","","34","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"From Automation to Innovation: Harnessing The Synergy of Generative AI and Retrieval-Augmented Generation For Citizen-centric Governance","M. Swapnil; S. Garima; S. Shashank","NITI Aayog, India; NITI Aayog, India; NITI Aayog, India",2024 ITU Kaleidoscope: Innovation and Digital Transformation for a Sustainable World (ITU K),"13 Dec 2024","2024","","","1","8","Presented study explores the transformative potential of Generative Artificial Intelligence (AI) and RetrievalAugmented Generation (RAG) in India’s e-governance ecosystem. Employing a multi-faceted analytical approach, the research uncovers insights into AI’s growth dynamics, perceptions, and impact on service delivery and citizen engagement. The findings emphasize the need for robust security measures, ethical guidelines, and context-specific deployment strategies. By presenting a framework for sustainable AI-driven knowledge management practices, the study offers actionable recommendations to policymakers and practitioners, fostering public trust and paving the way for an efficient, citizen-centric governance model in India. This innovative research contributes significantly to the growing body of knowledge on AI-driven governance, positioning India at the forefront of this transformative journey.","","978-92-61-39091-4","10.23919/ITUK62727.2024.10772932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772932","Generative AI;Retrieval-Augmented Generation;E-Governance;Citizen-Centric Governance","Technological innovation;Ethics;Generative AI;Ecosystems;Government;Predictive models;ITU;Security;Artificial intelligence;Standards","","1","","18","","13 Dec 2024","","","IEEE","IEEE Conferences"
"Leveraging Generative AI to Enhance Automated Vulnerability Scoring","S. L. Mirtaheri; A. Pugliese","University of Calabria, Rende, Italy; University of Calabria, Rende, Italy","2024 IEEE Conference on Dependable, Autonomic and Secure Computing (DASC)","17 Dec 2024","2024","","","57","64","Vulnerability assessment is an important and well-studied subject in software security. Traditional methods use expert knowledge, which is time-consuming. Considering the constantly increasing number of vulnerabilities, automated machine learning (ML)-based solutions have been proposed to assess the severity of vulnerabilities. Existing methods concentrate on predicting the Common Vulnerability Scoring System (CVSS) score or its vector metrics using available vulnerability information. The quality and diversity of the vulnerability description data can greatly affect the accuracy of these predictions. Studies report that less than 60% of such descriptions follow the formal template. On the other hand, the performance of ML-based vulnerability scoring approaches is highly dependent on the quality of the data and the model’s architecture. In this paper, we aim to improve the performance of existing ML-based solutions in vulnerability assessment. We use generative artificial intelligence (AI) and feed the CVSS descriptions to a large-language model. We use GPT3.5Turbo to generate descriptions and propose a fine-tuned BERT-CNN model to predict the CVSS vector metrics. We conduct several experiments to assess the performance of the proposed method against the state-of-the-art. We use both the original dataset (6,370 descriptions) and the descriptions generated by GPT3.5Turbo. Our experiments show that our proposed architecture considerably improves accuracy.","2837-0740","979-8-3315-2272-8","10.1109/DASC64200.2024.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10798728","Vulnerability;Severity;Prediction;Generative AI;Convolutional Neural Networks(CNN);ChatGPT;Bert;Long Short-term Memory (LSTM);TextRNN-ATT","Measurement;Hands;Accuracy;Generative AI;Computer architecture;Predictive models;Vectors;Software;Security;Long short term memory","","1","","34","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Toward Synthetic Network Traffic Generating in NTN-Enabled IoT: A Generative AI Approach","D. Jiang; Z. Wang; X. Liu; Q. Xu; T. Zou; R. Zhang; L. Tan; P. Zhang","School of Information and Communication Engineering and the Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Zhejiang Lab, Hangzhou, China; Zhejiang Lab, Hangzhou, China; Zhejiang Lab, Hangzhou, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center, Qilu University of Technology, Jinan, China; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center, Qilu University of Technology, Jinan, China",IEEE Internet of Things Journal,"10 Jan 2025","2025","12","2","2174","2187","Nonterrestrial networks (NTNs) enabled Internet of Things (IoT) extends connectivity to remote and underserved areas, enhances network reliability and coverage, and supports diverse IoT applications in challenging environments, such as rural, maritime, and disaster-stricken regions. As an emerging and fast-evolving IoT scheme, NTN-enabled IoT requires extensive evaluation to ensure effective deployment in real-world scenarios, such as connectivity, performance, and security evaluation. Since conducting testing in remote and diverse environments is logistically challenging and costly, we propose a generative artificial intelligence (GAI)-based synthetic traffic generation framework that facilitates comprehensive traffic analysis and performance evaluation. The proposed framework employs a GAI model to learn the traffic pattern and generate synthetic traffic from historical data. Our approach includes an embedding-based model for representing network flow attributes and a conditional generative adversarial network (CGAN) for generating traffic flows. Considering both source-destination information and statistical features achieves more comprehensive characterization of traffic flows. Finally, the simulation results demonstrate that the proposed approach can generate high quality traffic that conforms to real data distribution and shows obvious difference between multiple applications.","2327-4662","","10.1109/JIOT.2024.3468209","National Natural Science Foundation of China(grant numbers:U22A2005,62471493); Natural Science Foundation of Shandong Province(grant numbers:ZR2023LZH017,ZR2022LZH015,2023QF025); Fund Projects(grant numbers:2020-JCJQ-ZD-016-11,315197107); Open Fund of Digital Media Art, Key Laboratory of Sichuan Province(grant numbers:20DMAKL01); Open Foundation of Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Qilu University of Technology(grant numbers:2023ZD010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716549","Generative AI (GAI);Internet of Things (IoT);nonterrestrial networks (NTNs);traffic generation;word embedding","Internet of Things;Testing;Telecommunication traffic;Generative adversarial networks;Vectors;Performance evaluation;Vocabulary;Training;Traffic control;Load modeling","","1","","35","IEEE","14 Oct 2024","","","IEEE","IEEE Journals"
"Out-Of-Distribution Detection in Documents Using Generative AI for Banking Applications","S. B. Udayagiri; A. Dhaka; Y. Bhatnagar; R. Das","Department of Data and AI Societe Generale, Global Solution Center, Bangalore, India; Department of Data and AI Societe Generale, Global Solution Center, Bangalore, India; Department of Data and AI Societe Generale, Global Solution Center, Bangalore, India; Department of Data and AI Societe Generale, Global Solution Center, Bangalore, India","2023 Fourth International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)","10 Jul 2024","2023","","","1","8","Out-of-distribution (OOD) detection is a task where the goal is to determine whether an input sample belongs to the same distribution as the training data or comes from a different distribution. OOD detection plays a crucial role in ensuring the robustness and security of machine learning models deployed in critical applications, such as banking. While several studies have explored OOD detection in various domains such as natural images, genomics, text etc., there is a notable lack of research dedicated to document data in the banking sector. In the banking sector, documents such as letter of credit, invoice, bill of lading, insurance document, airway bill, passports, identity cards etc. are required to be processed. These documents are semantically different from natural images in terms of their composition on high-level and low-level features. Due to this most of the methods which work for natural images may not work or show a different behavior when applied to document datasets. Moreover, there is also no OOD benchmark dataset available in the document domain. To address this gap, in this work, we have curated an in-distribution and out-of-distribution document datasets. On this data we have benchmarked various discriminative and generative approaches to test our hypothesis. From Our experimental results we demonstrate that the generative approaches outperform discriminative approaches by a large margin at OOD task. We have also established that those generative models which factor in high frequency features would perform well for document data.","","979-8-3503-8462-8","10.1109/ICSTCEE60504.2023.10585174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10585174","Out-of-distribution;in-distribution;banking;document intelligence;Generative AI;PixelCNN++;Likelihood ratio","Estimation;Training data;Banking;Benchmark testing;Robustness;High frequency;Security","","","","26","IEEE","10 Jul 2024","","","IEEE","IEEE Conferences"
"Prompt-Enhanced Software Vulnerability Detection Using ChatGPT","C. Zhang; H. Liu; J. Zeng; K. Yang; Y. Li; H. Li","Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China; Alibaba, China; Alibaba, China; Alibaba, China; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","276","277","With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Moreover, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3643065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554843","software vulnerability detection;prompt engineering;large language model;chatgpt","Economics;Chatbots;Software;Maintenance;Security;Task analysis;Software engineering","","17","","4","","20 Jun 2024","","","IEEE","IEEE Conferences"
"Bias Analysis in Language Models using An Association Test and Prompt Engineering","R. V. K. Bevara; T. Xiao; F. Hosseini; J. Ding","Department of Information Science, University of North Texas, Denton, Texas, USA; Department of Information Science, University of North Texas, Denton, Texas, USA; Department of Computer Science and Engineering, University of North Texas, Denton, Texas, USA; Department of Information Science, University of North Texas, Denton, Texas, USA","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","19 Feb 2024","2023","","","356","363","Despite significant progress in the fields of machine learning and deep learning, there remains a sense of mistrust regarding the use of these models in real-world scenarios. This mistrust can be partly attributed to semantic biases in text, especially within the realm of commercial natural language processing (NLP). In this work, we analyze genre bias in movie reviews using the Word Embedding Association Test (WEAT). We compare bias across foundational transformer models, including BERT, DistilBert, RoBERTa, T5, XLNet, and GPT2, along with traditional approaches like Glove and Word2Vec. Our analysis shows that while the underlying data contains bias, different models exhibit varied bias levels due to their distinct architectures and training objectives. To mitigate bias, we propose a simple yet effective prompt engineering technique. Incorporating prompts led to a noticeable reduction in bias across different genres, with the effect sizes indicating that using prompts decreased bias by approximately 35% on average compared to scenarios without prompts. Our work provides new analysis that sheds light on prompt engineering techniques to address the pressing issue of semantic bias in NLP models. We believe continued research in this direction can lead to more transparent and fair AI systems.","2693-9371","979-8-3503-5939-8","10.1109/QRS-C60940.2023.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430006","Prompt Learning;language models;Word Embedding Association Test (WEAT);Natural Language Processing (NLP)","Training;Analytical models;Semantics;Motion pictures;Transformers;Data models;Natural language processing","","1","","17","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"LLM-FaaS: A Secured Large Language Model assisted Fog-as-a-Service Framework","E. A. Mattar; P. Bhattacharya; P. K. Dutta; J. J. P. C. Rodrigues","Robotics and Cybernetics, College of Engineering, University of Bahrain, Bahrain; School of Engineering and Technology, Amity University Kolkata, Kolkata, India; School of Engineering and Technology, Amity University Kolkata, Kolkata, India; Federal University of Piauí (UFPI), Teresina, PI, Brazil",2025 10th International Conference on Smart and Sustainable Technologies (SpliTech),"30 Jul 2025","2025","","","1","6","Fog computing reduces latency and network congestion by bringing storage and computation closer to the source of data. In this work, we propose a fog-based architecture that safeguards the confidentiality, integrity, and availability of outsourced cloud data through a three-layer model spanning user devices, fog nodes, and the cloud. Our approach employs an XOR-based technique to divide data into multiple segments, which are later recombined to maintain robust security even if certain nodes are compromised. In addition, we integrate a large language model (LLM) at the fog layer to intelligently analyze incoming tasks, prioritize queries, and help manage congestion in real time. By storing critical data segments at user devices and distributing other pieces across the fog nodes, the design not only boosts privacy protections but also improves query responsiveness. This integrated framework, supported by dynamic congestion management and LLM-driven insights, provides a scalable, cost-effective solution that meets diverse Quality of Service (QoS) demands. As a result, organizations and end users can embrace Fog as a Service (FaaS) with greater confidence in its performance and security.","","978-953-290-142-9","10.23919/SpliTech65624.2025.11091727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091727","Fog Computing;Sensor Processing;Large Language Models;Quality-of-Service;XOR;Security","Cloud computing;Data privacy;Large language models;Quality of service;Organizations;Real-time systems;Data models;Security;Protection;Edge computing","","","","12","","30 Jul 2025","","","IEEE","IEEE Conferences"
"Exploring the Roles of Agents and Multi-Agent in Image Feature Extraction","S. H. Khaleefah; S. A. Mostafa; A. Mustapha; R. Daraman; M. F. Nasrudin; S. Ismail","Faculty of Computer Science and Information Technology Universiti Tun Hussein Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology Universiti Tun Hussein Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology Universiti Tun Hussein Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology Universiti Tun Hussein Onn Malaysia, Johor, Malaysia; Centre for Artificial Intelligence Technology Universiti Kebangsaan Malaysia, Selangor, Malaysia; University Malaysia of Computer Science & Engineering, Cyberjaya, Selangor, Malaysia","2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)","26 Oct 2021","2021","","","76","83","In the interior of image texture analysis methods, many types, behavior, strategies, modeling, and characteristics are employed. Several surveys and reviews are illustrated in the literature regarding image texture and feature extraction using an agent and multi-agent-based systems. Nevertheless, few determinations are made in the context of the image texture feature extraction. Hence, the overall aim of this paper is to provide an overview of the image texture and feature extraction concepts depicted in the literature for characterizing regions in images by exploring the role and strategies of the agent and multi-agent systems (MAS). This paper tackles two behavior types, reactive agent (RA) and goal-directed agent (GDA), in terms of the natural surroundings of agent and multi-agent communications. In terms of MAS modeling, this paper tackles three types, including facilitator agent architecture (FAA), mediator agent architecture (MAA), an autonomous agent architecture (AAA). While for the MAS characteristics, five types are presented: interaction, coordination, cooperation, collaboration, and collectiveness. Finally, a discussion for agent and multi-agent-based feature extraction in image analysis is presented by comparing the strategies and the frequent use of the strategies in the literature. Based on this review, most of the feature extraction multi-agent-based systems rely on either coordinating or emergent-based strategies, while feature selection agent-based systems rely on collaborative strategies. However, there are several aspects that we can consider to be classified as agent-based strategies. This review develops a classification scheme for systems used in specific tasks of feature extraction.","","978-1-6654-3632-8","10.1109/ISAMSR53229.2021.9567863","Universiti Tun Hussein Onn Malaysia(grant numbers:H786); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9567863","Image processing;image texture analysis;feature extraction;agent;multi-agent system","Image texture;Analytical models;Robot kinematics;Collaboration;FAA;Feature extraction;Autonomous agents","","1","","83","IEEE","26 Oct 2021","","","IEEE","IEEE Conferences"
"Business Compliance Detection of Smart Contracts in Electricity and Carbon Trading Scenarios","Y. Wu; H. Wang; Y. Zhang; X. Li; H. Wu; M. Fan; T. Liu","Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Xi’an Jiaotong University, China",2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW),"3 Dec 2024","2024","","","177","178","Business compliance in smart contracts for blockchain-based electricity and carbon trading (B-ECT) remains unexplored. We propose an automated Business Compliance Detection tool for smart contracts (BCDetection ) in B-ECT to address this gap. Our innovation encompasses the creation of a benchmark dataset containing both compliant and non-compliant smart contracts, coupled with the deployment of Agent-based Large Language Models (LLMs) to align smart contract codes with prevailing business regulations. The BCDetection tool employs a structured agent for compliance verification, including pre-judgment, feature extraction, fine-grained feature alignment, and consistency judgment. A case study demonstrates its effectiveness. As the field evolves, our approach shows promise for enhancing security and compliance.","2994-810X","979-8-3503-6704-1","10.1109/ISSREW63542.2024.00074","Research and Development; National Natural Science Foundation of China; Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771465","Smart Contract;Blockchain;Electricity and Carbon Trading;Business Compliance;LLMs","Technological innovation;Codes;Electricity;Smart contracts;Emissions trading;Benchmark testing;Feature extraction;Regulation;Software reliability;Business","","","","6","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"A Survey on Agent Workflow – Status and Future","C. Yu; Z. Cheng; H. Cui; Y. Gao; Z. Luo; Y. Wang; H. Zheng; Y. Zhao","School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China",2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD),"21 Jul 2025","2025","","","770","781","In the age of large language models (LLMs), autonomous agents have emerged as a powerful paradigm for achieving general intelligence. These agents dynamically leverage tools, memory, and reasoning capabilities to accomplish user-defined goals. As agent systems grow in complexity, agent workflows—structured orchestration frameworks have become central to enabling scalable, controllable, and secure AI behaviors. This survey provides a comprehensive review of agent workflow systems, spanning academic frameworks and industrial implementations. We classify existing systems along two key dimensions: functional capabilities (e.g., planning, multi-agent collaboration, external API integration) and architectural features (e.g., agent roles, orchestration flows, specification languages). By comparing over 20 representative systems, we highlight common patterns, potential technical challenges, and emerging trends. We further address concerns related to workflow optimization strategies and security. Finally, we outline open problems such as standardization, and multi-modal integration—offering insights for future research at the intersection of agent design, workflow infrastructure, and safe automation.","2769-3554","979-8-3315-1936-0","10.1109/ICAIBD64986.2025.11082076","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082076","Agent Workflow;Specification;Orchestration;Standardization;LLM;Optimization;Security;MAS","Surveys;Technological innovation;Reviews;Standardization;Market research;Specification languages;Planning;Security;Sparks;Optimization","","","","76","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"AGORA: An Approach for Generating Acceptance Test Cases from Use Cases","G. De Vito; G. Vassallo; F. Palomba; F. Ferrucci","Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy",2024 50th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),"27 Dec 2024","2024","","","126","133","This paper introduces AGORA, an innovative approach that leverages Large Language Models to automate the definition of acceptance test cases from use cases. AGORA consists of two phases that exploit prompt engineering to 1) identify test cases for specific use cases and 2) generate detailed acceptance tests cases. AGORA was evaluated through a controlled experiment involving industry professionals, comparing the effectiveness and efficiency of the proposed approach with the manual method. The results showed that AGORA can generate acceptance test cases with a quality comparable to that obtained manually but improving the process efficiency by over 90% in a fraction of the time. Furthermore, user feedback indicated high satisfaction with using the proposed approach. These findings underscore the potential of AGORA as a tool to enhance the efficiency and quality of the software testing process.","2376-9521","979-8-3503-8026-2","10.1109/SEAA64295.2024.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803349","User Acceptance Testing;Large Language Models;Automated Software Engineering","Industries;Software testing;Refining;Natural languages;Collaboration;Writing;Prompt engineering;Software engineering;Guidelines;Software development management","","","","21","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Generative AI for Secure and Privacy-Preserving Mobile Crowdsensing","Y. Yang; B. Zhang; D. Guo; H. Du; Z. Xiong; D. Niyato; Z. Han","Army Engineering University of PLA, China; Army Engineering University of PLA, China; Army Engineering University of PLA, China; NTU, Singapore; Singapore University of Technology and Design, Singapore; NTU, Singapore; University of Houston, USA",IEEE Wireless Communications,"11 Dec 2024","2024","31","6","29","38","Recently, generative AI has attracted much attention from both academic and industrial fields, due to its potential especially in data generation and synthesis aspects. Simultaneously, secure and privacy-preserving mobile crowdsensing (SPPMCS) has been widely applied in data collection/acquisition due to advantages of low deployment cost, flexible implementation, and high adaptability. Since generative AI can generate new synthetic data to replace the original data to be analyzed and processed, it can lower data attacks and privacy leakage risks for the original data. Therefore, integrating generative AI into SPPMCS is feasible and significant. Moreover, this article investigates an integration of generative AI in SPPMCS, where we present potential research focuses, solutions, and case studies. Specifically, we firstly review the preliminaries for generative AI and SPPMCS, where their integration potential is presented. Then, we discuss research issues and solutions for generative AI-enabled SPPMCS, including security defense against malicious data injection, illegal authorization, malicious spectrum manipulation at the physical layer, as well as privacy protection for data content and terminals' identification and location. Next, we propose a framework for sensing data content protection with generative AI. Simulation results have clearly demonstrated the effectiveness of this framework. Finally, we present major research directions for generative AI-enabled SPPMCS.","1558-0687","","10.1109/MWC.004.2400017","National Research Foundation, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579548","","Sensors;Generative AI;Artificial intelligence;Data privacy;Security;Wireless sensor networks;Data protection;Synthetic data;Simulation;Physical layer;Object recognition;Crowdsensing","","4","","15","IEEE","1 Jul 2024","","","IEEE","IEEE Magazines"
"An Empirical Study to Evaluate AIGC Detectors on Code Content","J. Wang; S. Liu; X. Xie; Y. Li","Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","844","856","Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.CCS CONCEPTS• General and reference → Empirical studies; • Security and privacy → Social aspects of security and privacy; • Computing methodologies → Natural language generation.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764906","AIGC Detection;Code Generation;Large Language Model","Privacy;Codes;Natural languages;Natural language generation;Detectors;Software;Maintenance;Security;Software engineering;Software development management","","","","73","","29 Nov 2024","","","IEEE","IEEE Conferences"
"A Scalable Agent Architecture","C. Sander; D. Meyer; B. Klauer","Faculty of Electrical Engineering, Helmut Schmidt University, Hamburg, Germany; Faculty of Electrical Engineering, Helmut Schmidt University, Hamburg, Germany; Faculty of Electrical Engineering, Helmut Schmidt University, Hamburg, Germany","2024 4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","23 Dec 2024","2024","","","1","6","Research in the field of Multi Agent Systems (MASs) focuses on the design of intelligent interactions between agents. Consequently, agents often require multiple layers of function-ality, subcomponents, and communication channels leading to more complex agents. The development process of such agents is also intricate and often results in unclear implementations, software errors, and security vulnerabilities. To mitigate this complexity and streamline the development process, the paper proposes a scalable and modular agent architecture that functions as a system in a system. The key concept here is to model an agent as a MAS of its own. Agents are divided into components such as resources, sensors, and conversations, which communicate and cooperate with each other like agents in a MAS. Applying this concept enhances the manageability of agent functionalities by adhering the principle of divide and conquer while remaining within the concept of MASs. The proposed approach enables the implementation of each agent as a system of sub-agents, making them deployable in a cloud based environment and leading to highly scalable systems.","","979-8-3503-9118-3","10.1109/ICECCME62383.2024.10796047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796047","Multi Agent Systems;Scalable Agent Architecture","Upper bound;Computational modeling;Scalability;Computer architecture;Oral communication;Software;Sensors;Complexity theory;Security;Software engineering","","","","22","IEEE","23 Dec 2024","","","IEEE","IEEE Conferences"
"Humans vs. ChatGPT: Evaluating Annotation Methods for Financial Corpora","J. Kaikaus; H. Li; R. J. Brunner","Informatics Program, University of Illinois Urbana-Champaign, Urbana, Illinois, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; Department of Accountancy, University of Illinois Urbana-Champaign, Urbana, Illinois, USA",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","2831","2838","Given the vast amount of unstructured financial text data available today, there is a high demand for reliable, quality annotations to facilitate robust model development. However, traditional methods can often be expensive and time-inefficient. In this study, we investigate annotations for emotion, sentiment, and cognitive dissonance generated by the large language models (LLMs), GPT-3.5 and GPT-4, for quarterly earnings conference calls and compare them against human annotations obtained via traditional methods. We also investigate different prompt engineering choices on LLM annotation quality, experimenting with 4 styles of prompts centered around varying the amount of contextual information given and how it is presented to the models. Our results show the GPT models are not only more consistent and reliable than human annotators, but also provide annotations in a more cost- and time-efficient manner.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386425","large language models;earnings calls;emotion recognition;sentiment analysis","Analytical models;Annotations;Big Data;Chatbots;Data models;Reliability;Task analysis","","3","","34","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Conversational AI and Cloud Platform","V. Durga Prasad Jasti; D. Pounraj; M. Jawarneh; Meenakshi; P. Venkata Hari Prasad; S. Ray","CSE Department, VR Siddhartha Engineering College, Vijayawada, India; Department of Computer Science and Engineering, BVC Engineering College (Autonomous), Andhrapradesh, India; Faculty of Computing Sciences, Gulf College, Al&#x2010;Khuwair, Oman; Apeejay Stya University, Sohna, Haryana, India; Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vijayvada, India; SBES, Pune, India",Conversational Artificial Intelligence,"","2024","","","635","653","Summary <p>The term “conversational AI” refers to a system that can accurately imitate natural discourse. This is made possible by natural language processing (NLP), a subfield of artificial intelligence that examines how computers learn to comprehend and evaluate human speech. Natural language processing, also known as NLP, is a technique that interprets both written and spoken language in order to generate relevant and appropriate responses to questions or comments. Cloud‐dependent chatbots are prevalent in many real‐world applications. This manuscript explores various security and privacy concerns in conversational AI and cloud platforms. This article also describes attacks against the cloud and chatbots.</p>","","9781394200795","10.1002/9781394200801.ch35","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10952444.pdf&bkn=10950236&pdfType=chapter","","Cloud computing;Servers;Software;Computer architecture;Chatbots;Computational modeling;Privacy;Hardware;Speech processing;Scalability","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Integrating LLM-Based Text Generation with Dynamic Context Retrieval for GUI Testing","J. Yoon; S. Kim; S. Kim; S. Jung; S. Yoo","KAIST, Daejeon, Korea; Samsung Research, Seoul, Korea; KAIST, Daejeon, Korea; Samsung Research, Seoul, Korea; KAIST, Daejeon, Korea","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","394","405","Automated GUI testing plays a crucial role for smartphone vendors who have to ensure that the widely used mobile apps-that are not essentially developed by the vendors-are compatible with new devices and system updates. While existing testing techniques can automatically generate event sequences to reach different GUI views, inputs such as strings and numbers remain difficult to generate, as their generation often involves semantic understanding of the app functionality. Recently, Large Language Models (LLMs) have been successfully adopted to generate string inputs that are semantically relevant to the test case. This paper evaluates the LLM-based input generation in the industrial context of vendor testing of both in-house and 3rd party mobile apps. We present DROIDFILLER, an LLM based input generation technique that builds upon existing work with more sophisticated prompt engineering and customisable context retrieval. DROIDFILLER is empirically evaluated using a total of 120 textfields collected from a total of 45 apps, including both in-house and 3rd party ones. The results show that DROIDFILLER can outperform both vanilla LLM based input generation as well as the existing resource pool approach. We integrate DROIDFILLER into the existing GUI testing framework used at Samsung, evaluate its performance, and discuss the challenges and considerations for practical adoption of LLM-based input generation in the industry.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989041","National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989041","GUI Testing;Large Language Models;Test Automation","Software testing;Industries;Automation;Accuracy;Large language models;Semantics;Mobile applications;Prompt engineering;Testing;Graphical user interfaces","","","","47","IEEE","20 May 2025","","","IEEE","IEEE Conferences"
"Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting","J. Zhang; Y. Chen; C. Liu; N. Niu; Y. Wang","Alibaba Business School, Hangzhou Normal University, Hangzhou, China; Alibaba Business School, Hangzhou Normal University, Hangzhou, China; Alibaba Business School, Hangzhou Normal University, Hangzhou, China; Department of Electrical Engineering and Computer Science, University of Cincinnati, Cincinnati, USA; Department of Computer Science and Technology, Shanghai University of Finance and Economics, Shanghai, China",2023 International Conference on Intelligent Computing and Next Generation Networks（ICNGN),"25 Jan 2024","2023","","","1","6","Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT undoubtedly is the most representative model. We empirically evaluate ChatGPT’s performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT’s promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools based on generative LLMs.","","979-8-3503-2907-0","10.1109/ICNGN59831.2023.10396810","Hangzhou Normal University; Ministry of Education; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396810","ChatGPT;Requirements Information Retrieval;Empirical Evaluation;Natural Language Processing for Requirements Engineering","Chatbots;Information retrieval;Requirements engineering;Task analysis;Next generation networking","","13","","27","IEEE","25 Jan 2024","","","IEEE","IEEE Conferences"
"AWaRE2-MM: A Meta-Model for Goal-Driven, Contract-Mediated, Team-Centric Autonomous Middleware Frameworks for Antifragility","A. V. Uzunov; M. Brennan; M. B. Chhetri; Q. B. Vo; R. Kowalczyk; J. Wondoh","Defence Science and Technology Group, Australia; Swinburne University of Technology, Australia; CSIRO Data61, Australia; Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; Swinburne University of Technology, Australia",2021 28th Asia-Pacific Software Engineering Conference (APSEC),"17 Feb 2022","2021","","","547","552","In this paper, we introduce a new meta-model that captures core concepts for constructing software architectures for general-purpose, autonomous middleware frameworks that realize internalized and externalized self-adaptivity at both a system- and meta-level in order to achieve antifragility. The proposed meta-model builds on, specializes, and complements existing multi-agent meta-models in line with a previously published reference model for antifragile systems in the cyber domain.","2640-0715","978-1-6654-3784-4","10.1109/APSEC53868.2021.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712156","multi-agent systems;self-adaptive computing;meta-modelling;software architecture;antifragility;middleware frameworks","Software architecture;Surveillance;Refining;Computer architecture;Middleware;Domain specific languages","","4","","37","IEEE","17 Feb 2022","","","IEEE","IEEE Conferences"
"Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering","J. O. Couder; W. C. Pate; D. A. Machado; O. Ochoa","Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University Daytona Beach, Florida, United States of America; Department of Electrical Engineering and Computer Science, Embry-Riddle Aeronautical University Daytona Beach, Florida, United States of America",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","8","During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements playa crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT -3.5 and GPT -4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10892858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892858","AI;Requirement Tracing;Education;Software Requirement Specification;Large Language Models","Training;Visualization;Atmospheric modeling;Prototypes;Software systems;Requirements engineering;Interviews;Artificial intelligence;Software engineering;Software development management","","","","50","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"VeriRAG: Design AI-Specific CPU Co-processor with RAG-Enhanced LLMs","K. Bai; P. Yan; L. Liu; T. Jia","School of Integrated Circuits Peking University, Beijing, China; School of Integrated Circuits Peking University, Beijing, China; School of Integrated Circuits Peking University, Beijing, China; School of Integrated Circuits Peking University, Beijing, China",2025 International Symposium of Electronics Design Automation (ISEDA),"8 Aug 2025","2025","","","76","82","In recent years, many industrial CPUs have integrated AI-specific co-processors as ""CPU for AI"" solution. However, the implementation details of co-processors are diverse. In this work, we present VeriRAG, an LLM-assisted agile design methodology for AI-specific CPU co-processor, which includes a summary-template RAG-enhanced LLM flow for agile RTL generation. We first present a general RISC-V-based AI-specific co-processor architecture template to support both Matrix and Nonlinear computations with detailed instruction extensions. Furthermore, we develop a series of AI-specific co-processors based on VeriRAG for different CPUs, i.e., high-end Xuantie C910 and low-power CVA6 CPU. It is observed that the AI coprocessors can be effectively and properly designed by VeriRAG within a short design cycle, and co-processors gain notable performance boost for AI tasks.","","979-8-3315-3696-1","10.1109/ISEDA65950.2025.11100783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11100783","Co-processor;Agile Design;Code Generation","Codes;Microarchitecture;Design automation;Design methodology;Computer architecture;Performance gain;Artificial intelligence;Coprocessors","","","","31","IEEE","8 Aug 2025","","","IEEE","IEEE Conferences"
"CPLS: Optimizing the Assignment of LLM Queries","Y. Liu; H. Zhang; Z. Li; Y. Miao","School of Information and Physical Sciences, The University of Newcastle, Newcastle, Australia; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Computer Science, Shaanxi Normal University, Shaanxi, China; School of Information and Physical Sciences, The University of Newcastle, Newcastle, Australia",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","151","162","Large Language Models (LLMs) like ChatGPT have gained significant attention because of their impressive capabilities, leading to a dramatic increase in their integration into intelligent software engineering. However, their usage as a service with varying performance and price options presents a challenging trade-off between desired performance and the associated cost. To address this challenge, we propose CPLS, a framework that utilizes transfer learning and local search techniques for assigning intelligent software engineering jobs to LLM-based services. CPLS aims to minimize the total cost of LLM invocations while maximizing the overall accuracy. The framework first leverages knowledge from historical data across different projects to predict the probability of an LLM processing a query correctly. Then, CPLS incorporates problem-specific rules into a local search algorithm to effectively generate Pareto optimal solutions based on the predicted accuracy and cost. To evaluate the proposed approach, we conduct extensive experiments on LLM-based log parsing, a typical software maintenance task. Our experimental results demonstrate that CPLS outperforms the baseline methods, providing solutions with the highest accuracy in 14 out of 16 instances. Compared to the baselines, CPLS achieves an accuracy improvement ranging from 1.24% to 485.54%, or reduces costs by 15.21% to 89.09% while maintaining the highest accuracy achieved by the baselines.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795114","Large Language Models;Query Assignment;Cross-project Prediction;Local Search;Log Parsing","Software maintenance;Costs;Accuracy;Source coding;Transfer learning;Software algorithms;Predictive models;Search problems;Prediction algorithms;Software engineering","","","","50","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Future of Connectivity: A Comprehensive Review of Innovations and Challenges in 7G Smart Networks","V. Chamola; M. Shall Peelam; M. Guizani; D. Niyato","Department of Electrical and Electronics Engineering, BITS Pilani, Pilani, India; Department of Electrical and Electronics Engineering, BITS Pilani, Pilani, India; Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore",IEEE Open Journal of the Communications Society,"29 Apr 2025","2025","6","","3555","3613","The evolution from 1G to 6G networks has transformed global communication, progressing from basic voice calls in 1G to the immersive, AI-enabled experiences of 6G. As emerging AI-driven applications like autonomous systems, the Internet of Everything (IoE), and immersive technologies demand unprecedented capabilities, 7G networks are set to redefine connectivity by overcoming the limitations of earlier generations. This paper comprehensively reviews the innovations and challenges in 7G networks, focusing on integrating advanced AI and machine learning paradigms such as meta-learning, incremental learning, distributed intelligence, and reinforcement learning to enhance adaptability, resource allocation, and edge performance. The review also examines the role of Large Language Models (LLMs) in enabling real-time actionable intelligence and optimizing edge devices within 7G. The paper highlights the use of technologies, including blockchain for decentralized security, quantum computing for robust encryption, terahertz communication for ultra-fast data transfer, zero-energy solutions for sustainability, and generative AI for intelligent network optimization and automation. By addressing these challenges and exploring cutting-edge strategies, this paper envisions 7G networks as the foundation for a secure, intelligent, and sustainable digital future, equipped to combat emerging cyber warfare threats, enhance resilience against technological disruptions, and support innovations across smart cities, autonomous systems, healthcare, and industrial IoT.","2644-125X","","10.1109/OJCOMS.2025.3560035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10963909","7G networks;intelligent networking;machine learning;AI;LLM;wireless;terahertz communication;meta-learning;self-sustaining;security;XAI;explainable AI;distributed intelligence;edge performance;real-time actionable intelligence;generative AI","6G mobile communication;Artificial intelligence;Terahertz communications;5G mobile communication;Real-time systems;Technological innovation;Quantum computing;Security;Quantum entanglement;Wireless communication","","","","406","CCBY","11 Apr 2025","","","IEEE","IEEE Journals"
"LLM-MalDetect: A Large Language Model-Based Method for Android Malware Detection","R. Feng; H. Chen; S. Wang; M. Monjurul Karim; Q. Jiang","College of Mathematics and Information Science, Hebei University, Baoding, China; School of Artificial Intelligence, Shenzhen Polytechnic University, Shenzhen, China; College of Mathematics and Information Science, Hebei University, Baoding, China; Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",IEEE Access,"13 May 2025","2025","13","","81347","81364","Android malware poses a significant cybersecurity threat, enabling unauthorized data access, financial fraud, and device compromise. Although deep learning methods are widely used for malware detection, they often struggle with stability and adaptability in the face of evolving threats. While large language models (LLMs) have shown promise in this area, their application to Android malware detection remains underexplored, particularly with regard to optimizing the semantic relationships within Android application packages (APKs). To address this gap, we introduce LLM-MalDetect, a novel framework that improves LLM-based APK analysis by explicitly modeling semantic dependencies and leveraging structured prompt engineering for optimized detection. Our approach formalizes LLM adaptation through a robust string-based feature extraction method and a tailored fine-tuning strategy to enhance precision. Evaluations on benchmark datasets demonstrate that LLM-MalDetect achieves up to 98.97% accuracy, outperforms existing methods in terms of robustness, and enables real-time analysis.","2169-3536","","10.1109/ACCESS.2025.3565526","Shenzhen Polytechnic University Research Fund(grant numbers:6023310010K,6023310009K,6023312038K); National Key Research and Development Program of China(grant numbers:2021YFF1200100,2021YFF1200104); Special Projects fund in Key Areas of Guangdong Provincial Department of Education(grant numbers:2023ZDZX2085); Natural Science Foundation of Hebei Province(grant numbers:F2021201055); Innovation Capacity Enhancement Program-Science and Technology Platform Project, Hebei(grant numbers:22567623H); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10979936","Android malware detection;data privacy;information security;large language model;prompt engineering","Malware;Feature extraction;Operating systems;Accuracy;Computational modeling;Adaptation models;Semantics;Data models;Transformers;Time factors","","","","57","CCBY","29 Apr 2025","","","IEEE","IEEE Journals"
"Knowledge Graph Reasoning and Security Assurance Decision-Making Based on Online Retrieval Augment Generation","C. Lu; J. Luo; D. Shang; T. Chen; X. Hui; R. Shi","China Academy of industrial Internet, Beijing, China; China Academy of industrial Internet, Beijing, China; China Academy of industrial Internet, Beijing, China; China Academy of industrial Internet, Beijing, China; China Academy of industrial Internet, Beijing, China; China Academy of industrial Internet, Beijing, China",2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM),"26 Mar 2025","2024","","","1007","1010","The Sino-Russian pipeline network, a critical infrastructure for energy supply, faces multifaceted risks ranging from geopolitical instability and international sanctions to sophisticated cyber threats. This paper proposes a novel framework leveraging Knowledge Graph (KG) reasoning and online Retrieval Augmented Generation (RAG) to enhance security assurance and risk prevention decision-making for this vital energy corridor. Our approach integrates a dynamically updated KG with large language models (LLMs) to facilitate real-time risk assessment, scenario planning, and proactive mitigation strategies. We detail the methodology for KG construction, LLM integration, and RAG implementation, and demonstrate its efficacy through simulated experiments. Our results indicate that this framework significantly improves the accuracy and timeliness of risk identification and response, offering a robust solution for safeguarding critical energy infrastructure.","","979-8-3315-4172-9","10.1109/AIIM64537.2024.10934575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934575","LLM;knowledge graph;RAG;planning","Prevention and mitigation;Large language models;Retrieval augmented generation;Pipelines;Decision making;Knowledge graphs;Cognition;Real-time systems;Planning;Critical infrastructure","","1","","10","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Security Issues in AI-Generated Source Codes","R. Nagy; T. Szadeczky","Doctoral School on Safety and Security Sciences, Óbuda University, Budapest, Hungary; Dept. of Instrumentation and Automation, Kando Kalman Faculty of Electrical Engineering, Obuda University, Budapest, Hungary",2023 IEEE 6th International Conference and Workshop Óbuda on Electrical and Power Engineering (CANDO-EPE),"7 Feb 2024","2023","","","000125","000130","We live in the age of AI (artificial intelligence). With the advent of ChatGPT, there are countless solutions to help solve our everyday problems and tasks. The AI bot, which uses LLMs (large language models), has become the fastest-growing consumer application in history, with over 100 million users by January 2023. The surge in users has also enabled the bot to be widely used. In this paper we explore these uses, focusing on the IT security implications.","2831-4506","979-8-3503-2875-2","10.1109/CANDO-EPE60507.2023.10418012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418012","AI;NLP;data science;data processing;CNN;neural networks","Power engineering;Source coding;Chatbots;Security;Artificial intelligence;Task analysis;Surges","","","","17","IEEE","7 Feb 2024","","","IEEE","IEEE Conferences"
"Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability","Z. Wan; J. Qian; Y. Du; J. Jabbour; Y. Du; Y. Zhao; A. Raychowdhury; T. Krishna; V. J. Reddi","Georgia Institute of Technology; Georgia Institute of Technology; University of Minnesota, Twin Cities; Harvard University; Harvard University; University of Minnesota, Twin Cities; Georgia Institute of Technology; Georgia Institute of Technology; Harvard University",2025 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),"5 Aug 2025","2025","","","26","37","Embodied systems, where generative autonomous agents engage with the physical world through integrated perception, cognition, action, and advanced reasoning powered by large language models (LLMs), hold immense potential for addressing complex, long-horizon, multi-objective tasks in realworld environments. However, deploying these systems remains challenging due to prolonged runtime latency, limited scalability, and heightened sensitivity, leading to significant system inefficiencies. In this paper, we aim to understand the workload characteristics of embodied agent systems and explore optimization solutions. We systematically categorize these systems into four paradigms and conduct benchmarking studies to evaluate their task performance and system efficiency across various modules, agent scales, and embodied tasks. Our benchmarking studies uncover critical challenges, such as prolonged planning and communication latency, redundant agent interactions, complex low-level control mechanisms, memory inconsistencies, exploding prompt lengths, sensitivity to self-correction and execution, sharp declines in success rates, and reduced collaboration efficiency as agent numbers increase. Leveraging these profiling insights, we suggest system optimization strategies to improve the performance, efficiency, and scalability of embodied agents across different paradigms. This paper presents the first system-level analysis of embodied AI agents, and explores opportunities for advancing future embodied system design.","2766-0486","979-8-3315-0294-2","10.1109/ISPASS64960.2025.00013","DARPA; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11096365","embodied ai;workload characterization;system optimization","Sensitivity;Runtime;Scalability;Memory management;Benchmark testing;Cognition;Software;Planning;Performance analysis;Optimization","","","","105","IEEE","5 Aug 2025","","","IEEE","IEEE Conferences"
"Ensemble Learning Framework for Phishing URL Detection and Leveraging LLMs for Explainability","A. R. Revathi; A. Arun; K. Sebastian","SCOPE, Vellore Institute of Technology, Chennai, India; SCOPE, Vellore Institute of Technology, Chennai, India; SCOPE, Vellore Institute of Technology, Chennai, India",2025 11th International Conference on Communication and Signal Processing (ICCSP),"29 Jul 2025","2025","","","566","571","Recent trends show that Phishing has become a main method for attack by which a malicious attacker can steal sensitive personal information, such as bank card numbers, by imitating people or respected institutions like banks, email providers, social networks, and other web services. Being one of the oldest types of online threats, phishing attacks remain a significant problem regarding online security. Despite the existence of several detection systems, phishing as an attack is difficult to completely resolve due to the ever-evolving nature of these attacks. This paper proposes a stacking based multilayered ensemble learning model which is trained for URL-based phishing detection. This model incorporates both boosting algorithms as well as deep learning techniques, in order to improve classification accuracy using a stacking approach. The predictions from the models in the first-layer are sent as the parameters for higher-level models, refining our resultant predictions through iterative learning. Results demonstrate that the resulting model can detect phishing websites, outperforming existing models, with recall, accuracy, recall, and F1-score exceeding 99.9%, while using fewer parameters making it applicable in a real time environment.","2836-1873","978-1-6654-5736-1","10.1109/ICCSP64183.2025.11089451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11089451","Phishing;URL-based Phishing;Ensemble Learning;Boosting Algorithms;CNN;Transformers;Cybersecurity;Performance Metrics","Uniform resource locators;Deep learning;Accuracy;Phishing;Stacking;Signal processing algorithms;Predictive models;Boosting;Real-time systems;Ensemble learning","","","","18","IEEE","29 Jul 2025","","","IEEE","IEEE Conferences"
"Event-Triggered Consensus Control for Multi-Agent Systems Against False Data-Injection Attacks","X. -M. Li; Q. Zhou; P. Li; H. Li; R. Lu","Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China",IEEE Transactions on Cybernetics,"15 Apr 2020","2020","50","5","1856","1866","In this article, the event-triggered security consensus problem is studied for time-varying multiagent systems (MASs) against false data-injection attacks (FDIAs) and parameter uncertainties over a given finite horizon. In the process of information transmission, the malicious attacker tries to inject false signals to destroy consensus by compromising the integrity of measurements and control signals. The randomly occurring stealthy FDIAs on sensors and actuators are modeled by the Bernoulli processes. In order to reduce the unnecessary utilization of communication resources, an event-triggered control mechanism with state-dependent threshold is adopted to update the control input signal. The main objective of this article is to design a controller such that, under randomly occurring FDIAs and admissible parameter uncertainties, the MASs achieve consensus. By utilizing stochastic analysis method, two sufficient criteria are derived to ensure that the prescribed H∞ consensus performance can be achieved. Then, the desired controller gains are derived by solving recursive linear matrix inequalities. Simulation results are presented to illustrate the effectiveness and applicability of the proposed control method.","2168-2275","","10.1109/TCYB.2019.2937951","National Natural Science Foundation of China(grant numbers:61973091,U1611262,61425009); Guangdong Natural Science Funds for Distinguished Young Scholar(grant numbers:2017A030306014); Innovative Research Team Program of Guangdong Province Science Foundation(grant numbers:2018B030312006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8840898","Event-triggered mechanism;false data-injection attacks (FDIAs);multiagent systems (MASs)","Sensors;Security;Time-varying systems;Frequency control;Multi-agent systems;Actuators","","314","","51","IEEE","17 Sep 2019","","","IEEE","IEEE Journals"
"Semantic Communications Using Foundation Models: Design Approaches and Open Issues","P. Jiang; C. -K. Wen; X. Yi; X. Li; S. Jin; J. Zhang","Southeast University, China; Institute of Communications Engineering, National Sun Yat-sen University, Taiwan; Southeast University, China; Southeast University, China; Southeast University, China; Hong Kong University of Science and Technology, Hong Kong",IEEE Wireless Communications,"14 Jun 2024","2024","31","3","76","84","Foundation models (FMs), including large language models, have become increasingly popular due to their wide-ranging applicability and ability to understand human-like semantics. While previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system layers, considering computation and memory complexity, requires further analysis. This study focuses on integrating FMs at the task application, semantic coding, and physical transmission layers, using universal knowledge to profoundly transform system design. Additionally, it examines the use of compact models to balance performance and complexity, comparing three separate approaches that employ FMs. Ultimately, the study highlights unresolved issues in the fields that need addressing.","1558-0687","","10.1109/MWC.002.2300460","National Key Research and Development Program of China(grant numbers:2018YFA0701602); National Natural Science Foundation of China (NSFC)(grant numbers:62261160576,623B2020); Key Technologies R&D Program of Jiangsu(grant numbers:BE2023022,BE2023022-1); Hong Kong Research Grants Council(grant numbers:AoE/E-601/22-R); NSFC/RGC Collaborative Research Scheme(grant numbers:CRS_HKUST603/22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10558822","","Performance evaluation;Analytical models;Frequency modulation;Computational modeling;Semantics;Memory management;Transforms;Large language models","","5","","15","IEEE","14 Jun 2024","","","IEEE","IEEE Magazines"
"A Novel Approach to Image Synthesis: Using Stack GAN to Enhance Cybersecurity Application of Generative AI","P. Bisherwal; P. Srivastava; R. Kumar; R. Jindal","Dept.of C0E, Delhi Technological University, New Delhi, India; Dept.of C0E, Delhi Technological University, New Delhi, India; Dept.of C0E, Delhi Technological University, New Delhi, India; Dept.of C0E, Delhi Technological University, New Delhi, India",2024 International Conference on Intelligent Systems for Cybersecurity (ISCS),"12 Jul 2024","2024","","","1","6","Text-to-image synthesis, a transformative application of artificial intelligence, converts textual descriptions into vivid images, with implications ranging from artistic expression to cybersecurity. This paper explores Stack Generative Adversarial Networks (Stack GAN) as an architectural innovation, enhancing image quality in two stages: Stage I and Stage II, each employing generators and discriminators to produce high-resolution images. With a focus on data integrity and system security, encryption algorithms safeguard sensitive input data. The Stack GAN model, coupled with conditional augmentation, improves resource efficiency, and content linkage, and mitigates over-fitting. Results demonstrate the model's robustness, offering utility in information and network security through enhanced fraud detection, user authentication, and visualization of security policies. This research underscores the fusion of AI's creative capabilities with traditional and innovative security measures.","","979-8-3503-7523-7","10.1109/ISCS61804.2024.10581108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581108","Artificial Intelligence in Cybersecurity;Machine Learning;Text-to-visual representation;Stacked GAN;Fraud Detection;User Authentication;GANs for Visual Security","Training;Visualization;Technological innovation;Refining;Text to image;Generative adversarial networks;Generators","","","","20","IEEE","12 Jul 2024","","","IEEE","IEEE Conferences"
"Edge Multi-agent Intrusion Detection System Architecture for IoT Devices with Cloud Continuum","G. Funchal; T. Pedrosa; F. D. L. Prieta; P. Leitao","Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Campus de Santa Apolönia, Bragança, Portugal; Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Campus de Santa Apolönia, Bragança, Portugal; BISITE Digital Innovation hub, University of Salamanca, Edificio I+D+i, C/ Espejos s/n, Salamanca, Spain; Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Campus de Santa Apolönia, Bragança, Portugal",2024 IEEE 7th International Conference on Industrial Cyber-Physical Systems (ICPS),"26 Aug 2024","2024","","","1","6","The Industry 4.0 has brought significant changes in production processes and business models worldwide. Advanced technologies, e.g., Collaborative Robotics, Artificial Intelligence, Cloud Computing, and Internet of Things (IoT) are playing a crucial role in improving efficiency and productivity. However, the adoption of these technologies, particularly IoT, introduces security vulnerabilities and potential attacks due to inadequate security measures. This paper addresses the need for dedicated cybersecurity mechanisms and secure device design in IoT networks, particularly emphasizing the challenges faced in implementing Intrusion Detection Systems (IDS) on resource-constrained IoT edge devices, limiting the use of traditional machine learning based detection methods. Moreover, the limited computational resources of IoT devices require lightweight techniques that have low power requirements but can accurately detect anomalies in the network. To tackle these challenges, a novel multi-agent based architecture is proposed, considering the distribution of nodes along the edge-cloud continuum, and enabling the collaboration among different processes to detect anomalies during attacks. The proposed architecture is evaluated at the edge level using the CICIoT2023 dataset. The results demonstrate the feasibility of using multi-agent systems for a collaborative detection of IoT attacks, contributing to enhance the security of IoT-based systems against cyber threats in Industry 4.0 environments by leveraging lightweight techniques.","2769-3899","979-8-3503-6301-2","10.1109/ICPS59941.2024.10639952","Foundation for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10639952","Intrusion Detection Systems;Multi-agent Systems;Internet of Things;Machine Learning","Productivity;Cloud computing;Service robots;Image edge detection;Intrusion detection;Collaboration;Systems architecture","","2","","13","IEEE","26 Aug 2024","","","IEEE","IEEE Conferences"
"Named Entity Recognition Based on Large Language Model and Instruction Tuning","S. Yang; M. Li; L. Xu","Artificial Intelligence Institute of China Electronics Technology, Group Corporation, Beijing, China; Artificial Intelligence Institute of China Electronics Technology, Group Corporation, Beijing, China; Artificial Intelligence Institute of China Electronics Technology, Group Corporation, Beijing, China",2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),"26 Nov 2024","2024","","","336","343","In recent years, with the development of deep learning technology, Large Language models (LLMs) perform well on many natural language processing tasks, but the performance on named entity recognition tasks are average. This is because of the difference between named entity recognition and Large Language Model: the former is essentially a sequence labeling task, while the latter is a text generation model. Therefore, in this paper, we propose a named entity recognition method based on large language model and instruction fine-tuning. First, combining the idea of prompt engineering, we reconstruct the data set format of named entities by constructing instruction prompt to fit the generation task. The instruction cue consists of four parts: task description, entity definition, task example and input text. In addition, this paper also adopts the method of instruction fine-tuning to further improve the capability of named entity recognition for large language models. Finally, this paper selects the CLUENER2020 Chinese named entity recognition dataset for experimental validation, and compares with several traditional named entity recognition methods. The results show that the method proposed in this paper performs better.","","979-8-3315-0661-2","10.1109/ICBASE63199.2024.10762152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10762152","component;large language model;named entity recognition;prompt engineering;instruction tuning","Training;Deep learning;Large language models;Named entity recognition;Hardware;Data models;Prompt engineering;Labeling;Tuning;Software engineering","","","","28","IEEE","26 Nov 2024","","","IEEE","IEEE Conferences"
"Using Self-Organizing Architectures to Mitigate the Impacts of Denial-of-Service Attacks on Voltage Control Schemes","C. Cameron; C. Patsios; P. C. Taylor; Z. Pourmirza","School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, U.K.",IEEE Transactions on Smart Grid,"22 Apr 2019","2019","10","3","3010","3019","Modern power systems are becoming increasingly decentralized, with a greater degree of observability provided through a network of sensors and local controllers in addition to existing centralized supervisory control and data acquisition platforms. However, the interconnectivity between sensors and controllers creates potential vulnerabilities which can be exploited by a cyber-attack. The majority of components installed on the grid were designed with little or no consideration for aspects of cyber-security and therefore leaving the network at risk of economic loss, asset damage or widespread blackouts. Present research in cyber-attack events and electrical grid resilience, often treats these in isolation. Furthermore, the ICT infrastructure in modern electrical networks is not tested as rigorously in terms of reliability and security as the physical assets. Therefore, an integrated approach is needed for the analysis of cyber-threats against power systems, linking the attack mechanisms in the ICT layer and the physical impacts at the electrical layer. This paper introduces a method of self-organizing communication architectures that for the first time has been applied to the problem of mitigating the negative impacts of denial of service cyber-attacks in the smart grid and demonstrates the benefits of this in a novel integrated environment connecting power system modeling and communication layer simulation. This paper demonstrates and quantifies the advantages of self-organization in terms of computational burden and voltage control in a distribution network experiencing multiple attack formats and increasing numbers of attackers.","1949-3061","","10.1109/TSG.2018.2817046","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330052","Smart grid;self-organizing systems;multi-agent systems;cyber-security;voltage control","Smart grids;Computer security;Voltage control;Sensors;Information and communication technology","","46","","44","CCBY","3 Apr 2018","","","IEEE","IEEE Journals"
"Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap","X. Wu; S. -H. Wu; J. Wu; L. Feng; K. C. Tan","Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong, China; Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong, China; Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; College of Computer Science, Chongqing University, Chongqing, China; Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong, China",IEEE Transactions on Evolutionary Computation,"1 Apr 2025","2025","29","2","534","554","Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.","1941-0026","","10.1109/TEVC.2024.3506731","National Key Research and Development Program of China(grant numbers:2022YFC3801700); Research Grants Council of the Hong Kong SAR(grant numbers:PolyU25216423,PolyU11211521,PolyU15218622,PolyU15215623,C5052-23G); Hong Kong Polytechnic University(grant numbers:P0043563,P0046094); National Natural Science Foundation of China(grant numbers:62306259,U21A20512); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767756","Algorithm generation;evolutionary algorithm (EA);large language model (LLM);neural architecture search (NAS);optimization problem;prompt engineering","Optimization;Closed box;Reviews;Evolutionary computation;Codes;Search problems;Collaboration;Surveys;Software engineering;Prompt engineering","","18","","164","IEEE","27 Nov 2024","","","IEEE","IEEE Journals"
"Cyberattack Correlation and Mitigation for Distribution Systems via Machine Learning","J. Appiah-Kubi; C. -C. Liu","The Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; The Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA",IEEE Open Access Journal of Power and Energy,"24 Jan 2023","2023","10","","128","140","Cyber-physical system security for electric distribution systems is critical. In direct switching attacks, often coordinated, attackers seek to toggle remote-controlled switches in the distribution network. Due to the typically radial operation, certain configurations may lead to outages and/or voltage violations. Existing optimization methods that model the interactions between the attacker and the power system operator (defender) assume knowledge of the attacker’s parameters. This reduces their usability. Furthermore, the trend with coordinated cyberattack detection has been the use of centralized mechanisms, correlating data from dispersed security systems. This can be prone to single point failures. In this paper, novel mathematical models are presented for the attacker and the defender. The models do not assume any knowledge of the attacker’s parameters by the defender. Instead, a machine learning (ML) technique implemented by a multi-agent system correlates detected attacks in a decentralized manner, predicting the targets of the attacker. Furthermore, agents learn optimal mitigation of the communication level through Q-learning. The learned attacker motive is also used by the defender to determine a new configuration of the distribution network. Simulations of the technique have been performed using the IEEE 123-Node Test Feeder. The simulation results validate the capability and performance of the algorithm.","2687-7910","","10.1109/OAJPE.2023.3236429","U.S. National Science Foundation(grant numbers:1837359); projects(grant numbers:PR6ZBSTA); 5GPG sponsored by Commonwealth Cyber Initiative (CCI), State of Virginia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015858","Intrusion detection;cyber security;anomaly detection;q-learning;reinforcement learning;multi-agent systems;entropy;distribution automation;distribution reconfiguration","Distribution networks;Cyberattack;Intrusion detection;Anomaly detection;Q-learning;Reinforcement learning;Multi-agent systems;Entropy","","9","","19","CCBY","12 Jan 2023","","","IEEE","IEEE Journals"
"Real-Time Implementation of Secure Distributed State Estimation for Networked Microgrids","M. Cintuglu; A. Kondabathini; D. Ishchenko",NA; NA; NA,2020 IEEE 6th World Forum on Internet of Things (WF-IoT),"13 Oct 2020","2020","","","1","6","State estimation functionality is essential for future power grids operations especially with the increased Distributed Energy Resources (DER) and microgrids penetration, as it can enhance system observability and help improving DER management and control in DER-rich energy delivery systems. Decentralized nature of the power grids with high DER penetration necessitates distributed implementation for both state estimation and control applications, as centralized approaches may no longer be feasible due to intermittent nature of DER and communication latencies of traditional SCADA. Secure distributed state estimation (SDSE) further enhances the distributed state estimation algorithm by addressing misbehaving node detection and isolation issues in a distributed environment.This paper presents a framework for real-time implementation and verification of cyber-physical SDSE. The developed framework is comprised of power-hardware-in-the-loop setup representing the electrical model of networked microgrids and the distributed microgrid control platform with peer-to-peer communications as the cyber infrastructure.Communication and interoperability architectures within a microgrid are established by leveraging IEC 61850 DER data model extensions. IEC 61850 GOOSE is applied for inter-microgrid communication, while intra-microgrid communication uses Open Field Message Bus (OpenFMB) data model with Data Distribution Service (DDS) protocol for distributed publish/subscribe messaging. The case study investigation illustrates how the developed real-time cyber-physical security framework addresses the emerging data integrity and false data injection security problems in a multi-microgrid environment.","","978-1-7281-5503-6","10.1109/WF-IoT48130.2020.9221310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9221310","cyber-physical security;OpenFMB;DDS;distributed state estimation;misbehaving node detection;IEC 61850;microgrid;multi-agent systems;privacy","Protocols;Data integrity;Microgrids;Real-time systems;Data models;Peer-to-peer computing;Security","","4","","20","IEEE","13 Oct 2020","","","IEEE","IEEE Conferences"
"""Revolutionizing Farming: GAN-Enhanced Imaging, CNN Disease Detection, and LLM Farmer Assistant""","C. Dhavale; T. Pawar; A. Singh; S. Pole; K. Sabat","Information Technology Department, Xavier Institute of Engineering, Mumbai, India; Information Technology Department, Xavier Institute of Engineering, Mumbai, India; Information Technology Department, Xavier Institute of Engineering, Mumbai, India; Information Technology Department, Xavier Institute of Engineering, Mumbai, India; Information Technology Department, Xavier Institute of Engineering, Mumbai, India","2024 2nd International Conference on Computer, Communication and Control (IC4)","4 Apr 2024","2024","","","1","6","Crop disease recognition is a crucial aspect of modern agriculture that can significantly impact crop yield, quality, and overall food security. This paper introduces an innovative approach to crop disease recognition and farmer support by combining Generative AI and Langchain Llama Model for chatbot development. In the proposed system, Generative AI, specifically deep learning models, are employed to analyze images of crop leaves for early signs of diseases. This approach enhances the accuracy and efficiency of disease diagnosis, enabling farmers to take timely corrective actions and reduce the use of pesticides. A Generative Adversarial Network (GAN) is employed for image augmentation due to the limited dataset size. A Convolutional Neural Network (CNN) is utilized for precise crop disease recognition based on image analysis. To bridge the gap between technology and farmers, the Langchain Llama Model, a state-of-the-art conversational AI model, is integrated to create an interactive and user-friendly chatbot interface. The results of this research project demonstrate the potential of cutting-edge AI technology to transform agriculture, making it more accessible, efficient, and environmentally friendly. By empowering farmers with a sophisticated chatbot interface, this system paves the way for a smarter and more sustainable agricultural future.","","979-8-3503-8793-3","10.1109/IC457434.2024.10486501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486501","Crop disease recognition;Generative AI;CNN;Langchain LLaMa Model;chatbot","Training;Generative AI;Computational modeling;Crops;Transforms;Chatbots;Generative adversarial networks","","5","","16","IEEE","4 Apr 2024","","","IEEE","IEEE Conferences"
"Decomposition, Synthesis, and Attack: A Multi-Instruction Fusion Method for Jailbreaking LLMs","S. Jiang; X. Chen; K. Xu; L. Chen; H. Ren; R. Tang","School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China; School of Cyber Science and Engineering, Sichuan University, Chengdu, China",IEEE Internet of Things Journal,"8 Apr 2025","2025","12","8","9420","9434","Large language models (LLMs) can transform natural language instructions into executable commands for IoT devices like autonomous aerial vehicles (AAVs), creating new development opportunities. However, safety concerns about LLMs translating commands into machine or program control instructions cannot be overlooked. Currently, jailbreak instructions used to test the LLM security are often restricted to specific modes or tasks, resulting in a lack of diversity and leaving some tasks unexplored. To address this issue, we introduce a multi-instruction fusion (MIF) method that can automatically fuse harmful prompts and various task instructions into jailbreaks. First, we adopt a reverse decomposition strategy to acquire sufficient supervised data for fusing harmful prompts and harmless task instructions into jailbreaks and construct a task instruction synthesizer based on it. Then, to determine the optimal instruction combinations in the vast combination space, we propose a representative-node-based selection strategy, ReNB, to rank and filter the instruction combinations on a few representative samples, thereby accelerating the identification of the valid ones. Experimental results demonstrate that MIF significantly improves the attack success rate (ASR), achieving over 90% on GPT-4o-mini, LLaMa2-70B, and Qwen2-7B models, outperforming the state-of-the-art (SOTA) baselines.","2327-4662","","10.1109/JIOT.2025.3525741","National Natural Science Foundation of China(grant numbers:62202320,62402331); Natural Science Foundation of Sichuan Province(grant numbers:2024NSFSC1449); Fundamental Research Funds for the Central Universities(grant numbers:SCU2024D012,YJ202429); Science and Engineering Connotation Development Project of Sichuan University(grant numbers:2020SCUNG129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824840","Jailbreak attacks;large language model (LLM);autonomous aerial vehicles (AAVs)","Autonomous aerial vehicles;Natural languages;Synthesizers;Safety;Airports;Security;Internet of Things;Planning;Drones;Translation","","2","","78","IEEE","3 Jan 2025","","","IEEE","IEEE Journals"
"Are LLMs Correctly Integrated into Software Systems?","Y. Shao; Y. Huang; J. Shen; L. Ma; T. Su; C. Wan","East China Normal University, Shanghai, China; The University of Tokyo, Tokyo, Japan; East China Normal University, Shanghai, China; The University of Tokyo, Tokyo, Japan; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1178","1190","Large language models (LLMs) provide effective solutions in various application scenarios, with the support of retrieval-augmented generation (RAG). However, developers face challenges in integrating LLM and RAG into software systems, due to lacking interface specifications, various requirements from software context, and complicated system management. In this paper, we have conducted a comprehensive study of 100 open-source applications that incorporate LLMs with RAG support, and identified 18 defect patterns. Our study reveals that 77 % of these applications contain more than three types of integration defects that degrade software functionality, efficiency, and security. Guided by our study, we propose systematic guidelines for resolving these defects in software life cycle. We also construct an open-source defect library HYDRANGEA [1].","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00204","National Natural Science Foundation of China(grant numbers:62402183,23CGA33); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029854","LLM;defects;empirical software engineering","Systematics;Large language models;Retrieval augmented generation;Software systems;Libraries;Security;Faces;Software engineering;Guidelines","","","","103","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Evaluating Prompt Engineering for Generalized Power Line Segmentation","A. Hossain; M. Hasan; R. Abdelfattah; D. Scott; K. Abdelfatah; A. Sherif","School of Computing Sciences and Computer Engineering, University of Southern, Mississippi, MS, USA; School of Computing Sciences and Computer Engineering, University of Southern, Mississippi, MS, USA; School of Computing Sciences and Computer Engineering, University of Southern, Mississippi, MS, USA; Department of Computing Sciences, University of South, Carolina, SC, USA; CareerBuilder, Canada, CO ON, Canada; School of Computing Sciences and Computer Engineering, University of Southern, Mississippi, MS, USA",SoutheastCon 2025,"25 Apr 2025","2025","","","508","513","Power line segmentation is a critical task in infrastructure monitoring, UAV-based inspection, and autonomous navigation. Detecting power lines in aerial images presents significant challenges due to their thin structure, varying backgrounds, and occlusions caused by environmental elements. Recent advancements in multimodal large language models (MLLMs) and vision-based segmentation frameworks provide a new direction for improving segmentation accuracy using natural language instructions. This study investigates the effectiveness of prompt engineering in enhancing power line segmentation performance within the Generalized Segmentation Vision Assistant (GSVA) framework. By systematically evaluating different prompt strategies, we analyze how varying levels of prompt specificity influence the balance between precision, recall, and segmentation completeness. Our findings demonstrate that carefully structured prompts improve the model's ability to detect power lines while minimizing false positives from visually similar structures. Furthermore, we propose an optimized prompt that enhances segmentation performance by leveraging contrast-awareness, structural positioning, and adaptive instruction tuning. The results highlight the importance of prompt optimization in improving segmentation accuracy, particularly for thin-object detection in complex backgrounds.","1558-058X","979-8-3315-0484-7","10.1109/SoutheastCon56624.2025.10971697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971697","","Image segmentation;Adaptation models;Accuracy;Natural languages;Reliability engineering;Power grids;Safety;Prompt engineering;Optimization;Monitoring","","","","23","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Generative AI for Education: A Retrieval-Augmented System for Effective Feedback in Self-Assessment","J. Martinez-Romo; L. Araujo; L. Plaza; F. López-Ostenero","Computer Systems and Languages Department, Universidad Nacional de Educación a Distancia (UNED), Madrid, Spain; Computer Systems and Languages Department, Universidad Nacional de Educación a Distancia (UNED), Madrid, Spain; Computer Systems and Languages Department, Universidad Nacional de Educación a Distancia (UNED), Madrid, Spain; Computer Systems and Languages Department, Universidad Nacional de Educación a Distancia (UNED), Madrid, Spain",2025 IEEE Global Engineering Education Conference (EDUCON),"3 Jun 2025","2025","","","1","9","The application of generative AI in education has shown significant potential to enhance learning outcomes by providing personalized, adaptive feedback to students. In this work, we present a novel Retrieval-Augmented Generation (RAG) system designed to improve the explanations and feedback provided to students during self-assessment activities. The system we developed is grounded in the course's reference material, ensuring that the feedback remains accurate, consistent, and contextually relevant to the student's curriculum. The system retrieves information directly from the textbook, reducing ambiguity and interpretation errors, and generates responses tailored to the specific needs of each student. The feedback is not only designed to correct misconceptions but also to reinforce key concepts, making the system a valuable tool for self-guided learning. In this study, we also explore the importance of prompt engineering in creating effective AI-generated feedback. We detail the iterative process used to optimize the prompts and the strategies employed to ensure high-quality, interpretable responses. The findings from this work suggest that generative AI, when integrated with subject-specific textbooks and careful prompt engineering, can significantly enhance the educational experience by providing dynamic, and contextually accurate feedback. This approach opens new possibilities for AI-driven education tools, contributing to more personalized and effective learning experiences.","2165-9567","979-8-3315-3949-8","10.1109/EDUCON62633.2025.11016446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016446","Self-assessment tools;formative feedback;computer science;generative IA;Retrieval augmented generation","Computer science;Accuracy;Retrieval augmented generation;Prompt engineering;Iterative methods;Engineering education","","1","","34","IEEE","3 Jun 2025","","","IEEE","IEEE Conferences"
"Establishing a Robust LLMOps Framework for Intelligent Automation: Strategies and Best Practices","D. Krishnamurthy; V. Neelanath","UST, London, UK; UST, Trivandrum, India",2025 Emerging Technologies for Intelligent Systems (ETIS),"21 Apr 2025","2025","","","1","5","This paper delves into the establishment of a robust LLMOps framework, crucial for intelligent automation in modern enterprises. LLMOps for intelligent automation platforms require composability and capability to use different architectures, embeddings, and Generative AI models seamlessly with domain focus. In this paper we discuss an architecture we designed for UST SmartOps, UST's Intelligent Automation platform, keeping automation at its core. Large Language Models (LLMs) have significantly advanced AI capabilities in natural language processing and understanding, but their deployment and management, integration and using them effectively for automation use cases pose unique challenges. LLMOps, similar to DevOps and MLOps, is dedicated to the operationalization of these models, addressing infrastructure, performance optimization, security, and continuous improvement. The paper outlines the strategies and best practices essential for LLMOps, emphasizing data management, model monitoring, and the integration of AI with automation to improve operational efficiency, reduce costs, and improve decision-making processes. Through systematic orches-tration of tools, processes, and infrastructure, the document aims to provide a comprehensive guide to effectively leverage LLMs within intelligent automation frameworks.","","979-8-3315-0754-1","10.1109/ETIS64005.2025.10961869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10961869","LLMOps;LLMOps framework;AI infrastructure;AI security;Model monitoring;LLM Cost Optimization","Intelligent automation;Costs;Systematics;Large language models;Organizations;Security;Reliability;Monitoring;Optimization;Best practices","","1","","12","IEEE","21 Apr 2025","","","IEEE","IEEE Conferences"
"Security Enhancement During Agents Communication in GNA Approach","K. A. Mohammed; M. Abdul Majid; M. S. Ahmad","Faculty of Computer Systems & Software Engineering, University Malaysia Pahang, Kuantan, Pahang, Malaysia; Faculty of Computer Systems & Software Engineering, University Malaysia Pahang, Kuantan, Pahang, Malaysia; College of Computer Science and Information Technology, Universiti Tenaga Nasional, Kajang, Selangor, Malaysia","2018 International Symposium on Agent, Multi-Agent Systems and Robotics (ISAMSR)","22 Nov 2018","2018","","","1","6","In many dynamic environments, where humans and agents coexist, agents cooperate for internal and external resources to achieve their goals. Consequently, the agents are exposed to the risk of being attacked by malicious agents. Such situation, if not mitigated, risks the entire model and threatens its long-term performances. Due to uncertainties of an agent and its potential behavior, especially in human-agent collaboration systems, an agent might behave fraudulently to its partners (humans or agents). Therefore, security is a crucial aspect in the initial development of a collaborative human-agent system. However, many research envisages the security aspects only in the implementation stage. At this stage, a security model is strictly formulated based on the techniques and constraints among agents. Over the past decade, many researchers propose various security models with outstanding features. However, despite the features of the proposed security models, there is still a lack of research effort in security aspects for multi-agent systems, which is considered as a critical challenge. In this paper, we attempt to enhance the Generic Nodal Abstraction (GNA) approach with a security model applied among agents in their communication. The enhancements include authentication, message encryption and interaction constraints.","","978-1-5386-7856-5","10.1109/ISAMSR.2018.8540550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540550","Security;human agent collaboration;malicious agent","Multi-agent systems;Collaboration;Task analysis;Robots;Authentication;Organizations","","","","14","IEEE","22 Nov 2018","","","IEEE","IEEE Conferences"
"RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation","C. Peng; Q. Wu; J. Liu; J. Liu; B. Jiang; M. Xu; Y. Wang; X. Liu; P. Yang","ByteDance, Beijing, China; ByteDance, Beijing, China; ByteDance, Hangzhou, China; ByteDance, Hangzhou, China; ByteDance, Shenzhen, China; East China Normal University, Shanghai, China; ByteDance, Shanghai, China; ByteDance, Shenzhen, China; ByteDance, Beijing, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2279","2283","Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as MarsCode can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users’ acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users’ recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764858","Code Completion;Prompt Engineering;Benchmark;Large Language Model","Measurement;Codes;Large language models;Benchmark testing;Programming;Recording;History;Software engineering","","","","22","","29 Nov 2024","","","IEEE","IEEE Conferences"
"OntoKGen: A Genuine Ontology and Knowledge Graph Generator Using Large Language Model","M. S. Abolhasani; R. Pan","School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ; School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ",2025 Annual Reliability and Maintainability Symposium (RAMS),"27 Mar 2025","2025","","","1","6","Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.","2577-0993","979-8-3503-6774-4","10.1109/RAMS48127.2025.10935139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935139","Large Language Model;Ontology and Knowledge Graph Generator;Prompt Engineering;Neo4J","Large language models;Retrieval augmented generation;Pipelines;Random access memory;Knowledge graphs;Ontologies;User interfaces;Reliability engineering;Generators;Prompt engineering","","","","7","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Adversarial Attacks on Generative AI Anomaly Detection in the Quantum Era","J. A; V. Ebenezer; A. J. Isaac; J. Marshell; P. Pradeepa; V. Naveen","Dept. of CSE, Karunya Institute of Tech. and Sci., Coimbatore, India; Dept. of CSE, Karunya Institute of Tech. and Sci., Coimbatore, India; Dept. of CSE, Karunya Institute of Tech. and Sci., Coimbatore, India; Dept. of Electronics and Instrumentation Engg, SRM Valliammai Engineering College, Kattankulathur; Dept. of IT, PSNA college of Engg and tech., Dindigul; Dept. of CSE, Sri Shakthi Institute of Engg. and Tech., Coimbatore, India","2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","9 Feb 2024","2023","","","1833","1840","In the context of the rapidly evolving quantum era, where the capabilities of quantum computing are expanding exponentially, the security and reliability of generative AI anomaly detection systems have come under significant threat from adversarial attacks. This paper delves into the vulnerabilities inherent in existing models in this quantum era and introduces a novel approach aimed at fortifying their resilience. The problem at hand pertains to the susceptibility of conventional generative AI anomaly detection systems to adversarial manipulations, raising critical concerns about their trustworthiness in critical applications. Traditional methods in this domain rely on static models that lack adaptability to defend against quantum-based adversarial attacks, rendering them inadequate for the evolving threat landscape. Our proposed solution combines quantum-resistant algorithms with advanced generative AI techniques to dynamically adapt to emerging attack strategies, resulting in demonstrably enhanced robustness against quantum-based adversarial attacks, as substantiated by our experimental findings. In conclusion, safeguarding generative AI anomaly detection systems against adversarial threats in the quantum era is paramount, and our innovative approach offers a promising avenue for bolstering the security and reliability of these systems in this challenging environment.","","979-8-3503-4060-0","10.1109/ICECA58529.2023.10395092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395092","Adversarial Attacks;Generative Artificial Intelligence;Anomaly Detection;Quantum Era;Quantum Computing","Adaptation models;Quantum computing;Generative AI;Computational modeling;Robustness;Security;Anomaly detection","","3","","30","IEEE","9 Feb 2024","","","IEEE","IEEE Conferences"
"Deep Learning Patterns Enabling AI for Science","G. Fox","Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA",2023 IEEE John Vincent Atanasoff International Symposium on Modern Computing (JVA),"15 Jan 2024","2023","","","15","16","Currently, AI and, in particular, deep learning play a major role in science, from data analytics and simulation surrogates to policy and system decisions. This role is likely to increase as ideas from early adopters spread across all academic fields. One can group the structure of “AI for Science” into a few patterns, where one needs to explore examples of each pattern, possibly leading to Foundation models for each or maybe all of them combined. We suggest examining each pattern and supporting it with high-performance, easy-to-use environments for end-to-end systems, including data engineering. This support would cover parallelism, storage and data movement, security, and the user interface. We discuss the relationship between patterns, Foundation models, and benchmarks.","","979-8-3503-2889-9","10.1109/JVA60410.2023.00014","NSF(grant numbers:2212550,2210266,2204115,2151597); Department of Energy(grant numbers:DE-SC0023452); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387556","AI for Science;Foundation Models;Patterns;Deep Learning;Time Series;Imagery;Simulation Surrogates","Deep learning;Data analysis;Computational modeling;User interfaces;Parallel processing;Benchmark testing;Data engineering","","","","7","IEEE","15 Jan 2024","","","IEEE","IEEE Conferences"
"Advancements in Secure Computing: Exploring Automated Repair Debugging and Verification Techniques for Hardware Design","A. A. Zeraatkar; P. S. Kamran; H. Al-Asaad","Electrical and Computer Engineering Department, University of California, Davis, California; Electrical and Computer Engineering Department, University of California, Davis, California; Electrical and Computer Engineering Department, University of California, Davis, California",2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC),"13 Feb 2024","2024","","","0357","0364","This paper studies the recent advancement of techniques for automating the repair and verification of hardware designs. Challenges associated with debugging and fixing bugs in hardware designs are discussed, emphasizing the importance of addressing these issues prior to the manufacturing of chips. It categorizes approaches according to their contributions and indicates specific techniques used in each approach. The studies demonstrate the effectiveness of different methodologies, such as fault localization, confidence scoring, Large Language Models, and learning-based approaches, in automatically repairing as well as hardware design verification. According to the results of each approach, these techniques have the potential to enhance the security and reliability of hardware systems.","","979-8-3503-6013-4","10.1109/CCWC60891.2024.10427806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427806","Fuzzing;Large Language Models;Formal Verification;Automatic Code Repair","Location awareness;Computer bugs;Debugging;Maintenance engineering;Hardware;Manufacturing;Security","","","","33","IEEE","13 Feb 2024","","","IEEE","IEEE Conferences"
"An Adaptable AI Assistant for Network Management","A. Abane; A. Battou; M. Merzouki","NIST, Maryland, USA; NIST, Maryland, USA; NIST, Maryland, USA",NOMS 2024-2024 IEEE Network Operations and Management Symposium,"2 Jul 2024","2024","","","1","3","This paper presents a network management AI assistant built with Large Language Models. It adapts at runtime to the network state and specific platform, leveraging techniques like prompt engineering, document retrieval, and Knowledge Graph integration. The AI assistant aims to simplify management tasks and is easily reproducible with available source code.","2374-9709","979-8-3503-2793-9","10.1109/NOMS59830.2024.10574957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574957","LLMs;text embeddings;RAG;network management;knowledge graph;Neo4j;graph database","Runtime;Databases;Source coding;Knowledge graphs;Task analysis","","1","","7","IEEE","2 Jul 2024","","","IEEE","IEEE Conferences"
"MATLAB-Based Simulation of Generative AI-Based Autonomous Vehicle Control for High-Speed Driving and Emergency Maneuvers","N. Kunchakuri","Clarksburg, Maryland","2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)","2 Apr 2025","2025","","","1191","1196","Recently, the automobile industry has been investigating the idea of autonomous vehicles extensively as a means of, for increasing gas mileage or gaining access to areas that are dangerous for human drivers. The autonomous industry has not widely adopted these methods despite the fact that they clearly provide improved control efficiency. This is because their first end-user adoption frequently poses significant obstacles. Although System Predictive Management (SPM) has historically been employed to regulate models with shorter dynamics, it is currently also being utilized in models with significantly quicker dynamics due to the development of stronger processors. A weight adaptable SPM depending on the PSO-BP neural network is developed in this study to enhance the monitoring adaptability of self-driving vehicles beneath varying road shape and vehicle high-speeds. It is composed of an optimum-weight adaptable regulating and a dynamics-oriented SPM. The PSO-BP NN is trained offline using the optimum load beneath various operational circumstances determined by automatic simulation in order to accomplish the online modification of SPM load. This approach depends on the employment of SPM to accomplish high-precision monitoring management. The adaptable management structures outperform the original conventional SPM regulation in terms of monitoring adaption, according to the test outcomes of the Prescan-Carsim-Simulink combined simulated system. The adaptable management approach increased monitoring speed while satisfying the vehicle's needs for real-time management and lateral security, according to the evaluation outcomes from an automated vehicle investigation system where the management method was additionally validated.","","979-8-3315-1857-8","10.1109/CE2CT64011.2025.10939159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10939159","Autonomous vehicle;High-Seed;Artificial intelligence;Neural network;Path tracking management","Industries;Roads;Decision making;Artificial neural networks;Predictive models;Real-time systems;Safety;Automobiles;Vehicle dynamics;Monitoring","","","","21","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"Envisioning the interactive convergence of Generative AI and Facial Expression Recognition","F. G; N. M; J. Khan; S. H. K M","Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India; Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India; Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India; Computer Science and Engineering, New Horizon College of Engineering, Bengaluru, India",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),"10 Jun 2024","2024","","","1","5","This paper explores and envisions a collaboration between Facial Expression Recognition (FER) and Generative Artificial Intelligence (GenAI) to create an advanced human-computer interaction (HCI) experience. Deep learning techniques are harnessed to interpret human emotions from facial expressions, using a structured approach that includes pre-processing, feature extraction, and classification. The data acquired from the FER stage informs GenAI systems—such as Generative Pre-trained Transformers (GPTs)—to produce contextually relevant and emotionally resonant responses. The proposed system leverages a sophisticated camera setup to capture facial data with high accuracy under diverse lighting conditions, ensuring robust and reliable emotion inference. Virtual avatars complete the multimodal experience by generating corresponding nonverbal cues. Special attention is given to creating a dynamic feedback loop for continuous learning and adaptation, leading to increasingly effective human-computer dialogues. Ethical implications, such as user privacy, data security, and bias mitigation, are central to the design and deployment of this system, underscoring the commitment to ethical standards and user empowerment. This paper aims to demonstrate how the convergence of FER and GenAI can revolutionize HCI, paving the way for enhanced empathetic and intelligent computing interfaces.","","979-8-3503-9447-4","10.1109/I2CT61223.2024.10543745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543745","Generative Artificial Intelligence (GenAI);Facial Expression Recognition (FER);Human-Computer Interaction (HCI);Emotional Intelligent Systems;Ethical Considerations in AI","Human computer interaction;Deep learning;Ethics;Data privacy;Generative AI;Face recognition;Data security","","","","10","IEEE","10 Jun 2024","","","IEEE","IEEE Conferences"
"VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference","Z. Liu; X. Luo; J. Guo; W. Ni; Y. Zhou; Y. Guan; C. Guo; W. Cui; Y. Feng; M. Guo; Y. Zhu; M. Zhang; C. Jin; J. Leng",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Duke University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of Rochester; University of Illinois Urbana-Champaign; Magik Compute; Shanghai Jiao Tong University,2025 IEEE International Symposium on High Performance Computer Architecture (HPCA),"8 Apr 2025","2025","","","1496","1509","Vector quantization (VQ), which treats a vector as a compression unit, gains increasing research interests for its potential to accelerate large language models (LLMs). Compared to conventional element-wise quantization methods, VQ algorithms can compress weight and KV cache tensors in LLMs with a greater ratio while maintaining the high model accuracy. However, translating a VQ algorithm’s memory reduction into the actual latency improvement is challenging. We profile and analyze the current approach of integrating VQ into computation kernels and show that its major inefficiency lies in the poor access efficiency of codebooks in VQ algorithms and uncoordinated computation dataflow. Meanwhile, the diversity of VQ algorithms (e.g., different vector sizes and entry counts) and LLMs, computation kernels (e.g matrix-matrix/vector multiplication and attention computation) makes it impractical to manually craft efficient kernel implementations for each specific case. In this work, we design and implement VQ-LLM, an efficient fused VQ kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU’s memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve the latency reduction of $\mathbf{6 4. 3 6 \%}$ to $\mathbf{9 9. 1 \%}$ compared to existing open-source implementations. A final comparison with state-of-the-art element-wise quantization methods like AWQ and QoQ shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.","2378-203X","979-8-3315-0647-6","10.1109/HPCA61900.2025.00112","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10946800","large language model;vector quantization;gpu acceleration","Codes;Accuracy;Translation;Vector quantization;Vectors;Computational efficiency;System-on-chip;Kernel;Optimization;Engines","","","","76","IEEE","8 Apr 2025","","","IEEE","IEEE Conferences"
"6G-Enabled IoT Networks Cyber Threat Prevention Using Generative AI","W. Badawy","School of Artificial Intelligence, Egyptian Russian University, Cairo, Egypt",2024 International Conference on Future Telecommunications and Artificial Intelligence (IC-FTAI),"14 Apr 2025","2024","","","1","4","In this paper, we present the applications of Generative Artificial Intelligence (GAl) in preotecting 6G-enabled loT networks from different cyber threats. The paper shall leverage Generative Adversarial Networks (GANs) and federated learning, in order to propose a novel framework for detecting therefore mitigates sophisticated cyber-attacks in real-time. Internet of Things (loT) devices using 6G wireless networks shall provide opportunities for enhancing connectivity by enabling secured smart applications. However, these 6G advancements also introduce significant cybersecurity challenges which require robust threat prevention strategies. The proposed framework takes advantages of the inherent 6G capability, such as high data rates, low latency, and network slicing. It implemented a distributed, AI-driven security mechanism that adapts to evolving threats. The simulation results and it analysis, demonstrates the “effectiveness” of the proposed framework in preventing a wide range of cyber-attacks, such as phishing, malware, and denial-of-service (DoS) attacks. It also ensures the integrity, confidentiality, and availability of loT services. The results of this study demonstrate the integration of AI in next-generation wireless networks, and offer insights into the design of resilient and the application of adaptive cybersecurity solutions.","","979-8-3315-3408-0","10.1109/IC-FTAI62324.2024.10950051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10950051","6G wireless networks;Cybersecurity;Internet of Things “IoT”;Generative Artificial Intelligence “GAI”;real-time detection","6G mobile communication;Adaptive systems;Generative AI;Wireless networks;Prevention and mitigation;Simulation;Real-time systems;Telecommunications;Internet of Things;Cyberattack","","","","45","IEEE","14 Apr 2025","","","IEEE","IEEE Conferences"
"Generative AI for Space-Air-Ground Integrated Networks","R. Zhang; H. Du; D. Niyato; J. Kang; Z. Xiong; A. Jamalipour; P. Zhang; D. I. Kim","Nanyang Technological University, Singapore; University of Hong Kong, Hong Kong; Nanyang Technological University, Singapore; Guangdong University of Technology, China; Singapore University of Technology and Design, Singapore; The University of Sydney, Australia; Beijing University of Posts and Telecommunications, China; Sungkyunkwan University, South Korea",IEEE Wireless Communications,"11 Dec 2024","2024","31","6","10","20","Recently, generative AI technologies have emerged as significant advancements in the artificial intelligence field, renowned for their language and image generation capabilities. Meantime, the space-air-ground integrated network (SAGIN) is an integral part of future B5G/6G for achieving ubiquitous connectivity. Inspired by this, this article explores an integration of generative AI in SAGIN, focusing on potential applications and a case study. We first provide a comprehensive review of SAGIN and generative AI models, highlighting their capabilities and opportunities for their integration. Benefiting from generative AI's ability to generate useful data and facilitate advanced decision-making processes, it can be applied to various scenarios of SAGIN. Accordingly, we present a brief survey on their integration, including channel modeling and channel state information (CSI) estimation, joint air-space-ground resource allocation, intelligent network deployment, semantic communications, image extraction and processing, and security and privacy enhancement. Next, we propose a framework that utilizes a generative diffusion model (GDM) to construct a channel information map to enhance quality of service for SAGIN. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss potential research directions for generative AI-enabled SAGIN.","1558-0687","","10.1109/MWC.016.2300547","National Natural Science Foundation of China (NSFC)(grant numbers:62102099,U22A2054,2023A04J1699); National Research Foundation, Singapore; National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670196","","Generative AI;Data models;Artificial intelligence;Satellites;Resource management;Adaptation models;Atmospheric modeling;Space-air-ground integrated networks","","23","","15","IEEE","9 Sep 2024","","","IEEE","IEEE Magazines"
"CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models","H. Huang; F. Lin; Y. Hu; S. Wang; Y. Gao","Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"25 Dec 2024","2024","","","9488","9495","Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io","2153-0866","979-8-3503-7770-5","10.1109/IROS58592.2024.10801352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801352","","Training;Learning systems;Knowledge engineering;Foundation models;Grasping;Robot sensing systems;Planning;Prompt engineering;Object recognition;Intelligent robots","","2","","58","IEEE","25 Dec 2024","","","IEEE","IEEE Conferences"
"Learning Approximate Execution Semantics From Traces for Binary Function Similarity","K. Pei; Z. Xuan; J. Yang; S. Jana; B. Ray","Columbia University, New York, NY, USA; Purdue University, West Lafayette, IN, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA",IEEE Transactions on Software Engineering,"18 Apr 2023","2023","49","4","2776","2790","Detecting semantically similar binary functions – a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics – requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions’ execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions’ traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8× faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex – on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.","1939-3520","","10.1109/TSE.2022.3231621","National Science Foundation(grant numbers:CCF-18-45893,CCF-18-22965,CCF-16-19123,CNS-18-42456,CNS-18-01426,CNS-16-18771,CNS-16-17670,CNS-15-64055,CNS-15-63843); ONR(grant numbers:N00014-17-1-2010,N00014-16-1-2263,N00014-17-1-2788); NSF CAREER; ARO Young Investigator; Google Faculty Fellowship; JP Morgan Faculty Research Award; DiDi Faculty Research Award; Google Cloud; Capital One Research; Amazon Web Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002189","Binary analysis;large language models;software security","Semantics;Task analysis;Computer architecture;Optimization;Codes;Behavioral sciences;Computational modeling","","17","","100","IEEE","28 Dec 2022","","","IEEE","IEEE Journals"
"Assessing GPT's Legal Knowledge in Japanese Real Estate Transactions Exam","K. Inoshita","Faculty of Data Science, Shiga University, Hikone, Japan","2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)","13 Jan 2025","2024","","","149","155","In recent years, large language models (LLMs) like GPT-3.5 and GPT-4, developed by OpenAI, have demonstrated impressive capabilities in tasks such as natural language understanding, code generation, and even passing professional exams. Trained on vast datasets, these models have shown high accuracy across various fields. One challenging area for LLMs is professional certification exams, which require specialized knowledge and understanding of complex regulations. The Real Estate Transaction Specialist Examination (RETSE) in Japan is a key certification for professionals managing real estate transactions. Passing the RETSE qualifies individuals to handle legal responsibilities like ensuring the legality of property deals. The exam covers civil law, tax law, and real estate regulations, making it a comprehensive test of legal knowledge. With a pass rate of around 15%, it is considered one of Japan's more difficult professional exams, making it a strong benchmark for evaluating LLM performance. This study evaluates the ability of GPT-3.5 and GPT-4 to answer RETSE questions, comparing their performance to official passing criteria and analyzing the effect of prompt engineering. While GPT-4 outperformed GPT-3.5, neither model met the passing standard. Prompt strategies such as “Consider customary laws” improved response accuracy, especially for complex legal questions. These findings suggest that while GPT models cannot fully replace human expertise in legal exams like RETSE, they offer potential as supplementary learning tools for verifying basic knowledge. To mitigate the risk of job displacement by AI, it is crucial for professionals to enhance their expertise in areas where LLMs struggle, such as recent legal changes and ethical judgment. This study highlights both the limitations and potential of GPT in legal contexts, providing insights into how AI can support learners and practitioners in real estate law.","2770-7466","979-8-3315-3313-7","10.1109/3ict64318.2024.10824669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824669","GPT;Legal Knowledge Assessment;Real Estate Transactions;Prompt Engineering;AI in Education","Ethics;Technological innovation;Accuracy;Law;Large language models;Regulation;Natural language processing;Prompt engineering;Certification;Standards","","","","20","IEEE","13 Jan 2025","","","IEEE","IEEE Conferences"
"From Application Security Verification Standard (ASVS) to Regulation Compliance: A Case Study in Financial Services Sector","V. Tan; C. Cheh; B. Chen","Singapore University of Technology and Design, Singapore, Singapore; Singapore University of Technology and Design, Singapore, Singapore; Singapore University of Technology and Design, Singapore, Singapore",2021 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),"14 Feb 2022","2021","","","69","76","The OWASP Application Security Verification Standard (ASVS) is a widely used web application development guideline regarding the technical security controls and secure development requirements. While software development teams refer to ASVS to secure their applications and development process, they also need to ensure the compliance of various security related regulations, including sector-specific ones. In this work, we study the synergy of these two activities, i.e., by following ASVS, how does a development team position their developed applications in meeting those regulation requirements. We take the highly regulated financial services sector as a case study. In particular, we look at two recent guidelines published by Monetary Authority of Singapore (MAS) - the Technology Risk Management (TRM) guidelines and Notice 655 Cyber Hygiene. We developed a systematic approach to map ASVS to those two sector-specific regulations. Our results show that by adopting ASVS, a development team can achieve a high degree of regulatory compliance (38.6 % for the MAS TRM guidelines and 47.6% for the MAS Notice 655, respectively). That demonstrates the viability of using international standards (like ASVS) to support compliance with the two sector-specific regulations. In addition, our mapping approach can be useful for organizations to support their compliance efforts.","","978-1-6654-2603-9","10.1109/ISSREW53611.2021.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700387","application security;compliance;verification standard;financial service","Regulators;Standards organizations;Organizations;Transmission line measurements;Regulation;Software;Application security","","1","","20","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"Generative AI-Powered Assistant for Developers: Accelerate software development with Amazon Q Developer","B. Irani; R. Sonawane",NA; NA,Generative AI-Powered Assistant for Developers: Accelerate software development with Amazon Q Developer,"","2024","","","","","Leverage Amazon Q Developer to boost productivity and maximize efficiency by accelerating software development life cycle tasksKey FeaturesFirst book on the market to thoroughly explore all of Amazon Q Developer’s featuresGain an understanding of Amazon Q Developer's capabilities across the software development life cycle through real-world examplesBuild apps with Amazon Q Developer by auto-generating code in various languages within supported IDEsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany developers face the challenge of managing repetitive tasks and maintaining productivity. This book will help you tackle both these challenges with Amazon Q Developer, a generative AI-powered assistant designed to optimize coding and streamline workflows. This book takes you through the setup and customization of Amazon Q Developer, demonstrating how to leverage its capabilities for auto-code generation, code explanation, and transformation across multiple IDEs and programming languages. You'll learn to use Amazon Q Developer to enhance coding experiences, generate accurate code references, and ensure security by scanning for vulnerabilities. The book also shows you how to use Amazon Q Developer for AWS-related tasks, including solution building, applying architecture best practices, and troubleshooting errors. Each chapter provides practical insights and step-by-step guidance to help you fully integrate this powerful tool into your development process. You’ll get to grips with effortless code implementation, explanation, transformation, and documentation, helping you create applications faster and improve your development experience. By the end of this book, you’ll have mastered Amazon Q Developer to accelerate your software development lifecycle, improve code quality, and build applications faster and more efficiently.What you will learnUnderstand the importance of generative AI-powered assistants in developers' daily workEnable Amazon Q Developer for IDEs and with AWS services to leverage code suggestionsCustomize Amazon Q Developer to align with organizational coding standardsUtilize Amazon Q Developer for code explanation, transformation, and feature developmentUnderstand code references and scan for code security issues using Amazon Q DeveloperAccelerate building solutions and troubleshooting errors on AWSWho this book is forThis book is for coders, software developers, application builders, data engineers, and technical resources using AWS services looking to leverage Amazon Q Developer's features to enhance productivity and accelerate business outcomes. Basic coding skills are needed to understand the concepts covered in this book.","","9781835081204","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10769302.pdf&bkn=10769301&pdfType=book","","","","","","","","27 Nov 2024","","","Packt Publishing","Packt Publishing eBooks"
"Analysis of Student-LLM Interaction in a Software Engineering Project","A. Naman; R. Shariffdeen; G. Wang; S. Rasnayaka; G. N. Iyer","School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","112","119","Large Language Models (LLMs) are becoming increasingly competent across various domains, educators are showing a growing interest in integrating these LLMs into the learning process. Especially in software engineering, LLMs have demonstrated qualitatively better capabilities in code summarization, code generation, and debugging. Despite various research on LLMs for software engineering tasks in practice, limited research captures the benefits of LLMs for pedagogical advancements and their impact on the student learning process. To this extent, we analyze 126 undergraduate students’ interaction with an AI assistant during a 13-week semester to understand the benefits of AI for software engineering learning. We analyze the conversations, code generated, code utilized, and the human intervention levels to integrate the code into the code base.Our findings suggest that students prefer ChatGPT over CoPilot. Our analysis also finds that ChatGPT generates responses with lower computational complexity compared to CoPilot. Furthermore, conversational-based interaction helps improve the quality of the code generated compared to auto-generated code. Early adoption of LLMs in software engineering is crucial to remain competitive in the rapidly developing landscape. Hence, the next generation of software engineers must acquire the necessary skills to interact with AI to improve productivity.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028221","LLM for Code Generation;LLM for Learning;AI for Software Engineering;Software Engineering Education","Productivity;Codes;Large language models;Education;Oral communication;Debugging;Chatbots;Software;Next generation networking;Software engineering","","1","","24","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"Using Instruction-Following LLM Hidden States as Conditioning for Video Diffusion Model","R. H. Bhushan; A. G. K; P. Ambiga; S. S. Malgikar; B. P. V.R.","Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India","2025 17th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","4 Aug 2025","2025","","","1","10","Video generation has applications in several fields. With the advent of Generative AI, we see extensive research being conducted on video generation using AI. Through this project, we experiment the usage of LLM Hidden states as conditioning to train a Video Latent Diffusion Model to study their ability of passing richer semantic information about the video samples. We performed a comparative study of context retention abilities of LLMs in case of embeddings and hidden states separately. We create a pipeline with three major components - the LLM, a custom Bridge Network and the Diffusion UNet. We conduct our study using two different datasets - the Captioned Moving MNIST and a subset of the Sakuga-42M dataset. We conclude by evaluating our model variants on standard benchmarks and metrics, and state our findings, which could serve as ground for future work.","2688-0253","979-8-3315-3352-6","10.1109/ECAI65401.2025.11095434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11095434","Video Generation;Generative AI;Artificial Intelligence;Large Language Model;Multimodal;Prompt Engineering;Diffusion;Variational Autoencoder;Hidden States;FVD;CLIP Score;UNet;Latent","Visualization;PSNR;Large language models;Semantics;Refining;Pipelines;Diffusion models;Prompt engineering;Standards;Videos","","","","38","IEEE","4 Aug 2025","","","IEEE","IEEE Conferences"
"22 The Intersection of Artificial Intelligence and Operating System Design","A. Banafa",NA,Artificial Intelligence in Action: Real-World Applications and Innovations,"","2025","","","147","154","This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.","","9788770046190","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948931.pdf&bkn=10948919&pdfType=chapter","","","","","","","","3 Apr 2025","","","River Publishers","River eBook Chapters"
"Intrusion Detection in the Internet of Vehicles Using Transformer Models","M. Alauthman; A. Mashaleh; N. Aslam; A. Aldweesh; A. Almomani","Department of Information Security, University of Petra, Jordan; Al-Balqa Applied University, Al-Salt, Jordan; Department of Computer and Information Sciences, Northumbria University, U.K.; College of computing and IT, Shaqra University, Shaqra, Saudi Arabia; Department of Computer Information Science, Higher Colleges of Technology, UAE",2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA),"2 Jun 2025","2025","","","1","6","The proliferation of Internet of Vehicles (IoV) systems has introduced critical cybersecurity vulnerabilities in connected vehicle infrastructures. This paper presents a novel application of Transformer architecture for intrusion detection in vehicular Controller Area Network (CAN) buses, specifically addressing Denial-of-Service and Spoofing attacks. We introduce a Transformer-based model optimized for CAN frame sequence analysis, evaluated on the CICIoV2024 dataset comprising real-world attack scenarios from a 2019 Ford vehicle. Our experimental results demonstrate superior detection capabilities compared to classical machine learning approaches and recurrent neural architectures, achieving 98.4% accuracy and 98.5% F1-score. The proposed architecture's self-attention mechanism effectively captures temporal dependencies in CAN frame sequences while maintaining computational efficiency (2.3ms inference time) suitable for automotive-grade ECUs. This research advances the state-of-the-art in IoV security through enhanced detection accuracy and practical deployment considerations for resource-constrained vehicular environments.","","979-8-3315-2365-7","10.1109/ICCIAA65327.2025.11013054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11013054","Internet of Vehicles;Intrusion Detection Systems;Transformer;CAN Bus Security;Machine Learning;CICIo V2024 Dataset","Analytical models;Adaptation models;Computational modeling;Intrusion detection;Computer architecture;Machine learning;Transformers;Controller area networks;Internet of Vehicles;Automotive engineering","","","","21","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers","S. Fakhoury; M. Kuppe; S. K. Lahiri; T. Ramananandro; N. Swamy","Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2535","2547","Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages. Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted. However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use. In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle. Symbolic test generation also helps in distinguishing multiple plausible solutions. Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C. We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified $C$ code at a non-trivial scale. A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated. symbolic analysis is tractable.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029881","Code Generation;Agentic AI Systems;Trust-worthy AI programming","Codes;Three-dimensional displays;Synthesizers;Natural languages;Transforms;Programming;Formal specifications;Test pattern generators;Artificial intelligence;Standards","","1","","38","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Advanced Learning Technologies for Intelligent Transportation Systems: Prospects and Challenges","R. A. Khalil; Z. Safelnasr; N. Yemane; M. Kedir; A. Shafiqurrahman; N. SAEED","Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE; Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE; Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE; Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE; Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE; Department of Electrical and Communication Engineering, United Arab Emirates University (UAEU), Al-Ain, UAE",IEEE Open Journal of Vehicular Technology,"25 Mar 2024","2024","5","","397","427","Intelligent Transportation Systems (ITS) operate within a highly intricate and dynamic environment characterized by complex spatial and temporal dynamics at various scales, further compounded by fluctuating conditions influenced by external factors such as social events, holidays, and weather. Navigating the intricacies of modeling the intricate interaction among these elements, creating universal representations, and employing them to address transportation issues. Yet, these intricacies comprise just one facet of the multifaceted trials confronting contemporary ITS. This paper offers an all-encompassing survey exploring Deep learning (DL) utilization in ITS, primarily focusing on practitioners' methodologies to address these multifaceted challenges. The emphasis lies on the architectural and problem-specific factors that guide the formulation of innovative solutions. In addition to shedding light on the state-of-the-art DL algorithms, we also explore potential applications of DL and large language models (LLMs) in ITS, including traffic flow prediction, vehicle detection and classification, road condition monitoring, traffic sign recognition, and autonomous vehicles. Besides, we identify several future challenges and research directions that can push the boundaries of ITS, including the critical aspects, including transfer learning, hybrid models, privacy and security, and ultra-reliable low-latency communication. Our aim for this survey is to bridge the gap between the burgeoning DL and transportation communities. By doing so, we aim to facilitate a deeper comprehension of the challenges and possibilities within this field. We hope that this effort will inspire further exploration of fresh perspectives and issues, which, in turn, will play a pivotal role in shaping the future of transportation systems.","2644-1330","","10.1109/OJVT.2024.3369691","Summer Undergraduate Research Experience; United Arab Emirates University(grant numbers:G00004359); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444919","Intelligent transportation systems;autonomous vehicles;deep learning;large language models;explainable AI;traffic flow prediction","Transportation;Surveys;Intelligent transportation systems;Autonomous vehicles;Vehicle dynamics;Large language models;Traffic control;Deep learning","","42","","263","CCBYNCND","26 Feb 2024","","","IEEE","IEEE Journals"
"Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud- Based AI Models","G. Uzor; H. Al-Qudah; Y. Ineza; A. Serwadda","Department of Computer Science, Texas Tech University, Lubbock, Texas; Department of Computer Science, Texas Tech University, Lubbock, Texas; Department of Computer Science, Texas Tech University, Lubbock, Texas; Department of Computer Science, Texas Tech University, Lubbock, Texas",2025 19th International Conference on Semantic Computing (ICSC),"19 Jun 2025","2025","","","235","242","The interactive nature of Large Language Models (LLMs), which closely track user data and context, has prompted users to share personal and private information in unprecedented ways. Even when users opt out of allowing their data to be used for training, these privacy settings offer limited protection when LLM providers operate in jurisdictions with weak privacy laws, invasive government surveillance, or poor data security practices. In such cases, the risk of sensitive information, including Personally Identifiable Information (PII), being mishandled or exposed remains high. To address this, we propose the concept of an “LLM gatekeeper”, a lightweight, locally run model that filters out sensitive information from user queries before they are sent to the potentially untrustworthy, though highly capable, cloud-based LLM. Through experiments with human subjects, we demonstrate that this dual-model approach introduces minimal overhead while significantly enhancing user privacy, without compromising the quality of LLM responses.","2472-9671","979-8-3315-2426-5","10.1109/ICSC64641.2025.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11036311","Large Language Models;Generative Artificial Intelligence;Personal Identifiable Information;Natural Language Processing","Training;Data privacy;Privacy;Large language models;Surveillance;Semantics;Oral communication;Logic gates;Protection;Identification of persons","","","","18","IEEE","19 Jun 2025","","","IEEE","IEEE Conferences"
"Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios","Z. Chen; L. Jiang","Centre for Research on Intelligent Software Engineering, School of Computing and Information Systems, Singapore Management University; Centre for Research on Intelligent Software Engineering, School of Computing and Information Systems, Singapore Management University","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","657","668","In recent years, AI-based software engineering has progressed from pre-trained models to advanced agentic workflows, with Software Development Agents representing the next major leap. These agents, capable of reasoning, planning, and interacting with external environments, offer promising solutions to complex software engineering tasks. However, while much research has evaluated code generated by large language models (LLMs), comprehensive studies on agent-generated patches, particularly in real-world settings, are lacking. This study addresses that gap by evaluating 4,892 patches from 10 top-ranked agents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on their impact on code quality. Our analysis shows no single agent dominated, with 170 issues unresolved, indicating room for improvement. Even for patches that passed unit tests and resolved issues, agents made different file and function modifications compared to the gold patches from repository developers, revealing limitations in the benchmark's test case coverage. Most agents maintained code reliability and security, avoiding new bugs or vulnerabilities; while some agents increased code complexity, many reduced code duplication and minimized code smells. Finally, agents performed better on simpler codebases, suggesting that breaking complex tasks into smaller sub-tasks could improve effectiveness. This study provides the first comprehensive evaluation of agent-generated patches on real-world GitHub issues, offering insights to advance AI-driven software development.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00068","Ministry of Education, Singapore(grant numbers:MOET32020-0004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992485","Software Development Agents;Patch Generation;Large Language Models;Code Quality;GitHub Issues","Gold;Codes;Large language models;Computer bugs;Focusing;Complexity theory;Security;Reliability;Software development management;Software engineering","","","","50","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"CDAP-PFMs: A Cross-Domain Authentication Protocol for Edge Inference-Based PFMs","X. Meng; Y. Xiao; W. Liang; W. Xu; Z. Xu; X. Li; N. Xiong","College of Computer Science and Engineering, Hunan University of Science and Technology Sanya Research Institute, and the Hunan Key Laboratory for Service Computing and Novel Software Technology, Hunan University of Science and Technology, Xiangtan, China; College of Computer Science and Engineering, Hunan University of Science and Technology Sanya Research Institute, and the Hunan Key Laboratory for Service Computing and Novel Software Technology, Hunan University of Science and Technology, Xiangtan, China; College of Computer Science and Engineering, Hunan University of Science and Technology Sanya Research Institute, and the Hunan Key Laboratory for Service Computing and Novel Software Technology, Hunan University of Science and Technology, Xiangtan, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; Computer and Communication Engineer Institute, Changsha University of Science and Technology, Changsha, China; School of Computer Science and Engineering, Institute for Cyber Security, University of Electronic Science and Technology of China, Chengdu, China; College of Computer Science and Engineering, Hunan University of Science and Technology Sanya Research Institute, and the Hunan Key Laboratory for Service Computing and Novel Software Technology, Hunan University of Science and Technology, Xiangtan, China",IEEE Internet of Things Journal,"8 Aug 2025","2025","12","16","33468","33481","The pretrained foundation models (PFMs) based on edge inference reduce communication latency from the cloud server to the user mobile devices (MDs), and support different edge nodes to provide adaptive artificial intelligence (AI) services to the users. The complex network environment has attracted widespread attention to the communication security of cross-domain AI services. Authentication protocol is an important way to ensure the communication security among entities. However, existing cross-domain authentication protocols have not taken into account the communication delay caused by complex cryptographic operations on the MDs. To address these issues, a cross-domain inference authentication (CDIA) protocol for PFMs is proposed in this article. The complex cryptographic operations are executed collaboratively by the servers from different domains to realizes decentralized authentication. Besides, the correctness and security of CDIA protocol is proved by formal analysis. The security of CDIA protocol is verified by using the automated validation of Internet security protocols and applications (AVISPAs) tool. The security of negotiated group key is proved by Real-or-Random (RoR) model. Finally, the performance of CDIA protocol is evaluated to show its efficiency. Compared with related protocols, the computing cost and communication overhead of CDIA protocol are at least 52.38% and 11.9%, respectively.","2327-4662","","10.1109/JIOT.2025.3578252","Joint Key Project of National Natural Science Foundation of China(grant numbers:U2468205); National Natural Science Foundation of China(grant numbers:62402064,62472168,61976087); Science and Technology Project of the Department of Communications of Hunan Provincial(grant numbers:202101); Key Research and Development Program of Hunan Province(grant numbers:2022GK2015); Natural Science Foundation of Hunan Province(grant numbers:2021JJ30141,2024JJ6066); Research Foundation of Education Bureau of Hunan Province of China(grant numbers:23B0288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029052","Authentication protocol;edge inference;Internet of Things (IoT);pretrained foundation models (PFMs);security analysis","Authentication;Protocols;Security;Servers;Mobile handsets;Industrial Internet of Things;Frequency modulation;Computer architecture;Edge computing;Cloud computing","","","","39","IEEE","9 Jun 2025","","","IEEE","IEEE Journals"
"Toward LLM-driven Adaptive Policy Orchestration for Host-based Intrusion Detection Systems in IoT Environments","B. Karunanayake; I. Khalil; X. Yi; K. -Y. Lam","School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; College of Computing and Data Science, Nanyang Technological University, 50 Nanyang Ave, Singapore",IEEE Network,"","2025","PP","99","1","1","Large language models (LLMs) have emerged as powerful tools for text generation, demonstrating remarkable capabilities in reasoning, function calling, and generating structured outputs. When equipped with access to functions and memory, they can be transformed into interactive and intelligent LLM agents. However, the applicability of such LLM agents in real-time Internet of things (IoT) applications remains constrained by resource limitations. One critical area is IoT security, particularly in intrusion detection systems (IDS), where LLM agents hold the potential to enhance the effectiveness of existing solutions significantly. Current IDS approaches in IoT, predominantly reliant on signature-based or anomaly-based techniques, struggle to adapt to the dynamic and evolving nature of modern threats. Recent studies reveal a prevalence of false positives in existing IDS, complicating the alert reasoning and decision-making process. To address these limitations, this research proposes a novel adaptive IDS policy orchestration framework that leverages LLM agents in conjunction with advanced prompting strategies. Our framework generates adaptive IDS policies, enhancing precision with intelligent alert reasoning. It enables efficient policy deployment without needing LLM inference during policy execution. We validate the effectiveness of the proposed framework through experiments conducted on five real-world IDS datasets. The results demonstrate that the framework achieves over 97% precision on three IoT network datasets and over 80% on two more challenging datasets. Notably, our framework reduces the detection latency by 50% compared to traditional machine learning (ML) models with the requirement of fewer than 10k total completion tokens, offering a highly cost-effective, efficient, and scalable solution. These findings establish a new benchmark for IoT-IDS frameworks utilising LLM agents, underscoring the potential for further advancements in this domain.","1558-156X","","10.1109/MNET.2025.3579532","Australian Research Council(grant numbers:DP220100215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11036113","Internet of Things;Intrusion Detection Systems;Large Language Models;LLM Agents;Policy Generation","Internet of Things;Cognition;Intrusion detection;Telecommunication traffic;Image edge detection;Training;Threat modeling;Reliability;Data mining;Codes","","","","","IEEE","13 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Research on TTP Data Augmentation Methods Based on the ATT&CK Framework","J. Zhang; X. Xue; J. Zhang; T. Qu; L. Li; R. Xi; H. Zhu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",2025 28th International Conference on Computer Supported Cooperative Work in Design (CSCWD),"23 Jun 2025","2025","","","1722","1727","As cyber threats escalate, rapid identification and response to attacks are increasingly vital. Cyber Threat Intelli-gence (CTI) is crucial for understanding the threat landscape, and standardized attack frameworks are essential for effective anal-ysis. The MITRE ATT &CK framework has gained widespread adoption for its systematic description of Tactics, Techniques, and Procedures (TTP), aiding security teams in tracking at-tack patterns. However, manual classification of TTP is time-consuming and costly, hindering response efficiency. Although artificial intelligence has advanced automated TTP classification, accuracy still needs improvement due to the scarcity of labeled data, resulting in small and imbalanced datasets. This study introduces a novel TTP data augmentation method to enhance classification accuracy through synthetic data gen-eration. We construct a dataset of 19,716 sentences from the ATT&CK knowledge base and real-world threat reports, Ini-tially, we leverage large language models (LLMs) combined with prompt techniques to generate high-quality synthetic data, followed by semantic filtering and dynamic sampling strategies to further enhance data quality and improve class balance. Experimental results show an average $\mathbf{F}_{1}$ score increase of 16.95 % across various classification models, significantly enhancing TTP classification performance.","2768-1904","979-8-3315-1305-4","10.1109/CSCWD64889.2025.11033315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11033315","Cyber Threat Intelligence;MITRE ATT&CK;TTP;Large Language Models;Data Synthesis","Accuracy;Systematics;Filtering;Large language models;Data integrity;Semantics;Knowledge based systems;Data augmentation;Cyber threat intelligence;Synthetic data","","","","25","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"LLM-Based Quiz Generation for Assessments in Learning Management System","S. Shirkar; D. Langhi; S. Agrawal","Department of Information Technology, Vidyalankar Institute of Technology, Mumbai, India; Department of Information Technology, Vidyalankar Institute of Technology, Mumbai, India; Department of Information Technology, Vidyalankar Institute of Technology",2025 Second International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),"29 Jul 2025","2025","","","464","470","An advanced AI system which employs Large Language Models (LLMs) to create automatic assessment questions through automation features, has been introduced in this study. The system combines MetaAI's llama- 3 model Groq API together with LangChain and structured prompting methods to produce quizzes which remain accurate to original contexts and logical sequences. The system delivers better efficiency alongside reduced manual work through the creation of quality questions across different formats which contain multiple-choice items, true-false formats and open-ended questions. The system maintains data security through environment variables in its API handling process to protect against unauthorized access. Smart testing through AI-based quiz creation methods produces customized assessments which use dynamic test content according to the established study.","","979-8-3315-0172-3","10.1109/ICC-ROBINS64345.2025.11086273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11086273","Large language models;llama-3;Structured prompting;AI-driven quiz generation","Vocabulary;Large language models;Manuals;Production;Real-time systems;Natural language processing;Reliability;Prompt engineering;Context modeling;Testing","","","","21","IEEE","29 Jul 2025","","","IEEE","IEEE Conferences"
"Securing Inverter-Based Resources via Knowledge-Driven Threat Modeling, Analysis, and Mitigation","R. L. Neupane; V. Pusapati; L. S. Edara; X. Cheng; K. Neupane; H. Chintapatla; R. Mitra; M. Korkali; H. Suk Na; S. Srinivas; P. Calyam","University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; Southeast Missouri State University, Cape Girardeau, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA; University of Missouri - Columbia, Columbia, USA",NOMS 2025-2025 IEEE Network Operations and Management Symposium,"15 Jul 2025","2025","","","1","9","Inverter-based Resources (IBRs) present unique cybersecurity challenges due to their digital control systems and connection to the electric grid. They rely on digital communications and control systems, making them vulnerable to cyberattacks. These attacks can disrupt grid operations and stability, compromise data, or cause physical damage to equipment. To address these challenges, it is essential to establish robust cybersecurity measures that meet and exceed existing industry standards. In this paper, we describe a comprehensive strategy to bolster the cybersecurity of IBRs through cutting-edge applications and technologies via a cybersecurity framework called “CIBR-Fort”, a knowledge-driven, interoperable, scalable, and manageable framework for modeling, analysis, and mitigation of cyber threats disrupting different components of IBR systems. Our knowledge-driven analysis consists of a fusion of knowledge graphs (KGs) in cybersecurity and the electric grid, achieved through link prediction leveraging Large Language Models (LLMs) and cosine similarity, attributed towards informed decision-making for threat mitigation. The evaluation results show how we can automate LLM-driven link prediction based on the fusion of two distantly separated ontologies, generating a dataset that can be used for scaling via graph learning that can be utilized for further security analyses of IBR systems. In addition, we show our knowledge-driven threat analysis can predict different attacks with 91.88% maximum accuracy. Lastly, we show how we can achieve real-time end-to-end threat mitigation with an average of 40 ms per traffic flow.","2374-9709","979-8-3315-3163-8","10.1109/NOMS57970.2025.11073642","National Security Agency(grant numbers:H98230-21-1-0260); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11073642","knowledge graph;cybersecurity;inverter-based resources;large language models;retrieval augmented generation","Threat modeling;Prevention and mitigation;Large language models;Stability criteria;Retrieval augmented generation;Knowledge graphs;Power system stability;Real-time systems;Computer security;Standards","","","","33","IEEE","15 Jul 2025","","","IEEE","IEEE Conferences"
"Can Digital Intelligence and Cyber-Physical-Social Systems Achieve Global Food Security and Sustainability?","Y. Wang; M. Kang; Y. Liu; J. Li; K. Xue; X. Wang; J. Du; Y. Tian; Q. Ni; F. -Y. Wang","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China; School of Grassland Science, Beijing Forestry University, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China; Faculty of Innovation Engineering, Macau University of Science and Technology, Macao, China",IEEE/CAA Journal of Automatica Sinica,"28 Sep 2023","2023","10","11","2070","2080","Plants sequester carbon through photosynthesis and provide primary productivity for the ecosystem. However, they also simultaneously consume water through transpiration, leading to a carbon-water balance relationship. Agricultural production can be regarded as a form of carbon sequestration behavior. From the perspective of the natural-social-economic complex ecosystem, excessive water usage in food production will aggravate regional water pressure for both domestic and industrial purposes. Hence, achieving a harmonious equilibrium between carbon and water resources during the food production process is a key scientific challenge for ensuring food security and sustainability. Digital intelligence (DI) and cyber-physical-social systems (CPSS) are emerging as the new research paradigms that are causing a substantial shift in the conventional thinking and methodologies across various scientific fields, including ecological science and sustainability studies. This paper outlines our recent efforts in using advanced technologies such as big data, artificial intelligence (AI), digital twins, metaverses, and parallel intelligence to model, analyze, and manage the intricate dynamics and equilibrium among plants, carbon, and water in arid and semiarid ecosystems. It introduces the concept of the carbon-water balance and explores its management at three levels: the individual plant level, the community level, and the natural-social-economic complex ecosystem level. Additionally, we elucidate the significance of agricultural foundation models as fundamental technologies within this context. A case analysis of water usage shows that, given the limited availability of water resources in the context of the carbon-water balance, regional collaboration and optimized allocation have the potential to enhance the utilization efficiency of water resources in the river basin. A suggested approach is to consider the river basin as a unified entity and coordinate the relationship between the upstream, midstream and downstream areas. Furthermore, establishing mechanisms for water resource transfer and trade among different industries can be instrumental in maximizing the benefits derived from water resources. Finally, we envisage a future of agriculture characterized by the integration of digital, robotic and biological farming techniques. This vision aims to incorporate small tasks, big models, and deep intelligence into the regular ecological practices of intelligent agriculture.","2329-9274","","10.1109/JAS.2023.123951","National Key Research and Development Program of China(grant numbers:2021ZDO113704); National Natural Science Foundation of China(grant numbers:62076239,42041005,62103411); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266658","Carbon-water balance;decision-support;digital intelligence (DI);foundation models;planning","Biological system modeling;Robot kinematics;Ecosystems;Food security;Rivers;Digital intelligence;Sustainable development","","24","","68","","28 Sep 2023","","","IEEE","IEEE Journals"
"LLMSecurityTester: A Tool for Detection of Vulnerabilities in LLM-based Chatbots","V. Lavrentiev; D. Levshun","Laboratory of Computer Security Problems SPC RAS, St. Petersburg, Russia; Laboratory of Computer Security Problems SPC RAS, St. Petersburg, Russia","2025 33rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)","29 Apr 2025","2025","","","608","615","The development of generative algorithms in the last few years, including large language models, imposes increasingly high requirements in the field of data security and protection. Vulnerabilities of information systems related to the generation of inappropriate information are becoming an increasingly serious challenge. They can lead to negative consequences, such as misinformation, creation of fake news or disclosure of sensitive data. This paper presents the architecture of a system designed to identify vulnerabilities in large language models and proposes an approach for vulnerability detection based on prompt engineering. The idea of the approach lies in making certain requests to large language models, the execution of which can help to use the algorithm for illegal purposes. The authors provide a detailed description of the system prototype for automating vulnerability detection, named LLMSecurityTester. Additionally, preliminary results of experiments on various models and datasets are presented, demonstrating the applicability of the solution.","2377-5750","979-8-3315-2493-7","10.1109/PDP66500.2025.00091","Russian Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974819","chatbot;vulnerability detection;prompt engineering;large language model;test automation","Automation;Large language models;Data security;Prototypes;Chatbots;Reflection;Prompt engineering;Fake news;Protection;Information systems","","","","28","IEEE","29 Apr 2025","","","IEEE","IEEE Conferences"
"AI Tools Building Cybercrime & Defenses","A. S. Kamruzzaman; K. Thakur; S. Mahbub","Dept. of Business & Economics, York College/CUNY, Jamaica, NY, USA; Department of Professional Security Studies, New Jersey City University, Jersey City, NJ, USA; Zicklin School of Business, CUNY Baruch College, New York, United States","2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","20 Mar 2024","2024","","","1","5","Developments of Artificial Intelligence (AI) have reformed the future of Cybersecurity, aiding both offensive and defensive security. While AI assists supplement security and bridge the gap on required Information Security talent, Cybercriminals also take advantage of the lack of antioffensive controls on Large Language Models (LLM) like ChatGPT. This paper dives into the most common tactics the Cybercriminals implemented to use AI for malicious purposes. Additionally, the paper gives an insight of how AI being used as a weapon to make harmful malwares to victims. To prevent harmful hacks, organizations and employees need to be aware and implement prevention tactics using Machine Learning a sub division of AI.","","979-8-3503-9452-8","10.1109/ACDSA59508.2024.10467401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467401","Artificial Intelligence;Machine Learning;Large Language Models;Cybercrime;Cybercriminals;Malware;Ransomware","Computer hacking;Weapons;Information security;Organizations;Machine learning;Production;Malware","","","","22","IEEE","20 Mar 2024","","","IEEE","IEEE Conferences"
"Linking Code and Documentation Churn: Preliminary Analysis","A. Hovhannisyan; Y. Fan; G. Rodíguez-Pérez; R. G. Kula","NAIST, Japan; NAIST, Japan; UBC, Canada; Osaka University, Japan",2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW),"3 Dec 2024","2024","","","107","108","Code churn refers to the measure of the amount of code added, modified, or deleted in a project and is often used to assess codebase stability and maintainability. Program comprehension or how understandable the changes are, is equally important for maintainability. Documentation is crucial for knowledge transfer, especially when new maintainers take over abandoned code. We emphasize the need for corresponding documentation updates, as this reflects project health and trustworthiness as a third-party library. Therefore, we argue that every code change should prompt a documentation update (defined as documentation churn). Linking code churn changes with documentation updates is important for project sustainability, as it facilitates knowledge transfer and reduces the effort required for program comprehension. This study investigates the synchrony between code churn and documentation updates in three GitHub open-source projects. We will use qualitative analysis and repository mining to examine the alignment and correlation of code churn and documentation updates over time. We want to identify which code changes are likely synchronized with documentation and to what extent documentation can be auto-generated. Preliminary results indicate varying degrees of synchrony across projects, highlighting the importance of integrated concurrent documentation practices and providing insights into how recent technologies like AI, in the form of Large Language Models (i.e., LLMs), could be leveraged to keep code and documentation churn in sync. The novelty of this study lies in demonstrating how synchronizing code changes with documentation updates can improve the development lifecycle by enhancing diversity and efficiency.","2994-810X","979-8-3503-6704-1","10.1109/ISSREW63542.2024.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771325","code churn;software maintenance;knowledge transfer;automated documentation generation","Codes;Large language models;Documentation;Stability analysis;Libraries;Software reliability;Synchronization;Sustainable development;Knowledge transfer;Software development management","","","","10","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Identifying Design Pattern for Agent Based Production System Control","A. Lüder; J. Zawisza; A. C. S. Luis; M. Seitz; B. Vogel-Heuser","Otto-v.-Guericke University, Faculty of Mechanlcal Engineering, Magdeburg, Germany; Otto-v.-Guericke University, Faculty of Mechanlcal Engineering, Magdeburg, Germany; Department of Mechanical Engineering, Technical University of Munich, Garching, Germany; Department of Mechanical Engineering, Technical University of Munich, Garching, Germany; Department of Mechanical Engineering, Technical University of Munich, Garching, Germany",IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,"30 Dec 2018","2018","","","2896","2901","Multi-Agent Systems (MAS) are an implementation paradigm frequently discussed to be used within control system design for flexible production systems. Over the years, on the one hand different kind of MAS-based control architectures, and on the other hand, different agent system design methodologies have been developed with different intentions and focus e.g. Agent-Oriented Software Engineering (AOSE). Nevertheless, only a few practical applications of MAS in control can be observed. In fact, a MAS developers' problem is the missed availability of design support for agent-based engineering, to control complex production systems. Therefore, this paper intends to highlight a path towards such a MAS development support exploiting design patterns.","2577-1647","978-1-5090-6684-1","10.1109/IECON.2018.8591336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8591336","Production system control;multi-agent system;design pattern","Production systems;Control systems;Computer architecture;Design methodology;Process control;Unified modeling language;Multi-agent systems","","1","","40","IEEE","30 Dec 2018","","","IEEE","IEEE Conferences"
"Talk to Your Brain: Artificial Personalized Intelligence for Emotionally Adaptive AI Interactions","S. Varma; S. Shivam; S. Natarajan; B. Ray; B. Kumar; O. Dabral","ZS Associates, Pune, India; ZS Associates, Pune, India; Rockwell Automation, Pune, India; ZS Associates, Pune, India; Manipal University, Jaipur, India; Manipal University, Jaipur, India",2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI),"11 Dec 2024","2024","","","1","6","The process of =making an Artificial intelligent system to replicating human emotional understanding and adapt its responses contextually by tailoring its responses based on individual cognitive and emotional states is called Artificial Personalized Intelligence (API). In this paper, we present a finetuned emotionally adaptive AI pipeline capable of generating personalized, human-like responses. Using a custom dataset based on six universal emotions—Sadness, Happiness, Fear, Anger, Surprise, and Disgust—collected from interviews with 30 participants, we have explored two approaches: fine-tuning a LLaMA-3 8B model with the Low-Rank Adapter (LoRA) technique and employing a Retrieval-Augmented Generation (RAG) agent-based framework over the same LLaMA-3 8B model. Emotion classification, prompt engineering, and model fine-tuning were used over the dataset to capture emotional and personalized subtleties in the responses based on different individuals. We conducted a comprehensive analysis, evaluating performance across candidates, emotions, and overall performance, using metrics such as Mean Squared Error (MSE) and Pearson Correlation Coefficient to measure the difference among the outputs from both pipelines and actual human emotional responses. Our results show that prompt engineering combined with LoRA-based fine-tuning significantly enhances the ability to engage in emotionally intelligent and personalized conversations, whereas the RAG-based approach underperformed due to the pretrained LLAMA-3 model’s restrictive neutral and adversarial training. This highlights that fine-tuned LLMs are effective in replicating human emotions for personalized intelligence.","","979-8-3503-7687-6","10.1109/CVMI61877.2024.10781964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781964","Artificial Personalized Intelligence;Generative Artificial Intelligence;Large Language Models;Cognitive Intelligence;Retrieval-Augmented Generation;Fine-tuning","Training;Adaptation models;Computational modeling;Pipelines;Measurement uncertainty;Brain modeling;Prompt engineering;Interviews;Intelligent systems;Machine intelligence","","1","","10","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"LLM-Driven SAT Impact on Phishing Defense: A Cross-Sectional Analysis","H. İŞ","Department of Computer Engineering, Batman University, Batman, Turkey",2024 12th International Symposium on Digital Forensics and Security (ISDFS),"15 May 2024","2024","","","1","5","Amidst the growing sophistication of phishing threats that exploit human vulnerabilities, this study investigates the effectiveness of Security Awareness Training (SAT) enhanced by Large Language Models (LLMs). Targeting a diverse group of 1,270 participants, including academicians, officers, and students, it aims to evaluate whether LLM-driven SAT can strengthen phishing defenses and cultivate a more resilient digital environment. Initial assessments revealed a baseline Phish Prone Percentage (PPP) of 18.3%, indicating a pronounced vulnerability across participant groups. The deployment of an LLM-enhanced SAT program, characterized by its adaptive and interactive training modules, led to a significant post-training reduction in PPP to 6.3%. This outcome demonstrates the program's success in mitigating phishing risks and underscores the necessity of evolving SAT strategies to combat the dynamic nature of phishing attacks. The study's findings, illustrating a substantial improvement in phishing defense capabilities through LLM-integrated SAT, advocate for the integration of advanced technologies in cybersecurity education. By effectively lowering phishing vulnerability from 18.3% to 6.3%, this research highlights the critical role of innovative training methodologies in enhancing digital security across varied academic and professional landscapes.","2768-1831","979-8-3503-3036-6","10.1109/ISDFS60797.2024.10527274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527274","Phishing;Large Language Model(LLM);Cyber Security;Security Awareness Training(SAT);Artificial Intelligence","Training;Phishing;Digital forensics;Organizations;Testing","","4","","12","IEEE","15 May 2024","","","IEEE","IEEE Conferences"
"Unit Test Generation Multi-Agent AI System for Enhancing Software Documentation and Code Coverage","D. Stojanović; B. Pavković; N. Četić; M. Krunić; L. Vidaković","Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; LabSoft, Novi Sad, Serbia; LabSoft, Novi Sad, Serbia",2024 32nd Telecommunications Forum (TELFOR),"2 Jan 2025","2024","","","1","4","Software development necessitates a robust testing plan though test development can be laborious and nonappealing task. We explore the utilization of the application artificial intelligence agents for generating and executing unit tests, enhancing the “Mostly Basic Python Problems” dataset. We employ behavior-driven development within a three-agent system to generate user stories and unit tests. Empirical results indicate improvements in branch coverage, illustrating the effective utilization of large language models in software testing and development processes.","2994-5828","979-8-3503-9106-0","10.1109/TELFOR63250.2024.10819096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819096","AI agents;unit test generation;code generation;SW test automation;transformers;BDD","Software testing;Codes;Large language models;Documentation;Telecommunications;Test pattern generators;Software development management;Python","","","","7","IEEE","2 Jan 2025","","","IEEE","IEEE Conferences"
"Conversational AI Threat Identification at Industrial Internet of Things","B. Smail; Meenakshi; J. L. Arias&#x2010;Gonz&#xe1;les; M. Jawarneh; P. Venkata Hari Prasad; H. Pallathadka","Research Center on Scientific and Technical Information (DTISI), CERIST, Algiers, Algeria; Apeejay STYA University Sohna Haryana, India; University of British Columbia, Lima, Peru; Faculty of Computing Sciences, Gulf College, Al&#x2010;Khuwair, Oman; Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Manipur International University, Manipur, India",Conversational Artificial Intelligence,"","2024","","","513","532","Summary <p>Industrial internet of things (IIoT) is bringing about the next industrial revolution, and its use in business has made it easier for people to do business from anywhere and at any time. Through machine automation and robotics, IIOT has helped the manufacturing industry. Also, most data processing and computing services in the internet of things (IoT) ecosystem are done in the cloud. Because of this, the service‐oriented architecture also uses the cloud to virtualize its usefulness s ervice‐oriented architecture (SOA). So, IoT helps us get to Industry 4.0, which is the next generation of business and manufacturing. Also, when IoT and cloud computing are used with existing industry infrastructure, they help the robust data system make new products and get good results in real time. Along with this growth, security problems are also getting worse and worse. Researchers are paying attention to how quickly cyberattacks on IoT are growing. In the IoT ecosystem, security is much more complex and needs to cover many different areas, such as data security, the availability of data and services, access control, and the security of physical devices. IoT ecosystems can be set up as either open or closed ecosystems, so more research is needed for security deployments in both areas. Artificial intelligence and machine learning are important for attack detection and providing security solutions in IIoT. This manuscript provides an investigation of various security issues and security attacks in IIoT. A detailed study is conducted to identify all existing attacks in IIoT. These attacks are categorized on the basis of layers of IoT architecture.</p>","","9781394200795","10.1002/9781394200801.ch30","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10951559.pdf&bkn=10950236&pdfType=chapter","","Security;Business;Industrial Internet of Things;Ecosystems;Logic gates;Computer architecture;Cloud computing;Real-time systems;Conversational artificial intelligence;Servers","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly","P. LIU; L. Qian; X. Zhao; B. Tao","State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Department of Mechanical, Huazhong University of Science and Technology, Wuhan, China; School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Department of Mechanical, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Department of Mechanical, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Industrial Informatics,"5 Jun 2024","2024","20","6","8160","8169","In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.","1941-0050","","10.1109/TII.2024.3366977","National Natural Science Foundation of China(grant numbers:52275020,62293514,52188102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463190","Data-driven;fault localization;intelligent fault diagnosis;knowledge graph (KG);large language model (LLM)","Task analysis;Fault diagnosis;Training;Hidden Markov models;Cognition;Knowledge engineering;Knowledge graphs","","43","","30","IEEE","8 Mar 2024","","","IEEE","IEEE Journals"
"Securing Federated Diffusion Model With Dynamic Quantization for Generative AI Services in Multiple-Access Artificial Intelligence of Things","J. He; B. Lai; J. Kang; H. Du; J. Nie; T. Zhang; Y. Yuan; W. Zhang; D. Niyato; A. Jamalipour","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Software Engineering, Beijing Jiaotong University, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Computer Engineering, The University of Sydney, Sydney, NSW, Australia",IEEE Internet of Things Journal,"22 Aug 2024","2024","11","17","28064","28077","Generative diffusion models (GDMs) have emerged as potent tools for generating high-quality, creative content across various media, including audio, images, videos, and 3-D models. Their application in artificial intelligence-generated content (AIGC) marks a pivotal advancement in the evolution from the Internet of Things (IoT) to the Artificial Intelligence of Things (AIoT). Considering the inherent multiple-access nature of AIoT, training GDMs via federated learning and deploying them collaboratively is paramount. However, such approaches introduce considerable security risks and energy consumption challenges. To address these issues, we propose a comprehensive architecture for GDMs, encompassing both training and sampling stages. This architecture, termed secure and sustainable diffusion (SS-Diff), aims to thwart trigger-based security threats, such as backdoor attacks and trojan attacks, while simultaneously reducing energy consumption in multiple-access AIoT. The SS-Diff architecture incorporates a dynamic quantization mechanism within the training phase, significantly reducing communication overhead and thereby improving both spectrum and energy efficiency. During the sampling stage, a detection-based defense strategy is employed to identify and negate trigger inputs associated with malicious attacks. Through extensive simulations, we evaluate the performance of the SS-Diff architecture. The results demonstrate that the SS-Diff can effectively train GDMs and eliminate the impact of the attacks, compared with existing schemes.","2327-4662","","10.1109/JIOT.2024.3420696","National Natural Science Foundation of China (NSFC)(grant numbers:62102099,U22A2054); Guangzhou Basic Research Program(grant numbers:2023A04J1699); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515140137); Talent Fund of Beijing Jiaotong University(grant numbers:2023XKRC050); China Postdoctoral Science Foundation(grant numbers:2024T170047,GZC20230223,2024M750165); Open Fund of Anhui Engineering Research Center for Intelligent Applications and Security of Industrial Internet(grant numbers:IASII24-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596048","Artificial intelligence of things (AIoT);energy efficiency;generative diffusion model (GDM);multiple access;security","Artificial intelligence;Training;Internet of Things;Diffusion models;Security;Energy consumption;Computer architecture","","9","","48","IEEE","11 Jul 2024","","","IEEE","IEEE Journals"
"Security, Privacy, and Ethical Challenges of Artificial Intelligence in Large Language Model Scope: A Comprehensive Survey","T. T. Nguyen; H. T. T. Vu; H. N. Nguyen","Department of Information Systems, VNU University of Engineering and Technology, Hanoi, Vietnam; Hanoi Financial and Banking University, Hanoi, Vietnam; Department of Information Systems, VNU University of Engineering and Technology, Hanoi, Vietnam",2024 1st International Conference On Cryptography And Information Security (VCRIS),"27 Dec 2024","2024","","","1","6","Artificial intelligence (AI) models like ChatGPT and LLama have changed how we understand and create human-like language. They understand language well, can generate text like a person, know the context, and solve problems effectively. This makes them useful in many areas like search engines, customer service, and translation. Recently, these models have also caught the attention of the security field, uncovering weaknesses in security and proving useful for security tasks. This paper offers an extensive review of the security, privacy, and ethical issues associated with AI, focusing on LLMs and their impact on various domains. We explore the vulnerabilities of LLMs, propose poten-tial defense mechanisms, and highlight the broader implications of AI on human society. This survey intends to offer a clear perspective for researchers and practitioners, and policymakers on responsibly developing and deploying AI technologies.","","979-8-3315-2994-9","10.1109/VCRIS63677.2024.10813382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10813382","Large Language Model;LLM Security;LLM Privacy;LLM Ethical;Artificial Intelligence;ChatGPT","Surveys;Ethics;Privacy;Translation;Prevention and mitigation;Computational modeling;Training data;Security;Cryptography;Artificial intelligence","","","","33","IEEE","27 Dec 2024","","","IEEE","IEEE Conferences"
"Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security","C. Yan; R. Ren; M. H. Meng; L. Wan; T. Y. Ooi; G. Bai","University of Queensland, Australia; University of Queensland, Australia; Technical University of Munich, Germany; University of Queensland, Australia; University of Queensland, Australia; University of Queensland, Australia",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1370","1382","ChatGPT has enabled third-party developers to create plugins to expand ChatGPT’s capabilities. These plugins are distributed through OpenAI’s plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the Chat-GPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.CCS CONCEPTS• Security and privacy → Software and application security.","2643-1572","979-8-4007-1248-7","","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765055","Large Language Model;Testing;Security;Deployment","Privacy;Biological system modeling;Source coding;Ecosystems;Reverse engineering;Chatbots;Software;Security;Sustainable development;Software engineering","","","","59","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Contextualized AI for Cyber Defense: An Automated Survey Using LLMs","C. Y. Haryanto; A. M. Elvira; T. D. Nguyen; M. H. Vu; Y. Hartanto; E. Lomempow; A. Arakala","School of Science, RMIT University, Melbourne, Australia; School of Science, RMIT University, Melbourne, Australia; School of Science, RMIT University, Melbourne, Australia; School of Science, RMIT University, Melbourne, Australia; Faculty of Engineering and IT, University of Tehnology, Sydney, Australia; ZipThought, Melbourne, Australia; School of Science, RMIT University, Melbourne, Australia",2024 17th International Conference on Security of Information and Networks (SIN),"13 Feb 2025","2024","","","1","8","This paper surveys the potential of contextualized AI in enhancing cyber defense capabilities, revealing significant research growth from 2015 to 2024. We identify a focus on robustness, reliability, and integration methods, while noting gaps in organizational trust and governance frameworks. Our study employs two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for exploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for full-text analysis. We discuss the effectiveness and challenges of using LLMs in academic research, providing insights for future researchers.","","979-8-3315-0973-6","10.1109/SIN63213.2024.10871242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871242","cyber security;artificial intelligence;retrieval augmented generation;cyber defense strategy;meta-analysis","Surveys;Filtering;Chatbots;Robustness;Security;Artificial intelligence;Computer crime","","","","102","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"Syntactic-Guided Chain of Thought for Iterative Implicit and Explicit Target Detection in Aspect-Based Sentiment Analysis","M. Radi; N. Omar; W. Kaur","Center for Artificial Intelligence Technology (CAIT), Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia; Center for Artificial Intelligence Technology (CAIT), Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia; Center for Artificial Intelligence Technology (CAIT), Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia",IEEE Access,"22 May 2025","2025","13","","84738","84751","Prompt engineering is essential for optimizing the performance of large-language models (LLMs), particularly in tasks requiring complex interpretations such as aspect-based sentiment analysis (ABSA). However, existing methodologies often struggle to detect implicit targets, especially in multi-opinion sentences where sentiments are directed toward aspects that are not explicitly mentioned. This study addresses this gap by proposing the Iterative Syntactic-Guided Chain of Thought (IS-COT) framework, which integrates dependency parsing with modular prompt engineering to enhance LLMs’ reasoning capabilities. IS-COT leverages syntactic structures and iterative refinement to detect both explicit and implicit targets while resolving ambiguities in multi-opinion contexts. Experimental evaluations on benchmark datasets, Sem-Eval 2015 (Res15) and Sem-Eval 2016 (Res16), demonstrated the effectiveness of the framework, achieving superior performance with 80.43 F1 scores on Res15 and 84.47 F1 scores on Res16, significantly outperforming state-of-the-art models. These results highlight IS-COT’s potential of IS-COT as a comprehensive and interpretable solution for ABSA, addressing the critical limitations of existing approaches and advancing the field through innovative syntactic and semantic integration.","2169-3536","","10.1109/ACCESS.2025.3568695","Universiti Kebangsaan Malaysia (UKM)(grant numbers:TAP-K007009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10994766","Aspect-based sentiment analysis (ABSA);prompt engineering;chain of thought (COT);explicit opinion;implicit opinion;target-aspect-sentiment (TASD);dependency relations","Prompt engineering;Syntactics;Sentiment analysis;Semantics;Iterative methods;Cognition;Accuracy;Object detection;Analytical models;Knowledge engineering","","","","61","CCBY","9 May 2025","","","IEEE","IEEE Journals"
"Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought","M. Zeng; M. Xie; X. Zheng; C. Li; C. Zhang; L. Zhu","Guangxi Power Grid Co., Ltd, China; Guangxi Power Grid Co., Ltd, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, China; Guangxi Engineering Research Center of Industrial Internet Security and Blockchain, Guilin University of Electronic Technology, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","The rapid development of Internet of Things (IoT) technology has transformed people’s way of life and has a profound impact on both production and daily activities. However, with the rapid advancement of IoT technology, the security of IoT devices has become an unavoidable issue in both research and applications. Although some efforts have been made to detect or mitigate IoT security vulnerabilities, they often struggle to adapt to the complexity of IoT environments, especially when dealing with dynamic security scenarios. How to automatically, efficiently, and accurately understand these vulnerabilities remains a challenge. To address this, we propose an IoT security assistant driven by a Large Language Model (LLM), which, through the ICoT process, enhances the LLM’s understanding of IoT security vulnerabilities and related threats. The ICoT method we propose aims to enable the LLM to understand security issues by breaking down the various dimensions of security vulnerabilities and generating responses tailored to the user’s specific needs and expertise level. By incorporating ICoT, LLM can gradually analyze and reason through complex security scenarios, resulting in more accurate, in-depth, and personalized security recommendations and solutions. Experimental results show that, compared to methods relying solely on LLMs, our proposed LLM-driven IoT security assistant significantly improves the understanding of IoT security issues and provides personalized solutions based on user identity through the ICoT approach. From the evaluator’s perspective, it performs better across five dimensions.","2327-4662","","10.1109/JIOT.2025.3577675","Young Elite Scientists Sponsorship Program by CAST(grant numbers:2023QNRC001); Open Foundation of Key Laboratory of Cyberspace Security, Ministry of Education of China(grant numbers:KLCS20240209); National Natural Science Foundation of China(grant numbers:62472032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028909","Internet of Things;Chain-of-Thought;Security;Large Language Model","Internet of Things;Security;Cognition;Accuracy;Feature extraction;Complexity theory;Training;Systems architecture;Microprogramming;Fuzzing","","","","","IEEE","9 Jun 2025","","","IEEE","IEEE Early Access Articles"
"Dynamic Event-Triggered Byzantine-Resilient Output Regulation in Continuous-Time High-Order Multiagent Systems With Static/Dynamic Leader","C. Yan; L. Yan; Y. Lv; Y. Xia","School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; MIIT Key Laboratory of Complex-Field Intelligent Sensing, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Circuits and Systems I: Regular Papers,"26 Nov 2024","2024","71","12","5532","5545","As a critical security threat, Byzantine attacks in nodes sending tampered data to their neighbors in networks, significantly undermining the robustness of networked systems. This paper focuses on the leader-follower resilient consensus of continuous-time heterogeneous multiagent systems against Byzantine attacks on nodes. Considering the excessive communication burden in practice, a distributed dynamic event-triggered mechanism is devised to enable intermittent communication among agents. Building upon this mechanism, two protocols are applied to achieve the output consensus when the leader converges to dynamic and static states, respectively. The latter protocol can decrease both communication data and the frequency of event triggers when the leader converges to a static state. In addition, the output regulation control for uncertain system dynamics is achieved by utilizing the technique of internal model principle. Moreover, the designed event-triggered mechanism can be employed to the networks with bounded communication delays. Finally, the effectiveness of the designed protocols is demonstrated by some numerical simulations.","1558-0806","","10.1109/TCSI.2024.3381191","Beijing Natural Science Foundation(grant numbers:4242049); National Natural Science Foundation of China(grant numbers:62073036,62088101,62273045,U2341213); Beijing Nova Program(grant numbers:20230484481); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494506","Multi-agent systems;networked security;Byzantine attacks;dynamic event-triggered control;output control","Protocols;Computer security;Regulation;Multi-agent systems;Control systems;Robustness;Event detection;Output feedback;Cyberattack","","7","","38","IEEE","8 Apr 2024","","","IEEE","IEEE Journals"
"Strategically Revealing Intentions in General Lotto Games","K. Paarporn; R. Chandan; D. Kovenock; M. Alizadeh; J. R. Marden","Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA; Economic Science Institute, Argyros School of Business and Economics, Chapman University, Orange, CA, USA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA",IEEE Transactions on Automatic Control,"29 Jul 2024","2024","69","8","5396","5407","Strategic decision-making in uncertain and adversarial environments is crucial for the security of modern systems and infrastructures. A salient feature of many optimal decision-making policies is a level of unpredictability, or randomness, which helps to keep an adversary uncertain about the system's defensive strategies. This article seeks to explore decision-making policies on the other end of the spectrum—namely, whether there are benefits in revealing one's strategic intentions to an opponent before engaging in competition. We study these scenarios in a well-studied model of competitive resource allocation problem known as General Lotto games. In the classic formulation, two competing players simultaneously allocate their assets to a set of battlefields, and the resulting payoffs are derived in a zero-sum fashion. Here, we consider a three-stage extension where one of the players has the option to publicly precommit assets in a binding fashion to battlefields before play begins. In response, the opponent decides which of these battlefields to secure (or abandon) by matching the precommitment with its own assets. They then engage in a General Lotto game over the remaining set of battlefields. Interestingly, this article highlights many scenarios where strategically revealing intentions in this fashion can actually significantly improve one's payoff. Thus, we show precommitments can serve as alternate decision-making policies that contrast with conventional policies where randomness is the central component of strategy in adversarial environments.","1558-2523","","10.1109/TAC.2024.3367651","Office of the President, University of California(grant numbers:LFR-18-548175); ONR(grant numbers:#N00014-20-1-2359); Air Force Office of Scientific Research(grant numbers:#FA9550-20-1-0054,#FA9550-21-1-0203); Army Research Laboratory(grant numbers:CRA#W911NF-17-2-0181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440558","Multi-agent systems;game theory;security;decision-making","Game theory;Cost accounting;Resource management;Security;Uncertainty;Multi-agent systems;Decision making","","1","","45","IEEE","20 Feb 2024","","","IEEE","IEEE Journals"
