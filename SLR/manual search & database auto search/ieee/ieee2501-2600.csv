"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Learning and AI","F. Cady",Allen Institute for Artificial Intelligence; Stanford University; Carnegie Mellon,The Data Science Handbook,"","2025","","","309","329","Summary <p>Artificial intelligence has taken the world by storm, and this chapter will give you the foundation to understand and take advantage of this revolution. It shows in detail how to build and train deep neural networks, applying them to classic problems in classification and image analysis. An overview is given of the key concepts underlying modern AI applications, such as diffusion models and transformers, and how they are put together to make generative AI tools like Stable Diffusion. Finally, there is a discussion of Large Language Models (LLMs) and prompt engineering using the LangChain library.</p>","","9781394234516","10.1002/9781394234523.ch24","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10766914.pdf&bkn=10766879&pdfType=chapter","","Deep learning;Neurons;Data models;Training;Neural networks;Artificial intelligence;Graphics processing units;Libraries;Codes;Adaptation models","","","","","","25 Nov 2024","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"SQL Injection Vulnerability Detection Based on Pissa-Tuned Llama 3 Large Language Model","Z. Tian; S. Lou; Y. Zhang; Z. Jiang; X. Cheng","Guangdong Branch of National Computer Network Emergency, Response Technical Team/Coordination Center of China, Guangzhou, Guangdong, China; Guangdong Branch of National Computer Network Emergency, Response Technical Team/Coordination Center of China, Guangzhou, Guangdong, China; Guangdong Branch of National Computer Network Emergency, Response Technical Team/Coordination Center of China, Guangzhou, Guangdong, China; Guangdong Branch of National Computer Network Emergency, Response Technical Team/Coordination Center of China, Guangzhou, Guangdong, China; Guangdong Branch of National Computer Network Emergency, Response Technical Team/Coordination Center of China, Guangzhou, Guangdong, China",2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC),"13 Mar 2025","2024","","","255","259","SQL injection vulnerability is a common cyber security vulnerability. The existing SQL injection attack detection methods have problems such as high false positive rate and insufficient migration. In view of the powerful natural language processing ability of large language models, this paper proposes a SQL injection vulnerability detection method based on PiSSA fine-tuning of Llama3 large language models. The experimental results show that the proposed method achieves 99.81% accuracy and 0.19% false positive rate on the common benchmark data set, which verifies the good performance of the large language model in the field of SQL injection attack detection.","","979-8-3315-4175-0","10.1109/ICFTIC64248.2024.10912886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912886","SQL injection;vulnerability detection;Llama 3;PiSSA","Performance evaluation;Accuracy;Large language models;Data preprocessing;SQL injection;Benchmark testing;Natural language processing;Data models;Prompt engineering","","","","19","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Exploring the Impact of Large Language Model-Simulated Personalities on Recommendation","Z. Feng; Z. Wang; H. Mao; B. Liu","Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Electrical Engineering and Computer Science, Ningbo University, Ningbo, China",2025 28th International Conference on Computer Supported Cooperative Work in Design (CSCWD),"23 Jun 2025","2025","","","965","971","In recent years, Large Language Model-based Recommendation Systems (LLMRec) have achieved remarkable advancements. By designing effective prompts, LLMs can understand user interests and either directly generate recommendations based on users' interests or assist traditional recommendation systems in enhancing recommendation performance. However, existing LLMRec predominantly leverage LLMs as static tools for user interest understanding, ignoring the dynamic behavioral patterns of LLMs when simulating human personalities. While some studies explore LLMs' human-like behaviors in conversational scenarios, their impacts on recommendation systems remain underexplored. To fill these gaps, this paper aims to investigate how LLM-simulated personalities interact with users of varying personalities in recommendation systems, particularly focusing on whether LLM-simulated personalities exhibit behaviors align with Social Homophily Theory which is well-documented in human social interactions but unexplored in LLMRec. We conducted experiments on two public datasets, inducing LLMs to simulate different human personalities based on the Big Five Personality Traits through prompt engineering, and incorporating the generated user profiles into the Knowledge Augmented Recommendation framework to evaluate performance across various recommendation models. Experimental results demonstrate that inducing LLMs to simulate personality in the KAR framework significantly influences recommendation performance. Moreover, LLM-simulated personalities exhibit a preference for users with similar personalities, mirroring the Social Homophily Theory observed in human behavior.","2768-1904","979-8-3315-1305-4","10.1109/CSCWD64889.2025.11033598","Natural Science Foundation of Zhejiang Province(grant numbers:LZ20F020001); Natural Science Foundation of Ningbo(grant numbers:2021J091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11033598","Recommendation Systems;Large Language Model;Big Five Personality Traits","Knowledge engineering;Federated learning;Large language models;Computational modeling;Focusing;Behavioral sciences;Prompt engineering;Recommender systems","","","","29","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"The Role of Cyber-Physical-Social Systems in Smart Energy Future","X. Yu; N. Liu; Y. Xue","School of Engineering, RMIT University, Melbourne, VIC, Australia; North China Electric Power University, Beijing, China; State Grid Electric Power Research Institute, Nanjing, China",IEEE Transactions on Industrial Cyber-Physical Systems,"8 Feb 2024","2024","2","","35","42","Future energy systems (FESs) require greater interaction, integration, and cooperation between physical infrastructure, cyber technologies, and human participants from prosumers to communities and governments. Cyber-Physical-Social Systems (CPSSs) will be the enabling technology to ensure the efficiency, effectiveness, sustainability, security and safety of energy generation and use integrating human and social factors into consideration. In this paper, we will first present an overview of the challenges in CPSSs. We will then outline potential contributions that CPSSs can make to FESs, as well as the opportunities that FESs present to CPSSs.","2832-7004","","10.1109/TICPS.2024.3357666","Australian Research Council(grant numbers:DP200101199,DP230101107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412685","Cyber-physical systems;cyber-social systems;renewable energy;smart grid;human dynamics;generative AI;transformer;ChatGPT;information technology;holism;reductionism","Cyber-physical systems;Security;Safety;Behavioral sciences;Smart grids;Social factors;Renewable energy sources;Human factors;Generative AI;Chatbots;Information technology;Energy management","","14","","36","CCBY","23 Jan 2024","","","IEEE","IEEE Journals"
"Big Data and AI Algorithms for Sustainable Development Goals: A Topic Modeling Analysis","P. Nedungadi; S. Surendran; K. -Y. Tang; R. Raman","Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, Kerala, India; Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, Kerala, India; Graduate Institute of Library and Information Science, National Chung Hsing University, Taichung City, Taiwan; Amrita School of Business, Amrita Vishwa Vidyapeetham, Amritapuri, Kerala, India",IEEE Access,"20 Dec 2024","2024","12","","188519","188541","This study makes significant contributions to the field by examining the transformative role of big data and artificial intelligence (AI) in advancing Sustainable Development Goals (SDGs), particularly healthcare (SDG3), sustainable energy (SDG7), and industry and infrastructure (SDG9). Using BERTopic modeling, a machine learning technique, this research systematically analyzes literature from 2013 to 2024, providing an overview of AI and big data applications mapped to SDGs which is a first. This structured approach identifies key SDGs impacted by these technologies and highlights interdisciplinary methods that further enhance SDG outcomes. AI applications notably improve healthcare by advancing disease tracking, tailored treatments, and precision medicine, fostering universal healthcare and reducing noncommunicable disease mortality. In energy, AI-driven solutions optimize forecasting, grid management, and renewable integration, while in industry, they bolster infrastructure resilience through innovations like predictive maintenance and automated quality control within Industry 4.0 frameworks. The integration of automated text analysis and semantic context captures broad trends, contributing both methodologically and substantively at the intersection of AI and sustainability. Despite these advancements, the study underscores ethical concerns, including data privacy, security, and algorithmic biases. Interdisciplinary collaboration among healthcare professionals, engineers, environmental scientists, and AI experts is crucial to developing ethical, scalable AI solutions. The study suggests future research focus on AI transparency, scaling across diverse sectors, and integrating advanced techniques such as neurosymbolic AI and quantum neural networks to enhance system reliability. These insights offer practical implications, reinforcing the potential of AI and big data to address global challenges sustainably while calling for balanced attention to ethical and regulatory dimensions.","2169-3536","","10.1109/ACCESS.2024.3516500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794768","Sustainable development goal;big data;artificial intelligence;healthcare;resilient energy;resilient infrastructure;industrial innovation;generative AI","Artificial intelligence;Big Data;Sustainable development;Medical services;Data models;Technological innovation;Ethics;Market research;Climate change;Analytical models","","13","","128","CCBY","12 Dec 2024","","","IEEE","IEEE Journals"
"Automation of AD-OHC Dashbord and Monitoring of Cloud Resources using Genrative AI to Reduce Costing and Enhance Performance","P. Chavan; P. Chavan",BE Computer Science; Data Science,2024 International Conference on Innovations and Challenges in Emerging Technologies (ICICET),"6 Aug 2024","2024","","","1","9","In this extensive review, the incorporation of Generative Artificial Intelligence (AI) into ad-hoc dashboards and cloud resource monitoring is investigated in depth. A purpose of this work is to investigate a current research and highlight the transformational potential of generative artificial intelligence for optimising costs, automating tasks, and improving overall cost efficiency. This article includes a comprehensive analysis of a development of ad-hoc cloud computing, a relevance of cloud resource monitoring, and the role that generative artificial intelligence plays in reducing costs. Notable advantages include better user satisfaction, resource optimisation, adaptive learning, and efficient automation. Additionally, precise decision-making and resource optimisation are highlighted. By presenting actual evidence, the study highlights the association between the quality of generative artificial intelligence and the influence it has on society. It finishes with an analysis of generative artificial intelligence applications in ad-hoc dashboards, with a focus on increased resource utilisation, scalability, and timely consistency of cloud resources. In addition, the article addresses constraints and makes suggestions for future paths. These include models that protect users’ privacy, efficient resource utilisation, explainable artificial intelligence, dynamic autonomy, and security-driven generative models. The paper’s goal is to provide the groundwork for further study and improvement.","","979-8-3503-1901-9","10.1109/ICICET59348.2024.10616299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616299","AD-OHC Dashbord;Cloud Resources;Generative AI;Cloud Computing;Cost Reductions;Cloud Monitoring.","Cloud computing;Technological innovation;Costs;Automation;Generative AI;Scalability;Resource management","","3","","42","IEEE","6 Aug 2024","","","IEEE","IEEE Conferences"
"Intelligent Systems for Data Driven Agriculture: Enhancing Farmer Productivity Through Automation and Artificial Intelligence","S. Lokesh; A. Madhavan; R. P. Ramanathan; K. Anand","Department of Computer Science and Engineering, PSG Institute of Technology and Applied Research, Tamilnadu, India; Department of Computer Science and Engineering, PSG Institute of Technology and Applied Research, Tamilnadu, India; Department of Computer Science and Engineering, PSG Institute of Technology and Applied Research, Tamilnadu, India; Department of Computer Science and Engineering, PSG Institute of Technology and Applied Research, Tamilnadu, India","2024 International Conference on Smart Systems for Electrical, Electronics, Communication and Computer Engineering (ICSSEECC)","3 Sep 2024","2024","","","433","438","Numerous obstacles endanger the livelihoods of smallholder farmers and global food security. Crop output losses due to plant diseases alone are estimated to be between 15% to 25% every year. However, farmers' capacity to recognize and treat illnesses in a timely manner is hampered by things like a lack of agronomic expertise, limited access to actionable insights, and information gaps. The work proposes a comprehensive end-to-end web application that uses machine learning models to help farmers with configurable expert advice, smart farm management, and automatic disease detection. Thus, enabling farmers to identify the infection using their smart phones and obtain crop - fertilizer recommendations for the crops to be treated in the farmer's area based on historical data analysis of yield-influencing elements such as geography, weather, and soil. The proposed system will employ Generative Artificial Intelligence, such as ChatGPT's natural language processing and generation capabilities, to engage in conversational interactions with farmers. The work achieves close to 95% accuracy after utilizing 30 epochs in the area of disease detection and 97% accurate models are achieved for fertilizer outputs. These features along with it's high accuracy can assist farmers in making informed decisions, utilising the data obtained to reduce crop output losses.","","979-8-3503-7817-7","10.1109/ICSSEECC61126.2024.10649461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10649461","Machine Learning;ChatGPT;Generative AI;Convoluted Neural Network","Smart agriculture;Productivity;Plant diseases;Accuracy;Neural networks;Crops;Soil","","1","","19","IEEE","3 Sep 2024","","","IEEE","IEEE Conferences"
"Access Control Policy Generation for IoT Using Deep Generative Models","K. Ragothaman; Y. Wang; D. Zeng; J. Liu; B. Rimal","University of Wisconsin Parkside, Kenosha, WI; Dakota State University, Madison, SD; Dakota State University, Madison, SD; Dakota State University, Madison, SD; University of Idaho, Moscow, ID",2025 IEEE 22nd Consumer Communications & Networking Conference (CCNC),"5 May 2025","2025","","","1","8","Internet of Things (IoT) is commonly utilized in domestic, industrial, and public environments to automate various tasks. Due to this, an enormous amount of data is being generated and transmitted through IoT networks. These data may contain sensitive information depending on the context. Access control is one of the frontline security measures that any information system should adopt. The dynamic nature of the IoT requires access control policies to be able to adapt to their environments. However, it is very challenging for a human administrator to specify access control policies for all scenarios manually because of their dynamic nature. Current literature suggests the need for automating the process of policy generation. Machine Learning and Deep Learning techniques can enable the required automation. We conducted a case study using two baseline Tabular Generative Adversarial Network (GAN) models, namely CTGAN and CopulaGAN, to generate access control policy data. We utilized the CAV policies dataset published by Cunnington et al. We evaluated our results using both quantitative and manual evaluation. Our initial results identified a significant number of policy violations to underlying environmental constraints. We later trained the models by applying constraints. Our final results demonstrate that the models were able to generate policies without violating the specified environmental constraints.","2331-9860","979-8-3315-0805-0","10.1109/CCNC54725.2025.10976144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976144","IoT;Access Control;Policies;Machine Learning;Generative AI","Access control;Training;Hands;Deep learning;Generative AI;Manuals;Generative adversarial networks;Internet of Things;Stress;Information systems","","1","","31","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Trust Model-Based Consensus Optimization for Vehicle Platooning Networks: A Novel Deep Reinforcement Learning Approach With GenAI","H. Chen; X. Fu; Q. Yuan; Z. Zhuang; J. Kang; Z. Liu; J. Wang; D. Niyato","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Guangdong University of Technology, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Intelligent Transportation Systems,"8 Aug 2025","2025","26","8","11356","11371","Vehicle platooning has emerged as a promising solution for efficient traffic management. Multiple platoons traveling in a cooperative way can alleviate congestion and enhance driving safety by information sharing and consensus. To address the data security and privacy concerns, blockchain could be applied to enable secure data sharing and consensus across multiple platoons. However, existing performance of blockchain is insufficient to ensure reliable and efficient data consensus among multiple platoons. First, the hierarchical structure of platoons with different roles of vehicles complicates the trust establishment between platoons, making it challenging to evaluate their trustworthiness and ensure consensus reliability. Additionally, data sharing in vehicle platooning networks demands timely information and efficient consensus-building. To tackle above challenges, we design a role-adaptive trust model for trust evaluation of platoons in consideration of different roles of vehicles within a platoon. Based on the proposed model, we formulate a blockchain consensus optimization problem to facilitate both reliability and efficiency of data consensus among multiple platoons. Leveraging Generative Artificial Intelligence (GenAI) techniques, we then propose the Diffusion Enhanced Soft Actor-Critic (DESAC) by integrating the diffusion model and SAC, to further improve the performance of blockchain consensus. Experiment results demonstrate the effectiveness and efficiency of the proposed consensus optimization approach.","1558-0016","","10.1109/TITS.2025.3559672","Natural Science Foundation of China(grant numbers:62272053,62272195); Beijing Nova Program(grant numbers:20230484364); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10972169","Vehicle platooning;consensus;blockchain;trust model;deep reinforcement learning (DRL);generative AI (GenAI)","Blockchains;Reliability;Consensus protocol;Optimization;Vehicle dynamics;Trust management;Heuristic algorithms;Data models;Diffusion models;Computational modeling","","","","52","IEEE","21 Apr 2025","","","IEEE","IEEE Journals"
"SAGA-Integrating AI for Enhanced Recruitment: A System with Audio-Video Proctoring and Comprehensive Evaluation","N. Mathur; A. Sharma; S. Kaushik; V. S. Bakkialakshmi; V. Kavadiya; N. Aneja","Department of Computing Technologies, S.R.M Institute of Science and Technology,Kattankulathur, Chennai, India; Department of Computing Technologies, S.R.M Institute of Science and Technology,Kattankulathur, Chennai, India; Department of Computing Technologies, S.R.M Institute of Science and Technology,Kattankulathur, Chennai, India; Department of Computing Technologies, S.R.M Institute of Science and Technology,Kattankulathur, Chennai, India; Department of Computing Technologies, S.R.M Institute of Science and Technology,Kattankulathur, Chennai, India; Department of Computer Science, Purdue University, West Lafayette, IN, USA","2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)","13 Mar 2025","2024","1","","395","399","AI has transformed recruitment, overcoming logictical issues and interviewer prejudice. The COVID-19 epidemic compounded matters. Advanced AI interview systems use natural language processing and computer vision algorithms to assess candidates’ verbal and non-verbal cues using audio and video. Systems have uniform standards, high integrity, security, and scalability. In addition, speech, sentiment, face, and emotion detection are provided. Online learning and telemedicine could be added to recruitment apps. These technologies improve feedback quality and transparency for safe, efficient, and balanced global change.","","979-8-3315-4121-7","10.1109/ICAICCIT64383.2024.10912328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912328","Proctoring Systems;Generative AI;Comparative Evaluation;Unbiased Assessment;Automated Analysis;Environmental Monitoring","Generative AI;Scalability;Face recognition;Telemedicine;Natural language processing;Information and communication technology;Security;Interviews;Standards;Recruitment","","","","20","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Generative Architecture for Data Imputation in Secure Blockchain-Enabled Spatiotemporal Data Management","S. Li; W. Liu; Y. Wu; J. Zhao","School of Computer and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Computer and Information Security, Guilin University of Electronic Technology, Guilin, China; Unit 95795 of PLA, Guilin, China; School of Computer and Information Security, Guilin University of Electronic Technology, Guilin, China",Journal of Web Engineering,"2 Apr 2024","2024","23","1","111","163","In the era of big data, one of the most critical challenges is ensuring secure access, retrieval, and sharing of linked spatiotemporal data. To address this challenge, this paper introduces a groundbreaking blockchain-enabled evolutionary indirect feedback graph algorithm for the secure management of interconnected spatiotemporal datasets. The algorithm utilizes a generative neural network model for data imputation, predicting and generating plausible values to improve dataset completeness and integrity. The core architecture utilizes blockchain technology to optimize data retrieval efficiency and uphold robust access control mechanisms. The algorithm incorporates indirect feedback mechanisms, allowing users to provide implicit feedback through their interactions, enhancing the relevance and efficiency of data retrieval. In addition. sophisticated graph-based techniques are used to model intricate relationships between data entities, facilitating seamless data retrieval and sharing in interwoven datasets. The algorithm's data security approach includes comprehensive access control mechanisms, encryption, and authentication mechanisms, safeguarding data confidentiality and integrity. Extensive evaluations show significant enhancements in retrieval performance and access control precision, making the proposed model a promising solution for the secure management of expansive interconnected spatiotemporal data.","1544-5976","","10.13052/jwe1540-9589.2315","GUET Graduate Education(grant numbers:2023YCXS063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488442","Indirect feedback graph algorithm;linked spatiotemporal data;big data analysis;secure access;data retrieval;generative AI;blockchain","Access control;Data privacy;Machine learning algorithms;Data security;Authentication;Transportation;Prediction algorithms","","","","34","","2 Apr 2024","","","River Publishers","River Publishers Journals"
"Empowering Smart Vehicles: Giving Authority with LLM and SSI","A. Altaf; A. Anjum","Institute of Information Technology, Quaid-i-Azam University, Islamabad, Pakistan; Institute of Information Technology, Quaid-i-Azam University, Islamabad, Pakistan",2024 International Conference on Frontiers of Information Technology (FIT),"17 Jan 2025","2024","","","01","06","The spread of smart vehicles has introduced an unprecedented flow of data that holds promise for enhancing safety, efficiency, and user services. Traditional, centralized data management models, however, raise critical privacy and security concerns, while existing decentralized systems often fall short in empowering users with true control over their data. This paper proposes a novel framework that integrates Self-Sovereign Identity (SSI) and Large Language Models (LLMs) to create a decentralized, user-driven access control system tailored for connected vehicles. By equipping users with digital wallets that grant them explicit authority over their data and leveraging LLMs for adaptive, context-aware data-sharing decisions, this approach advances beyond rigid, rule-based systems. This research presents findings that demonstrate the effectiveness of the SSI-LLM integration in addressing privacy and security concerns while maximizing data utility.","2473-7569","979-8-3315-1050-3","10.1109/FIT63703.2024.10838412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838412","Smart Vehicle;Self-Sovereign Identity (SSI);Differential Privacy in IOV;Zero Knowledge Proof(ZKP)","Data privacy;Privacy;Large language models;Biological system modeling;Scalability;Ecosystems;Real-time systems;Safety;Time factors;Interoperability","","1","","17","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Toward Extracting Learning Pattern: A Comparative Study of GPT-4o-mini and BERT Models in Predicting CVSS Base Vectors","S. Isogai; S. Ogata; Y. Kashiwa; S. Yazawa; K. Okano; T. Okubo; H. Washizaki","Shinshu University, Nagano, Japan; Shinshu University, Nagano, Japan; Nara Institute of Science and Technology, Nara, Japan; Voice Research, Inc., Tokyo, Japan; Shinshu University, Nagano, Japan; Institute of Information Security, Kanagawa, Japan; Waseda University, Tokyo, Japan",2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW),"3 Dec 2024","2024","","","127","134","Software vulnerabilities represent significant threats to the security of systems and data. Timely mitigation of these vulnerabilities is crucial to prevent exploitation. The Common Vulnerability Scoring System (CVSS) is widely used to quantify the severity of vulnerabilities, but manually assigning CVSS base vectors to newly discovered vulnerabilities is time-consuming and error-prone. This paper investigates the potential of large language models (LLMs) to automate this task. We conducted a comparative study of GPT-4o-mini and BERT models for predicting CVSS base vectors. Both models were trained and evaluated on a dataset of vulnerabilities extracted from the National Vulnerability Database (NVD), using vulnerability descriptions as input. Our results demonstrate that BERT outperforms GPT-4o-mini in terms of prediction accuracy for CVSS base vectors. BERT’s ability to capture contextual information within the vulnerability descriptions enables it to make more accurate predictions. To gain insights into the decision-making process of the GPT-4o-mini model, we performed a keyword analysis. This analysis provides a foundation for further research into the explainability of LLMs in security applications.","2994-810X","979-8-3503-6704-1","10.1109/ISSREW63542.2024.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771484","CVE;CVSS;Vulnerability;GPT-4o-mini;BERT;Fine-Tuning","Accuracy;Databases;Prevention and mitigation;Large language models;Decision making;Predictive models;Vectors;Software;Software reliability;Security","","","","20","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"Collaborative Vehicular Threat Sharing: A Long-Term Contract-Based Incentive Mechanism With Privacy Preservation","C. He; Y. Wang; J. Hu; T. H. Luan; Y. Bi; Z. Su","School of Cyber Engineering, Xidian University, XiÃan, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China",IEEE Transactions on Intelligent Transportation Systems,"27 Nov 2024","2024","25","12","21528","21544","The rapid development of the Internet of Vehicles (IoV) has spurred innovations in Intelligent Transportation Systems (ITS), but it also faces increasingly sophisticated cybersecurity threats. Traditional defense mechanisms often fall short in handling emerging and complex attacks due to the lack of flexibility to adapt to the rapidly evolving IoV environment. An emerging solution is to employ Large Language Models (LLMs), such as ChatGPT, to enhance IoV security, which depends on the quality, quantity, and freshness of the threat data used for fine-tuning. In this paper, we introduce a collaborative vehicular threat sharing framework that utilizes vehicular honeypots to gather threat data for fine-tuning LLMs, thereby bolstering IoV security. Local differential privacy is leveraged to safeguard the vehicles’ privacy. Given that vehicles have different privacy preferences that may change over time, it is critical to design an appropriate incentive mechanism to encourage sustainable participation in the dynamic IoV environment. Moreover, since privacy preferences are the private information of the vehicles, an information asymmetry exists between the vehicles and the IDS cloud server. To address this challenge, we propose a dynamic contract-based incentive mechanism that considers the dynamically changing privacy preference during long-term participation. The optimal contract is derived to maximize the expected utility of the IDS cloud server. Extensive simulation results demonstrate the feasibility of our proposed dynamic contract based incentive mechanism and validate the effectiveness of the LLM-based threat classification in handling complex threats.","1558-0016","","10.1109/TITS.2024.3461855","National Natural Science Foundation of China (NSFC)(grant numbers:62171352,62302387); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696923","Internet of Vehicles;vehicular honeypot;dynamic contract theory;privacy-preserving;incentive mechanism","Privacy;Contracts;Vehicle dynamics;Security;Collaboration;Costs;Servers","","","","45","IEEE","26 Sep 2024","","","IEEE","IEEE Journals"
"Large Model-Based Data Augmentation for Imbalanced Text Classification","D. Zhang; R. Mi; P. Zhou; D. Jin; M. Zhang; T. Song","School of Computer Science, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China; National Computer Network Emergency Response Technical, Team/Coordination Center of China, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","11 Jul 2024","2024","","","1006","1010","This study focuses on the application of large models to deal with imbalanced data problems in text classification. In view of the central position of text in web data and the negative impact of class imbalance on classifier performance, researchers have explored the method of using large models to generate high-quality minority class samples to enhance model performance. This paper reviews the technical progress of machine learning, deep learning, and large language models and their applications in text classification tasks. Although large models perform well in complex tasks due to their excellent language understanding ability, traditional machine learning and deep learning methods are popular in text classification scenarios that require fast response due to their simple structure and higher computational efficiency. This study proposes a data augmentation technique inspired by SMOTE, which uses a large language model combined with a simple prompt engineering strategy to generate high-quality minority samples. The experimental results show that the proposed method significantly improves the macro average precision, recall and F1 score on multiple text classification models, and effectively alleviates the challenge of class imbalance.","","979-8-3503-8555-7","10.1109/AINIT61980.2024.10581735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581735","component;Data augmentation;Imbalanced text classification;Large model","Deep learning;Seminars;Computational modeling;Large language models;Text categorization;Data augmentation;Data models","","","","12","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"Imperceptible Content Poisoning in LLM-Powered Applications","Q. Zhang; C. Zhou; G. Go; B. Zeng; H. Shi; Z. Xu; Y. Jiang","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Central South University, Changsha, China; Central South University, Changsha, China; Nanchang University, Nanchang, China; Tsinghua University, Beijing, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","242","254","Large Language Models (LLMs) have shown their superior capability in natural language processing, promoting extensive LLM-powered applications to be the new portals for people to access various content on the Internet. However, LLM-powered applications do not have sufficient security considerations on untrusted content, leading to potential threats. In this paper, we reveal content poisoning, where attackers can tailor attack content that appears benign to humans but causes LLM-powered applications to generate malicious responses. To highlight the impact of content poisoning and inspire the development of effective defenses, we systematically analyze the attack, focusing on the attack modes in various content, exploitable design features of LLM application frameworks, and the generation of attack content. We carry out a comprehensive evaluation on five LLMs, where content poisoning achieves an average attack success rate of 89.60%. Additionally, we assess content poisoning on four popular LLM-powered applications, achieving the attack on 72.00% of the content. Our experimental results also show that existing defenses are ineffective against content poisoning. Finally, we discuss potential mitigations for LLM application frameworks to counter content poisoning.CCS CONCEPTS• Computing methodologies → Machine learning; • Security and privacy;","2643-1572","979-8-4007-1248-7","","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764993","LLM Applications;Content Poisoning","Privacy;Prevention and mitigation;Large language models;Focusing;Machine learning;Natural language processing;Internet;Security;Portals;Software engineering","","","","57","","29 Nov 2024","","","IEEE","IEEE Conferences"
"RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model","Y. Lu; S. Liu; Q. Zhang; Z. Xie",Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology,2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC),"25 Mar 2024","2024","","","722","727","Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.","2153-697X","979-8-3503-9354-5","10.1109/ASP-DAC58780.2024.10473904","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473904","","Design automation;Natural languages;Asia;Benchmark testing;Syntactics;Chatbots;Hardware","","41","","11","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"Enhanced Sentiment Intensity Regression Through LoRA Fine-Tuning on Llama 3","D. Lin; Y. Wen; W. Wang; Y. Su","Faculty of Arts and Sciences, Beijing Normal University, Zhuhai, China; Faculty of Arts and Sciences, Beijing Normal University, Zhuhai, China; Faculty of Arts and Sciences, Beijing Normal University, Zhuhai, China; Faculty of Arts and Sciences, Beijing Normal University, Zhuhai, China",IEEE Access,"9 Aug 2024","2024","12","","108072","108087","Sentiment analysis and emotion detection are critical research areas in natural language processing (NLP), offering benefits to numerous downstream tasks. Despite the widespread application of pre-trained models and large language models (LLMs) in sentiment analysis, most previous works have focused on sentiment polarity or emotion classification, neglecting the finer-grained task of sentiment intensity regression, which prevents the precise capture of sentiment intensity and hindering model performance in complex scenarios and diverse applications. To address this issue, we enhance the Roberta model with an efficient additive attention mechanism and an adaptive weighted Huber loss function, notably improving its performance in sentiment intensity regression. Based on the SemEval 2017 and 2018 datasets, we employ prompt engineering to construct fine-tuned datasets, which are further enriched with outputs from the enhanced Roberta model. We then fine-tune the Llama 3 model using Low-Rank Adaptation (LoRA) within the Unsloth framework. Experimental results demonstrate that our enhanced RoBERTa model significantly outperforms baseline models. Furthermore, the enriched and LoRA fine-tuned Llama 3-8B model outperforms other LLMs with similar parameter scales. Our method improves MAE by 0.015 and MSE by 0.0054 on the SemEval 2018 dataset, achieving a Pearson correlation coefficient of 0.8441. On the SemEval 2017 dataset, it improves MAE by 0.0416 and MSE by 0.043, with a Pearson correlation coefficient increased to 0.8268, which demonstrates the superior predictive power and robustness of our approach.","2169-3536","","10.1109/ACCESS.2024.3438353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623139","LoRA;sentiment analysis;Llama 3;RoBERTa;adaptive weighted Huber loss function","Task analysis;Adaptation models;Sentiment analysis;Analytical models;Biological system modeling;Predictive models;Training","","3","","56","CCBYNCND","5 Aug 2024","","","IEEE","IEEE Journals"
"Can Developers Prompt? A Controlled Experiment for Code Documentation Generation","H. -A. Kruse; T. Puhlfürϐ; W. Maalej","Universitat Hamburg, Hamburg, Germany; Universitat Hamburg, Hamburg, Germany; Universitat Hamburg, Hamburg, Germany",2024 IEEE International Conference on Software Maintenance and Evolution (ICSME),"19 Dec 2024","2024","","","574","586","Large language models (LLMs) bear great potential for automating tedious development tasks such as creating and maintaining code documentation. However, it is unclear to what extent developers can effectively prompt LLMs to create concise and useful documentation. We report on a controlled experiment with 20 professionals and 30 computer science students tasked with code documentation generation for two Python functions. The experimental group freely entered ad-hoc prompts in a Chat- GPT-like extension of Visual Studio Code, while the control group executed a predefined few-shot prompt. Our results reveal that professionals and students were unaware of or unable to apply prompt engineering techniques. Especially students perceived the documentation produced from ad-hoc prompts as significantly less readable, less concise, and less helpful than documentation from prepared prompts. Some professionals produced higher quality documentation by just including the keyword Docstring in their ad-hoc prompts. While students desired more support in formulating prompts, professionals appreciated the flexibility of ad-hoc prompting. Participants in both groups rarely assessed the output as perfect. Instead, they understood the tools as support to iteratively refine the documentation. Further research is needed to understand which prompting skills and preferences developers have and which support they need for certain tasks.","2576-3148","979-8-3503-9568-6","10.1109/ICSME58944.2024.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795104","Software Documentation;Large Language Model;Program Comprehension;Developer Study;AI4SE","Computer science;Visualization;Software maintenance;Codes;Large language models;Documentation;Prompt engineering;Iterative methods;Python","","1","","71","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"LLM-based PID controller optimization","I. Kamenko; S. Ilic; V. Congradac","The Institute for Artificial Intelligence Research and Development of Serbia, Novi Sad, Serbia; The Institute for Artificial Intelligence Research and Development of Serbia, Novi Sad, Serbia; Department of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia",2025 10th International Conference on Smart and Sustainable Technologies (SpliTech),"30 Jul 2025","2025","","","1","6","This study explores the application of Large Language Models (LLMs) to optimize Proportional-Integral-Derivative (PID) controller parameters, aiming to automate a process traditionally reliant on manual, iterative tuning. Conventional methods often depend on heuristics and trial-and-error, requiring significant time and expertise. To address this, an LLM-based approach was developed that iteratively adjusts controller gains using real-time system feedback, substantially reducing the need for manual input. An adaptive tuning strategy, featuring predefined modes and levels of aggressiveness, was designed to balance response speed, minimize overshoot, and ensure system stability. The method was validated through computer simulations across a range of carefully selected process models and benchmarked against state-of-the-art expert-driven tuning techniques, demonstrating its effectiveness in reducing tuning time according to the selected mode while maintaining or improving system robustness. Results showed that LLMs, without additional model fine-tuning and guided solely by prompt engineering, can effectively optimize PID parameters. These findings demonstrate the promise of LLM-driven methods for advancing automation in PID controller tuning.","","978-953-290-142-9","10.23919/SpliTech65624.2025.11091752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091752","LLM;AI;PID controller;tuning;optimization","Adaptation models;Tuners;Computational modeling;Manuals;Robustness;Real-time systems;Stability analysis;Prompt engineering;Tuning;Optimization","","","","18","","30 Jul 2025","","","IEEE","IEEE Conferences"
"Experimental Study of In-Context Learning for Text Classification and Its Application to Legal Document Review in Construction Delay Disputes","N. Huber-Fliflet; J. Zhang; P. Gronvall; F. Wei; P. Spinelli","Data & Technology, Ankura Consulting Group, LLC, Washington, D.C., USA; Data & Technology, Ankura Consulting Group, LLC, Washington, D.C., USA; Data & Technology, Ankura Consulting Group, LLC, Washington, D.C., USA; Data & Technology, Ankura Consulting Group, LLC, Washington, D.C., USA; Construction Disputes & Advisory, Ankura Consulting Group, LLC, New York City, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","2119","2125","Text classification is a well-established area of machine learning that involves automatically categorizing text into predefined categories, such as positive or negative in sentiment analysis. It typically involves applying a machine learning algorithm to learn a predictive model from a set of labeled training texts and using the model to classify new texts. Large language models (LLMs) have been successfully applied to various natural language processing tasks, including text classification. There are two main approaches to text classification with LLMs: In-Context Learning and Fine-Tuning. In-Context Learning involves prompt engineering, allowing the model to learn a new task using only a few demonstration examples, while Fine-Tuning adjusts the model’s parameters with additional labeled data. Retrieval-Augmented Generation (RAG) is a retrieval process that enhances LLM performance by selecting relevant examples. This paper presents our work on utilizing In-Context Learning and RAG to identify delay-related statements during the document review process of construction delay disputes. We also report the results of our experiments comparing the accuracy of In-Context Learning with that of traditional machine learning algorithms, such as logistic regression and KNN.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10826061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10826061","Text classification;In-Context Learning;few-shot learning;zero-shot learning;Legal document review;construction delay dispute","Training;Sentiment analysis;Machine learning algorithms;Reviews;Text categorization;Retrieval augmented generation;Predictive models;Data models;Delays;Prompt engineering","","","","12","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"ChatGPT, Finances, and Degree Attainment: Increasing Generative Artificial Intelligence (AI) Utilization and Implications for Students' Decision-Making","T. Fletcher","School of Universal Computing Construction and Engineering Education (SUCCEED) Florida International University, Miami, FL",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","7","Students’ decision-making related to attending college for science, technology, engineering, and mathematics (STEM) degrees and retaining them within those fields continue to be of focus, especially when considering the national goal of satisfying industry, private, and public sector workforce needs. This is particularly important when considering growing national security concerns and ongoing high cybersecurity and climate change workforce demands. However, despite concerted efforts, low enrollment and student attrition continue to be a challenge, even more so since the onset of the global pandemic. Simultaneously, the utilization of generative artificial intelligence (AI) has seen a sharp increase since 2022 by various stakeholders, most notably college students. Additionally, faculty are encouraged by their higher education institutions to incorporate these tools within classrooms for several reasons, including demands from industry for students to gain knowledge and experience with generative AI tools. Given the importance of understanding the ongoing issue of STEM degree attrition and the increasing utilization of generative AI, this research study explored these two areas using ChatGPT, one of the world's most utilized generative AI language models, within the process. The first research question (RQ#l) is: What is the top reason why students do notfinish college? The response provided solely emphasized finances and/or financial constraints. Based on that answer, the second research question (RQ #2) was: Is getting a college degree worth the cost? RQ #2 was explored to allow a deeper understanding of the initial research question from a different angle. Both questions were analyzed through the lens of two theoretical frameworks, the human capital theory and the principal-agent theory, and results were compared to the literature as a form of research validation. This research study is essential as there is an ongoing increase in the utilization of generative AI in homes, K-12 institutions, and higher education for everyday knowledge, formal and informal education. When considering that soon-to-be and current college students may be using these generative AI tools to make decisions related to pursuing and/or retaining their educational status, it is essential to explore how generative AI system outputs align with what has been researched and documented in the literature. Therefore, this research study explored the application of ChatGPT and the system's responses to research questions as a form of document analysis and analyzed those results using triangulation as a form of validation. The analysis and results of this study contributed directly to the recommendations and feedback to the owners of ChatGPT - OpenAI. Additionally, the results highlighted a need for increased focus on AI ethics related to data input, data output, access to the system, and who has a seat at the table as the company recommends, approves, and implements changes. Limitations of this study are provided along with an overarching concluding statement.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10892857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892857","Attrition;STEM;Artificial Intelligence (AI);AI Ethics and Bias","Industries;Training;Ethics;Text analysis;Generative AI;Decision making;Chatbots;Regulation;Stakeholders;STEM","","2","","52","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"AADC-Net: A Multimodal Deep Learning Framework for Automatic Anomaly Detection in Real-Time Surveillance","D. Tri Phan; V. Hoang Minh Doan; J. Choi; B. Lee; J. Oh","Department of Biomedical Engineering, Pukyong National University, Busan, South Korea; Smart Gym-Based Translational Research Center for Active Senior’s Healthcare, Pukyong National University, Busan, South Korea; Smart Gym-Based Translational Research Center for Active Senior’s Healthcare, Pukyong National University, Busan, South Korea; Digital Healthcare Research Center, Institute of Information Technology and Convergence, Pukyong National University, Busan, South Korea; Ohlabs Corporation, Busan, South Korea",IEEE Transactions on Instrumentation and Measurement,"16 Apr 2025","2025","74","","1","13","Automatic anomaly detection (AAD) has emerged as an advanced vision-based measurement method with diverse applications in healthcare and security. However, current AAD methods still face challenges related to data limitations and labeled-data imbalances, which limit the accuracy and reliability of AAD in real-life applications. Additionally, labeling and training large datasets for video anomaly detection (VAD) is computationally demanding and time-consuming. To address these challenges, this work introduces AADC-Net, a multimodal deep neural network for automated abnormal event detection and categorization. The key contributions of this research are as follows: 1) AADC-Net leverages pretrained large language models (LLMs) and vision-language models (VLMs) to mitigate VAD dataset limitations and imbalances; 2) a pretrained object detection model [DEtection TRansformer (DETR)] is integrated for visual feature extraction, eliminating the need for bounding box supervision; 3) the experimental results demonstrate the state-of-the-art (SOTA) performance of the proposed AADC-Net with an area under the curve (AUC) of 83.2% and an average precision (AP) of 83.8% on the public UCF-Crime and XD-Violence datasets, respectively; and 4) additionally, AADC-Net can be integrated into existing video surveillance systems, such as those in smart gyms and healthcare facilities, to automatically detect anomalies in real time with minimal supervision, enhancing security, monitoring, and reducing labor costs while minimizing human error. In summary, our results demonstrate that AADC-Net not only achieves high accuracy in anomaly detection but also provides a practical solution for real-world surveillance applications.","1557-9662","","10.1109/TIM.2025.3551832","National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:2022R1A5A8023404); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10946244","Automatic anomaly detection (AAD);real-time surveillance;video anomaly detection (VAD);vision-based methods;vision-language model (VLM)","Visualization;Anomaly detection;Training;Surveillance;Feature extraction;Data models;Real-time systems;Computational modeling;Predictive models;Large language models","","","","77","IEEE","31 Mar 2025","","","IEEE","IEEE Journals"
"Exploiting Opportunistic Scheduling Schemes on Cooperative NOMA Networks Under Active Eavesdropper","K. Shim; S. -H. Park; B. -S. Kim; B. An","School of Computer Engineering and Applied Mathematics, Hankyong National University, Anseong, Republic of Korea; Department of Computer Science, Kookmin University, Seoul, Republic of Korea; Department of Software and Communications Engineering, Hongik University, Sejong, Republic of Korea; Department of Software and Communications Engineering, Hongik University, Sejong, Republic of Korea",IEEE Access,"2 Jul 2025","2025","13","","111484","111507","In this paper, we investigate secure transmission in a cooperative non-orthogonal multiple access (NOMA) system in the presence of an active eavesdropper. To enhance secrecy performance, we propose two approaches. The first is a novel cooperative NOMA architecture, in which the cell-edge user generates a jamming signal to degrade the eavesdropper’s channel condition. The second is a novel opportunistic scheduling scheme, referred to as the minimum antenna selection (MAS) scheme, which selects the transmit antenna at the source node to minimize the maximum eavesdropper channel capacity between the cell-center and cell-edge users. We derive closed-form expressions for the secrecy outage probability (SOP) of both the cell-center and cell-edge users to characterize the impact of various network parameters on secrecy performance. Through numerical results, we analyze the impact of the source, cell-center, and cell-edge users’ transmit power on the SOP; the impact of the power allocation coefficient, target secrecy rate, and eavesdropper transmit power on the total SOP; and the impact of the distance between the source node and the eavesdropper, as well as the distance between the cell-center user and the eavesdropper, on the secrecy sum throughput (SST). These results demonstrate that the proposed cooperative NOMA architecture and the MAS scheme can effectively protect confidential messages against active eavesdropping attacks.","2169-3536","","10.1109/ACCESS.2025.3582184","National Research Foundation of Korea (NRF); Korean Government Ministry of Science and Information and Communications Technology (MSIT)(grant numbers:NRF-2022R1A2B5B01001190); BK21 FOUR; Korea Institute of Marine Science and Technology Promotion (KIMST); Ministry of Oceans and Fisheries(grant numbers:KIMST RS-2021-KS211509); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11045892","Active eavesdropping;cooperative NOMA;opportunistic scheduling scheme;physical layer security","NOMA;Eavesdropping;Transmitting antennas;Relays;Scheduling;Jamming;Base stations;Wireless networks;Surveillance;Oral communication","","","","34","CCBY","20 Jun 2025","","","IEEE","IEEE Journals"
"Intention is All you Need: Refining your Code from your Intention","Q. Guo; X. Xie; S. Liu; M. Hu; X. Li; L. Bu","Tianjin University, P.R. China; Singapore Management University, Singapore; State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China; Singapore Management University, Singapore; Tianjin University, P.R. China; State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1127","1139","Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements. As software projects scale in size and complexity, the traditional iterative exchange between re-viewers and developers becomes increasingly burdensome. While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents. This paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments. Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation. Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions. Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification. Extensive experiments with five LLMs (G PT 40, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79 % accuracy in intention extraction and up to 66 % in code refinement generation. Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029878","code refinement;intention-based generation;large language model","Deep learning;Codes;Accuracy;Large language models;Data integrity;Refining;Software;Complexity theory;Iterative methods;Software engineering","","1","","49","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Thelma: Threat Hunting Enhanced Language Models for Hunt Automation","R. Lin",National Taiwan University of Science and Technology,2024 IEEE Conference on Communications and Network Security (CNS),"31 Oct 2024","2024","","","1","9","Threat hunting is a proactive security approach that seeks to identify and respond to cyber threats beyond traditional defenses. It’s related to digital forensics, focusing on collecting evidence of malicious activities. Despite various tools and methods available, threat hunting often demands skilled human intervention for complex threats. This paper introduces Thelma, an agent framework that enhances the threat-hunting process by employing large language models (LLMs). Thelma uses two LLMs: one for prioritizing threat hypotheses and another for generating specific threat-hunting queries. This approach streamlines the threat-hunting cycle, starting with hypothesis prioritization, followed by query generation and execution, which then informs the status update of the targeted environment. The cycle repeats until no threats remain. It is demonstrated that with minimal prompt-tuning and fine-tuning, the LLMs can reason about the environment and generate threat-hunting queries to be executed on diverse devices.","2994-5895","979-8-3503-7596-1","10.1109/CNS62487.2024.10735642","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735642","","Automation;Large language models;Digital forensics;Focusing;Network security","","1","","22","IEEE","31 Oct 2024","","","IEEE","IEEE Conferences"
"Enhancing Project-Specific Code Completion by Inferring Internal API Information","L. Deng; X. Ren; C. Ni; M. Liang; D. Lo; Z. Liu","State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Ant Group, China; School of Computing and Information Systems, Singapore Management University, Singapore; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China",IEEE Transactions on Software Engineering,"","2025","PP","99","1","17","Project-specific code completion, which aims to complete code based on the context of the project, is an important and practical software engineering task. The state-of-the-art approaches employ the retrieval-augmented generation (RAG) paradigm and prompt large language models (LLMs) with information retrieved from the target project for project-specific code completion. In practice, developers always define and use custom functionalities, namely internal APIs, to facilitate the implementation of specific project requirements. Thus, it is essential to consider internal API information for accurate project-specific code completion. However, existing approaches either retrieve similar code snippets, which do not necessarily contain related internal API information, or retrieve internal API information based on import statements, which usually do not exist when the related internal APIs haven’t been used in the file. Therefore, these project-specific code completion approaches face challenges in effectiveness or practicability. To this end, this paper aims to enhance project-specific code completion by locating internal API information without relying on import statements. We first propose a method to infer internal API information. Our method first extends the representation of each internal API by constructing its usage examples and functional semantic information (i.e., a natural language description of the function’s purpose) and constructs a knowledge base. Based on the knowledge base, our method uses an initial completion solution generated by LLMs to infer the API information necessary for completion. Based on this method, we propose a code completion approach that enhances project-specific code completion by integrating similar code snippets and internal API information. Furthermore, we developed a benchmark named ProjBench, which consists of recent, large-scale real-world projects and is free of leaked import statements. We evaluated the effectiveness of our approach on ProjBench and an existing benchmark CrossCodeEval. Experimental results show that our approach outperforms the base-performing approach by an average of +5.91 in code exact match and +6.26 in identifier exact match, corresponding to relative improvements of 22.72% and 18.31%, respectively. We also show our method complements existing ones by integrating it into various baselines, boosting code match by +7.77 (47.80%) and identifier match by +8.50 (35.55%) on average.","1939-3520","","10.1109/TSE.2025.3592823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11096713","Project-specific code completion;Large language model;API information;Retrieval-augmented generation","Codes;Benchmark testing;Semantics;Large language models;Knowledge based systems;Training;Retrieval augmented generation;Data mining;Software development management;Programming","","","","","IEEE","25 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Lightweight LLM-Based Anomaly Detection Framework for Securing IoTMD Enabled Diabetes Management Control Systems","I. W. A. J. Pawana; P. V. Astillo; I. You","Department of Financial Information Security, Kookmin University, Seoul, South Korea; Department of Computer Engineering, University of San Carlos, Cebu, Philippines; Department of Financial Information Security, Kookmin University, Seoul, South Korea",IEEE Journal of Biomedical and Health Informatics,"","2025","PP","99","1","12","The adoption of Implantable Internet of Things Medical Devices (IoTMD) has revolutionized chronic disease management by enabling continuous monitoring and real-time data transmission, allowing patients to optimize treatment strategies. However, these advancements come with significant security risks, as IoTMD systems remain vulnerable to cyber threats that could compromise patient data and device functionality. Addressing this challenge, this study evaluates the fine-tuning performance of various lightweight Large Language Models (LLMs) for anomaly detection in IoTMD-enabled diabetes management control systems (DMCS). Among the evaluated models, LLaMA 3.2 1B-Instruct, fine-tuned with Low-Rank Adaptation (LoRA), achieves the highest performance, with 99.91% accuracy, perfect precision (100.00%), and a false positive rate of 0%. Comparative analysis against other lightweight LLMs—GPT-2, Phi-1 (1.3B), and Gemma 2B-Instruct—as well as traditional deep learning models such as IL-MLP, IL-CNN, FL-MLP, and FL-CNN, highlights the superior adaptability and robustness of transformer-based architectures in anomaly detection. These findings demonstrate the effectiveness of LLMs in securing IoTMD systems, providing a powerful solution for mitigating cyber threats while ensuring system reliability. The results underscore the potential of LLM-based anomaly detection in strengthening IoTMD cybersecurity, paving the way for safer and more reliable implantable medical devices in modern healthcare settings.","2168-2208","","10.1109/JBHI.2025.3577604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11027716","Large language model;intrusion detection system;artificial intelligence;healthcare","Medical services;Anomaly detection;Diabetes;Insulin;Computer security;Medical devices;Internet of Things;Insulin pumps;Control systems;Intrusion detection","","","","","IEEE","9 Jun 2025","","","IEEE","IEEE Early Access Articles"
"SoK: On-Device Large Language Model Personalization for Meta Computing","Y. Shi; Z. Jia; Z. Shen; M. Zhao","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China",2024 International Conference on Meta Computing (ICMC),"9 Jul 2025","2024","","","267","276","Meta computing has been developed to fully exploit all available computing resources, especially the computational capability of terminal IoT devices, to complete the targeting task. This computing paradigm aims to provide efficient, fault-tolerant, and personalized services while maintaining robust security and privacy for any computing task. Meanwhile, Large Language Models (LLMs), which are among the most widely used AI applications, have shown exceptional capabilities in tasks such as natural language understanding, language generation, and complex reasoning. LLMs have the potential to significantly impact our society. As LLMs become more prevalent as personalized intelligent assistants, deploying them on resource-constraint terminal devices and enabling on-device personalization (i.e., customizing models through fine-tuning) will be increasingly demanding due to the advantages of quick response and data privacy preservation. When the meta computing paradigm comes across the AI application, an urgent but open question is: How to fully utilize the capabilities of resource-constrained computing platforms to perform on-device LLM personalization? Current research on this topic is fragmented and lacks a comprehensive overview. This paper seeks to fill this gap by organizing and synthesizing previous studies in this area. We offer a thorough review of on-device LLM personalization research, categorizing the literature into a taxonomy that includes four main categories, each covering interconnected topics from data-focused, model-focused, optimizer-focused, and distribution-focused perspectives. Our goal is to serve as a valuable resource, helping researchers and practitioners gain a comprehensive understanding of the advancements in on-device LLM personalization and inspiring further contributions to this vital and dynamic field.","","979-8-3503-5599-4","10.1109/ICMC60390.2024.00036","Natural Science Foundation of Shandong Province(grant numbers:ZR2022LZH010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11062748","Meta computing;large language model;on-device learning;personalization;IoT","Surveys;Technological innovation;Privacy;Reviews;Large language models;Computational modeling;Taxonomy;Memory management;Natural language processing;Security","","","","75","IEEE","9 Jul 2025","","","IEEE","IEEE Conferences"
"The Digital Cybersecurity Expert: How Far Have We Come?","D. Wang; G. Zhou; X. Li; Y. Bai; L. Chen; T. Qin; J. Sun; D. Li",Zhongguancun Laboratory; Zhongguancun Laboratory; Zhongguancun Laboratory; Zhongguancun Laboratory; Zhongguancun Laboratory; Zhongguancun Laboratory; Zhongguancun Laboratory; Tsinghua University,2025 IEEE Symposium on Security and Privacy (SP),"16 Jun 2025","2025","","","3273","3290","The increasing deployment of large language models (LLMs) in the cybersecurity domain underscores the need for effective model selection and evaluation. However, traditional evaluation methods often overlook specific cybersecurity knowledge gaps that contribute to performance limitations. To address this, we develop CSEBenchmark, a fine-grained cyber-security evaluation framework based on 345 knowledge points expected of cybersecurity experts. Drawing from cognitive science, these points are categorized into factual, conceptual, and procedural types, enabling the design of 11,050 tailored multiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find that even the best-performing model achieves only 85.42% overall accuracy, with particular knowledge gaps in the use of specialized tools and uncommon commands. Different LLMs have unique knowledge gaps. Even large models from the same family may perform poorly on knowledge points where smaller models excel. By identifying and addressing specific knowledge gaps in each LLM, we achieve up to an 84% improvement in correcting previously incorrect predictions across three existing benchmarks for two cybersecurity tasks. Furthermore, our assessment of each LLM's knowledge alignment with specific cybersecurity roles reveals that different models align better with different roles, such as GPT-4o for the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon Privacy Engineer. These findings underscore the importance of aligning LLM selection with the specific knowledge requirements of different cybersecurity roles for optimal performance.","2375-1207","979-8-3315-2236-0","10.1109/SP61157.2025.00198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023378","large language model;benchmark;cybersecurity","Knowledge engineering;Privacy;Analytical models;Accuracy;Large language models;Benchmark testing;Cognitive science;Internet;Computer security","","","","73","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"On Text Granularity and Metric Frameworks for Large Language Model Content Detection","L. Le; D. Tran","College of Computing and Software Engineering, Kennesaw State University, Kennesaw, Georgia, USA; Brigham and Women’s Hospital, Harvard Medical School, Boston, Massachusetts, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","8301","8308","Breakthroughs in Large Language Models (LLMs) have allowed Artificial Intelligence (AI) assistant systems to provide quality information with conveniences. An issue is paralleling the advantages, however. One among the problems of LLM generated content is that they seem indistinguishable from that of human which leads to numerous issues in areas like science, education, information security, etc. Furthermore, approaches in LLM content detection are either computationally expensive or need the LLMs’ internal computations which make them more difficult to be used by the public. Addressing the research gap, we present a metric learning framework for LLM text detection that is balanced for resources, accessibility, and performance. Specifically, the detection framework relies on metric learning to evaluate the similarity between a given text to an equivalent example from LLMs and verify whether the former is from human or AI. The framework can be trained in triplets or pairs of text instances from the same contexts at either the full-text or the sentence granularity levels. For benchmarking, five corpora totalling over 95,000 contexts and responses from human and GPT-3.5 TURBO or GPT-4 TURBO are developed. In term of performance, our architectures maintain 0.87 to 0.95 F1 scores throughout multiple experiment settings. Our framework also requires much less time in training and inference compared to RoBERTa, LLaMA 3, and Ghostbuster, while having 90% to 150% performances of the best benchmark.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825505","LLM text detection;metric learning;text granularity;triplet learning;contrastive learning","Measurement;Training;Computational modeling;Large language models;Training data;Text detection;Computer architecture;Benchmark testing;Transformers;Vectors","","","","47","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Compromising LLM Driven Embodied Agents With Contextual Backdoor Attacks","A. Liu; Y. Zhou; X. Liu; T. Zhang; S. Liang; J. Wang; Y. Pu; T. Li; J. Zhang; W. Zhou; Q. Guo; D. Tao","State Key Laboratory of Complex and Critical Software Environment (SKLCCSE) and the School of Computer Science and Engineering (SCSE), Beihang University, Beijing, China; State Key Laboratory of Complex and Critical Software Environment (SKLCCSE) and the School of Computer Science and Engineering (SCSE), Beihang University, Beijing, China; State Key Laboratory of Complex and Critical Software Environment (SKLCCSE) and the School of Computer Science and Engineering (SCSE), Beihang University, Beijing, China; State Key Laboratory of Complex and Critical Software Environment (SKLCCSE) and the School of Computer Science and Engineering (SCSE), Beihang University, Beijing, China; School of Computing, National University of Singapore, Queenstown, Singapore; Zhongguancun Laboratory, Beijing, China; Zhongguancun Laboratory, Beijing, China; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore; School of Cyberspace Security, University of Science and Technology of China, Hefei, China; School of Cyberspace Security, University of Science and Technology of China, Hefei, China; Agency for Science, Technology and Research (A*STAR), Fusionopolis, Singapore; College of Computing and Data Science, Nanyang Technological University, Jurong West, Singapore",IEEE Transactions on Information Forensics and Security,"21 Apr 2025","2025","20","","3979","3994","Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations (such as rationales and solution examples) developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called Contextual Backdoor Attack. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a closed-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM’s contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning. To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers. We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents. To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning. Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems. The contextual backdoor threat introduced in this study poses serious risks for millions of downstream embodied agents, given that most publicly available LLMs are third-party-provided. This paper aims to raise awareness of this critical threat. Our code and demos are available at https://contextual-backdoor.github.io/.","1556-6021","","10.1109/TIFS.2025.3555410","National Natural Science Foundation of China(grant numbers:62206009); Fundamental Research Funds for the Central Universities, Grants from the State Key Laboratory of Complex and Critical Software Environment (SKLCCSE); Chinese Aeronautical Establishment(grant numbers:20230017051001); Outstanding Research Project of the Shen Yuan Honors College, Beihang University.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943262","Embodied agent;contextual backdoor attacks;incontext learning","Visualization;Codes;Training;Context modeling;Toxicology;Data models;Cognition;Autonomous vehicles;Closed box;Threat modeling","","","","65","IEEE","27 Mar 2025","","","IEEE","IEEE Journals"
"LLM-Driven Testing for Autonomous Driving Scenarios","N. Petrovic; K. Lebioda; V. Zolfaghari; A. Schamschurko; S. Kirchner; N. Purschke; F. Pan; A. Knoll","Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany; Chair of Robotics, Artificial Intelligence and Real-Time Systems, Technical University of Munich, Munich, Germany",2024 2nd International Conference on Foundation and Large Language Models (FLLM),"28 Jan 2025","2024","","","173","178","In this paper, we explore the potential of leveraging Large Language Models (LLMs) for automated test generation based on free-form textual descriptions in area of automotive. As outcome, we implement a prototype and evaluate the proposed approach on autonomous driving feature scenarios in CARLA open-source simulation environment. Two pre-trained LLMs are taken into account for comparative evaluation: GPT-4 and Llama3. According to the achieved results, GPT-4 outperforms Llama3, while the presented approach speeds-up the process of testing (more than 10 times) and reduces cognitive load thanks to automated code generation and adoption of flexible simulation environment for quick evaluation.","","979-8-3503-5479-9","10.1109/FLLM63129.2024.10852505","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10852505","autonomous driving;CARLA;Generative Pre-Trained Transformer (GPT);Large Language Model (LLM);Llama3;Model-Driven Engineering (MDE)","Codes;Large language models;Prototypes;Transformers;Cognitive load;Test pattern generators;Autonomous vehicles;Testing;Load modeling;Automotive engineering","","","","19","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Architectural Framework and Standardization of LLM-Enhanced Intelligent Data Analysis Systems","X. Han; J. Ma; S. Yu; Y. Liu; P. Ma; C. Wang","China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China; China Academy of Information and Communications Technology, Beijing, China",2024 International Conference on Ubiquitous Computing and Communications (IUCC),"2 Jul 2025","2024","","","573","578","In the era of data as a pivotal resource, analyzing and mining data have become critical to enterprises' digital and intelligent transformation. However, with the continuous growth of data types and volumes, traditional Business Intelligence (BI) tools face challenges such as slow processing speeds, inability to handle complex queries, and limitations in multimodal data integration. The rise of Large Language Model (LLM) technology has drawn widespread attention to its potential to empower the field of data analysis. This paper proposes an intelligent data analysis system that integrates LLMs with BI tools, leveraging the capabilities of large models in multimodal data processing, natural language understanding, automated code generation, and intelligent data insights. The system simplifies data analysis workflows and enhances both efficiency and accuracy. From the perspectives of system architecture design, core technological capabilities, standardization requirements, and evaluations of multiple products, this paper aims to explore the current state and future trends of intelligent data analysis systems powered by large language models.","","979-8-3315-1199-9","10.1109/IUCC65928.2024.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11049478","Large Language Model;Business Intelligence;Data Analysis","Industries;Large language models;Data integration;Systems architecture;Standardization;Lead;Ubiquitous computing;Market research;Natural language processing;Business intelligence","","","","18","IEEE","2 Jul 2025","","","IEEE","IEEE Conferences"
"Enhancing Identifier Naming Through Multi-Mask Fine-Tuning of Language Models of Code","S. Vijayvargiya; M. Saad; T. Sharma","BITS Pilani, Hyderabad, India; Dalhousie University, Halifax, Canada; Dalhousie University, Halifax, Canada",2024 IEEE International Conference on Source Code Analysis and Manipulation (SCAM),"19 Dec 2024","2024","","","71","82","Code readability strongly influences code compre-hension and, to some degree, code quality. Unreadable code makes software maintenance more challenging and is prone to more bugs. To improve the readability, using good identifier names is crucial. Existing studies on automatic identifier re-naming have not considered aspects such as the code context. Additionally, prior research has done little to address the typical challenges inherent in the identifier renaming task. In this paper, we propose a new approach for renaming identifiers in source code by fine-tuning a transformer model. Through the use of perplexity as an evaluation metric, our results demonstrate a significant decrease in the perplexity values for the fine-tuned approach compared to the baseline, reducing them from 363 to 36. To further validate our method, we conduct a developers' survey to gauge the suitability of the generated identifiers, comparing original identifiers with identifiers generated with our approach as well as two state-of-the-art large language models, GPT-4 Turbo and Gemini Pro. Our approach generates better identifier names than the original names and exhibits competitive performance with state-of-the-art commercial large language models. The proposed method carries significant implications for software developers, tool vendors, and researchers. Software developers may use our proposed approach to generate better variable names, increasing the clarity and readability of the software. Researchers in the field may use and build upon the proposed approach for variable renaming.","2470-6892","979-8-3315-2850-8","10.1109/SCAM63643.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795342","","Training;Surveys;Software maintenance;Codes;Source coding;Large language models;Predictive models;Transformers;Usability;Software development management","","","","56","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"SSRFSeek: An LLM-based Static Analysis Framework for Detecting SSRF Vulnerabilities in PHP Applications","Y. Zhou; E. Wang; S. Ma","College of Computer Science and Technology National University of Defense Technology, Changsha, China; College of Computer Science and Technology National University of Defense Technology, Changsha, China; College of Computer Science and Technology National University of Defense Technology, Changsha, China","2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","23 Jun 2025","2025","","","939","944","Server-side request Forgery (SSRF) vulnerabilities pose significant security risks to web applications, and the attack surface for SSRF continues to expand with the widespread adoption of microservices architecture and cloud services. However, existing research on SSRF vulnerability detection still has significant shortcomings, particularly the lack of systematic modeling for framework-specific implementations and precise analysis of complex code logic. This paper proposes SSRFSeek, a PHP static analysis framework based on large language models, to detect SSRF vulnerabilities in web applications. SSRFSeek consists of two phases: in the vulnerability modeling phase, a chain-of-thought-based document analysis method is used to guide the large language model in automatically extracting source and SSRF sink from API documentation; in the vulnerability analysis phase, taint analysis based on code property graphs is combined with taint flow effectiveness inference based on large language models to reduce false positives effectively. We evaluated SSRFSeek (built upon the DeepSeek R1 model) on 6 PHP applications, successfully identifying 2 known vulnerabilities and 5 previously unknown vulnerabilities while reducing the false positive rate by 41.7 %, confirming the framework's capability in SSRF vulnerability detection. All discovered vulnerabilities have been responsibly reported to the affected vendors and received 3 new CVE numbers.","","979-8-3315-2228-5","10.1109/AINIT65432.2025.11035424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035424","Server-Side Request Forgery;Large Language Model;Static Analysis","Analytical models;Codes;Text analysis;Systematics;Large language models;Static analysis;Documentation;Forgery;Security;Logic","","","","24","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Protocol-Agnostic and Data-Free Backdoor Attacks on Pre-Trained Models in RF Fingerprinting","T. Zhao; N. Wang; J. Zhang; X. Wang","Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, US; Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, US; Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, US; Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, FL, US",IEEE INFOCOM 2025 - IEEE Conference on Computer Communications,"1 Jul 2025","2025","","","1","10","While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.","2641-9874","979-8-3315-4305-1","10.1109/INFOCOM55648.2025.11044704","NSF(grant numbers:CNS-2415209,CNS-2321763,CNS-2317190,IIS-2306791,CNS-2319343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11044704","Backdoor Attack;Pre-trained Model;Radio Frequency Fingerprinting;Security","Radio frequency;Training;Time-frequency analysis;Protocols;Large language models;LoRa;Focusing;Implants;Fingerprint recognition;Wireless fidelity","","","","43","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"An Invitation to Deep Reinforcement Learning","B. Jaeger; A. Geiger",NA; NA,An Invitation to Deep Reinforcement Learning,"","2025","","","","","Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning if the target objective is differentiable. However, this is not the case for many interesting problems. Common objectives like intersection over union (IoU) and bilingual evaluation understudy (BLEU) scores or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection, or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time-intensive to approach due to the large range of methods, as well as the often highly theoretical presentation. This monograph takes an alternative approach that is different from classic RL textbooks. Rather than focusing on tabular problems, RL as a generalization of supervised learning is introduced, which is first applied to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this monograph.","","9781638284413","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10829522.pdf&bkn=10829521&pdfType=book","Optimization","","","","","","","6 Jan 2025","","","now","Now Foundations and Trends Books"
"Confidential and Protected Disease Classifier using Fully Homomorphic Encryption","A. Malik; N. Ratha; B. Yalavarthi; T. Sharma; A. Kaushik; C. Jutla","University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; IBM Research, USA",2024 IEEE Conference on Artificial Intelligence (CAI),"30 Jul 2024","2024","","","365","370","With the rapid surge in the prevalence of Large Language Models (LLMs), individuals are increasingly turning to conversational AI for initial insights across various domains, including health-related inquiries such as disease diagnosis. Many users seek potential causes on platforms like ChatGPT or Bard before consulting a medical professional for their ailment. These platforms offer valuable benefits by streamlining the diagnosis process, alleviating the significant workload of healthcare practitioners, and saving users both time and money by avoiding unnecessary doctor visits. However, Despite the convenience of such platforms, sharing personal medical data online poses risks, including the presence of malicious platforms or potential eavesdropping by attackers. To address privacy concerns, we propose a novel framework combining FHE and Deep Learning for a secure and private diagnosis system. Operating on a question-and-answer-based model akin to an interaction with a medical practitioner, this end-to-end secure system employs Fully Homomorphic Encryption (FHE) to handle encrypted input data. Given FHE’s computational constraints, we adapt deep neural networks and activation functions to the encryted domain. Further, we also propose a faster algorithm to compute summation of ciphertext elements. Through rigorous experiments, we demonstrate the efficacy of our approach. The proposed framework achieves strict security and privacy with minimal loss in performance.","","979-8-3503-5409-6","10.1109/CAI59869.2024.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605346","Disease classifier;Fully Homomorphic Encryption;Deep Learning;Privacy","Deep learning;Adaptation models;Accuracy;Computational modeling;Discrete Fourier transforms;Classification algorithms;Cryptography","","1","","27","IEEE","30 Jul 2024","","","IEEE","IEEE Conferences"
"Recommendation of Best Practices for Industrial Agent Systems based on the IEEE 2660.1 Standard","P. Leitão; T. I. Strasser; S. Karnouskos; L. Ribeiro; J. Barbosa; V. Huang","Research Center in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Bragança, Portugal; AIT Austrian Institute of Technology, TU Wien, Vienna, Austria; SAP, Walldorf, Germany; Linköping University, Linköping, Sweden; MORE Colab, Bragança, Portugal; Sage Technology Resources, CA, USA",2021 22nd IEEE International Conference on Industrial Technology (ICIT),"18 Jun 2021","2021","1","","1157","1162","Cyber-Physical Systems (CPS) is a key concept in Industry 4.0, acting as a backbone to develop smart processes, machines and products. Multi-Agent Systems (MAS) is a suitable paradigm for the realization of such industrial CPS systems, supporting the distribution of intelligence and decision-making capabilities among a network of autonomous and cooperative agents. Standardization is a key factor for the acceptance of industrial CPS and agent-based systems, assuming a critical role in establishing specifications for the hardware integration, which is an important requirement for industrial agents. This paper focuses on the recently established IEEE 2660.1-2020 standard that defines a recommended practice to solve the interface problem when applying industrial agents, namely integrating intelligent software agents with low-level automation devices in the CPS context. The paper illustrates the applicability of the standard in three different application scenarios related to power and energy systems, factory automation and building automation, and discusses future directions in terms of standardization in the field of industrial agents, that are required for its wider adoption in the realization of industrial CPS solutions.","","978-1-7281-5730-6","10.1109/ICIT46573.2021.9453511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453511","","Conferences;Decision making;Cyber-physical systems;Software agents;Hardware;Standards;Manufacturing automation","","8","","28","IEEE","18 Jun 2021","","","IEEE","IEEE Conferences"
"Out-of-Distribution Detection for Neurosymbolic Autonomous Cyber Agents","A. Samaddar; N. Potteiger; X. Koutsoukos","Department of Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Computer Science, Vanderbilt University, Nashville, TN, USA",2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC),"29 Jan 2025","2025","","","1","9","Autonomous agents for cyber applications take advantage of modern defense techniques by adopting intelligent agents with conventional and learning-enabled components. These intelligent agents are trained via reinforcement learning (RL) algorithms, and can learn, adapt to, reason about and deploy security rules to defend networked computer systems while maintaining critical operational workflows. However, the knowledge available during training about the state of the operational network and its environment may be limited. The agents should be trustworthy so that they can reliably detect situations they cannot handle, and hand them over to cyber experts. In this work, we develop an out-of-distribution (OOD) Monitoring algorithm that uses a Probabilistic Neural Network (PNN) to detect anomalous or OOD situations of RL-based agents with discrete states and discrete actions. To demonstrate the effectiveness of the proposed approach, we integrate the OOD monitoring algorithm with a neurosymbolic autonomous cyber agent that uses behavior trees with learning-enabled components. We evaluate the proposed approach in a simulated cyber environment under different adversarial strategies. Experimental results over a large number of episodes illustrate the overall efficiency of our proposed approach.","","979-8-3315-1888-2","10.1109/ICAIC63015.2025.10849024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849024","cyber-security;neurosymbolic AI;out-of-distribution (OOD);probabilistic neural network (PNN)","Training;Knowledge engineering;Hands;Neural networks;Reinforcement learning;Probabilistic logic;Intelligent agents;Reliability;Computer security;Monitoring","","","","24","IEEE","29 Jan 2025","","","IEEE","IEEE Conferences"
"Transformer-Based Federated Learning Models for Recommendation Systems","M. Sujaykumar Reddy; H. Karnati; L. Mohana Sundari","School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India",IEEE Access,"13 Aug 2024","2024","12","","109596","109607","In today’s data-driven environment, safeguarding user privacy is a top priority, particularly in machine learning applications. Our study introduces an innovative approach that combines the privacy-preserving attributes of federated learning with the advanced capabilities of transformer-based models, specifically tailored for recommendation systems. Federated learning emerges as a decentralized alternative to traditional machine learning, enhancing both user privacy and data security. Our research employs two distinct transformer models: BERT (Bidirectional Encoder Representations from Transformers) and BST (Behavior Sequence Transformer), within a federated learning context. The models performance is analyzed using the Amazon Customer Review and movielens-1m datasets. The empirical results are compelling: the federated BERT model achieves a notable 87% and 76% accuracy in the global model for 2 different datasets. Similarly, the federated BST model demonstrates a performance with an mean absolute error of 0.8. This research not only highlights the effectiveness of federated learning in boosting model accuracy but also emphasizes its crucial role in preserving user privacy. Our findings illustrate that integrating federated learning can lead to enhanced performance in recommendation systems without sacrificing data privacy. Consequently, this research marks a significant step forward in developing more effective, privacy-conscious machine learning solutions, contributing to the broader field of ethical and responsible AI.","2169-3536","","10.1109/ACCESS.2024.3439668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630471","Federated learning;recommendation systems;information privacy;data security;transformers;deep learning","Transformers;Recommender systems;Federated learning;Accuracy;Privacy;Data privacy;Biological system modeling","","6","","27","CCBYNCND","7 Aug 2024","","","IEEE","IEEE Journals"
"Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference","J. Kundu; W. Guo; A. BanaGozar; U. De Alwis; S. Sengupta; P. Gupta; A. Mallik","IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; IMEC, Leuven, Belgium; UCLA, California, US; IMEC, Leuven, Belgium",2024 IEEE International Symposium on Workload Characterization (IISWC),"28 Nov 2024","2024","","","57","67","Aligning future system design with the ever-increasing compute needs of large language models (LLMs) is undoubtedly an important problem in today’s world. Here, we propose a general performance modeling methodology and workload analysis of distributed LLM training and inference through an analytical framework that accurately considers compute, memory sub-system, network, and various parallelization strategies (model parallel, data parallel, pipeline parallel, and sequence parallel). We validate our performance predictions with published data from literature and relevant industry vendors (e.g., NVIDIA). For distributed training, we investigate the memory footprint of LLMs for different activation re-computation methods, dissect the key factors behind the massive performance gain from A100 to B200 (∼ 35x speed-up closely following NVIDIA’s scaling trend), and further run a design space exploration at different technology nodes (12 nm to 1 nm) to study the impact of logic, memory, and network scaling on the performance. For inference, we analyze the compute versus memory boundedness of different operations at a matrix-multiply level for different GPU systems and further explore the impact of DRAM memory technology scaling on inference latency. Utilizing our modeling framework, we reveal the evolution of performance bottlenecks for both LLM training and inference with technology scaling, thus, providing insights to design future systems for LLM training and inference.","2835-2238","979-8-3503-5603-8","10.1109/IISWC63097.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763669","","Training;Performance evaluation;Analytical models;Computational modeling;Large language models;Systems architecture;Telecommunication traffic;Data models;Space exploration;System analysis and design","","2","","35","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"A Hybrid Transformer Ensemble Approach for Phishing Website Detection","K. S. Mandapati; S. Meesala; D. Maddela; K. Ponnada; H. Neyyala; E. A. Shaik","Dept.of CSE GMR Institue of Technology, Rajam, India; Dept.of CSE GMR Institue of Technology, Rajam, India; Dept.of CSE GMR Institue of Technology, Rajam, India; Dept.of CSE GMR Institue of Technology, Rajam, India; Dept.of CSE GMR Institue of Technology, Rajam, India; Dept.of CSE GMR Institue of Technology, Rajam, India",2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),"6 Dec 2023","2023","","","1","8","The internet has developed into a veritable informational gold mine in the era of the digital age, and URLs (Uniform Resource Locator) act as access points to enormous volumes of material. For a variety of applications, including web content classification, fraud detection, and information retrieval, it is essential to extract useful properties from URLs. The proposed approach investigates the potential for extracting useful information from URLs using transformer-based models. Transformers can be configured to decode the underlying patterns and semantics in URLs, which frequently display complicated structures and variations. Due to their exceptional abilities in natural language processing, they are used to effectively encode and represent the properties of the provided URLs. The approach uses pre-trained transformer models like BERT (Bidirectional Encoder Representation from Transformers), optimizing their feature extraction performance on unique URLs. Later, a stacking strategy is used that combines machine learning models to further improve accuracy. Stacking is a potent ensemble approach and makes use of the diverse strengths of various models to provide predictions that are more reliable and accurate. By providing a quick and accurate method to identify phishing websites, the proposed approach makes contribution to field of internet security and protects consumers from potential cyber risks.","","979-8-3503-0085-7","10.1109/ICSSAS57918.2023.10331880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331880","Phishing Detection;Machine Learning;BERT;Accuracy;Cyber Security","Uniform resource locators;Phishing;Stacking;Semantics;Predictive models;Transformers;Ensemble learning","","1","","23","IEEE","6 Dec 2023","","","IEEE","IEEE Conferences"
"CDA: Covert Deception Attacks in Multi-Agent Resource Scheduling","W. Hao; J. Liu; W. Li; L. Chen","Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China",IEEE Robotics and Automation Letters,"18 Sep 2024","2024","9","11","9215","9222","In this letter, we address the critical security concerns in multi-agent systems, where illegal infiltration is commonly used to convert agents into malicious entities. Existing research predominantly focuses on explicit malicious attack patterns. Our work introduces a covert deception attack framework in the context of multi-agent resource scheduling scenarios. We first highlight vulnerabilities in scheduling strategies based on time and path costs. Exploiting these weaknesses, an infiltrated agent clandestinely gathers motion characteristics of other agents while posing as a teammate. Using these motion characteristics, the infiltrated agent employs an LSTM architecture to learn and predict congestion areas, thereby designing attack paths with greater time efficiency. This approach allows the infiltrated agent to secure additional resources and evade capture more effectively. Validation through simulation and real-world experiments demonstrates the feasibility and effectiveness of our approach, underscoring the importance of evaluating covert attacks in risk assessments within multi-agent systems.","2377-3766","","10.1109/LRA.2024.3455765","National Natural Science Foundation of China(grant numbers:62072231); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669201","Cooperating robots;robot safety;swarm robotics;task and motion planning","Job shop scheduling;Planning;Predictive models;Heat maps;Feature extraction;Costs;Schedules","","1","","29","IEEE","6 Sep 2024","","","IEEE","IEEE Journals"
"Leveraging Deep Learning and Feature Extraction for Robust Anomaly Detection in Network Traffic","S. Agarwala; A. T. Quanita; O. Jahan Sagor; A. S. M. Shahiduzzaman Sajid; A. Yakin Srizon; M. F. Faruk; S. M. Mahedy Hasan","Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh",2024 27th International Conference on Computer and Information Technology (ICCIT),"10 Jun 2025","2024","","","2208","2213","Anomaly detection in network traffic is critical for identifying potential security breaches, such as phishing, malware, or Denial of Service (DoS) attacks. Traditional signature-based Intrusion Detection Systems (IDS) have proven inadequate in identifying novel threats, prompting the adoption of machine learning (ML) and deep learning (DL) approaches. This study explores the integration of advanced ML and DL models with feature extraction techniques such as Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec to enhance anomaly detection capabilities. Models including Random Forest (RF), Support Vector Machines (SVM), Dense Neural Networks (DNN), and Long Short-Term Memory (LSTM) were applied to a real-world dataset of malicious URLs. The experiments demonstrate that deep learning models, particularly DNN and LSTM, out-perform traditional methods in detecting complex anomalies, achieving accuracies of 89.66% and 89.27%, respectively. This research provides insights into the application of hybrid ML and DL techniques for robust and scalable anomaly detection systems. The study also highlights the potential of future advancements in Transformer models and ensemble learning for improving real-time detection in dynamic and high-dimensional network environments.","2474-9656","979-8-3315-1909-4","10.1109/ICCIT64611.2024.11022455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11022455","Anomaly detection;Intrusion Detection Systems;Machine Learning;Long Short-Term Memory;Word2Vec;Network Security;Transformers","Deep learning;Support vector machines;Intrusion detection;Telecommunication traffic;Transformers;Feature extraction;Real-time systems;Anomaly detection;Long short term memory;Random forests","","","","11","IEEE","10 Jun 2025","","","IEEE","IEEE Conferences"
"Generative Phishing URL Detection Based on Large Language Model","B. Zhou; J. Liu","Shenzhen Institute of Advanced Technology, CAS, Sangfor Technologies Inc., Shenzhen, China; Shenzhen Institute of Advanced Technology, CAS, Shenzhen, China","2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","23 Jun 2025","2025","","","1838","1842","Phishing is a form of cyberattack in which the attacker pretends to be a trustworthy entity to trick users into providing sensitive information. The most common form of phishing is URL-based phishing, which uses fake URLs to trick users into entering sensitive information, leading to serious security risks. The core of the phishing URL detection task is to accurately identify forged URLs and prevent potential threats. Although existing detection methods have achieved certain results, there are still some shortcomings, including only giving labels without giving reasons for judgment, and not being able to handle complex and changeable URLs. To alleviate these deficiencies, and inspired by the deep semantic understanding capabilities of large language models, we propose a phishing URL detection model based on large language models. In this work, we transform the traditional classification paradigm in phishing URL detection into a generation paradigm to increase the interpretability of model judgment and improve the detection performance. At the same time, by doing so, we also fill the gap of underutilization of large models in phishing URL detection. We have conducted sufficient experiments, and the results show that the proposed model is superior to existing models in terms of detection accuracy and other metrics. At the same time, the proposed model requires less training data and also shows better generalization ability.","","979-8-3315-2228-5","10.1109/AINIT65432.2025.11035267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11035267","phishing url;generation;large language model","Uniform resource locators;Measurement;Seminars;Phishing;Large language models;Semantics;Training data;Transforms;Security;Information technology","","","","20","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Augmenting Cybersecurity and Fraud Detection Using Artificial Intelligence Advancements","K. Kumar; Kuldeep; B. Bhushan","Department of Computer Science and Engineering, Sharda University, Greater Noida, India; Department of Computer Science and Engineering, Sharda University, Greater Noida, India; Department of Computer Science and Engineering, Sharda University, Greater Noida, India","2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)","15 Feb 2024","2023","","","1207","1212","In 2023, the rapid advancement of Generative AI (GenAI) models, including ChatGPT and Google Bard, has reshaped the digital landscape. GenAI tools are now used both defensively and offensively, raising crucial questions about their societal, ethical, and privacy implications. Artificial Intelligence (AI) plays a pivotal role in the Fourth Industrial Revolution, enhancing the security of computer networks. This study investigates these connections, finding that e-Governance partially mediates the link between AI and cybersecurity, with stakeholders influencing this dynamic. AI's increasing integration into daily life, spanning healthcare, finance, and surveillance, emphasizes the importance of data privacy. The handling of extensive datasets by AI systems brings about unique privacy and ethical challenges, necessitating stringent safeguards. This concise abstract underscore the profound impact of GenAI models on cybersecurity in 2023, emphasizing their implications for society, ethics, and privacy. It highlights nuanced relationships between AI, e-Governance, and cybersecurity, while acknowledging the need for robust privacy protection. The paper calls attention to the potential of advanced techniques to fortify cybersecurity against ever-evolving threats.","","979-8-3503-0611-8","10.1109/ICCCIS60361.2023.10425069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425069","Artificial Intelligence;Cyber-security;GenAI","Privacy;Ethics;Data privacy;Internet;Stakeholders;Artificial intelligence;Computer security","","6","","25","IEEE","15 Feb 2024","","","IEEE","IEEE Conferences"
"Voltage Control for Distribution Networks Based on Large Language Model-Assisted Deep Reinforcement Learning","L. Yan; C. Cheng","School of Electrical Information Engineering, Northeast Petroleum University, Daqing, China; School of Electrical Information Engineering, Northeast Petroleum University, Daqing, China",IEEE Access,"6 May 2025","2025","13","","76072","76084","With the continuous integration of large-scale distributed energy resources into distribution networks, numerous challenges arise regarding security, stability, and economic performance, particularly voltage violations and increased network losses. Furthermore, existing deep reinforcement learning (DRL) methods often rely on extensive real-world operational data for agent training. Yet, the lack of diversity in the collected data can significantly limit the generalization ability of agents under varying operating conditions. This paper proposes a regional voltage optimization control strategy for distribution networks to address these issues based on DRL supported by large language model (LLM). By integrating LLM technologies with DRL, the approach leverages prompt engineering to guide large-language models in generating customized datasets for DRL agent training, enabling data augmentation. This reduces the dependence on real-world data while improving the generalizability of agents. The proposed control strategy was validated on modified IEEE 33-bus and 123-bus distribution systems. The experimental results effectively mitigate voltage violations and reduce network losses while exhibiting strong robustness and generalization under various operating conditions.","2169-3536","","10.1109/ACCESS.2025.3565280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10979848","Deep reinforcement learning;large language model;voltage control;multi-agent;data augmentation;data-driven","Distribution networks;Voltage control;Optimization;Training;Training data;Renewable energy sources;Prompt engineering;Data models;Biological system modeling;Knowledge engineering","","1","","36","CCBY","29 Apr 2025","","","IEEE","IEEE Journals"
"DKFuzz: An Automated Approach for Testing the Underlying Kernels of Deep Learning Frameworks","H. Li; X. Li; Y. Nie; S. Du; Q. Wu","NKLSTISS, Institute of Systems Engineering, Academy of Military Sciences, Beijing, China; NKLSTISS, Institute of Systems Engineering, Academy of Military Sciences, Beijing, China; NKLSTISS, Institute of Systems Engineering, Academy of Military Sciences, Beijing, China; NKLSTISS, Institute of Systems Engineering, Academy of Military Sciences, Beijing, China; NKLSTISS, Institute of Systems Engineering, Academy of Military Sciences, Beijing, China","2025 2nd International Conference on Algorithms, Software Engineering and Network Security (ASENS)","27 May 2025","2025","","","163","175","Fuzz testing has emerged as an important technique for security testing in deep learning frameworks. However, existing fuzz testing methods primarily focus on model-level testing and API-level testing, while overlooking the core components of high-level APIs (kernels). Notably, vulnerabilities in kernels often involve high-priority memory safety issues, which can directly cause system crashes, thereby posing significant threats to the security and stability of deep learning system.In this paper, we propose DKFuzz, a novel method designed to discover vulnerabilities in the underlying kernels of deep learning frameworks. DKFuzz builds upon and improves the state-of-the-art fuzz testing engine, IvySyn. Specifically, DKFuzz introduces two key innovations:First, to target the underlying kernels, DKFuzz designs a set of diverse mutation strategies to expand the kernel test input data pool. Upon detecting a kernel crash, DKFuzz parses the core configuration files of the deep learning framework to construct a bidirectional mapping relationship between the kernel and high-level APIs. Based on this mapping relationship, DKFuzz can automatically synthesize high-level API code snippets. By executing these snippets, DKFuzz can accurately reproduce the original kernel crash.Second, for uncovered kernels, DKFuzz adopts a directed kernel coverage approach. By leveraging the bidirectional mapping relationship obtained through configuration file parsing, DKFuzz identifies the corresponding high-level APIs and combines them with prompt engineering techniques powered by large language models to generate high-level unit test cases for these APIs. By executing these test cases, DKFuzz further enhances kernel coverage. To evaluate the effectiveness of DKFuzz, we conducted experiments on two widely used deep learning frameworks, PyTorch and TensorFlow, and compared the results with IvySyn. The experimental results demonstrate that DKFuzz achieves significant improvements in both kernel coverage and vulnerability discovery capabilities. These findings highlight the potential of DKFuzz as a robust and effective tool for security testing in deep learning frameworks.","","979-8-3315-2029-8","10.1109/ASENS64990.2025.11011223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011223","Deep Learning Framework;Kernel;Fuzz","Deep learning;Codes;Large language models;Fuzzing;Computer crashes;Stability analysis;Prompt engineering;Kernel;Thermal stability;Testing","","","","17","IEEE","27 May 2025","","","IEEE","IEEE Conferences"
"The role of library versions in Developer-ChatGPT conversations","R. Raj; D. E. Costa","Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),"18 Jun 2024","2024","","","172","176","The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.","2574-3864","979-8-4007-0587-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555730","","Vocabulary;Codes;Reviews;Oral communication;Chatbots;Libraries;Software","","","","23","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Diffusion-Geo: A Two-Stage Controllable Text-To-Image Generative Model for Remote Sensing Scenarios","M. Cai; W. Zhang; T. Zhang; Y. Zhuang; H. Chen; L. Chen; C. Li","National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China; Advanced Research Institute of Multidisciplinary Sciences, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China; National Key Laboratory of Science and Technology on Space-Born Intelligent Information Processing, Beijing Institute of Technology, Beijing, China",IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium,"5 Sep 2024","2024","","","7003","7006","Image generation is a crucial task to facilitate intelligent interpretation in remote sensing domain. Expanding dataset size through image generation can enhance model performance of downtown task. However, current generative models in remote sensing are mostly unconditional or guided by simple text, resulting in generated images lacking spatial and semantic constraints. This lack of control can negatively optimize downstream task models. To tackle these challenges, a two-stage controllable text-image generative model called Diffusion-Geo is presented. In the first stage, an extensive image-text generation dataset called RS-Control is created through prompt engineering of multimodal large language models (MLLMs) and manual prompts for existing datasets, incorporates diverse conditional controls with rich spatial and semantic information. Then RS-Control dataset is utilized to train a universal controllable image generative model. The second stage involves efficient tuning the universal model for different task datasets, minimizing fine-tuning costs while preserving diversity and high-quality features. Experiments conducted on the RSICD caption dataset and WHU change detection dataset demonstrate the superiority of Diffusion-Geo over other state-of-the-art models in image generation.","2153-7003","979-8-3503-6032-5","10.1109/IGARSS53475.2024.10641523","National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10641523","controllable text-to-image generation;diffusion;remote sensing","Training;Image synthesis;Semantics;Text to image;Data models;Sensors;Prompt engineering","","1","","8","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"Integrating ChatGPT-4: A Novel XAI Interface for Enhanced Clinician Understanding of MRI Image Segmentation Results","G. Grillo; T. Torda; C. Voena; A. Ciardiello; S. Giagu","Istituto Nazionale di Fisica Nucleare, Università Campus Bio-Medico di Roma, Rome, Italy; Physics Department, Sapienza University of Rome, Rome, Italy; Rome Division, Istituto Nazionale di Fisica Nucleare, Rome, Italy; Physics Department, Sapienza University of Rome, Rome, Italy; Physics Department, Sapienza University of Rome, Rome, Italy",2024 IEEE 37th International Symposium on Computer-Based Medical Systems (CBMS),"25 Jul 2024","2024","","","320","325","The poor explainability or interpretability of deep learning makes it difficult to introduce these technologies into the hospital workflow. Explainable Artificial Intelligence (XAI) addresses this problem by providing interpretable explanations of AI decisions. We present a system that employs large language models (LLM) with an image-to-text approach, converting AI results of medical segmentation into understandable natural language descriptions. This study first explores how ChatGPT-4 can interpret and describe Magnetic Resonance Image (MRI) brain scans and their segmentations both without previous input and with specific details. In particular, the effectiveness of 'prompt engineering' in improving the accuracy and relevance of language-based artificial intelligence model responses was investigated. Through a series of controlled experiments, we aim to demonstrate how the accurate and strategic formulation of prompts can significantly influence the behaviour of even ChatGPT-4, highlighting its potential but above all the care required when using it. Simultaneously, a user-friendly interface was developed to facilitate interactions between radiologists and the AI segmentation model via ChatGPT-4 generated explanations. This integrated approach aims to improve the reliability, transparency, and usability of AI-supported medical image analysis, potentially leading to more informed clinical decisions and better patient outcomes.","2372-9198","979-8-3503-8472-7","10.1109/CBMS61543.2024.00060","CHIST-ERA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601103","LLM;explainable AI;user interface;brain tumour","Image segmentation;Accuracy;Explainable AI;Magnetic resonance imaging;Natural languages;Reliability;Prompt engineering","","","","17","IEEE","25 Jul 2024","","","IEEE","IEEE Conferences"
"Introducing ChatGPT: The AI Revolution in Project Management","K. Bainey",NA,AI-Driven Project Management: Harnessing the Power of Artificial Intelligence and ChatGPT to Achieve Peak Productivity and Success,"","2024","","","3","8","Summary <p>This chapter explores the essence of ChatGPT: what it is, how to access it, and why every modern project manager should grasp its potential and utilize it. Project managers face challenges every day, but with these challenges come great opportunities for innovation and growth through generative artificial intelligence tools like ChatGPT. This introduction presents an overview of the key concepts discussed in the subsequent chapters of this book. The book gives an overview of the foundation, laying the groundwork by emphasizing the revolutionary features of ChatGPT as well as impacts and relevant ethical issues relating to project management‐artificial Intelligence (PM‐AI). It offers a deep look into practical applications using ChatGPT for accurate project forecasting, professional development, and blending human interaction for PM‐AI. The book introduces PM‐AI modality model, which integrates AI technologies like large language models and prompt engineering.</p>","","9781394232239","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10950555.pdf&bkn=10950176&pdfType=chapter","","Chatbots;Artificial intelligence;Project management;Decision making;Translation;Time factors;Productivity;Planning;Data analysis;Uniform resource locators","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Movable Antenna Enhanced Secure Simultaneous Wireless Information and Power Transfer","X. Dong; W. Lyu; R. Yang; Y. Xiu; W. Mei; Z. Zhang","National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China, Chengdu, China",IEEE Communications Letters,"","2025","PP","99","1","1","In this letter, we investigate a secure simultaneous wireless information and power transfer (SWIPT) system with movable antennas (MAs). To improve the energy harvesting (EH) while ensuring physical layer security (PLS), we aim to maximize the harvested energy by jointly optimizing the transmit beamforming vectors, artificial noise (AN) covariance matrix, and MA positions. To solve this non-convex problem, we propose a block coordinate descent (BCD) algorithm based on successive convex approximation (SCA) technique. Numerical results show that the proposed method converges fast and achieves significant performance improvement in terms of the harvested energy compared to other benchmarks.","1558-2558","","10.1109/LCOMM.2025.3595280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11108293","Movable antenna;physical layer security;artificial noise;simultaneous wireless information and power transfer","Array signal processing;Simultaneous wireless information and power transfer;Optimization;Vectors;Transmitting antennas;Radio frequency;Receivers;MIMO;Internet of Things;Covariance matrices","","","","","IEEE","4 Aug 2025","","","IEEE","IEEE Early Access Articles"
"An End-to-End GPT-4o Informed Pipeline for Manipulation Task Planning with Sim2Real Validation","S. Guo; T. Tian; J. Zhou; Z. Liu; L. Zhou; R. Zhao; Z. Huang; C. Yuan; M. H. Ang","Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore; Advanced Robotics Centre, National University of Singapore, Singapore",2025 10th International Conference on Control and Robotics Engineering (ICCRE),"31 Jul 2025","2025","","","99","104","Nowadays, the integration of task planning for robotic manipulation with LLMs is at the frontier of research, where LLMs act as critics or assistants, serving various purposes such as feasibility prediction, object pose estimation, motion trajectory planning, and obstacle avoidance. Despite the promising progress in this field, most of these works still focus on leveraging LLMs to enhance the performance of a single part of robotics task planning, while neglecting the integration of LLMs into the overall task planning process from scratch to finish. Thus resulting in potential risks to the system’s generalizability and adaptability in real-world experiments. To address the above challenges, we propose a novel end-to-end pipeline where GPT40 are globally integrated into the entire task planning process. This pipeline encompasses scene setup and description, task planning, relevant code generation, and sim-to-real validation, reducing labor costs while improving the efficiency of task planning, thereby demonstrating the feasibility and advantages of our pipeline. The demo video of our work can be seen here: https://youtu.be/u2rkymOjVQ4","2835-3722","979-8-3315-4351-8","10.1109/ICCRE65455.2025.11093514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11093514","Manipulation Task Planning;Large Language Model;Sim2real","Codes;Costs;Trajectory planning;Large language models;Pipelines;Pose estimation;Cognition;Planning;Collision avoidance;Robots","","","","21","IEEE","31 Jul 2025","","","IEEE","IEEE Conferences"
"Optimized Frequency Collaborative Strategy Drives AI Image Detection","J. Li; W. Jiang; L. Shen; Y. Ren","School of Computer, Artificial Intelligence Innovation Team, Beijing Information Science and Technology University, Beijing, China; School of Computer, Artificial Intelligence Innovation Team, Beijing Information Science and Technology University, Beijing, China; School of Computer, Artificial Intelligence Innovation Team, Beijing Information Science and Technology University, Beijing, China; School of Computer, Artificial Intelligence Innovation Team, Beijing Information Science and Technology University, Beijing, China",IEEE Internet of Things Journal,"20 May 2025","2025","12","11","16192","16203","Artificial intelligence (AI) image generation powered by large language model (large language modelss (LLMs)) enables highly realistic image synthesis and manipulation, posing significant security risks to Internet of Things (IoT) systems, particularly in identity authentication and data integrity. Although multidomain synthetic image detection has advanced, how spatial and frequency domain features affect decision making is still an open question, causing models to emphasize less critical areas and fall into local optimality. Through multidomain empirical analysis, we reveal the common contrastive differences in image textures and further demonstrate that frequency analysis helps capture the spectral differences in images. Building on this, we propose the collaborative spatial and frequency detector (CSFD). First, the image is decomposed into strong and weak texture regions in the spatial domain. Second, it aggregates different components in the frequency domain, using weighted channel attention to enhance spatial reasoning. Finally, the texture regions are combined to discriminate synthetic images. Experimental results demonstrate that incorporating channel attention based on frequency information improves the detection of synthetic images with spectral defects. On a comprehensive AI-generated image detection benchmark, the proposed method improves accuracy by 2.61% over current methods. Our code is available at https://github.com/JackPotProject/Frequency-Collaborative.","2327-4662","","10.1109/JIOT.2025.3531053","Basic Research Project of the Translational Application Project of the “Wise Eyes Action”(grant numbers:F2B6A194); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10848481","artificial intelligence (AI) image detection;deep learning;domain fusion;Internet of Things (IoT) security","Frequency-domain analysis;Feature extraction;Internet of Things;Detectors;Filters;Security;Collaboration;Semantics;Image synthesis;Discrete cosine transforms","","","","72","IEEE","21 Jan 2025","","","IEEE","IEEE Journals"
"Commit Message Generation via ChatGPT: How Far are We?","Y. Wu; Y. Li; S. Yu","Peking University, Beijing, China; Peking University, Beijing, China; Shenzhen (CUHK-Shenzhen), The Chinese University of Hong Kong, Shenzhen, China",2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","124","129","Commit messages concisely describe code changes in natural language and are important for software maintenance. Various automatic commit message generation approaches have been proposed, such as retrieval-based, learning-based, and hybrid approaches. Recently, large language models have shown impressive performance in many natural language processing tasks. Among them, ChatGPT is the most popular one and has attracted wide attention from the software engineering community. ChatGPT demonstrates the ability of in-context learning (ICL), which allows ChatGPT to perform downstream tasks by learning from just a few demonstrations without explicit model tuning. However, it remains unclear how well ChatGPT performs in the commit message generation task via ICL. Therefore, in this paper, we conduct a preliminary evaluation of ChatGPT with ICL on commit message generation. Specifically, we first explore the impact of two key settings on the performance of ICL on commit message generation. Then, based on the best settings, we compare ChatGPT with several state-of-the-art approaches. The results show that a carefully-designed demonstration can lead to substantial improvements for ChatGPT on commit message generation. Furthermore, ChatGPT outperforms all the retrieval-based and learning-based approaches in terms of BLEU, METEOR, ROUGE-L, and Cider, and is comparable to hybrid approaches. Based on our findings, we outline several open challenges and opportunities for ChatGPT-based commit message generation.","","979-8-4007-0536-6","10.1145/3650105.3652300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599581","Commit Message Generation;Large Language Model;In-Context Learning","Software maintenance;Codes;Terminology;Large language models;Chatbots;Hybrid power systems;Meteors","","2","","51","","30 Jul 2024","","","IEEE","IEEE Conferences"
"Learning to Communicate Using Action Probabilities for Multi-Agent Cooperation","Y. Bai; T. Sugawara","Dept. Computer Science and Engineering, Waseda University, Tokyo, Japan; Dept. Computer Science and Engineering, Waseda University, Tokyo, Japan",2023 IEEE International Conference on Agents (ICA),"29 Jan 2024","2023","","","31","36","While communication is an essential tool for cooperation in multi-agent systems (MAS), existing approaches generally assume high-dimensional messages genereated by deep networks, which are uninterpretable for humans and require expensive transmission and complex encoding/decoding networks in agents. Uninterpretability may in turn raise reliability and security issues for the systems. To achieve low-dimensional and interpretable communication, we demonstrate that each agent's action probabilities can be used as messages, inspired by the fact that humans often share likely actions during collaboration. Our proposed method, communication based on action probabilities (CAP), is a simple yet effective communication architecture for coordination and collaboration in MAS, which facilitates our understanding of the agents' learned coordinated and cooperative behaviors. Moreover, because messages can be generated by the actor network, which is usually implemented in deep reinforcement learning agents, we can eliminate the requirement of extra message-generator networks. Our experiments show that CAP can achieve comparable performance to those of the state-of-the-art methods, with quicker convergence, simpler network structures and better interpretability.","","979-8-3503-1278-2","10.1109/ICA58824.2023.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405416","Multi-agent deep reinforcement learning;Cooperation;Communication;Interpretability","Deep learning;Collaboration;Reinforcement learning;Security;Reliability;Multi-agent systems;Convergence","","1","","22","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Attribution-guided Adversarial Code Prompt Generation for Code Completion Models","X. Li; G. Meng; S. Liu; L. Xiang; K. Sun; K. Chen; X. Luo; Y. Liu","Institute of Information Engineering, CAS, School of Cybersecurity, UCAS, China; Institute of Information Engineering, CAS, School of Cybersecurity, UCAS, China; Nanyang Technological University, Singapore; Institute of Information Engineering, CAS, School of Cybersecurity, UCAS, China; Institute of Information Engineering, CAS, School of Cybersecurity, UCAS, China; Institute of Information Engineering, CAS, School of Cybersecurity, UCAS, China; The Hong Kong Polytechnic University, China; Nanyang Technological University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1460","1471","Large language models have made significant progress in code completion, which may further remodel future software development. However, these code completion models are found to be highly risky as they may introduce vulnerabilities unintentionally or be induced by a special input, i.e., adversarial code prompt. Prior studies mainly focus on the robustness of these models, but their security has not been fully analyzed.In this paper, we propose a novel approach AdvPro that can automatically generate adversarial code prompts for these code completion models. AdvPro incorporates 14 code mutation strategies at the granularity of five levels. The mutation strategies are ensured to make no modifications to code semantics, which should be insensitive to the models. Moreover, we leverage gradient attribution to localize the important code as mutation points and speed up adversarial prompt generation. Extensive experiments are conducted on 13 state-of-the-art models belonging to 7 families. The results show that our approach can effectively generate adversarial prompts, with an increased rate of 69.6% beyond the baseline ALERT. By comparing the results of attribution-guided localization, we find that the recognition results of important tokens in input codes are almost identical among different models. This finding reduces the limitation of using open-source alternative models to guide adversarial attacks against closed-source models. The results of the ablation study on the components of AdvPro show that CCMs focus on variable names, but other structures are equally crucial.CCS CONCEPTS• Software and its engineering → Software testing and debugging; • Security and privacy → Software security engineering.","2643-1572","979-8-4007-1248-7","","Nova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764985","Adversarial prompts;code completion models;attribution-guided localization","Location awareness;Software testing;Analytical models;Codes;Semantics;Software;Robustness;Security;Software engineering;Software development management","","","","48","","29 Nov 2024","","","IEEE","IEEE Conferences"
"Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities","H. Zhou; C. Hu; Y. Yuan; Y. Cui; Y. Jin; C. Chen; H. Wu; D. Yuan; L. Jiang; D. Wu; X. Liu; J. Zhang; X. Wang; J. Liu","School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; School of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada; Standards and Mobility Innovation Laboratory, Samsung Research America, Plano, TX, USA; Department of Electrical and Computer Engineering, Western University, London, ON, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada",IEEE Communications Surveys & Tutorials,"12 Jun 2025","2025","27","3","1955","2005","Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.","1553-877X","","10.1109/COMST.2024.3465447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685369","Large language model;telecommunications;generation;classification;prediction;optimization","Telecommunications;6G mobile communication;Optimization;Surveys;Sensors;Training;Reinforcement learning","","45","","229","IEEE","23 Sep 2024","","","IEEE","IEEE Journals"
"Failure Management Overview in Optical Networks","S. Cruzes","Department of Optical Network Engineering, Ciena Corporation, Ciena Office Brazil, São Paulo, Brazil",IEEE Access,"19 Nov 2024","2024","12","","169170","169193","Conventional optical networks are limited by static operational methods that hinder their scalability and effectiveness. As networks operate with reduced margins to maximize resource utilization, the risk of hard failures increases, necessitating efficient failure prediction systems and accurate quality of transmission (QoT) estimation. Effective management requires the detection of soft failures, accurate bit error rate (BER) predictions, and dynamic network operations to maintain minimal margins. Machine learning (ML) offers promising solutions for automating these tasks, significantly enhancing failure management and network reliability. This article provides an extensive overview of ML techniques applied to optical networks, specifically focusing on failure management. The key ML techniques discussed include network kriging (NK) for performance estimation and failure localization, support vector machine (SVM) for classification tasks, convolutional neural networks (CNNs) for signal analysis and soft failure identification, and generative adversarial networks (GANs) for synthetic data generation and soft failure detection. It also explores the application of artificial neural networks (ANNs), autoencoders (AEs), Gaussian process (GP), long short-term memory (LSTM), and gated recurrent units (GRUs) in optical networks. This study surveys ML techniques for early-warning and failure prediction, failure detection, identification, localization, magnitude estimation, and soft failure detection and prediction. Emphasizing automation, it discusses how ML algorithms can streamline failure management processes, reducing manual intervention and service disruptions. The potential of large language models (LLMs) and digital twins (DTs) for further advancements in automating failure management, optimizing performance, and network optimization in optical networks is also examined. LLMs significantly advance network management by improving network design, diagnosis, security, and autonomous optimization through the integration of comprehensive domain resources and intelligent agents. These advancements are paving the way towards achieving artificial general intelligence and fully automated optical network management.","2169-3536","","10.1109/ACCESS.2024.3498704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10752984","Optical networks;failure management;quality of transmission;machine learning","Location awareness;Support vector machines;Maximum likelihood estimation;Accuracy;Failure analysis;Optical fiber networks;Optical imaging;Optical fiber filters;Optimization;Long short term memory","","4","","115","CCBYNCND","14 Nov 2024","","","IEEE","IEEE Journals"
"ECG: Augmenting Embedded Operating System Fuzzing via LLM-Based Corpus Generation","Q. Zhang; Y. Shen; J. Liu; Y. Xu; H. Shi; Y. Jiang; W. Chang","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; KLISS, BNRist, School of Software, Tsinghua University, Beijing, China; KLISS, BNRist, School of Software, Tsinghua University, Beijing, China; KLISS, BNRist, School of Software, Tsinghua University, Beijing, China; School of Electronic Information, Central South University, Changsha, China; KLISS, BNRist, School of Software, Tsinghua University, Beijing, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,"6 Nov 2024","2024","43","11","4238","4249","Embedded operating systems (Embedded OSs) power much of our critical infrastructure but are, in general, much less tested for bugs than general-purpose operating systems. Fuzzing Embedded OSs encounter significant roadblocks due to much less documented specifications, an inherent ineffectiveness in generating high-quality payloads. In this article, we propose ECG, an Embedded OS fuzzer empowered by large language models (LLMs) to sufficiently mitigate the aforementioned issues. ECG approaches fuzzing Embedded OS by automatically generating input specifications based on readily available source code and documentation, instrumenting and intercepting execution behavior for directional guidance information, and generating inputs with payloads according to the pregenerated input specifications and directional hints provided from previous runs. These methods are empowered by using an interactive refinement method to extract the most from LLMs while using established parsing checkers to validate the outputs. Our evaluation results demonstrate that ECG uncovered 32 new vulnerabilities across three popular open-source Embedded OS (RT-Linux, RaspiOS, and OpenWrt) and detected ten bugs in a commercial Embedded OS running on an actual device. Moreover, compared to Syzkaller, Moonshine, KernelGPT, Rtkaller, and DRLF, ECG has achieved additional kernel code coverage improvements of 23.20%, 19.46%, 10.96%, 15.47%, and 11.05%, respectively, with an overall average improvement of 16.02%. These results underscore ECG’s enhanced capability in uncovering vulnerabilities, thus contributing to the overall robustness and security of the Embedded OS.","1937-4151","","10.1109/TCAD.2024.3447220","National Key Research and Development Program of China(grant numbers:2023YFB4503704); NSFC Program(grant numbers:62202500); Natural Science Foundation of Hunan Province(grant numbers:2023JJ40772); Open Fund of Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation(grant numbers:CSSAE-2023-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10745813","Embedded operating system (Embedded OS);fuzz testing;vulnerability detection","Integrated circuits;Source coding;Large language models;Computer bugs;Electrocardiography;Fuzzing;Robustness;Security;Kernel;Payloads","","3","","34","IEEE","6 Nov 2024","","","IEEE","IEEE Journals"
"Integrating LLM with CNN-RNN for Optimizing Crop Production","K. Gupta; K. Rani; P. Ajmani; V. Sharma; A. Alkhayyat","Computer Science and Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; Computer Science and Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; Vivekananda Institute of Professional Studies-TC; Computer Science Department, CHRIST University, Bengaluru, India; College of Technical Engineering, The Islamic University, Najaf, Iraq",2024 International Conference on Intelligent & Innovative Practices in Engineering & Management (IIPEM),"21 Mar 2025","2024","","","1","6","This study establishes a hybrid methodology combining LLM, CNN, and RNN for predicting crop yield, and detailed coverage of each area of the hybrid is set as follows-. In general, traditional methods have great difficulties with analyzing agricultural data presented as complex non-linear data. Our suggested approach utilizes the extraction capabilities of CNN, the temporal sequence modeling of RNN, and the sophisticated data processing of LLM, and this hybrid approach produced high accuracy levels for tested crops including wheat, maize, millet, rice, and barley, as well as significantly better precision, recall and f-score cutoff levels than traditional methods. This research contributes to ongoing developments toward improved food security and agriculture output through the implementation of sophisticated machine learning models. This paper presents a novel hybrid model for predicting crops yields by combining Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) with Large Language Models (LLMs). Because agricultural data is challenging to model as traditional algorithms typically struggle with complex, inherently non-linear data, our modeling approach leverages the capabilities of sophisticated LLMs for processing data, temporal sequence modeling capabilities of RNNs, and data extraction capabilities of CNNs. Our hybrid model can produce high accuracy outcomes from the model experimentation using standard measures of output accuracy from a number of crop species, including barley, rice, millet, maize and wheat, and the model also produced significant and meaningful improvements over previous studies with traditional methods.","","979-8-3503-9004-9","10.1109/IIPEM62726.2024.10925778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925778","Artificial Intelligence;large language model;Convolutional Neural Network;Accuracy;Agricultural yield","Accuracy;Recurrent neural networks;Large language models;Food security;Machine learning;Predictive models;Data models;Convolutional neural networks;Data mining;Crop yield","","1","","18","IEEE","21 Mar 2025","","","IEEE","IEEE Conferences"
"PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning","Z. Sun; T. Cong; Y. Liu; C. Lin; X. He; R. Chen; X. Han; X. Huang","The Hong Kong University of Science and Technology, Guangzhou; BNRist, Tsinghua University; The Hong Kong University of Science and Technology, Guangzhou; Xi'an Jiaotong University; The Hong Kong University of Science and Technology, Guangzhou; National University of Defense Technology; Nanyang Technological University; Jinan University",2025 IEEE Symposium on Security and Privacy (SP),"16 Jun 2025","2025","","","1713","1731","Fine-tuning is an essential process to improve the performance of Large Language Models (LLMs) in specific domains, with Parameter-Efficient Fine-Tuning (PEFT) gaining popularity due to its capacity to reduce computational demands through the integration of low-rank adapters. These lightweight adapters, such as LoRA, can be shared and utilized on open-source platforms. However, adversaries could exploit this mechanism to inject backdoors into these adapters, resulting in malicious behaviors like incorrect or harmful outputs, which pose serious security risks to the community. Unfortunately, few current efforts concentrate on analyzing the backdoor patterns or detecting the backdoors in the adapters. To fill this gap, we first construct and release PADBench, a comprehensive benchmark that contains 13, 300 benign and backdoored adapters fine-tuned with various datasets, attack strategies, PEFT methods, and LLMs. Moreover, we propose PEFTGuard, the first backdoor detection framework against PEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard outperforms existing detection methods, achieving nearly perfect detection accuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot transferability on three aspects, including different attacks, PEFT methods, and adapter ranks. In addition, we consider various adaptive attacks to demonstrate the high robustness of PEFTGuard. We further explore several possible backdoor mitigation defenses, finding fine-mixing to be the most effective method. We envision that our benchmark and method can shed light on future LLM backdoor detection research. 11Our code and dataset are available at: https://github.com/Vincent-HKUSTGZ/PEFTGuard.","2375-1207","979-8-3315-2236-0","10.1109/SP61157.2025.00161","National Natural Science Foundation of China(grant numbers:62425205,62376210,62402273); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023344","","Privacy;Codes;Accuracy;Prevention and mitigation;Large language models;Benchmark testing;Robustness;Security","","","","89","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Formal Modelling of the Multiple Trains Following Operation in Uncertain Environments","X. Wang; Y. Xu; J. Liu; G. Wu; S. Chen","School of Computing and Artificial Intelligence, Southwest Jiaotong University National- Local Joint Engineering Laboratory of System Credibility Automatic Verification, Chengdu, China; School of Mathematics, Southwest Jiaotong University National- Local Joint Engineering Laboratory of System Credibility Automatic Verification, Chengdu, China; School of Computing, Ulster University, Northern Ireland; School of Mathematics, Southwest Jiaotong University National- Local Joint Engineering Laboratory of System Credibility Automatic Verification, Chengdu, China; School of Mathematics, Southwest Jiaotong University National- Local Joint Engineering Laboratory of System Credibility Automatic Verification, Chengdu, China",2023 18th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),"8 Apr 2024","2023","","","450","457","Ensuring the safety of the following operation under moving block signal is very important for safe operation. Now that the Train Control system under Moving Block Signal (TC-MBS) is a distributed, complex, dynamic, and highly interactive system, the proposed method uses the Multi-Agent System (MAS) technique to design, model, and analyse the TC-MBS system. This paper describes implementing formal methods for modelling the TC-MBS system that operates in an uncertain environment and its safety verification based on model checking techniques. First, the multi-mode following operation in an uncertain environment is analysed using the decision table technique. Second, we introduce a creative modelling approach for MAS called UML-MAS to build the semi-formal model of the TC-MBS system. UML-MAS acts as a standard interface for the system designers and infrastructure managers to increase their requirement comprehension, analysis, system design, and scheme planning at the conceptual level. Thirdly, a set of rules is suggested to convert UML-MAS model into Vector-based Interpreted Systems Programming Language (VISPL) model, which serves as the input for the model checker MCMAS- T. In order to demonstrate the practicality and effectiveness of the suggested approach, some case studies on modelling the TC-MBS systems while considering various scenarios in an uncertain environment and their verification employing MCMAS- Tare presented.","","979-8-3503-1840-1","10.1109/ISKE60036.2023.10481339","National Natural Science Foundation of China(grant numbers:61976130,62366017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481339","train control system;uncertain environment;formal models;model checking;multi-agent system","Knowledge engineering;Interactive systems;Unified modeling language;Model checking;Control systems;Safety;Planning","","","","26","IEEE","8 Apr 2024","","","IEEE","IEEE Conferences"
"Stealthy Attacks in Cloud-Connected Linear Impulsive Systems","A. Duz; S. Phillips; A. Fagiolini; R. G. Sanfelice; F. Pasqualetti","Mechanical Engineering Department, University of California, Riverside; Computer Engineering Department, University of California, Santa Cruz; Department of Energy, Computer Science, and Mathematical Models, University of Palermo, Italy; Computer Engineering Department, University of California, Santa Cruz; Mechanical Engineering Department, University of California, Riverside",2018 Annual American Control Conference (ACC),"16 Aug 2018","2018","","","146","152","This paper studies a security problem for a class cloud-connected multi-agent systems, where autonomous agents coordinate via a combination of short-range ad-hoc communication links and long-range cloud services. We consider a simplified model for the dynamics of a cloud-connected multi-agent system and attacks, where the states evolve according to linear time-invariant impulsive dynamics, and attacks are modeled as exogenous inputs designed by an omniscent attacker that alters the continuous and impulsive updates. We propose a definition of attack detectability, characterize the existence of stealthy attacks as a function of the system parameters and attack properties, and design a family of undetectable attacks. We illustrate our results on a cloud-based surveillance example.","2378-5861","978-1-5386-5428-6","10.23919/ACC.2018.8431900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431900","","Mathematical model;Cloud computing;Cyber-physical systems;Authentication;Control theory;Monitoring","","6","","36","","16 Aug 2018","","","IEEE","IEEE Conferences"
"Towards Complex Adaptive Control Systems for Human-Robot-Interaction in Intralogistics","T. Kirks; J. Jost; T. Uhlott; M. Jakobs","Fraunhofer IML, Department of Automation and Embedded Systems, Dortmund, Germany; Fraunhofer IML, Department of Automation and Embedded Systems, Dortmund, Germany; Fraunhofer IML, Department of Automation and Embedded Systems, Dortmund, Germany; Fraunhofer IML, Department of Automation and Embedded Systems, Dortmund, Germany",2018 21st International Conference on Intelligent Transportation Systems (ITSC),"9 Dec 2018","2018","","","2968","2973","Due to the rising importance of flexible industrial systems and the involved occurrence of complex robotic systems, the human worker has to be integrated into the system design and its implementation to retain control over changing configurations. This paper introduces a novel way of human-machine-interaction involving humans and autonomous transport vehicles for intralogistics using multi-agent systems. The presented work shows how decentrally organized robots can adapt to human workers sharing the same environment. For that purpose, each entity, also the human worker, is represented by an agent including its specific capabilities and needs. In the resulting multi-agent system, all instances share their information with each other, hence, allowing to adapt dynamically to one another.","2153-0017","978-1-7281-0323-5","10.1109/ITSC.2018.8569949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8569949","","Robots;Multi-agent systems;Task analysis;Decentralized control;Safety;Logistics;Personnel","","6","","16","IEEE","9 Dec 2018","","","IEEE","IEEE Conferences"
"Exploration of the Use of Conversational Text-Based AI Tools for Academic Teachers: A Use Case and Study","S. Venkatraman; S. K. Bamrah; G. A. J. Arulraj","Dept. of Electronics, Dolcera ITES Pvt. Ltd., Telangana, India; Information Systems, Nissan Motor India Pvt. Ltd., Tamil Nadu, India; Dept. of Chemical Engineering, PSN Engineering College, Tamil Nadu, India",2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT),"4 Oct 2024","2024","","","192","197","The article explores the scope of integrating conversational AI (Artificial Intelligence) tools into the daily activities of academic teachers. The article presents a use case of adopting AI tools for performing some of the day to day, or regular activities of academic teachers. Today, AI is leveraged for multitude of activities including training robots to perform or mimic some human activities, processing large datasets to derive quick and intelligent decisions, and automating, or semi-automating some routine and repeated human tasks. Since 2021, conversational AI tools have witnessed huge amounts of usage activities from people across different industries. ChatGPT and Claude AI were used for prompt engineering to automate the use cases considered in this work and the results are presented in good detail. The accuracy and relevancy of ChatGPT and Claude AI outputs are good and this stands as a testament to the suitability of utilizing AI tools for some of the day-to-day activities of teachers.","","979-8-3503-6501-6","10.1109/IC2SDT62152.2024.10696651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696651","AI;Artificial Intelligence;ChatGPT;Claude AI;conversational;Google Gemini;teachers","Training;Industries;Accuracy;Service robots;Chatbots;Prompt engineering","","","","6","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Seeing the Unseen: A Forecast of Cybersecurity Threats Posed by Vision Language Models","M. Taeb; J. Wang; M. H. Weatherspoon; S. Bernadin; H. Chi","Cybersecurity and Information Technology University of West Florida, Pensacola, FL, USA; Communication, Culture & Technology Georgetown University, Washington, DC, USA; Electrical & Computer Engineering FAMU-FSU College of Engineering, Tallahassee, FL, USA; Electrical & Computer Engineering FAMU-FSU College of Engineering, Tallahassee, FL, USA; Computer and Information Sciences Florida A&M University, Tallahassee, FL, USA",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","5664","5673","Despite the proven efficacy of large language models (LLMs) like GPT in numerous applications, concerns have emerged regarding their exploitation in creating phishing emails or network intrusions, which have shown to be detrimental. The multimodal functionalities of large vision-language models (LVLMs) enable them to grasp visual commonsense knowledge. This study investigates the feasibility of using two widely available commercial LVLMs, LLAVA, and multimodal GPT4, for effectively bypassing CAPTCHAs or producing bot-driven fraud through malicious prompts. It was found that these LVLMs can interpret and respond to the visual information presented in image, puzzle, and text-based CAPTCHA and reCAPTCHA, thereby potentially circumventing the challenge-response authentication security measure. This capability suggests that such systems could facilitate unauthorized access to secured accounts via remote digital methods. Remarkably, these attacks can be executed with the standard, unaltered versions of the LVLMs, eliminating the need for previous adversarial methods like jailbreaking.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825034","Arm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825034","Cyber Intelligence;Vision Language Model;CAPTCHA;Social Engineering;Multi-modal Learning.","Visualization;Protocols;Statistical analysis;Generative AI;Phishing;Large language models;Predictive models;Network intrusion;Standards;CAPTCHAs","","1","","24","USGov","16 Jan 2025","","","IEEE","IEEE Conferences"
"Vision Transformer Based Hash Coding for Efficient Image and Audio Retrieval with Global and Local Equilibrium Distance Constraints","Y. Liu; Y. Pan; J. Yin","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China","2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20 Jan 2025","2024","","","3330","3335","With the application of efficient retrieval in information systems and retrieval augmented generation with vector database for large language models, hash coding algorithms have made progress in recent years. The rise of transformer technology in the field of deep learning has brought the possibility to further improve the effect of hash coding algorithms. We introduce the vision transformer framework to both images and audios, and propose a novel approach for the tasks of multi-label image retrieval and audio event retrieval. In the proposed hash coding model, global and local equilibrium distance constraints are integrated, so that the hash codes for images can be better obtained through the global hash centers and local similar samples. In order to realize end-to-end training and hash code generation for audios, we adopt the adapter of mel spectrogram, thus the proposed approach can be simply converted and applied to audio hash coding. Comparative experiments verify that better results can be achieved on multiple image and audio datasets.","","978-1-6654-1020-5","10.1109/SMC54092.2024.10831803","National Natural Science Foundation of China(grant numbers:U2001211,U22B2060); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2023A1515011400); Research Foundation of Science and Technology(grant numbers:2023B01J0001,2024B01W0004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10831803","Hash coding;Vision transformer;Equilibrium distance constraint;Efficient retrieval;Deep learning","Training;Computer vision;Image coding;Codes;Large language models;Image retrieval;Transformers;Encoding;Vectors;Spectrogram","","","","33","IEEE","20 Jan 2025","","","IEEE","IEEE Conferences"
"Application and Interpretable Research of Capsule Network in Situational Understanding","P. Li; Q. Fei; Z. Chen; J. Ru","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing",2024 43rd Chinese Control Conference (CCC),"17 Sep 2024","2024","","","8679","8684","In the context of multi-agent collaborative adversarial scenarios, the accurate and rapid assessment of situations is a crucial prerequisite for unmanned clusters to achieve autonomous decision-making. Leveraging deep learning techniques, multi-agent systems can achieve precise understanding of complex situations. However, the inherently non-interpretable black-box structure of deep learning makes it challenging to apply in domains with stringent security requirements. In this paper, we propose a threat situation classification network based on Capsule Networks to categorize different scenario situations, and conduct a comprehensive analysis of the interpretability of this network. The network introduces a novel convolutional “Flatten Layer” to ensure that feature capsules are distributed within planes that maintain the same relative spatial relationships as the input image. This establishes the characteristic plane matrix heatmaps and the characteristic volume matrix heatmaps, which, along with the coupling coefficient matrix heatmaps, collectively demonstrate the network’s sparse interpretability during the classification process. Experimental results show that the proposed network can effectively accomplish situation classification tasks while maintaining interpretability, providing insights for research in situation understanding in domains with high-security requirements.","1934-1768","978-9-8875-8158-1","10.23919/CCC63176.2024.10661727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10661727","threat situation classification;Capsule Network;interpretability analysis","Heating systems;Deep learning;Couplings;Visualization;Decision making;Collaboration;Closed box","","","","14","","17 Sep 2024","","","IEEE","IEEE Conferences"
"CODING AND PROGRAMMING: THE AI REVOLUTION","B. Marr","Advanced Performance Institute, Buckinghamshire, UK",Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society,"","2024","","","201","210","Summary <p>GenAI can turn natural language instructions into functioning code, which essentially turns English (or whatever our native tongue is) into a computer programming language. Clearly, GenAI is going to be a revolutionary tool for programmers. But it may also prove a useful tool for everyday folks who want to, say, build an app or create a piece of software, but don't know any programming languages. Seattle‐based real estate company, Redfin, has used large language models including ChatGPT to help with programming tasks. Codacy is an AI code quality/code analysis tool that helps to automate code analysis. CodeWhisperer is Amazon's code generation tool, developed using open‐source code and Amazon's own data. The tool takes natural language prompts and creates code based on the programmer's objectives, in their style. It can also be used for code completion.</p>","","9781394254255","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10953050.pdf&bkn=10950173&pdfType=chapter","","Codes;Encoding;Software;Artificial intelligence;Software development management;Companies;Programming profession;Testing;Generative AI;Chatbots","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"PyTy: Repairing Static Type Errors in Python","Y. W. Chow; L. Di Grazia; M. Pradel","University of Stuttgart, Germany; University of Stuttgart, Germany; University of Stuttgart, Germany",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","1058","1070","Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based re-pair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3639184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548748","Automatic Program Repair;Type Annotation;Transfer Learning","Deep learning;Runtime;Transfer learning;Manuals;Debugging;Maintenance engineering;Writing","","4","","56","","14 Jun 2024","","","IEEE","IEEE Conferences"
"Secure MLLM Semantic Communication Enabled Air-Space-Ground-Maritime Networks","M. Chen; Z. Sun; X. He; B. Li; L. Wang; S. Mumtaz","Nanjing University of Posts and Telecommunications, China; Nanjing University of Posts and Telecommunications, China; Nanjing University of Posts and Telecommunications, China; Nanjing University of Posts and Telecommunications, China; Nanjing University of Posts and Telecommunications, China; Silesian University of Technology, Poland",IEEE Communications Magazine,"27 Jun 2025","2025","63","7","26","33","With the advancement of 6G, future mobile networks are witnessing a seamless integration between terrestrial and non-terrestrial networks to establish a global air-space-ground-maritime network. The dynamic channel and vast geographic coverage in these networks require efficient transmission of the multi-modal data. Meanwhile, semantic communication is a promising solution for this issue since it focuses on meaning rather than symbol-level reconstruction. However, due to wireless transmission and additional vulnerabilities introduced by semantic processing, these networks face critical security challenges and open issues in both conventional communication and the semantic level. Therefore, we propose a secure multimodal semantic communication framework leveraging multimodal large language models (MLLMs) to address these challenges. We provide a systematic analysis of three security aspects: security in system and model, security in data and semantics, and security in communication and transmission. In addition, we have developed a novel framework that integrates semantic representation with joint source-channel coding (JSCC) for semantic information transmission. Then, a comprehensive protection mechanism is proposed that jointly optimizes security at both physical and semantic layers through artificial noise. Finally, experimental results demonstrate that our approach enhances the efficiency and security of communication. It shows that the achievable secrecy rate is improved by up to 40 percent while maintaining a gap exceeding 10 dB between the legitimate receiver and eavesdropper.","1558-1896","","10.1109/MCOM.001.2400703","National Natural Science Foundation of China(grant numbers:62402246); Natural Science Foundation of Jiangsu Province(grant numbers:BK20241886,BE2023035,BE2023087); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11053517","","Wireless communication;Analytical models;Systematics;Noise;Noise reduction;Receivers;Semantic communication;Data models;Security;Protection","","","","15","IEEE","27 Jun 2025","","","IEEE","IEEE Magazines"
"Sustainable Cloud Development: Optimize cloud workloads for environmental impact in the GenAI era","P. G. Patel; I. K. Dua; S. David",NA; NA; NA,Sustainable Cloud Development: Optimize cloud workloads for environmental impact in the GenAI era,"","2025","","","","","Reduce your carbon footprint, optimize workloads, and align with organizational environmental goals by leveraging carbon footprint–friendly software design, generative AI, and cost-saving strategiesKey FeaturesDiscover sustainable cloud practices, including carbon footprint analysis, optimization, and securityExplore best practices, insights, and case studies for implementing sustainable solutions like generative AI workloadsLearn cost-saving strategies through efficient resource use and business alignmentPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWritten by three seasoned AWS solution architects, sustainability mentors, and thought leaders, Sustainable Cloud Development equips cloud professionals with actionable strategies to design, build, and optimize workloads that minimize environmental impact, while maintaining performance and scalability. This book combines practical insights, best practices, and case studies to help you align your cloud operations with global sustainability goals. From foundational concepts such as carbon footprint measurement to advanced techniques such as sustainable software architecture, generative AI lifecycle optimization, and cost-efficient cloud practices, this book covers every aspect of sustainable cloud development. You’ll get to grips with key tools, including AWS Cost Explorer, for analyzing costs and usage over time to right-size deployments; auto scaling for automatically scaling compute resources dynamically based on demand; Amazon Trusted Advisor for reviewing optimization recommendations across critical areas such as cost, performance, and security; and Amazon CloudWatch for detailed monitoring and threshold-based alerting around all resources and applications. This book serves as a practical blueprint for optimizing your cloud workloads for both high performance and a minimal environmental footprint.What you will learnExplore the principles of sustainable cloud computing and application performance analysisDiscover best practices for data lifecycle management, storage optimization, and networking efficiencyUnderstand and analyze the carbon footprint of cloud applicationsImplement sustainable software architecture and coding patternsOptimize the lifecycle and consumption of generative AI modelsAlign cloud services with sustainability goals and global regulationsExplore eco-friendly generative AI practices, including efficient model deploymentWho this book is forThis book is for cloud architects, engineers, DevOps professionals, and IT sustainability specialists who want to align their cloud practices with environmental goals. It also caters to software developers eager to build green, efficient solutions. A basic understanding of cloud services and IT infrastructure is necessary.","","9781836208402","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948544.pdf&bkn=10948543&pdfType=book","","","","","","","","3 Apr 2025","","","Packt Publishing","Packt Publishing eBooks"
"Relaxing Platform Dependencies in Agent-Based Control Systems","M. P. Hernández; D. Mcfarlane; A. K. Parlikad; M. Herrera; A. K. Jain","Institute for Manufacturing, University of Cambridge, Cambridge, U.K.; Institute for Manufacturing, University of Cambridge, Cambridge, U.K.; Institute for Manufacturing, University of Cambridge, Cambridge, U.K.; Institute for Manufacturing, University of Cambridge, Cambridge, U.K.; Institute for Manufacturing, University of Cambridge, Cambridge, U.K.",IEEE Access,"23 Feb 2021","2021","9","","30511","30527","Agent-based systems have been widely used to develop industrial control systems when they are required to address issues such as flexibility, scalability and portability. The most common approach to develop such control systems is with agents embedded in a platform that provides software libraries and runtime services that ease the development process. These platforms also bring challenges to the agent-based control system engineering. For example, they might introduce default design features, such as a global directory of agents. Furthermore, agents are generally locked in a platform and depend on the platform’s available support for deployment across computing infrastructures. This article addresses these challenges through an approach for building agent-based control systems, that relaxes the dependencies in multiagent system (MAS) platforms, through the use of container-based virtualisation. The proposed approach is elaborated via a reference architecture that enables the implementation of agents as self-contained applications that can be deployed, on-demand, in independent environments but still are able to communicate and coordinate with other agents of the control system. We built a prototype using this approach and evaluated it in the context of a case study for the supervisory control of digital network infrastructures. This case study enabled us to demonstrate feasibility of the approach and to show the flexibility, of the resulting control system, to adopt several topologies as well as to operate at different scales, over emulated networks. We also concluded that designing agents as individual deployment units is also cost-effective especially in control scenarios with low number of stable agents.","2169-3536","","10.1109/ACCESS.2021.3059273","Engineering and Physical Sciences Research Council (EPSRC) through the BT Prosperity Partnership Project: Next Generation Converged Digital Infrastructure(grant numbers:EP/R004935/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354153","Agent-based control;multiagent systems;micro services;container-based virtualisation industrial control;system containers","Control systems;Computer architecture;Scalability;Industrial control;Multi-agent systems;Cloud computing;Tools","","5","","63","CCBY","12 Feb 2021","","","IEEE","IEEE Journals"
"Is ChatGPT Trustworthy Enough? a Review","G. Zhou; Y. Liu; Z. Yan; E. Gelenbe","Xidian, China; Xidian, China; Xidian, China; Polish Academy of Sciences, China",IEEE Consumer Electronics Magazine,"","2024","PP","99","1","11","ChatGPT, as an advanced model that seamlessly integrates into diverse digital interactions, shows great potential to enhance the performance of consumer technology and reshape its landscape. The critical question of its trustworthiness and associated challenges becomes increasingly prominent. However, the literature still lacks a thorough review to study its trust. This comprehensive review explores whether ChatGPT is trustworthy enough by navigating the multifaceted realm of large language models, placing a significant focus on its exemplar model, ChatGPT. Our exploration traverses the complex interplay of factors impacting user trust, including trust in ChatGPT itself and trust in its utilization and dissemination. By delving into insightful perspectives on the ChatGPT trustworthiness regarding a set of evaluation criteria including fundamental properties, subjective properties, security and privacy, we find that ChatGPT is far from trustworthy based on related literature review. This paper sheds light on the weakness of ChatGPT trust and provides the trajectory of future development in the realm of large language models.","2162-2256","","10.1109/MCE.2024.3485257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10731562","","Chatbots;Consumer electronics;Reviews;Artificial intelligence;Surveys;Reliability;Security;Data models;Accuracy;Ethics","","","","","IEEE","23 Oct 2024","","","IEEE","IEEE Early Access Articles"
"A secure and private ensemble matcher using multi-vault obfuscated templates","B. P. Gilkalaye; S. Mukherjee; R. Derakhshani","School of Science and Engineering, University of Missouri-Kansas City; School of Science and Engineering, University of Missouri-Kansas City; School of Science and Engineering, University of Missouri-Kansas City",2024 IEEE International Joint Conference on Biometrics (IJCB),"11 Nov 2024","2024","","","1","10","Generative AI has revolutionized modern machine learning by providing unprecedented realism, diversity, and efficiency in data generation. This technology holds immense potential for biometrics, including for securing sensitive and personally identifiable information. Given the irrevocability of biometric samples and mounting privacy concerns, biometric template security and secure matching are among the most sought-after features of modern biometric systems. This paper proposes a novel obfuscation method using Generative AI to enhance biometric template security. Our approach utilizes synthetic facial images generated by a Generative Adversarial Network (GAN) as ""random chaff points"" within a secure vault system. Our method creates n sub-templates from the original template, each obfuscated with m GAN chaff points. During verification, s closest vectors to the biometric query are retrieved from each vault and combined to generate hash values, which are then compared with the stored hash value. Thus, our method safeguards user identities during the training and deployment phases by employing the GAN-generated synthetic images. Our protocol was tested using the AT&T, GT, and LFW face datasets, achieving ROC areas under the curve of 0.99, 0.99, and 0.90, respectively. Our results demonstrate that the proposed method can maintain high accuracy and reasonable computational complexity comparable to those unprotected template methods while significantly enhancing security and privacy, underscoring the potential of Generative AI in developing proactive defensive strategies for biometric systems.","2474-9699","979-8-3503-6413-2","10.1109/IJCB62174.2024.10744494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10744494","","Biometrics;Training;Privacy;Data privacy;Protocols;Generative AI;Face recognition;Generative adversarial networks;Security;Synthetic data","","","","54","IEEE","11 Nov 2024","","","IEEE","IEEE Conferences"
"Monitoring and Storage of Health Data in Secured Cloud Environment: A Detailed Survey","M. Radha; K. Murugan","Department of Electronics and Communication Engineering, Kalasalingam Academy of Research and Education, Virdhunagar, Tamilnadu, India; Department of Electronics and Communication Engineering, Kalasalingam Academy of Research and Education, Virdhunagar, Tamilnadu, India","2024 2nd International Conference on Computer, Communication and Control (IC4)","4 Apr 2024","2024","","","1","6","One of the numerous cloud-based services is the e-health system, which stores and shares patient medical data among healthcare professionals and patients. It operates mostly through computer or electronic systems and cloud technologies. The semi-trusted third-party supplier (the cloud) stores medical information. Consequently, security has emerged as the primary worry because no unauthorized individual ought to be able to obtain the data. To help to advance the field of e-health system security, this paper purposes to give a brief overview of the security features of cloud founded e-health systems. A study of the research on safe cloud-based healthcare systems using several techniques, including Extended Semi-shadow Images (ESSI), Medical Cloud Multi-Agent Systems (MCMAS), Adaptive Deep Convolutional Neural Networks (ADCNN), Structural Health Monitoring (SHM), etc., is presented in this paper. Additionally, certain security rules pertaining to health are discussed along with security techniques, their benefits and their drawbacks for each category. This study provides guidance for conducting additional research in the field of e-health system research because it analyses security legislation, security mechanisms, and their benefits and drawbacks.","","979-8-3503-8793-3","10.1109/IC457434.2024.10486629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486629","ESSI –(Extended Semi-shadow Images);MCMAS –(Medical Cloud Multi-Agent System);ADCNN –(Adaptive Deep Convolutional Neural Network);SHM –(Structural Health Monitoring)","Surveys;Computational modeling;Legislation;Security;Convolutional neural networks;Computer security;Monitoring","","","","25","IEEE","4 Apr 2024","","","IEEE","IEEE Conferences"
"Demystifying Chains, Trees, and Graphs of Thoughts","M. Besta; F. Memedi; Z. Zhang; R. Gerstenberger; G. Piao; N. Blach; P. Nyczyk; M. Copik; G. Kwaśniewski; J. Müller; L. Gianinazzi; A. Kubicek; H. Niewiadomski; A. O'Mahony; O. Mutlu; T. Hoefler","ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; Dell, USA; ETH Zurich, Zurich, Switzerland; Cledar, Poland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; BASF SE, Germany; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; Cledar, Poland; Dell, USA; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland",IEEE Transactions on Pattern Analysis and Machine Intelligence,"","2025","PP","99","1","20","The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.","1939-3539","","10.1109/TPAMI.2025.3598182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11123142","","","","","","","IEEE","12 Aug 2025","","","IEEE","IEEE Early Access Articles"
"Privacy Protected Modified Double Ratchet Algorithm for Secure Chatbot Application","M. Shah; M. Panchal","Computer Engineering (Cyber Security), Gujarat Technological University, Graduate School of Engg & Tech, Chandkheda, Ahmedabad, Gujarat, India; Computer Engineering (Cyber Security), Gujarat Technological University, Graduate School of Engg & Tech, Chandkheda, Ahmedabad, Gujarat, India",2022 3rd International Conference on Smart Electronics and Communication (ICOSEC),"22 Nov 2022","2022","","","747","754","Our world has been revolutionized as a result of digitalization. Large amounts of data are regularly generated as a result of the well-known World Wide Web, various social networking sites, and an infinite number of apps. Personal assistants are proof of the growing popularity of chatbots. Conversational AI developers need to be aware of any potential threats or flaws in existing designs, as well as how hackers may exploit those flaws, if they want their creations to be secure. The concept of “chatbot” refers to a piece of software that is executed on a computer and has the ability to engage in conversation with human users while also processing the content of such talks. Users are also able to connect and communicate with digital gadgets via the use of chatbots (such as smartphones and tablets). According to the authors, it is also referred to as “one of the most sophisticated and promising representations of interactions between people and robots.” Its primary function is to assist students by responding to their inquiries. The chatbots that are currently available are not entirely secure, and it is possible that they could provide opportunities for information flowing through the chatbot interface to be accessed by cybercriminals or hackers. As a direct consequence of this, the chatbot will be made secure by utilizing authentication (session) timeouts in combination with encryption mechanisms. During the transport of data from one end system to another, encryption is used to prevent outsiders from gaining access to the information being sent. The data is encrypted as soon as it leaves the machine of the sender, and the only person who will be able to decrypt it is the recipient to whom it is intended to be sent. It is probable to set a time limit on how long an authorized user may remain “logged in,” which prevents cyber attackers from being able to guess their way into someone else’s protected account by using authentication (session) timeout. “In this research paper, the Double Ratchet algorithm is modified with Paillier Cryptosystems. Our Privacy Protected Modified model is secure in key as well as message data through network attacks.","","978-1-6654-9764-0","10.1109/ICOSEC54921.2022.9952106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952106","Chatbot Security;Response;Query;End to End Encryption;Paillier Cryptosystems;Double Ratchet;Privacy;Preserving.","Privacy;Computer hacking;Social networking (online);Authentication;Chatbots;Software;Encryption","","8","","17","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Responding to Artificial Intelligence Cyber Threats","C. Brooks",Georgetown University,"Inside Cyber: How AI, 5G, IoT, and Quantum Computing Will Transform Privacy and Our Security","","2025","","","103","114","Summary <p>In the never‐ending struggle for tactical and technological supremacy in cybersecurity, attackers and defenders now face competition from artificial intelligence (AI) and machine learning (ML). Cybersecurity systems need to be regularly evaluated and updated in order to stop these machine‐driven hacker attacks. A holistic approach to layering emerging solutions to meet the growing AI‐generated cyber threats is becoming a must for most companies. Security teams are becoming more precise, effective, and productive in their cyber defense efforts thanks to generative AI. One of the most popular applications of generative AI nowadays is threat detection. Patch analysis and application processes can be automated with generative AI. Incident response is a successful area in which generative AI is used in cybersecurity. AI‐enabled cybersecurity needs to be considered in devices at the hardware level to better mitigate attacks.</p>","","9781394254965","10.1002/9781394310562.ch14","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10951766.pdf&bkn=10950206&pdfType=chapter","","Artificial intelligence;Computer security;Internet;Electronic mail;Business;Training;Software;Risk management;Hands;Games","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Face Morphing Attack Detection and Localization Based on Feature-Wise Supervision","L. Qin; F. Peng; M. Long","Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China",IEEE Transactions on Information Forensics and Security,"18 Oct 2022","2022","17","","3649","3662","To strengthen the security of face recognition systems to morphing attacks (MAs), many countermeasures were proposed. However, in the existing face morphing attack detection (MAD), the deep networks trained by classical score-level losses are weak in characterizing the intrinsic morphing patterns of different MAs, and they also cannot be directly applied to differential MAD scenarios. To this end, this paper presents a method for detecting and locating face MAs by the use of feature-wise supervision. It constructs the fine-grained classification loss on the basis of different morphing patterns, and designs the similarity-based and distance-based differential losses according to the properties of differential MAD scenarios. The experimental results and analysis show that the fine-grained classification loss can locate the local morphed areas after detecting MAs, while the differential losses are able to improve the generalization ability of MAD methods to unseen MAs, and can enhance the robustness of MAD methods to low-resolution and non-frontal probe face images.","1556-6021","","10.1109/TIFS.2022.3212276","National Natural Science Foundation of China(grant numbers:U1936115,62072055,92067104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912424","Face recognition;forensics;morphing attack detection;morphing attacks","Feature extraction;Face recognition;Faces;Probes;Security;Location awareness;Training","","18","","39","IEEE","5 Oct 2022","","","IEEE","IEEE Journals"
"Retrieval Augmented Generation on Hybrid Cloud: A New Architecture for Knowledge Base Systems","C. -C. Chuang; K. -C. Chen","High Performance Computing Division, National Center for High-performance Computing, Taichung, Taiwan; High Performance Computing Division, National Center for High-performance Computing, Taichung, Taiwan",2024 16th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI),"15 Oct 2024","2024","","","68","71","Retrieval Augmented Generation (RAG) is a novel approach that combines the strengths of large language models and external documents to generate responses grounded in retrieved information. This technique is particularly useful for building knowledge base systems. However, deploying such systems requires a large amount of computational resources for hosting language models, and keep privacy of the document is also a concern. Through the nature of hybrid cloud, the knowledge base system can maintain data privacy while benefiting from the scalability of public clouds. This study leverages a hybrid cloud architecture, and utilizes Kubernetes as a cloud orchestrator to schedule the workload across different cloud environments. The implemented RAG system demonstrates the feasibility of the proposed architecture, showcasing enhanced privacy, flexibility, and security.","2472-0070","979-8-3503-7790-3","10.1109/IIAI-AAI63651.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707974","Hybrid Cloud Computing;Retrieval Augmented Generation (RAG);Kubernetes;Knowledge Base System","Cloud computing;Privacy;Schedules;Scalability;Large language models;Knowledge based systems;Computer architecture;Hybrid power systems;Security;Informatics","","2","","17","IEEE","15 Oct 2024","","","IEEE","IEEE Conferences"
"Trustworthy Machine Learning: From Data to Models","B. Han; J. Yao; T. Liu; B. Li; S. Koyejo; F. Liu",NA; NA; NA; NA; NA; NA,Trustworthy Machine Learning: From Data to Models,"","2025","","","","","The success of machine learning algorithms relies not only on achieving good performance but also on ensuring trustworthiness across diverse applications and scenarios. Trustworthy machine learning seeks to handle critical problems in addressing the issues of robustness, privacy, security, reliability, and other desirable properties. The broad research area has achieved remarkable advancement and brings various emerging topics along with the progress. This monograph provides a systematic overview of the research problems under trustworthy machine learning, covering the perspectives from data to model. Starting with fundamental data-centric learning, this work reviews learning with noisy data, long-tailed distribution, out-of-distribution data, and adversarial examples to achieve robustness. Delving into private and secured learning, the monograph elaborates on core methodologies such as differential privacy, different attacking threats, and learning paradigms, to realize privacy protection and enhance security. Finally, it introduces several trendy issues related to the foundation models, including jailbreak prompts, watermarking, and hallucination, as well as causal learning and reasoning. This work integrates commonly isolated research problems in a unified manner, which provides general problem setups, detailed sub-directions, and further discussion on its challenges or future developments. The comprehensive investigation presented in this work can serve as a clear introduction for the problem evolution from data to models, and also bring new insight for developing trustworthy machine learning.","","9781638285496","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10981586.pdf&bkn=10981585&pdfType=book","","","","","","","","1 May 2025","","","now","Now Foundations and Trends Books"
"Customization of Your Foundation Model","O. Bergeret; A. Abbasi; J. Farvault",NA; NA; NA,GenAI on AWS: A Practical Approach to Building Generative AI Applications on AWS,"","2025","","","243","262","<p>Model customization is essential because pre‐trained foundation models are designed to be general‐purpose and versatile, capable of performing a wide range of tasks across different domains. While these models possess broad knowledge and language capabilities, they lack the specificity needed to excel in domain‐specific applications. Without customization, models may struggle with precise language generation, fail to recognize key terms, or generate irrelevant content, ultimately limiting their practical usefulness. There are several effective methods for adapting foundation models, like Anthropic's Claude, Meta's Llama, and Amazon's Titan, to perform better in specific domains or tasks. These methods include continued pre‐training, fine‐tuning, prompt engineering, and retrieval‐augmented generation (RAG). The ideal service for customizing a model depends on the dataset and the task for which the model is intended.</p>","","9781394281305","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10982342.pdf&bkn=10964414&pdfType=chapter","","Adaptation models;Computational modeling;Retrieval augmented generation;Data models;Training;Prompt engineering;Foundation models;Internet;Context modeling;Computational efficiency","","","","","","2 May 2025","","","Wiley","Wiley AI eBook Chapters"
"SmartInv: Multimodal Learning for Smart Contract Invariant Inference","S. J. Wang; K. Pei; J. Yang","Columbia University, NY; Columbia University, NY; Columbia University, NY",2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","2217","2235","Smart contracts are software programs that enable diverse business activities on the blockchain. Recent research has identified new classes of ""machine un-auditable"" bugs that arise from source code not meeting underlying transaction contexts. Existing detection methods require human understanding of underlying transaction logic and manual reasoning across different sources of context (i.e., modalities), such as code and natural language specifying the expected transaction behavior.To automate the detection of ""machine un-auditable"" bugs, we present SmartInv, an accurate and fast smart contract invariant inference framework. Our key insight is that the expected behavior of smart contracts, as specified by invariants, relies on understanding and reasoning across multimodal information, such as source code and natural language. We propose a new finetuning and prompting strategy to foundation models, Tier of Thought (ToT), to reason across multiple modalities of smart contracts and to generate invariants. SmartInv then localizes potential vulnerabilities by checking the violation of those generated invariants.We evaluate SmartInv on real-world smart contract bugs that resulted in financial losses over the past 2.5 years (from January 1, 2021 to May 31, 2023). Extensive evaluation shows that SmartInv can generate useful invariants to effectively localize ""machine un-auditable"" bugs, from which SmartInv uncovers 119 zero-day bugs. We sampled eight bugs and reported them to the respective developers. Six vulnerabilities were quickly fixed by the developers, five of which are confirmed as ""high severity.""","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646885","machine learning;programming languages;smart contracts;security;blockchain","Privacy;Source coding;Smart contracts;Computer bugs;Natural languages;Manuals;Cognition","","8","","93","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing","M. Rostami; M. Chilese; S. Zeitouni; R. Kande; J. Rajendran; A. -R. Sadeghi",Technical University of Darmstadt; Technical University of Darmstadt; Technical University of Darmstadt; Texas A&M University; Texas A&M University; Technical University of Darmstadt,"2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)","10 Jun 2024","2024","","","1","6","Modern computing systems heavily rely on hardware as the root of trust. However, their increasing complexity has given rise to security-critical vulnerabilities that cross-layer attacks can exploit. Traditional hardware vulnerability detection methods, such as random regression and formal verification, have limitations. Random regression, while scalable, is slow in exploring hardware, and formal verification techniques are often concerned with manual effort and state explosions. Hardware fuzzing has emerged as an effective approach to exploring and detecting security vulnerabilities in large-scale designs like modern processors. They outperform traditional methods regarding coverage, scalability, and efficiency. However, state-of-the-art fuzzers struggle to achieve comprehensive coverage of intri-cate hardware designs within a practical timeframe, often falling short of a 70 % coverage threshold. To address this challenge, we propose a novel ML-based hardware fuzzer, ChatFuzz. Our approach leverages large language models (LLMs) to understand processor language and generate data/control flow entangled yet random machine code sequences. Reinforcement learning (RL) is integrated to guide the input generation process by rewarding the inputs using code coverage metrics. Utilizing the open-source RISC-V-based RocketCore and BOOM cores as our testbed, ChatFuzz achieves 75% condition coverage in RocketCore in just 52 minutes. This contrasts with state-of-the-art fuzzers, which demand a 30-hour timeframe for comparable condition coverage. Notably, our fuzzer can reach a 79.14% con-dition coverage rate in RocketCore by conducting approximately 199k test cases. In the case of BOOM, ChatFuzz accomplishes a remarkable 97.02% condition coverage in 49 minutes. Our analysis identified all detected bugs by The Huzz, including two new bugs in the RocketCore and discrepancies from the RISC-VISA Simulator.","1558-1101","978-3-9819263-8-5","10.23919/DATE58400.2024.10546625","Deutsche Forschungsgemeinschaft (DFG)(grant numbers:SFB 1119-236615297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546625","","Program processors;Codes;Scalability;Computer bugs;Reinforcement learning;Manuals;Fuzzing","","6","","15","","10 Jun 2024","","","IEEE","IEEE Conferences"
"Using Natural Language Processing Tools to Infer Adversary Techniques and Tactics Under the Mitre ATT&CK Framework","R. Gabrys; M. Bilinski; S. Fugate; D. Silva",Naval Information Warfare Center Pacific; Naval Information Warfare Center Pacific; Naval Information Warfare Center Pacific; Naval Information Warfare Center Pacific,2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC),"13 Feb 2024","2024","","","0541","0547","This paper presents a novel approach to interpreting Intrusion Detection System (IDS) rules and predicting likely attacker Tactics, Techniques, and Procedures (TTPs) through the use of Large Language Models (LLMs) trained to associate IDS rules to TTPs in the MITRE ATT&CK framework. Our methodology focuses on automatically describing IDS rules to enhance explainability and provide a natural interface for exploring the potential operational impacts that an unmitigated attack may have on network security and operations. Using natural language processing techniques, we trained several models on a dataset of 972 labeled IDS rules, enabling it to generate descriptive narratives to aid in explaining individual rules and providing a foundation for predicting associated TTPs to aid in defensive cyber operations. We present our approach and initial results, discuss the potential implications in the broader context of cyber defense, and propose a conceptual extension where identified TTPs are used to predict and suggest proactive and reactive defense strategies that may be acted upon by human and autonomous defenders. We believe this and related work pave the way for more dynamic and informed decision-making processes in cyber defense, potentially transforming how Security Operations Centers (SOCs) and autonomous cyber defense systems operate.","","979-8-3503-6013-4","10.1109/CCWC60891.2024.10427746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427746","","Support vector machines;Couplings;Machine learning algorithms;Intrusion detection;Predictive models;Natural language processing;Computer security","","5","","16","USGov","13 Feb 2024","","","IEEE","IEEE Conferences"
"Context-Aware Authentication Framework for Secure V2V and V2I Communications in Autonomous Vehicles Using LLM","A. Sharma; S. Rani","Chitkara University Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, India",IEEE Transactions on Intelligent Transportation Systems,"","2025","PP","99","1","8","The rapid adoption of autonomous vehicles necessitates secure and efficient communication frameworks to protect against cyber threats and to ensure the privacy of data. The following work presents a context-aware authentication framework for Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications, integrating Large Language Models (LLMs) with advanced cryptographic techniques. By leveraging real-time contextual data, the framework dynamically adapts authentication mechanisms to varying traffic conditions and potential security threats, reducing latency while enhancing accuracy. The proposed approach employs lightweight cryptographic protocols to ensure computational efficiency, making it suitable for resource-constrained vehicular environments. Comparative evaluations demonstrate that the framework outperforms existing authentication mechanisms in terms of security, scalability, and computational overhead. This solution contributes to the development of a more resilient and intelligent transportation system, addressing key challenges in autonomous vehicular networks. The significance of our results emphasises the necessity of a comprehensive strategy that includes organizational, governmental alongwith technological actions to protect the future of intelligent transportation systems. For different number of vehicles the service failure for 110 vehicles is 14.2%, communication cost is 2345 bytes and service endurance is 83.21%.","1558-0016","","10.1109/TITS.2025.3563913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11000443","CPS;transportation;CITS;T-CPS;LLM","Security;Authentication;Transportation;Data privacy;Safety;Privacy;Real-time systems;Vehicle dynamics;Anomaly detection;Access control","","","","","IEEE","12 May 2025","","","IEEE","IEEE Early Access Articles"
"A Case Study of Soil Parameters for Efficient Crop Recommendation Using ML and IoT","S. B. Shinde; S. K. Wagh","Smt. Kashibai Navale College of Engineering, SPPU University, Pune, India; MES Wadia College of Engineering, Pune, SPPU University, Pune, India",2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL),"27 Mar 2025","2025","","","1002","1008","India's livelihood depends on agriculture, and the integration of advanced technologies, especially machine learning has the potential to increase the efficiency and security of precision agriculture. The system uses IoT sensors to monitor and add real-time soil parameters such as nitrogen (N), phosphorus (P), potassium (K), pH, electrical conductivity (EC), organic carbon (OC), and other elements to energy supply. Provide a better understanding of soil health and the environment through geospatial data through APIs. This data is securely sent to the cloud, where it is analyzed using machine learning by large language models (LLMs) trained on different agricultural data to generate crop recommendations. The recommendations guide farmers in choosing the best crops, planting plans, fertilization, and permaculture methods, promoting economic savings and sustainable agriculture. Hosted on a shared website, the system provides instant integration of IoT sensors, cloud infrastructure, LLM servers, and user interface, providing farmers with efficient, data-driven insights to improve decision-making and support agriculture that can contribute to long-term food security, safety and environmental sustainability.","","979-8-3315-2392-3","10.1109/ICSADL65848.2025.10933365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933365","Machine Learning (ML) Internet of Things(IoT);LLM;Precision Agriculture;Crop Recommendation;Soil Parameters;Nitrogen;Phosphorus;Potassium;Sustainability","Temperature sensors;Precision agriculture;Crops;Machine learning;Soil;Real-time systems;Sensor systems;Sensors;Internet of Things;Potassium","","","","19","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Do Chase Your Tail! Missing Key Aspects Augmentation in Textual Vulnerability Descriptions of Long-Tail Software Through Feature Inference","L. Han; S. Pan; Z. Xing; J. Sun; S. Yitagesu; X. Zhang; Z. Feng","College of Intelligence and Computing, Tianjin University, Tianjin, China; CSIRO’s Data61, Canberra, ACT, Australia; CSIRO’s Data61, Canberra, ACT, Australia; CSIRO’s Data61, Canberra, ACT, Australia; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",IEEE Transactions on Software Engineering,"13 Feb 2025","2025","51","2","466","483","Augmenting missing key aspects in Textual Vulnerability Descriptions (TVDs) is crucial for effective vulnerability analysis. For instance, in TVDs, key aspects include Attack Vector, Vulnerability Type, among others. These key aspects help security engineers understand and address the vulnerability in a timely manner. For software with a large user base (non-long-tail software), augmenting these missing key aspects has significantly advanced vulnerability analysis and software security research. However, software instances with a limited user base (long-tail software) often get overlooked due to inconsistency software names, TVD limited avaliability, and domain-specific jargon, which complicates vulnerability analysis and software repairs. In this paper, we introduce a novel software feature inference framework designed to augment the missing key aspects of TVDs for long-tail software. Firstly, we tackle the issue of non-standard software names found in community-maintained vulnerability databases by cross-referencing government databases with Common Vulnerabilities and Exposures (CVEs). Next, we employ Large Language Models (LLMs) to generate the missing key aspects. However, the limited availability of historical TVDs restricts the variety of examples. To overcome this limitation, we utilize the Common Weakness Enumeration (CWE) to classify all TVDs and select cluster centers as representative examples. To ensure accuracy, we present Natural Language Inference (NLI) models specifically designed for long-tail software. These models identify and eliminate incorrect responses. Additionally, we use a wiki repository to provide explanations for proprietary terms. Our evaluations demonstrate that our approach significantly improves the accuracy of augmenting missing key aspects of TVDs for log-tail software from 0.27 to 0.56 (+107%). Interestingly, the accuracy of non-long-tail software also increases from 64% to 71%. As a result, our approach can be useful in various downstream tasks that require complete TVD information.","1939-3520","","10.1109/TSE.2024.3523284","Open Project of China Academy of Railway Sciences Corporation Limited(grant numbers:RITS2023KF05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10817126","Software vulnerability;long-tail software;textual vulnerability descriptions;natural language inference;software feature","Software;Heavily-tailed distribution;Vectors;Tail;Operating systems;Databases;Large language models;Security;Feature extraction;Accuracy","","","","86","IEEE","27 Dec 2024","","","IEEE","IEEE Journals"
"Analyzing Logs of Large-Scale Software Systems Using Time Curves Visualization","D. Borysenkov; A. Vogel; S. Henning; E. Perez-Wohlfeil","Dynatrace Research, Linz, Austria; JKU/Dynatrace Co-Innovation Lab, Johannes Kepler University, Linz, Austria; Dynatrace Research, Linz, Austria; Dynatrace Research, Linz, Austria","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","327","337","Logs are crucial for analyzing large-scale software systems, offering insights into system health, performance, security threats, potential bugs, etc. However, their chaotic na-ture-characterized by sheer volume, lack of standards, and variability-makes manual analysis complex. The use of clustering algorithms can assist by grouping logs into a smaller set of templates, but lose the temporal and relational context in doing so. On the contrary, Large Language Models (LLMs) can provide meaningful explanations but struggle with processing large collections efficiently. Moreover, representation techniques for both approaches are typically limited to either plain text or traditional charting, especially when dealing with large-scale systems. In this paper, we combine clustering and LLM summarization with event detection and Multidimensional Scaling through the use of Time Curves to produce a holistic pipeline that enables efficient and automatic summarization of vast collections of software system logs. The core of our approach is the proposal of a semimetric distance that effectively measures similarity between events, thus enabling a meaningful representation. We show that our method, based on logs collected from different applications, can explain the behavior of a system over time without prior knowledge. We also show how the approach can be used to detect general trends as well as outliers in parallel and distributed systems by overlapping multiple projections. As a result, we expect a significant reduction in the time required to analyze system-wide issues, identify performance bottlenecks and security risks, debug applications, etc.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992386","log analysis;visualization;set similarity distance;time curves","Visualization;Software algorithms;Pipelines;Text summarization;Manuals;Software systems;Market research;Security;Proposals;Standards","","","","32","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"KUBETEUS: An Intelligent Network Policy Generation Framework for Containers","B. Kim; H. Park; S. Lee","Incheon National University, Republic of Korea; Incheon National University, Republic of Korea; Incheon National University, Republic of Korea",IEEE INFOCOM 2025 - IEEE Conference on Computer Communications,"1 Jul 2025","2025","","","1","10","Containers have become the standard for delivering cloud-native services by taking advantage of their scalability, portability, and resource efficiency. However, particularly in network policies, they have also become major targets for various security attacks that exploit misconfigurations and vulnerabilities. Especially in complex cloud-native environments, manually managing network policies is prone to errors, and existing studies that automate policy generation often have limitations in accuracy. In this paper, we present KUBETEUS,a highly automated, intelligent network policy generation framework. Our system operates in an intent-driven manner, enhanced by natural language processing (NLP) and fine-tuned Large Language Models (LLMs), enabling the generation of network policies without needing to understand complex configurations. Furthermore, our system devises a multi-stage validation process to fundamentally prevent misconfigurations in network policy enforcement. The evaluation of KUBETEUS demonstrates its effectiveness, with the most improved fine-tuned LLM achieving a 360% increase in BLEU score and a 233% increase in ROUGE-2 score compared to the baseline model. We believe that the approach presented in this paper is applicable to the wide range of container-native policy platforms in used today, and that its broader adoption will help address more complex security policy generation concerns.","2641-9874","979-8-3315-4305-1","10.1109/INFOCOM55648.2025.11044592","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11044592","Cloud-native Architecture;Container Network;Intent-based Network Policy;LLM-based Policy Generation","Measurement;Intelligent networks;Accuracy;Scalability;Static analysis;Containers;Natural language processing;Software;Prompt engineering;Standards","","","","56","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"The Software Genome Project: Unraveling Software Through Genetic Principles","Y. Wu; C. Liu; Z. Xu; L. Zhang; Y. Zhang; Z. Zhu; Y. Liu","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","2319","2323","Open-source software is crucial to modern development, but its complexity creates challenges in quality, security, and management. Current governance approaches excel at collaboration but struggle with decentralized management and security. With the rise of large language models (LLM)-based software engineering, the need for a finer-grained understanding of software composition is more urgent than ever. To address these challenges, inspired by the Human Genome Project, we treat the software source code as software DNA and propose the Software Genome Project (SGP), which is geared towards the secure monitoring and exploitation of open-source software. By identifying and labeling integrated and classified code features at a fine-grained level, and effectively identifying safeguards for functional implementations and non-functional requirements at different levels of granularity, the SGP could build a comprehensive set of software genome maps to help developers and managers gain a deeper understanding of software complexity and diversity. By dissecting and summarizing functional and undesirable genes, SGP could help facilitate targeted software optimization, provide valuable insight and understanding of the entire software ecosystem, and support critical development tasks such as open source governance. SGP could also serve as a comprehensive dataset with abundant semantic labeling to enhance the training of LLMs for code. Based on these, we expect SGP to drive the evolution of software development towards more efficient, reliable, and sustainable software solutions.CCS Concepts• Software and its engineering → Software design engineering.","2643-1572","979-8-4007-1248-7","","National Research Foundation; National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765003","Software Genes;Software Composition;OSS Governance","Codes;Ecosystems;Genomics;Software;Complexity theory;Security;Labeling;Bioinformatics;Open source software;Software engineering","","","","27","","29 Nov 2024","","","IEEE","IEEE Conferences"
"An In-Depth Analysis of the Role That ML and Big Data Play in Driving Digital Marketing’s Paradigm Shift","M. A. Tripathi; R. Tripathi; F. Effendy; G. Manoharan; M. John Paul; M. Aarif","Department of humanities and social sciences, Motilal Nehru National Institute of Technology, Allahabad, India; Department of humanities and social sciences, Motilal Nehru National Institute of Technology, Allahabad, India; STMIK Rosma, Karawang, Indonesia; School of Business, SR University, Telangana, India; Department of Management Studies, MG University; Global Research Network, Noida, UP, India",2023 International Conference on Computer Communication and Informatics (ICCCI),"24 May 2023","2023","","","1","6","Machine learning (ML) is an artificial neural network (ANN) that helps developers improve their software’s predictive abilities before they have all the data they need. Because information is so priceless, progress toward fully autonomous agents requires better methods for managing the omnipresent content infrastructures that exist today. All sorts of fields have benefited from advancements in computer vision and AI, from medical diagnosis to data presentation and operations to scientific study, and so on. Learning from polluted or erroneous data may be expensive, much as training for a sport can be dangerous to those who are vulnerable to injury. An organization will incur costs rather than see benefits if its algorithms are improperly taught, as explained in Approaching Data Science. Organizations need to be able to verify the quality and consistency of any large datasets, as well as their sources, to ensure the efficacy of any algorithm.","2473-7577","979-8-3503-4821-7","10.1109/ICCCI56745.2023.10128357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128357","machine learning;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probability;supervised learning;classification;regression;clustering;big data;digital transformation;inquiry;Data protection;Security","Training;Organizations;Artificial neural networks;Machine learning;Big Data;Information management;Medical diagnosis","","28","","28","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"Detailed Investigation of Influence of Machine learning (ML) and Big Data on Digital Transformation in Marketing","A. Jain; K. K. Ramachandran; S. Sharma; T. Sharma; P. Pareek; B. Pant","Department of Computer Science & Engineering, IFTM University, Moradabad, Uttar Pradesh, India; Commerce/ Management Science, DR G R D College of Science, India; Amity college of Commerce, Amity University, Haryana, India; University Institute of Media Studies, Chandigarh University, Mohali, Punjab, India; Department of ECE, Vishnu Institute of Technology, Bhimavaram, A.P., India; Department of Computer Science & Engineering, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","1061","1065","The study explores with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. Because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. Computer vision and computer vision have improved a wide range of industries, including medical diagnoses, data display and procedures, science and research, and so. Just as preparing for a sport may be risky for individuals who are prone to injury, learning from contaminated or erroneous data can be costly. As described in the article Approaching Data Science, incorrectly trained algorithms result in expenses for a corporation rather than savings. Because incorrectly labeled, missing, or irrelevant data might impair the accuracy of any algorithm, organizations must be able to vouch for the quality and integrity of any data sets, along with their sources.","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823810","machine learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;digital marketing;big data;digital transformation;investigation;Data protection;Security","Computer vision;Technological innovation;Machine learning algorithms;Digital transformation;Organizations;Machine learning;Big Data","","8","","22","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Develop a Model for Assessing the Most Efficient Diseases Diagnosis using Machine Learning","L. Vives; N. K. Basha; Poonam; A. Gehlot; V. Chole; K. Pant","Peruvian University of Applied Sciences, Lima-Peru; Department of Electronics and Communication Engineering, Srinivasa Ramanujan Institute of Technology, Anantapur, AP, India; Department of Computer Science, Arya PG College, Panipat; Uttaranchal Institute of Technology, Uttaranchal University, India; G H Raisoni Institute of Engineering and Technology, Nagpur; Department of Biotechnology, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","2458","2462","so, machine learning techniques are being developed to improve performance and maintenance prediction. Increasing our knowledge of the relationship between humans and algorithms, Because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. Numerous researchers recently developed numerous computer-aided diagnostic algorithms employing various supervised learning approaches. Early identification of sickness may help to reduce the number of people who die as a result of these illnesses. Using machine learning techniques, this research creates an efficient automated illness diagnostic algorithm. We chose three key disorders in this paper: coronavirus, cardiovascular diseases, and diabetes. The data are inputted into a mobile application in the suggested model, the investigation is then done in a real-time dataset that used a pre-trained model machine learning technique trained within the same dataset then implemented in firebase, and lastly, the illness identification result can be seen in the mobile application. Logistic regression is a method of prediction calculation","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823933","Deep learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;Internet of things;Healthcare;Data protection;Security","Machine learning algorithms;Supervised learning;Machine learning;Predictive models;Prediction algorithms;Data models;Mobile applications","","2","","20","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Machine Learning Applications For Protecting The Information Of Health Care Department Using Smart Internet Of Things Appliances -A REVIEW","J. Surve; D. Umrao; M. Madhavi; T. S. Rajeswari; S. L. Bangare; M. K. Chakravarthi","Department of Information Technology, International Institute of Information Technology, Hinjewadi, Pune, India; Department Computer Science and Engineering Institute/ Organization: Naraina Vidya Peet, Engineering & Management Institute, Ganga Gunj Panki Kanpur; Department of Computer Science and Enginnering, VR Siddhartha Engineering College, Chalasani Nagar, Kanuru, Vijayawada, India; Department of English, Koneru Lakshmaiah Education Foundation, Vaddeswaram, AP, India; Department of Information Technology, Sinhgad Academy of Engineering, Savitribai Phule Pune University, Pune, India; School of Electronics Engineering, VIT-AP University, Amaravathi, India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","893","898","So, let us begin with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. Because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. Computer vision and computer vision have improved a wide range of industries, including medical diagnoses, data display and procedures, science and research, and so on A number of tiny IoT devices on the market for heart rate sensor, allowing patients to roam about freely while their hearts are continually monitored. It is still difficult to guarantee ultra-accurate findings, however most current instruments can achieve accuracy rates of over 90 percentage... Monitoring cardiac rhythms, as well as glucose levels, may be challenging, and even those who are represented at medical institutions. Intermittent heart rate assessments cannot protect against sudden changes in vital signs, and standard techniques of heart rhythm surveillance used in hospitals require patients to be permanently attached to wired apparatus, limiting their mobility.","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823642","machine learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;Internet of things;Health care;Data protection;Security","Computer vision;Limiting;Heart beat;Surveillance;Neural networks;Robot sensing systems;Software","","1","","19","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
