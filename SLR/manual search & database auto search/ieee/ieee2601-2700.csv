"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"AI-Based convolute Neural Approach Management To Predict The RNA Structure","S. Rishi; S. Debnath; S. Dewani; D. S. David; R. A. Jaleel; M. M. A. Zahra","Department of Microbiology, Government Medical College Srinagar, Kashmir, India; Department of Genetics and Plant Breeding, Genetics & Plant Breeding, Palli Siksha Bhavana (Institute of Agriculture), Visva-Bharati University, Sriniketan, Birbhum, West Bengal, India; Department of Physiology, Government Medical College, Srinagar, Kashmir, India; Department of Information Technology, Vel Tech Multi Tech Dr. Rangarajan Dr.Sakunthala Engineering College; Department of Information and Communication Engineering, Al-Nahrain University, Iraq; Computer Techniques Engineering Department, Al-Mustaqbal University College, Hillah, Iraq",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","2224","2228","Let us begin with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. Because data is so valuable, improving strategies for intelligently having to manage the now-Ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. In a nutshell, deep learning is a subset of machine learning that solves problems that machine learning alone cannot. Acquiring RNA secondary spatial relationships has been more significant in RNA and functional genomics studies in recent years. Although some RNA secondary sequences may be discovering approaches, most of the time, quick and accurate computational approaches are utilized to predict the structure of DNA strands. Current methods for determining RNA structure of proteins are generally based on the lowest power storage strategy, which seeks the optimal RNA folded form in vivo and employs an incremental process to satisfy the lowest source of critical energy and related aspects.","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823922","Deep learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;Healthcare;Data protection;Security;RNA structure;AI;convolute neural network","Deep learning;Proteins;Sequential analysis;In vivo;RNA;Neural networks;Hydrogen","","1","","17","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Practice and Prompts","J. Blount; A. Iannarino",Sales Gravy; thesalesblog.com,"The AI Edge: Sales Strategies for Unleashing the Power of AI to Save Time, Sell More, and Crush the Competition","","2024","","","87","99","Summary <p>Prompt engineering is the process of structuring text into a format that can be interpreted and understood by generative AI. AI tools are trained on massive datasets of text and code. They can generate text, translate languages, write different kinds of creative sales content, and answer our questions in an informative way. A prompt is a natural language text that describes the task that an AI should perform. The goal of prompt engineering is to write prompts that are clear, concise, and produce the information we seek on the first try. The prompt should be specific enough to tell the AI tool what to do, but it should not be so specific that it limits creativity. The prompt should also be grammatically correct and free of errors. Accordingly, the path to mastering AI prompts to make the readers faster, better, and stronger is the decision to jump in and get started.</p>","","9781394244492","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10950738.pdf&bkn=10950183&pdfType=chapter","","Artificial intelligence;Prompt engineering;Robots;Translation;Psychology;Codes;Buildings;Tires;Stars;Sports","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Empirical Analysis of Deep learning Techniques for Enhancing Patient Treatment Facilities in Healthcare Sector","C. J. Shelke; K. S. Kumar; G. R. Karetla; M. N. S. Sulthana; R. Beohar; K. Pant","Department of IT, Alliance University, Bengaluru, India; MBA Department, Panimalar Engineering College, Varadarajapuram, Poonamallee, Chennai, India; School of Computers Data and Mathematical Sciences, Western Sydney University, Sydney, Australia; Department of CSE, JNTUH, HYDERABAD; Faculty of Peace Studies, Dr Vishwanath D Karad MIT World Peace University, Pune, India; Department of Biotechnology, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","1314","1318","The study investigates with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. Because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. In a nutshell, deep learning is a subset of machine learning that solves problems that machine learning alone cannot. Deep learning use neural networks to boost computing labour while delivering accurate results. NLP, speech recognition, and facial recognition are just a few of the fantastic uses of deep learning. For example, when you submit a photo of yourself and a buddy to Facebook, Facebook dynamically tags your colleague and proposes a name for you to use. To recognise a face, Facebook employs deep learning algorithms. Deep learning techniques comprehend spoken human languages and transform them to text. Deep learning, in tandem with IoT, might lead to a slew of game-changing advancements in the future. Monitoring cardiac rhythms, as well as glucose levels, may be challenging, and even those who are represented at medical institutions. Intermittent heart rate assessments cannot protect against sudden changes in vital signs, and standard techniques of heart rhythm surveillance used in hospitals require patients to be permanently attached to wired apparatus, limiting their mobility.","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823489","Deep learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;Healthcare;Data protection;Security","Deep learning;Social networking (online);Heart beat;Face recognition;Surveillance;Neural networks;Transforms","","","","18","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"A review of Using Deep learning Applications in detecting Cancers at earlier stage","H. Khan; N. C. Sattaru; R. Ramachandran; G. R. Dhcekshana; S. Jeyalakshmi; K. Yadav","CSE, RCET, Bhilai; Aurora & PG College (Affiliated to OU), Hyderabad, Telangana; Anna University, Chennai, Tamilnadu, India; Department of Corporate Secretaryship, Psg College of Arts and Science Affiliated to Bharathiyar University; Department of Corporate Secretaryship, Psg College of Arts and Science Affiliated to Bharathiyar University; Department of Zoology, Dr. Ghanshyam Singh College of education, Varanasi, U.P., India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"18 Jul 2022","2022","","","2239","2243","Let us begin with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. With today's data availability, machine learning techniques are being developed to improve performance and maintenance prediction. Increasing our knowledge of the relationship between humans and algorithms, because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. In a nutshell, deep learning is a subset of machine learning that solves problems that machine learning alone cannot. As numerous academics have proved, automation (ML) in Healthcare is becoming increasingly significant. ML is being used in applications like Electroencephalogram and tumor detection/analysis. Monitoring cardiac rhythms, as well as glucose levels, may be challenging, and even those who are represented at medical institutions. Intermittent heart rate assessments cannot protect against sudden changes in vital signs, and standard techniques of heart rhythm surveillance used in hospitals require patients to be permanently attached to wired apparatus, limiting their mobility.","","978-1-6654-3789-9","10.1109/ICACITE53722.2022.9823441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823441","Deep learning;automatic assistance;Data Acquisition;Data processing;Algorithm;Data Management;Interpretation;statistics;probabilities;data wrangling;imputation;supervised learning;classification;regression;clustering;Internet of things;Health care;Data protection;Security","Deep learning;Machine learning algorithms;Hospitals;Heart beat;Surveillance;Software algorithms;Software","","","","20","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"’Ignore All and Accept My Resume’: The Impact of Prompt Injection in Automatic Resume Screening","L. Aminou; A. Daaif; M. Soulami; M. Youssfi","IIACS Laboratory, ENSET of Mohammedia, Hassan II University of Casablanca, Mohammedia, Morocco; IIACS Laboratory, ENSET of Mohammedia, Hassan II University of Casablanca, Mohammedia, Morocco; IIACS Laboratory, ENSET of Mohammedia, Hassan II University of Casablanca, Mohammedia, Morocco; IIACS Laboratory, ENSET of Mohammedia, Hassan II University of Casablanca, Mohammedia, Morocco","2025 5th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)","26 May 2025","2025","","","1","5","Users of generative AI models—such as ChatGPT, Claude, or any other model—are becoming familiar with prompt manipulation, contributing to the Prompt Engineering as an emerging technical field. Indeed, every new technology possesses a dual nature and can be able of being exploited for both beneficial and harmful purposes. The harmful aspect in the field of prompt engineering is known as Prompt Injection. The latter, which is a harmful practice used in different situations, involves manipulating AI systems by injecting malicious instructions. In the context of resume screening, job candidates often exploit this technique to manipulate the applicant tracking system (ATS) by inserting a harmful prompt into the resume to be approved. By examining recent studies, this review seeks to understand this new area of concern by examining the influence of prompt injection on resume screening. The topic is still general and not tailored to a specific use case, such as resume screening, owing to its novelty. Therefore, we provide an overview to shed light on how this could affect the quality of the hiring decisions.","","979-8-3315-3297-0","10.1109/IRASET64571.2025.11008146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008146","Prompt Injection;Large Language Model;Prompt;ATS;resume;job candidate;resume screening","Sensitivity;Reviews;Large language models;Resumes;Organizations;Prompt engineering;Security;Protection;Recruitment;Investment","","","","24","IEEE","26 May 2025","","","IEEE","IEEE Conferences"
"Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain","Y. Tang; A. A. B. Da Costa; X. Zhang; I. Patrick; S. Khastgir; P. Jennings","WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom",2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC),"13 Feb 2024","2023","","","3893","3900","Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by “chatting” with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains.","2153-0017","979-8-3503-9946-2","10.1109/ITSC57777.2023.10422308","UKRI(grant numbers:MR/S035176/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10422308","large language model;domain ontology distillation;autonomous driving","Knowledge engineering;Runtime;Manuals;Ontologies;Chatbots;Autonomous vehicles;Intelligent transportation systems","","13","","28","IEEE","13 Feb 2024","","","IEEE","IEEE Conferences"
"Large Language Model Based Multi-Objective Optimization for Integrated Sensing and Communications in UAV Networks","H. Li; M. Xiao; K. Wang; D. I. Kim; M. Debbah","Division of Information Science and Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Information Science and Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, Brunel University of London, Uxbridge, U.K.; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer and Information Engineering, KU 6G Research Center, Khalifa University, Abu Dhabi, UAE",IEEE Wireless Communications Letters,"10 Apr 2025","2025","14","4","979","983","This letter investigates an un-crewed aerial vehicle (UAV) network with integrated sensing and communication (ISAC) systems, where multiple UAVs simultaneously sense the locations of ground users with radars and provide communication services. To find the trade-off between communication and sensing (C&S) in the system, we formulate a multi-objective optimization problem (MOP) to maximize the total network utility and the localization Cramér-Rao bounds (CRB) of ground users, which jointly optimizes the deployment and power control of UAVs. Inspired by the huge potential of large language models (LLM) for prediction and inference, we propose an LLM-enabled decomposition-based multi-objective evolutionary algorithm (LEDMA) for solving the highly non-convex MOP. We first adopt a decomposition-based scheme to decompose the MOP into a series of optimization sub-problems. We second integrate LLMs as black-box search operators with MOP-specifically designed prompt engineering into the framework of MOEA to solve optimization sub-problems simultaneously. Numerical results demonstrate that the proposed LEDMA can find the clear trade-off between C&S and outperforms baseline MOEAs in terms of obtained Pareto fronts and convergence.","2162-2345","","10.1109/LWC.2025.3529082","Horizon Europe MSCA Project “Secured and Intelligent Massive Machine-to-Machine Communication for 6G (SCION);”; “Autonomous Vehicular Edge Computing and Networking for Intelligent Transportation (ASCENT);” in part by the Horizon Europe COVER Project(grant numbers:101086228); UK Research and Innovation(grant numbers:EP/Y028031/1); Royal Society Industry Fellow Scheme(grant numbers:IF-R2-23200104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839306","Integrated sensing and communications;multi-objective optimization;large language model","Autonomous aerial vehicles;Optimization;Integrated sensing and communication;Vectors;Radar cross-sections;Power control;Evolutionary computation;Air to ground communication;Sorting;Signal to noise ratio","","4","","13","IEEE","13 Jan 2025","","","IEEE","IEEE Journals"
"PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought","J. Yao; Y. Liu; Z. Dong; M. Guo; H. Hu; K. Keutzer; L. Du; D. Zhou; S. Zhang","Peking University; Nanjing University; University of California, Berkeley; Stanford University; Peking University; University of California, Berkeley; Nanjing University; Bytedance; Peking University",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","7027","7037","Diffusion-based generative models have exhibited remarkable capability in the production of high-fidelity visual content such as images and videos. However, their performance is significantly contingent upon the quality of textual inputs, commonly referred to as ‘'prompts'. The process of traditional prompt engineering necessitates empirical exper-tise and poses challenges for inexperienced users. In this paper, we introduce PromptCoT, an innovative enhancer that autonomously refines prompts for users. PromptCoT is designed based on the observation that prompts, which re-semble the textual information of high-quality images during training, lead to superior generation performance. Therefore, we fine-tune the Large Language Models (LLM) using a curated text dataset that comprises descriptions of high-quality visual content. Consequently, the LLM can capture the distribution of high-quality texts, enabling it to boost the original texts. Nonetheless, one drawback of LLMs is their tendency to generate irrelevant information. We employ a tailored Chain-of-Thought (CoT) mechanism to address the problem. Our CoT can extract and amalgamate crucial information from the prompt candidates, enabling a reasonable process based on the contextual cues to produce a more comprehensive and nuanced output. Considering computational efficiency, instead of allocating a dedicated LLM to each individual model or dataset, we integrate adapters that facil-itate task-specific adaptation, leveraging a shared LLM as the foundation for this process. With independent fine-tuning of adapters, we can adapt PromptCoT to new datasets while minimally increasing training costs and memory usage. We evaluate the effectiveness of PromptCoT by assessing on widely-used latent diffusion models for visual generation. The results demonstrate significant improvements in key performance metrics.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.00671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10656469","","Training;Adaptation models;Visualization;Computational modeling;Large language models;Semantics;Text to image","","2","","58","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"A Foundation Model Approach to Detect Machine Generated Text","J. Pan","Home Team Science and Technology Agency, Singapore",TENCON 2023 - 2023 IEEE Region 10 Conference (TENCON),"22 Nov 2023","2023","","","405","408","Large Language Models with autoregression generative capabilities like ChatGPT have garnered lots of attention from its launch. However, the cyber security community is also wary of the threats that it poses with cybercriminal and cyber security threat related activities. It could generate highly deceptive phishing and social engineering attacks that could evade human detection and render existing phishing or social engineering detection tools useless. Inspired by the approach used to develop Foundation Model that resulted with amazing capabilities from the contemporary model constructs like ChatGPT, our research endeavour demonstrates a model construct developed using Foundation model approach could yield potential as defensive tool to detect GPT generated text. Preliminary evaluation results show promising results.","2159-3450","979-8-3503-0219-6","10.1109/TENCON58879.2023.10322333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322333","Foundation Model;Machine Generated Text Detector;Transformer","Phishing;Chatbots;Computer crime;IEEE Regions","","","","10","IEEE","22 Nov 2023","","","IEEE","IEEE Conferences"
"BERT-PhishFinder: A Robust Model for Accurate Phishing URL Detection With Optimized DistilBERT","A. Aljofey; S. A. Bello; J. Lu; C. Xu","Shenzhen Key Laboratory of Advanced Machine Learning and Applications, College of Mathematics and Statistics, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Machine Learning and Applications, College of Mathematics and Statistics, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Machine Learning and Applications, College of Mathematics and Statistics, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Machine Learning and Applications, College of Mathematics and Statistics, Shenzhen University, Shenzhen, China",IEEE Transactions on Dependable and Secure Computing,"10 Jul 2025","2025","22","4","4315","4329","Phishing URL detection has become a critical challenge in cybersecurity, with existing methods often struggling to maintain high accuracy while generalizing across diverse datasets. In this article, we introduce BERT-PhishFinder, a novel and efficient transformer-based model designed to tackle this problem. While most traditional approaches rely heavily on lexical features or complex convolutional architectures, BERT-PhishFinder leverages the power of DistilBERT, a lightweight yet highly effective transformer, to capture rich contextual representations of URL sequences. To enhance the model’s robustness and reduce overfitting, we strategically incorporate SpatialDropout1D in the embedding layers, along with global average pooling and global max pooling techniques to extract both comprehensive and key discriminative features. The pooled representations are thoughtfully concatenated to form a comprehensive feature representation. Through this carefully crafted design, our model adopts ensemble learning, as it undergoes multiple parallel dense layers, each with distinct parameters and dropout regularization. This facilitates learning diverse patterns and features from the input URL sequence, culminating in exceptional phishing URL detection performance. Extensive evaluations against conventional deep learning algorithms, transformer models (XLNet, RoBERTa, ALBERT), and other existing methods on five benchmark datasets show that BERT-PhishFinder not only achieves the state of-the-art real phishing URL detection but also accomplishes this with reduced label dependency.","1941-0018","","10.1109/TDSC.2025.3545771","National Natural Science Foundation of China(grant numbers:62372302,U21A20455,12326619); Natural Science Foundation of Guangdong Province of China(grant numbers:2024A1515011913,2023A1515011691); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904020","Cyber security;phishing URL detection;deep learning;transformers;DistilBERT","Phishing;Uniform resource locators;Feature extraction;Transformers;Accuracy;Convolutional neural networks;Machine learning;Encoding;Deep learning;Computational modeling","","","","42","IEEE","25 Feb 2025","","","IEEE","IEEE Journals"
"Comprehensive AI-Powered Healthcare Management System","B. D. Rao; G. Madhukar; B. N. Reddy; S. K. Voni; N. S. V. Kumar","Department of CSE, MLR Institue of Technology, Hyderabad, India; Department of CSE, MLR Institue of Technology, Hyderabad, India; Department of CSE, MLR Institue of Technology, Hyderabad, India; Department of CSE, MLR Institue of Technology, Hyderabad, India; Department of CSE, MLR Institue of Technology, Hyderabad, India",2025 International Conference on Intelligent Computing and Control Systems (ICICCS),"12 May 2025","2025","","","1017","1022","This is the Comprehensive AI-Powered Healthcare Management System intended to revolutionize healthcare delivery, overcoming the shortcomings of present systems by integrating multiple sources of information to predict disease onset and administering personalized care, all based on advanced technologies such as Artificial Intelligence and Machine Learning integrated into Blockchain. The system will have early disease detection and tailor-made treatment plans and holistic patient care. The key algorithms include Random Forest, Support Vector Machine, and Neural Networks. It will deploy Convolutional Neural Networks for the analysis of medical images and Natural Language Processing techniques through the application of transformer models such as BERT. Key Technologies to be used are PyTorch, TensorFlow, DialogFlow, and Ethereum. This project shall be developed in phases starting from collecting and integrating diverse health data. Expected output is a fully functional healthcare management platform for the enhancement of patient outcomes, facilitation of greater efficiency by health providers, and secure health data management. These diverse applications have functionalities in improving diagnostic accuracy and patient management in clinics, remote monitoring of patients with chronic diseases, prediction of mental health crisis incidents, and safe storage of patient data through blockchain integration.","","979-8-3315-1208-8","10.1109/ICICCS65191.2025.10984436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10984436","Healthcare Informatics;Artificial Intelligence in Healthcare;Machine Learning for Disease Prediction;Predictive;Analytics in Healthcare;Medical Image Analysis;Blockchain in Healthcare;Data Security and Privacy in Healthcare","Data privacy;Accuracy;Data security;Medical services;Transforms;Transformers;Natural language processing;Blockchains;Medical diagnostic imaging;Diseases","","","","7","IEEE","12 May 2025","","","IEEE","IEEE Conferences"
"Temperature Forecasting of Grain in Storage: An Improved Approach Based on Broad Learning Network","Q. Wang; M. Hou; Y. Qin; F. Lian","College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; College of Information Science and Engineering, Henan University of Technology, Zhengzhou, China",IEEE Access,"29 Aug 2024","2024","12","","115112","115123","Temperature forecasting of grain in storage is crucial for timely granary temperature control, mitigating adverse effects of extreme temperatures on grain quality. Traditional machine learning methods struggle with stability and high error rates in grain storage temperature forecasting, while deep learning models are more accurate but time-consuming and have heavy parameters. To address these problems, an improved model with light weight and good accuracy is proposed in this paper, which broad learning network is combined with one-dimensional convolution module and multi-head self-attention mechanism (BLN-1DCNN-MHSA). Firstly, we employ a one-dimensional convolution module at the feature nodes of the model to extract local temporal correlations, compensating for temporal sequence learning limitations of the BLN. Secondly, a multi-head self-attention mechanism at the enhancement nodes to captures important features dependencies and global temporal correlations. Lastly, our model achieves better prediction through enhanced representation ability of model nodes. The results with real grain storage temperature data demonstrate that the RMSE, MAPE, and MAE of the proposed model are 0.341, 0.54%, 0.28, respectively, which represent more than 2 times improvement in accuracy compared to the BLN, and it also reduces training time by more than 90% compared with LSTM and Transformer models. Additionally, the generalization and robustness of the improved approach are demonstrated through promising results in a classification experiment on the MNIST dataset. In general, the model provides a certain feasibility for early warning of grain storage risks by predicting its temperature trends.","2169-3536","","10.1109/ACCESS.2024.3417533","National Natural Science Foundation of China(grant numbers:62373136); Special Project for Key Research and Development of Henan Province(grant numbers:221111230300); Special Project for Scientific Research and Development of Henan Academy of Sciences(grant numbers:230607048); Science and Technology Opening Cooperation Project of Henan Academy of Sciences(grant numbers:220907016); Open Fund of the Key Laboratory of Grain Information Processing and Control(grant numbers:KFJJ-2021-103); Key Scientific Research Project Program of Universities of Henan Province(grant numbers:22A510014); Innovative Funds Plan of Henan University of Technology(grant numbers:2022ZKCJ02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568119","Grain storage temperature forecasting;grain storage security;broad learning network;multi-head self-attention;convolutional neural network","Predictive models;Feature extraction;Forecasting;Accuracy;Data models;Temperature distribution;Mathematical models;Storage management;Convolutional neural networks","","","","44","CCBYNCND","21 Jun 2024","","","IEEE","IEEE Journals"
"FLY-LLM Sim: A Novel Integration of UAV and LLM Lab Platform","K. M. Guntupalli; A. Raja","Computer Information Technology and Graphics, Purdue University Northwest, Hammond, Indiana, USA; Computer Information Technology and Graphics, Purdue University Northwest, Hammond, Indiana, USA",2025 IEEE World AI IoT Congress (AIIoT),"12 Aug 2025","2025","","","0259","0265","Recent developments in Large Language Models (LLMs) have created new possibilities to integrate integrate these models into robotics platforms, allowing unmanned aerial vehicles (UAVs) to have enhanced control and decision-making capabilities. The UAV market has experienced significant growth in recent years, accompanied by a surge in UAV-related job opportunities. The need for LLM-powered UAVs is expected to increase significantly as LLM advances. Therefore, it is essential to provide the upcoming generation of professionals, students, and educators with the knowledge and abilities they need to develop, implement, and operate LLM-driven UAV systems efficiently. Nevertheless, there is a absence of training materials and education on LLM-powered UAVs, particularly for practical, hands-on learning. We proffer a novel Fly-Llm Sim lab platform that provides effective and efficient hands-on practice. Our lab platform comprises different simulation environments and preconfigured lab modules. The outcome of these lab modules is to educate and train users on LLM-powered UAV to perform advanced aerial route planning and perform vision-based cyber attack analysis. The lab platform will be open source, allowing other developers to customize it to their needs. Hence, our platform adopts a plug-in-based design to support the customization of the lab modules. Our evaluation results show that our platform is effective and efficient in training and educating students and professionals in operating LLM-powered UAVs via natural language commands and developing custom task-specific prompts using prompt engineering techniques.","","979-8-3315-2508-8","10.1109/AIIoT65859.2025.11105292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11105292","UAV;LLM;Interactive learning;Simulation;Laboratory","Training;Industries;Large language models;Natural languages;Decision making;Autonomous aerial vehicles;Planning;Prompt engineering;Surges;Robots","","","","23","IEEE","12 Aug 2025","","","IEEE","IEEE Conferences"
"AI-Assisted Outcome Engineering for Mapping Natural Language to Radar Configuration Files","D. L. Young; E. C. Larson; M. A. Thornton","Darwin Deason Institute of Cybersecurity, Southern Methodist University, Dallas, TX, USA; Darwin Deason Institute of Cybersecurity, Southern Methodist University, Dallas, TX, USA; Darwin Deason Institute of Cybersecurity, Southern Methodist University, Dallas, TX, USA",2025 IEEE 18th Dallas Circuits and Systems Conference (DCAS),"25 Jun 2025","2025","","","1","6","This paper shows how to use Large Language Models (LLMs) and DSPy prompt engineering to bridge the gap between expert knowledge and user natural language inputs. This approach improves radar system usability by leveraging the DSPy ReAct module and MIPROv2 prompt optimization to facilitate the dynamic generation of radar technical parameters.","2766-5186","979-8-3315-9934-8","10.1109/DCAS65331.2025.11045499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11045499","","Knowledge engineering;Circuits and systems;Large language models;Natural languages;Radar applications;Prompt engineering;Usability;Optimization","","","","11","IEEE","25 Jun 2025","","","IEEE","IEEE Conferences"
"Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models","G. Tziafas; H. Kasaei","Department of Artificial Intelligence, University of Groningen, the Netherlands; Department of Artificial Intelligence, University of Groningen, the Netherlands",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","515","522","Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully handcrafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611448","","Limiting;Heuristic algorithms;Large language models;Memory modules;Libraries;Complexity theory;Prompt engineering","","","","70","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning for intelligent resource allocation in IIoT networks","J. Rosenberger; M. Urlaub; D. Schramm","Automation and Electrification Solutions, Bosch Rexroth AG, Lohr am Main, Germany; Automation and Electrification Solutions, Bosch Rexroth AG, Lohr am Main, Germany; Chair of Mechatronics, University of Duisburg-Essen, Duisburg, Germany",2021 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT),"31 Jan 2022","2021","","","118","119","In the industrial Internet of Things (IIoT), a high number of devices with limited resources, like computational power, memory, bandwidth and, in case of wireless sensor networks, also energy, communicate. At the same time, the amount of data as well as the demand for data processing in the edge is rapidly increasing. To enable Industry 4.0 (I4.0) and the IIoT, an intelligent resource allocation is required to make optimal use of the available resources. For this purpose, a multi-agent system (MAS) based on deep reinforcement learning (DRL) is proposed. Multi-agent reinforcement learning (MARL) is already taken into account in different communication networks, e.g. for intelligent routing. Despite its great potential, little attention is paid to these methods in industry so far. In this work, DRL is applied for resource allocation and load balancing for industrial edge computing. An optimal usage of the available resources of the IIoT devices should be achieved. Due to the structure of IIoT systems as well as for security reasons, a MAS is preferred for decentralized decision making. In subsequent steps, it is planned to add and remove devices during runtime, to change the number of tasks to be executed as well as evaluations on single- and multi-policy-approaches. The following aspects will be considered for evaluation: (1) improvement of the resource usage of the devices and (2) overhead due to the MAS.","","978-1-6654-3841-4","10.1109/GCAIoT53516.2021.9692913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692913","multi-agent-system;deep reinforcement learning;resource allocation;load balancing;industrial internet of things;streaming data","Wireless sensor networks;Runtime;Reinforcement learning;Load management;Routing;Resource management;Security","","8","","14","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"Towards Adaptive AI Governance: Comparative Insights from the U.S., EU, and Asia","V. Kulothungan; D. Gupta","Capitol Technology University, Laurel, Maryland, USA; Dept. of Computer Information Systems, Texas A&M University - Central Texas, Texas, USA",2025 IEEE 11th Conference on Big Data Security on Cloud (BigDataSecurity),"21 May 2025","2025","","","32","38","Artificial intelligence (AI) trends vary significantly across global regions, shaping the trajectory of innovation, regulation, and societal impact. This variation influences how different regions approach AI development, balancing technological progress with ethical and regulatory considerations. This study conducts a comparative analysis of AI trends in the United States (US), the European Union (EU), and Asia, focusing on three key dimensions: generative AI, ethical oversight, and industrial applications. The US prioritizes market-driven innovation with minimal regulatory constraints, the EU enforces a precautionary risk-based framework emphasizing ethical safeguards, and Asia employs state-guided AI strategies that balance rapid deployment with regulatory oversight. Although these approaches reflect different economic models and policy priorities, their divergence poses challenges to international collaboration, regulatory harmonization, and the development of global AI standards. To address these challenges, this paper synthesizes regional strengths to propose an adaptive AI governance framework that integrates risk-tiered oversight, innovation accelerators, and strategic alignment mechanisms. By bridging governance gaps, this study offers actionable insights for fostering responsible AI development while ensuring a balance between technological progress, ethical imperatives, and regulatory coherence.","","979-8-3315-9510-4","10.1109/BigDataSecurity66063.2025.00018","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006825","Artificial intelligence (AI);Regulations;Governance;Security and Privacy","Economics;Ethics;Technological innovation;Biological system modeling;Asia;Coherence;Market research;Regulation;Security;Artificial intelligence","","1","","27","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"A Multi-Agent System Architecture for Deregulated Electricity Market Communications in Nigeria","J. O. Dadam","Department of Electrical & Computer Engineering, Afe Babalola University, Ado-Ekiti, Nigeria",2019 IEEE PES/IAS PowerAfrica,"8 Dec 2019","2019","","","36","41","The deregulation of electricity market in Nigeria has opened up the requirement for information exchange among the market players in order to guarantee the reliability and security of the interconnected power system. Information and data exchange in heterogeneous data environment of the diverse market players are difficult due different data model and organization. This paper presents a bottom-up modeling approach of information exchange system for Nigerian deregulated electric power market using Multi-Agents System (MAS). To construct the MAS system, the functionalities of the agents, the interactions among the agents, and agent architecture are designed. A MAS for data communication based on the proposed architecture is an effective way of integrating heterogeneous data sources from the diverse players in the electricity market in Nigeria. MAS is well suitable for considering different scenarios of information exchange among market players as well as testing different requirements of information exchange in the electricity market.","","978-1-7281-1010-3","10.1109/PowerAfrica.2019.8928636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928636","Data Communication;Deregulated Electricity Market;Information Exchange;Multi-Agent System;Ontology Language","Electricity supply industry;Meters;Information exchange;Data communication;Multi-agent systems;Power systems;Memory","","","","30","IEEE","8 Dec 2019","","","IEEE","IEEE Conferences"
"Social Media Governance and Fake News Detection Integrated with Artificial Intelligence Governance","B. Thuraisingham; T. Thomas","Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas; Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas",2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI),"8 Oct 2024","2024","","","190","197","Social Media Systems such as Facebook, Instagram, and Twitter (i.e., X) are exploding. These systems need proper governance so that the users are safe and post accurate information. This paper focuses on social media governance with an emphasis on Artificial Intelligence. First, we discuss various aspects of governance of such policies, procedures and risk and then address a key topic which is detecting fake news on social media. In order for the users to be safe using social media we have to ensure that the governance aspects also include fake news detection. Many of the fake news detection techniques utilize Machine Learning (ML) and Artificial Intelligence (AI) and more recently Generative AI (GenAI) techniques. Therefore, the AI systems that implement the various techniques have to be trustworthy. That means these systems have to be secure as well as ensure fairness, privacy and integrity. Therefore, the paper will also discuss AI Governance as an integral part of Social Media Governance. Finally, to support the various applications and frameworks, both data and the cloud are critical. Large amounts of data are stored and managed by the social media systems as well as used to train the AI models. Furthermore, we need massive amounts of computing power that can be provided by the cloud. Therefore, we will also discuss data and cloud governance that provides the infrastructure for social media and AI governance.","2835-5776","979-8-3503-5118-7","10.1109/IRI62200.2024.00048","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10703211","Social Media Governance;Fake News Detection;Machine Learning;Artificial Intelligence Governance;Security;Privacy;Fairness;Integrity","Privacy;Social networking (online);Generative AI;Machine learning;Data science;Rough surfaces;Web sites;Multimedia communication;Artificial intelligence;Fake news","","","","22","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Cloning Object Detectors with Limited Access to In-Distribution Samples","A. Aarts; W. Michiels; P. Roelse","Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Irdeto B.V., Hoofddorp, The Netherlands",2023 IEEE 13th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),"2 Jan 2024","2023","","","62","67","An object detector identifies and locates objects in images. Object detectors are widely deployed in practice, for example in driver assistance systems. Recently it has been shown that state-of-the-art object detectors based on neural networks can be cloned successfully if the adversary has oracle access to the detector, and if the adversary has access to either: (i) sufficiently many images drawn from the same distribution as the images of the detector's train set, also referred to as in-distribution samples, or (ii) a publicly available generative AI network capable of generating images that are close to in-distribution samples. This paper presents a new cloning attack that uses images from a publicly and freely available dataset, referred to as out-of-distribution samples, and a limited number of in-distribution samples. The new attack includes a strategy for combining in-and out-of-distribution samples during training and a calibration step to better mimic the functionality of the oracle detector. Our experiments show that CenterNet and RetinaNet object detectors trained with the Oxford-IIIT Pet, the WIDER FACE, or the Tsinghua-Tencent 100K dataset can be cloned successfully using images from the ImageNet-1K dataset supplemented with a limited number of in-distribution samples.","2166-6822","979-8-3503-2415-0","10.1109/ICCE-Berlin58801.2023.10375681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375681","Object detection;security;model extraction","Training;Neural networks;Cloning;Detectors;Calibration;Object recognition;Artificial intelligence","","","","17","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"Forensic Investigations in the Age of AI: Identifying and Analyzing Artifacts from AI-Assisted Crimes","A. Alnaqbi; M. Alblooshi; H. A. Nasser; N. Habtom; F. Iqbal","Zayed University, Abu Dhabi, UAE; Zayed University, Abu Dhabi, UAE; Zayed University, Abu Dhabi, UAE; Zayed University, Abu Dhabi, UAE; Zayed University, Abu Dhabi, UAE",2025 13th International Symposium on Digital Forensics and Security (ISDFS),"2 Jun 2025","2025","","","1","6","This paper examines the possible manipulation of generative AI models, in particular ChatGPT, to support cybercriminal activities. The research investigates the possibility of coercing AI into assisting malicious actors by setting up a suspect in a cybercrime scenario. In this paper, using a Windows machine and an Android Mobile, we simulate a scenario in which ChatGPT is manipulated to assist a cybercriminal and conduct forensic analysis to reveal evidence of AI involvement. Through integrated memory, mobile, and network forensics, we successfully extract AI-generated artifacts including session tokens, interaction histories, and manipulated prompts that circumvent AI safety measures. Key findings demonstrate how forensic techniques can effectively identify AI involvement in cybercrimes through volatile memory analysis and network traffic examination and how prompt engineering can successfully bypass AI safety measures, exposing vulnerabilities in current safeguards. The paper highlights both opportunities and risks for generative AI in criminal contexts through experimental analysis and forensic techniques. It further discusses ethical and legal issues concerning accountability, misuse of technology, and the potential for harmful behaviors facilitated by AI. These findings demonstrate an imperative need for robust safeguards and specialized forensic approaches to address the exploitation of AI in criminal activities.","2768-1831","979-8-3315-0993-4","10.1109/ISDFS65363.2025.11012109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11012109","ChatGPT;Forensics;AI;Mobile Forensics;Memory Forensics;Network Forensics","Ethics;Forensics;Current measurement;Digital forensics;Telecommunication traffic;Chatbots;Safety;Prompt engineering;History;Computer crime","","","","9","IEEE","2 Jun 2025","","","IEEE","IEEE Conferences"
"Analysis of ChatGPT-Generated Codes Across Multiple Programming Languages","S. Almanasra; K. Suwais","Faculty of Computer Studies, Arab Open University, Riyadh, Saudi Arabia; Faculty of Computer Studies, Arab Open University, Riyadh, Saudi Arabia",IEEE Access,"10 Feb 2025","2025","13","","23580","23596","Our research focuses on the intersection of artificial intelligence (AI) and software development, particularly the role of AI models in automating code generation. With advancements in large language models like ChatGPT, developers can now generate code from natural language prompts, a task that traditionally required significant manual input and expertise. AI-generated code promises to boost productivity by enabling faster prototyping and automating repetitive coding tasks. However, as these models are increasingly adopted in real-world applications, questions surrounding their efficiency and code quality become critical. This research investigates ChatGPT-4o, a state-of-the-art language model, and its ability to generate functional, high-quality code in different programming languages. By comparing performance between Python and Java, the study seeks to shed light on AI’s capabilities and limitations in code generation, addressing not only functional correctness but also broader software engineering concerns such as memory usage, runtime efficiency, and maintainability. The study addresses key questions related to the performance, code quality, and error management of AI-generated code by analyzing solutions for 300 data structure problems and 300 problems from the LeetCode platform. The findings reveal notable performance differences between the two languages: Java demonstrated superior runtime performance, particularly for medium and hard problems, while Python exhibited better memory efficiency across all complexity levels. The research also highlighted significant gaps in code quality, with both languages showing deficiencies in documentation and exception management. This study contributes to the literature by offering a comprehensive cross-language analysis of ChatGPT-4o’s programming capabilities, addressing a gap in the evaluation of AI-generated code performance.","2169-3536","","10.1109/ACCESS.2025.3538050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870152","Artificial intelligence;software development;ChatGPT;large language model;programming","Codes;Programming;Encoding;Chatbots;Runtime;Python;Software development management;Java;Complexity theory;Standards","","3","","35","CCBY","3 Feb 2025","","","IEEE","IEEE Journals"
"STEP: Generating Semantic Text Embeddings with Prompt","W. Cao; Q. Li; S. Zhang; R. Xu; Y. Li","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Tianjin Jinnan Meteorological Service, Tianjin, China; Peking University, Beijing, China; National Computer Network Emergency Response Technical Team/Coordination Center, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China",2023 Eleventh International Conference on Advanced Cloud and Big Data (CBD),"8 May 2024","2023","","","180","185","In recent years, semantic embeddings for text has played a bigger role in the field of natural language processing (NLP), additionally, it has shown great potential in real-life applications like search and recommendation systems. Therefore, models for generating semantic text embeddings have received extensive study. State-of-the-art solutions for text embeddings have evolved from traditional methods (like Word2Vec, Glove, etc.) to deep neural network based solutions (such as LSTM, Transformer, and pre-trained models like BERT and RoBERTa, etc), besides, frameworks like Sentence Transformer have already lowered the bar of training models for semantic text representation using customized models and datasets. In this paper, we investigated several well trained models according to Massive Text Embedding Benchmark (MTEB) in Huggingface website. Enlighted by the extensive use of prompt engineering in large language models like Llama or GPT3, we proposed STEP: a novel method using prompt to improve performance of text embeddings on downstream tasks, making it applicable to almost any pre-trained language models for text embeddings. Besides, STEP does not need to modify base model structure. In the experiment, we applied STEP to five pre-trained models chosen from MTEB, trained and evaluated our approach on two separated datasets, final results indicated that our approach could improve performance of tasks related to semantic text similarity.","","979-8-3503-5337-2","10.1109/CBD63341.2023.00040","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516563","embedding;prompt;semantic;NLP","Training;Intent recognition;Semantics;Big Data;Transformers;Natural language processing;Task analysis","","","","31","IEEE","8 May 2024","","","IEEE","IEEE Conferences"
"LLM Assistance for Memory Safety","N. Mohammed; A. Lal; A. Rastogi; R. Sharma; S. Roy","Microsoft Research, India; Microsoft Research, India; Microsoft Research, India; Microsoft Research, India; IIT-Kanpur, India",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1717","1728","Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029895","Automating Software Engineering;Memory Safety;Annotation Inference","Trusted computing;Codes;Runtime;Annotations;Large language models;Static analysis;Cognition;Safety;Software reliability;Formal verification","","1","","35","IEEE","23 Jun 2025","","","IEEE","IEEE Conferences"
"Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code","J. K. Das; S. Mondal; C. K. Roy","Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 May 2025","2025","","","68","79","Large language models (LLMs) like ChatGPT have shown the potential to assist developers with coding and debugging tasks. However, their role in collaborative issue resolution is underexplored. In this study, we analyzed 1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine the diverse usage of ChatGPT and reliance on its generated code. Our contributions are fourfold. First, we manually analyzed 289 conversations to understand ChatGPT's usage in the GitHub Issues. Our analysis revealed that ChatGPT is primarily utilized for ideation, whereas its usage for validation (e.g., code documentation accuracy) is minimal. Second, we applied BERTopic modeling to identify key areas of engagement on the entire dataset. We found that backend issues (e.g., API management) dominate conversations, while testing is surprisingly less covered. Third, we utilized the CPD clone detection tool to check if the code generated by ChatGPT was used to address issues. Our findings revealed that ChatGPT-generated code was used as-is to resolve only 5.83% of the issues. Fourth, we estimated sentiment using a RoBERTa-based sentiment analysis model to determine developers' satisfaction with different usages and engagement areas. We found positive sentiment (i.e., high satisfaction) about using ChatGPT for refactoring and addressing data analytics (e.g., categorizing table data) issues. On the contrary, we observed negative sentiment when using ChatGPT to debug issues and address automation tasks (e.g., GUI interactions). Our findings show the unmet needs and growing dissatisfaction among developers. Researchers and ChatGPT developers should focus on developing task-specific solutions that help resolve diverse issues, improving user satisfaction and problem-solving efficiency in software development.","2640-7574","979-8-3315-3510-0","10.1109/SANER64311.2025.00015","Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10992523","ChatGPT;Issue Tracker;Dev-GPT Conversation;Code Reliance","Analytical models;Codes;Data analysis;Automation;Collaboration;Oral communication;Debugging;Chatbots;Software development management;Testing","","","","80","IEEE","21 May 2025","","","IEEE","IEEE Conferences"
"One Sentence Can Kill the Bug: Auto-Replay Mobile App Crashes From One-Sentence Overviews","Y. Huang; J. Wang; Z. Liu; M. Li; S. Wang; C. Chen; Y. Hu; Q. Wang","State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China; York University, Toronto, Canada; Technical University of Munich, Munich, Germany; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Software Engineering,"17 Apr 2025","2025","51","4","975","989","Crash reports play a crucial role in software maintenance as they inform developers about the issues encountered in mobile applications. Developers must reproduce the reported crash before fixing it, which is extremely time-consuming and tedious. Existing studies have focused on automatic crash reproduction with step-by-step instructions. However, a non-neglectable portion of crash reports only provides a one-sentence overview, which merely describes the final crash-triggering action. These reports require developers to invest more effort in understanding and fixing the issues while existing techniques cannot handle them due to the lack of step-by-step guidance, thus calling for a greater need for automatic support. Leveraging the capability of Large Language Models (LLMs) in combining acting and reasoning, we propose ReActDroid, an automated approach to reproduce mobile application crashes directly from the crash overview. ReActDroid utilizes ReAct prompting to augment the app-specific knowledge and exploration history, enabling the LLM to derive the necessary steps for triggering the crash from a comprehensive and historical perspective. We evaluate ReActDroid on 102 crash reports from 69 popular Android apps and successfully reproduce 57.8% of the crashes, surpassing the performance of state-of-the-art baselines by 69% to 321%. Besides, the average reproducing time is 51.8 seconds, outperforming the baselines by 73% to 949%. We also evaluate the usefulness of ReActDroid with promising results.","1939-3520","","10.1109/TSE.2025.3535938","National Natural Science Foundation of China(grant numbers:62232016,62072442,62272445); Youth Innovation Promotion Association of the Chinese Academy of Sciences; Basic Research Program of ISCAS(grant numbers:ISCAS-JCZD-202304); Major Program of ISCAS(grant numbers:ISCAS-ZD-202302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869838","Mobile application testing;issue report;large language model","Computer crashes;Computer bugs;Graphical user interfaces;Mobile applications;Visualization;Large language models;Recording;Codes;Navigation;Adaptation models","","","","61","IEEE","3 Feb 2025","","","IEEE","IEEE Journals"
"Trust but Verify: Cryptographic Data Privacy for Mobility Management","M. Tsao; K. Yang; S. Zoepf; M. Pavone","Stanford University, Stanford, CA, USA; Stanford University, Stanford, CA, USA; Lacuna Technologies, Palo Alto, CA, USA; Stanford University, Stanford, CA, USA",IEEE Transactions on Control of Network Systems,"26 May 2022","2022","9","1","50","61","The era of big data has brought with it a richer understanding of user behavior through massive datasets, which can help organizations optimize the quality of their services. In the context of transportation research, mobility data can provide municipal authorities (MAs) with insights on how to operate, regulate, or improve the transportation network. Mobility data, however, may contain sensitive information about end users and trade secrets of mobility providers (MPs). Due to this data privacy concern, MPs may be reluctant to contribute their datasets to MA. Using ideas from cryptography, we propose an interactive protocol between an MA and an MP, in which MA obtains insights from mobility data without MP having to reveal its trade secrets or sensitive data of its users. This is accomplished in two steps: 1) a commitment step and 2) a computation step. In the first step, Merkle commitments and aggregated traffic measurements are used to generate a cryptographic commitment. In the second step, MP extracts insights from the data and sends them to MA. Using the commitment and zero-knowledge proofs, MA can certify that the information received from MP is accurate, without needing to directly inspect the mobility data. We also present a differentially private version of the protocol that is suitable for the large query regime. The protocol is verifiable for both MA and MP in the sense that dishonesty from one party can be detected by the other. The protocol can be readily extended to the more general setting with multiple MPs via secure multiparty computation.","2325-5870","","10.1109/TCNS.2022.3141027","National Science Foundation(grant numbers:CMMI-1454737,CNS-1837135); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:P400P2_199332); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672747","Cyber-physical systems;networked control systems;security and privacy;transportation networks","Protocols;Cryptography;Urban areas;Differential privacy;Vehicles;Privacy;Data models","","12","","30","IEEE","6 Jan 2022","","","IEEE","IEEE Journals"
"Investigations on Secrecy Performance of Downlink Overlay CR-NOMA System With SIC Imperfections","K. K. Godugu; S. Vappangi","School of Electronics Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; School of Electronics Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India",IEEE Access,"7 Feb 2024","2024","12","","18051","18072","Cognitive radio (CR) and non-orthogonal multiple access (NOMA) are two technologies witnessed to offer tremendous possibilities for the next generation wireless networks to maximise their usage of available spectrum. In this work, we evaluate the performance of a downlink overlay secure CR-NOMA system while the secondary transmitter (ST) is used as a decode-and-forward (DF) relay to assist the primary transmitter (PT) to transmit information to the destination i.e., primary user (PU), while covertly transmitting its own information to the secondary user (SU) against the eavesdropper (Eve) of PT. The secrecy performance comparison between two users i.e., PU and SU are obtained under perfect and imperfect successive interference cancellation (SIC), respectively. Furthermore, this paper investigates the secrecy performance comparison between the proposed overlay downlink CR-NOMA system comprising of single antenna (SA) and multiple antennas (MA) in terms of various performance metrics such as secrecy sum rate (SSR), the average secrecy rate (ASR), the average secrecy sum rate (ASSR), strictly positive secrecy rate (SPSC) and secrecy outage probability (SOP). It is to be noted that in our proposed system, MAs are equipped at both PT and ST with the main purpose to provide cooperative diversity and we employ both maximal ratio combining (MRC) and selection combining (SC) diversity techniques for processing the received signals at the PU/Eve which further improves the system’s capacity and enhances the secrecy performance. In addition, for characterizing the secrecy performance of the proposed overlay CR-NOMA network, we present thoroughly the derivations of novel closed-form analytical expressions of the performance metrics such as SOP, SPSC and the ASR by taking into account both perfect SIC (pSIC) and imperfect SIC (ipSIC) scenarios. Based on the analytical frameworks, the numerical and simulation results are obtained under different network parameters. Towards this end, the outcomes of the simulation are shown to prove both the reliability of the mathematical analysis and the accuracy of the suggested technique.","2169-3536","","10.1109/ACCESS.2024.3361038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418214","Non-orthogonal multiple access;physical layer security;overlay cognitive radio;secrecy rate;average secrecy rate;strictly positive secrecy rate","NOMA;Security;Relays;Jamming;Interference cancellation;Uplink;Wireless networks","","5","","43","CCBYNCND","1 Feb 2024","","","IEEE","IEEE Journals"
"An Integrated Localization and Control Framework for Multi-Agent Formation","Y. Cai; Y. Shen","Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; Beijing National Research Center for Information Science and Technology, Beijing, China",IEEE Transactions on Signal Processing,"5 Mar 2019","2019","67","7","1941","1956","High-accuracy formation is essential for multi-agent systems to accomplish certain tasks, and the accuracy of the formation is determined jointly by the network localization and formation control procedures. Existing studies commonly treat the two procedures separately in the system design, leading to suboptimal formation performance. This paper establishes a general framework for high-accuracy multi-agent formation via integrated localization and control. In particular, we propose a new metric called the formation error to characterize the minimum squared distance between two formations for arbitrary translation and rotation, and develop an integrated localization and control scheme to minimize the mean formation error (MFE). Theoretical bounds for the MFE are derived in a closed form, which guides the integrated design of the sensing strategy and control policy. In the case study, we develop efficient integrated algorithms for multi-agent formation under spectrum resource constraints. Numerical results validate the performance gain of the proposed algorithms over existing ones as well as demonstrate the effects of the network parameters on formation performance.","1941-0476","","10.1109/TSP.2019.2897968","National Natural Science Foundation of China(grant numbers:61871256,91638204,61811530329); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636259","Formation control;formation error;integrated localization and control;resource allocation","Current measurement;Position measurement;Sensors;Geometry;Time measurement;Task analysis","","32","","45","IEEE","6 Feb 2019","","","IEEE","IEEE Journals"
"Performance Evaluation of a Prototype UAV-Based Secure Communication System Employing ROS and Chaotic Communications","N. Souli; S. G. Stavrinides; P. Kardaras; R. Picos; M. Karatzia; P. Kolios; G. Ellinas","Department of Electrical and Computer Engineering, KIOS Research and Innovation Center of Excellence, University of Cyprus; Physics Department, International Hellenic University, Greece; Department of Electrical and Computer Engineering, KIOS Research and Innovation Center of Excellence, University of Cyprus; Department of Industrial Engineering and Construction, University of Balearic Islands, Spain; Department of Electrical and Computer Engineering, KIOS Research and Innovation Center of Excellence, University of Cyprus; Department of Computer Science, KIOS Research and Innovation Center of Excellence, University of Cyprus; Department of Electrical and Computer Engineering, KIOS Research and Innovation Center of Excellence, University of Cyprus",2024 International Conference on Unmanned Aircraft Systems (ICUAS),"19 Jun 2024","2024","","","1034","1041","Providing secure communications has become imperative in single- and multi-agent systems that need to ensure the integrity, confidentiality, and availability of the transmitted data, especially when these systems are employed in critical in-frastructure applications. This work presents a novel approach for secure communications between a group of unmanned aerial vehicles (UAVs) to safeguard the confidentiality of the data exchanged in such a multi-agent system, employing the robot operating system (ROS), a virtual private network (VPN), and a chaotic-based communication architecture. Specifically, a custom ROS-based framework is developed to collect and distribute the UAV sensor data, while a VPN network is deployed as the first layer of security for the system. Subsequently, a lightweight chaotic-based module is incorporated as the second layer of security to enable secure communications between the UAVs in real time. To evaluate the proposed system, a prototype multi-UAV system is designed, implemented, and extensively tested in a real-world environment. The proposed system achieves secure real-time communications, with low power consumption and minimal processing resources (CPU and RAM usage), demonstrating its applicability for the energy-constrained UAV-based system under consideration.","2575-7296","979-8-3503-5788-2","10.1109/ICUAS60882.2024.10556964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556964","Secure communications;chaotic communications;unmanned aerial vehicles;multi-agent system","Performance evaluation;Wireless communication;Wireless sensor networks;Chaotic communication;Prototypes;Autonomous aerial vehicles;Real-time systems","","1","","27","IEEE","19 Jun 2024","","","IEEE","IEEE Conferences"
"Using GitHub Copilot for Test Generation in Python: An Empirical Study","K. E. Haji; C. Brandt; A. Zaidman","Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands",2024 IEEE/ACM International Conference on Automation of Software Test (AST),"18 Jun 2024","2024","","","45","55","Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot’s test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately $45.28 \%$ of the tests generated by Copilot are passing tests; $54.72 \%$ of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that $92.45 \%$ of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.","2833-9061","979-8-4007-0588-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556390","","Codes;Runtime;Source coding;Writing;Syntactics;Software;Test pattern generators","","4","","30","","18 Jun 2024","","","IEEE","IEEE Conferences"
"Toward Automated Programming for Robotic Assembly Using ChatGPT","A. Macaluso; N. Cote; S. Chitta","Annabella Macaluso is With the University of California San Diego, La Jolla, CA, USA; Nicholas Cote and Sachin Chitta are With Autodesk Research (Robotics), Autodesk Inc.; Nicholas Cote and Sachin Chitta are With Autodesk Research (Robotics), Autodesk Inc.",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17687","17693","Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610554","","Robotic assembly;Codes;Debugging;Chatbots;Encoding;Cognition;Task analysis","","2","","23","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design","W. Chen; C. Liu; W. Huang; J. Lyu; M. Yang; Y. Du; L. Du; J. Yang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Integrated Circuits, Nanjing University, Nanjing, China; School of Integrated Circuits, Nanjing University, Nanjing, China; Beijing Microelectronics Technology Institute, Beijing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; National Center of Technology Innovation for EDA, Nanjing, China",2025 International Symposium of Electronics Design Automation (ISEDA),"8 Aug 2025","2025","","","201","207","Recent advancements have demonstrated the significant potential of large language models (LLMs) in analog circuit design. Nevertheless, testbench construction for analog circuits remains manual, creating a critical bottleneck in achieving fully automated design processes. Particularly when replicating circuit designs from academic papers, manual Testbench construction demands time-intensive implementation and frequent adjustments, which fails to address the dynamic diversity and flexibility requirements for automation. AnalogTester tackles automated analog design challenges through an LLM-powered pipeline: a) domain-knowledge integration, b) paper information extraction, c) simulation scheme synthesis, and d) testbench code generation with Tsinghua Electronic Design (TED). AnalogTester has demonstrated automated Testbench generation capabilities for three fundamental analog circuit types: operational amplifiers (op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while maintaining a scalable framework for adaptation to broader circuit topologies. Furthermore, AnalogTester can generate circuit knowledge data and TED code corpus, establishing fundamental training datasets for LLM specialization in analog circuit design automation.","","979-8-3315-3696-1","10.1109/ISEDA65950.2025.11100397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11100397","analog circuits;large language model;multi-agent;simulation;testbench generation","Training;Design automation;Codes;Systematics;Regulators;Photonic band gap;Scalability;Large language models;Pipelines;Analog circuits","","","","29","IEEE","8 Aug 2025","","","IEEE","IEEE Conferences"
"Active Model Learning for Software Interrogation and Diagnosis","A. A. Porter; A. F. Karr","Department of Computer Science, USA and Fraunhofer USA Center Mid-Atlantic, University of Maryland College Park; Department of Statistics, Operations, and Data Science, USA and Fraunhofer USA Center Mid-Atlantic, Temple University Philadelphia","2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","17 Sep 2024","2024","","","93","100","We propose extension of the concept of software correctness, and the associated task of software testing, to include interrogation and diagnosis, whereby actionable knowledge about the limitations and performance of the software is gleaned. The new paradigm is especially relevant for AI-based systems such as classifiers and large language models (LLMs) that involve interactions among complex software, underlying training/reference data and the analysis/input data. In this context, the need for both interrogation-to identify problematic outputs when there may be no measure of correctness-and diagnosisto determine where the problem lies-is evident. To make our rhetoric concrete, we present two case studies. Classifier boundaries enable characterization of robustness of classifier output and fragility of inputs. Cliques in metagenomic assembly provide novel insight into the software as well as the potential for performance improvement.","2159-4848","979-8-3503-4479-0","10.1109/ICSTW60967.2024.00026","University of Maryland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10675986","classifier;uncertainty;DNA reads;metagenomic assembly","Software testing;Analytical models;Uncertainty;Measurement uncertainty;Training data;Software systems;Data models","","","","10","IEEE","17 Sep 2024","","","IEEE","IEEE Conferences"
"FaultLLAMA2: A Fine Tuned Large Language Model for Code Recommendations to Reduce Java Lock Contention","A. Shawon; R. Liscano; A. Azim; V. Sundaresan; Y. -K. Chang","Electrical and Computer Engineering, Ontario Tech University, Oshawa, Canada; Electrical and Computer Engineering, Ontario Tech University, Oshawa, Canada; Electrical and Computer Engineering, Ontario Tech University, Oshawa, Canada; IBM Toronto Lab, IBM, Toronto, Canada; IBM Toronto Lab, IBM, Toronto, Canada",2024 34th International Conference on Collaborative Advances in Software and COmputiNg (CASCON),"17 Jan 2025","2024","","","1","9","Lock contention occurs when multiple threads or processes try to access the same lock simultaneously, often causing performance degradation in software that runs concurrent threads. In this paper, we propose a new recommendation model that leverages Large Language Models (LLMs) and suggests refactored source code to alleviate lock contention faults. Our proposed FaultLLAMA2 model is based on a fine-tuned customization of the generalized LLAMA2 LLM model. FaultLLAMA2 is fine-tuned on examples of lock contention anti-patterns and their associated recommendations. Our results show that the accuracy of the FaultLLAMA2 model was on average 2.7 times higher than the LLAMA2 baseline model based on the BLED and Pass@K metrics. We also compared the FaultLLAMA2 model recommendations with a CodeBERT code generation model fine-tuned for lock contention recommendations and its accuracy was 4 times better.","","979-8-3315-0483-0","10.1109/CASCON62161.2024.10837708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837708","Lock Contention;Refactored Code Recommendation;Anti-Patterns;Large Language Model","Measurement;Codes;Accuracy;Computational modeling;Large language models;Source coding;Refining;Reinforcement learning;Software;Tuning","","","","31","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph","H. Xu; W. Ma; T. Zhou; Y. Zhao; K. Chen; Q. Hu; Y. Liu; H. Wang","Huazhong University of Science and Technology, Wuhan, China; Singapore Management University, Singapore; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; The University of Tokyo, Tokyo, Japan; Nanyang Technological University, Singapore; Huazhong University of Science and Technology, Wuhan, China",2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"13 Jun 2025","2025","","","243","254","In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effective solution for vulnerability remediation and software quality improvement.","2574-1934","979-8-3315-3683-1","10.1109/ICSE-Companion66252.2025.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024256","bug detection;fuzzing;large language model","Codes;Computer bugs;Refining;Knowledge graphs;Software quality;Fuzzing;Maintenance engineering;Software reliability;Performance analysis;Testing","","","","46","IEEE","13 Jun 2025","","","IEEE","IEEE Conferences"
"Part: I Fundamentals of AI","A. Banafa",NA,Artificial Intelligence in Action: Real-World Applications and Innovations,"","2025","","","1","2","This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.","","9788770046190","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948968.pdf&bkn=10948919&pdfType=chapter","","","","","","","","3 Apr 2025","","","River Publishers","River eBook Chapters"
"Part: II AI Applications","A. Banafa",NA,Artificial Intelligence in Action: Real-World Applications and Innovations,"","2025","","","69","70","This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.","","9788770046190","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948945.pdf&bkn=10948919&pdfType=chapter","","","","","","","","3 Apr 2025","","","River Publishers","River eBook Chapters"
"Characterizing and Efficiently Accelerating Multimodal Generation Model Inference","Y. Lee; A. Golden; A. Sun; B. Hosmer; B. Acun; C. Balioglu; C. Wang; C. D. Hernandez; C. Puhrsch; D. Haziza; D. Guessous; F. Massa; J. Kahn; J. Wan; J. Reizenstein; J. Zhai; J. Isaacson; J. Schlosser; J. Pino; K. R. Sadagopan; L. Shamis; L. Ma; M. -J. Hwang; M. Chen; M. Elhoushi; P. Rodriguez; R. Pasunuru; S. Hsia; S. Yih; S. Popuri; X. Liu; C. -J. Wu","AI Research at Meta, Menlo Park, CA, USA; Visiting Researcher at Meta, Cambridge, MA, USA; Research Engineer at Meta, San Francisco, CA, USA; Research Scientist at Meta, Menlo Park, CA, USA; Research Scientist at Meta, Menlo Park, CA, USA; Research Engineer at Meta, New York, USA; Research Engineer at Meta, New York, USA; Research Engineer at Meta, Menlo Park, CA, USA; Machine Learning Engineer at Meta, Menlo Park, CA, USA; Research Engineer at Meta, Paris, France; Machine Learning Engineer at Meta, Menlo Park, CA, USA; Research Engineer at Meta, Paris, France; Research Engineer manager at Meta, New York, USA; Software Engineer at Meta, New York, USA; Research Engineer at Meta, London, UK; Software Engineer at Meta, New York, USA; Engineering Leader at Meta, Seattle, WA, USA; Machine Learning Engineer at Meta, Menlo Park, CA, USA; Researcher at Meta, Menlo Park, USA; Research Engineer at Meta, Menlo Park, CA, USA; Software Engineer at Meta, Menlo Park, CA, USA; Research Scientist at Meta, Menlo Park, CA, USA; Research Scientist at Meta, Seattle, WA, USA; Research Scientist at Meta, NYC, NY, USA; Research Engineer at Meta, New York, USA; Research Scientist at Meta, Seattle, WA, USA; Research Scientist at Meta, Seattle, WA, USA; Research Scientist at Meta, Menlo Park, CA, USA; Research Scientist at Meta, Seattle, WA, USA; Research Manager at Meta, Menlo Park, CA, USA; Research Scientist at Meta, Menlo Park, CA, USA; Director of AI Research at Meta, Cambridge, MA, USA",IEEE Micro,"","2025","PP","99","1","11","Generative artificial intelligence (AI) technology is revolutionizing the computing industry, posing new system design and optimization opportunities. In particular, AI’s ability to understand and respond in multiple modalities comes with significant system resource demands. To sustainably scale generative AI capabilities to billions of users in the world, inference must be fast and efficient. This paper pinpoints key system design and optimization opportunities by characterizing a family of emerging multi-modal generation models on real systems. Auto-regressive token generation is a critical latency performance bottleneck, typically dominated by GPU idle time. In addition to memory-intensive attention across the generative AI models, linear operations constitute significant inference latency due to the feed forward networks in Transformer-based models. We demonstrate that state-of-the-art optimization levers, spanning from applications to system software and hardware, set a 3.88× better baseline.","1937-4143","","10.1109/MM.2025.3596539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11120454","","Generative AI;Computational modeling;Translation;Artificial intelligence;Decoding;Graphics processing units;Codes;Vocoders;Transformers;Memory management","","","","","IEEE","8 Aug 2025","","","IEEE","IEEE Early Access Articles"
"Part: III Challenges and Opportunities","A. Banafa",NA,Artificial Intelligence in Action: Real-World Applications and Innovations,"","2025","","","111","112","This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.","","9788770046190","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10948959.pdf&bkn=10948919&pdfType=chapter","","","","","","","","3 Apr 2025","","","River Publishers","River eBook Chapters"
"NLP‐Driven Chatbots","A. Mary Sowjanya; K. Srividya","Dept. of CS&amp;SE, College of Engineering, Andhra University, Visakhapatnam, AP, India; Department of AI&amp;ML, GMR Institute of Technology, Rajam, AP, India",Conversational Artificial Intelligence,"","2024","","","713","725","Summary <p>The emergence of natural language processing (NLP) technologies has revolutionized the field of conversational AI, giving rise to sophisticated chatbot systems that simulate human‐like interactions. This paper delves into the diverse applications and profound implications of NLP‐driven chatbots in the realm of conversational AI. We explore how these chatbots have been harnessed across various sectors, including customer service, healthcare, e‐commerce, and education, to enhance user experiences, automate tasks, and provide real‐time assistance. Furthermore, we investigate the ethical and societal implications that arise with the increasing integration of NLP‐driven chatbots in our daily lives, addressing concerns related to privacy, data security, algorithmic bias, and the potential displacement of human roles. Through an in‐depth analysis, we highlight the transformative potential of NLP‐driven chatbots, emphasizing the need for a balanced approach that leverages their benefits while mitigating potential drawbacks. This paper contributes to a comprehensive understanding of the landscape surrounding NLP‐driven chatbots, offering insights into their evolving applications and the imperative considerations for a responsible and effective deployment in the field of conversational AI.</p>","","9781394200795","10.1002/9781394200801.ch39","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10952553.pdf&bkn=10950236&pdfType=chapter","","Chatbots;Oral communication;Conversational artificial intelligence;Natural language processing;Artificial intelligence;Knowledge based systems;Ethics;Software;Generators;Engines","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Efficient Audio Steganography Using Generalized Audio Intrinsic Energy With Micro-Amplitude Modification Suppression","W. Su; J. Ni; X. Hu; B. Li","School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Cyber Science and Technology, Sun Yat-sen University, Shenzhen, China; School of Computer Science, Guangdong Polytechnic Normal University, Guangzhou, China; College of Information Engineering, Guangdong Key Laboratory of Intelligent Information Processing, and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China",IEEE Transactions on Information Forensics and Security,"4 Jul 2024","2024","19","","6559","6572","Recent advances in content-adaptive Audio Steganography in Temporal Domain (ASTD) suggest that modification of micro-amplitude samples may compromise its security. To prevent the micro-amplitude samples from being modified, a targeted Large Amplitude First (LAF) rule was adopted in some audio steganographic schemes, e.g., DFR. However, it is observed that the results with LAF rule are often unstable across different datasets, we thus propose a new Micro-Amplitude Suppression (MAS) rule in this paper following the design philosophy of wet paper coding. Unlike DFR where the audio steganographic performance heavily depends on the adopted heuristic filters, we propose to evaluate the embedding cost of cover audio with the Generalized Audio Intrinsic Energy (GAIE), which is obtained by calculating the weighted sum of squared DCT coefficients for each segmented audio clip with carefully designed weights. Extensive experimental results demonstrate that the proposed MAS rule tends to be more general and consistent than the LAF rule, and the proposed GAIE also shows better empirical security performance and audio quality compared to the advanced AAC and DFR_res (a variant of DFR). In addition, by preventing the micro-amplitude samples from being modified, the proposed GAIE_MAS can not only outperform other hand-crafted audio steganographic schemes but also the recently emerged deep learning-based schemes, e.g., IAA.","1556-6021","","10.1109/TIFS.2024.3417268","National Natural Science Foundation of China(grant numbers:62202507,U23B2022,U22A2030); Natural Science Foundation of Guangdong Province, China(grant numbers:2022A1515011209); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2023B0303000010); China Post-Doctoral Science Foundation(grant numbers:2021M703767); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570225","Audio steganography;distortion cost function;micro-amplitude suppression;generalized audio intrinsic energy","Distortion;Steganography;Security;Cost function;Costs;Filters;Payloads","","5","","40","IEEE","24 Jun 2024","","","IEEE","IEEE Journals"
"Generative Artificial Intelligence Device to analysis the data for SAP based Data Management","H. Narne","UiPath Inc, Dallas, TX",2024 International Conference on Intelligent Computing and Emerging Communication Technologies (ICEC),"16 Jan 2025","2024","","","1","6","A table-native AI model that shortens time-to-value for predictive activities on tabular data is being built by SAP. It is called the SAP Foundation Model. The goal is to build a general-purpose prediction engine that can automatically generate predictions from tabular data with minimal or no more training data. Although LLMs excel at text generation, they struggle with business-related data that is tabular and has challenges with categorisation, regression, and prediction. In fact, the SAP Foundation Model can compete with and even surpass these conventional methods while providing the flexibility of foundation models; in contrast, state-of-the-art LLMs perform 20-50% worse than incumbent restricted AI approaches in comparable cases. SAP has a long track record of successfully meeting business needs, and the company's stated goal is to develop AI that is useful, trustworthy, and socially conscious so that it can assist clients in solving their business challenges. Another innovative strategy for AI development, federated learning is quickly becoming the standard for delivering AI apps to the periphery of industrial networks. Research into the development of ethical guidelines and frameworks for the deployment of AI in MIS should prioritize the safety and security of data. In order to reap the benefits of AI integration for businesses, one must first establish a cohesive digital business plan. Another is getting people to interact with AI despite obstacles.","","979-8-3315-0843-2","10.1109/ICEC59683.2024.10837215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837215","Generative artificial intelligence;GAI;artificial intelligence;supply chain;operations management","Video games;Generative AI;Foundation models;Virtual assistants;Training data;Predictive models;Safety;Standards;Business;Guidelines","","","","15","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Exploiting LLM Embeddings for Content-Based IoT Anomaly Detection","T. Wang; Z. Zhao; K. Wu","Department of Computer Science, University of Victoria, Canada; Department of Computer Science, University of Victoria, Canada; Department of Computer Science, University of Victoria, Canada","2024 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","1 Oct 2024","2024","","","1","6","The Internet of Things (IoT) consists of enormous special-purpose devices whose security is hard to guarantee due to their simple design. Compared to data content on the Internet, the data content generated from IoT devices reflects the special application context of this device and thus can be treated as the special “language” this device speaks. Leveraging the power of large language models, we use ChatGPT embeddings to extract the features in the IoT traffic payload, with which we train an effective and efficient IoT traffic anomaly model. We evaluate this content-based IoT anomaly model using real-world IoT attack data. Our experimental results demonstrate the high detection accuracy and low false rates of our method.","2154-5952","979-8-3503-6231-2","10.1109/PACRIM61180.2024.10690230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690230","Large Language Model (LLM);Embeddings;IoT Anomaly Detection","Accuracy;Large language models;Refining;Telecommunication traffic;Signal processing;Feature extraction;Chatbots;Vectors;Internet of Things;Anomaly detection","","2","","17","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"My Model is Malware to You: Transforming AI Models into Malware by Abusing TensorFlow APIs","R. Zhu; G. Chen; W. Shen; X. Xie; R. Chang","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore; Zhejiang University, Hangzhou, China",2025 IEEE Symposium on Security and Privacy (SP),"16 Jun 2025","2025","","","486","503","The rapid advancement of AI technologies has significantly increased the demand for AI models across various industries. While model sharing reduces costs and fosters innovation, it also introduces security risks, as attackers can embed malicious code within models, leading to potential undetected attacks when running the model. Despite these risks, the security of model sharing, particularly for TensorFlow, remains under-investigated. To address these security concerns, we present a systematic analysis of the security risks associated with TensorFlow APIs. We introduce the TensorAbuse attack, which exploits hidden capabilities of TensorFlow APIs, such as file access and network messaging, to construct powerful and stealthy attacks. To facilitate this, we developed two novel techniques: one for identifying persistent APIs in TensorFlow and another for leveraging large language models to accurately analyze and classify API capabilities. We applied these techniques to TensorFlow v2.15.0 and identified 1,083 persistent APIs with five main capabilities. We exploited 20 of these APIs to develop five attack primitives and four synthetic attacks, including file leak, IP exposure, arbitrary code execution, and shell access. Our tests revealed that Hugging Face, TensorFlow Hub, and ModelScan could not detect any of these attacks. We have reported these findings to Google, Hugging Face, and ModelScan, and are currently working with them to address these issues.","2375-1207","979-8-3315-2236-0","10.1109/SP61157.2025.00012","National Key R&D Program of China(grant numbers:2022YFE0113200); National Research Foundation, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11023358","AI model attack;TensorFlow API;capability abuse","Industries;Technological innovation;Privacy;Systematics;Large language models;Malware;Internet;Security;IP networks;Faces","","2","","51","IEEE","16 Jun 2025","","","IEEE","IEEE Conferences"
"Zero-Shot Anomaly Detection in a Forensic Timeline","I. K. Agus Ariesta Putra; R. M. Achmad; H. Studiawan","Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",2024 10th International Conference on Smart Computing and Communication (ICSCC),"1 Oct 2024","2024","","","16","21","A central aspect of digital forensics involves analyzing forensic timelines to uncover sequences of events in computer systems. A forensic timelines might contain several anomalous events. Detecting anomalies in these timelines is important for identifying potential security breaches and malicious activities. Current solutions for detecting anomalies in forensic timelines utilize deep learning and sentiment analysis. In this work, we propose an approach for performing anomaly detection using a zero-shot learning technique in forensic timelines. Our proposed framework uses pre-trained large language models to automatically identify anomalous events and activities without requiring prior training data that is specific to each anomaly. The experimental results show that the method we proposed achieves the highest accuracy score of 98.16% tested on five public datasets.","","979-8-3503-6310-4","10.1109/ICSCC62041.2024.10690546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690546","anomaly detection;forensic timeline;zero-shot learning","Sentiment analysis;Zero-shot learning;Forensics;Computational modeling;Large language models;Digital forensics;Training data;Predictive models;Computer security;Anomaly detection","","","","20","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"About the Authors and Editors","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","323","324","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123673.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"From Scientific Texts to Verifiable Code: Automating the Process with Transformers","C. Wang; M. Scazzariello; M. Chiesa",KTH Royal Institute of Technology; RISE Research Institutes of Sweden; KTH Royal Institute of Technology,2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code),"12 Jun 2025","2025","","","213","216","Despite the vast body of research literature proposing algorithms with formal guarantees, the amount of verifiable code in today’s systems remains minimal. This discrepancy stems from the inherent difficulty of verifying code, particularly due to the time-consuming nature and strict formalism of proof details that formal verification tools require. However, the emergence of Transformers in Large Language Models presents a promising solution to this challenge. In this position paper, we believe that Transformers have the potential to read research papers that propose algorithms with formal proofs and translate these proofs into verifiable code. We leverage Transformers to first build a formal structure of the proof using the original text from the paper, and then to handle the tedious, low-level aspects of proofs that are often omitted by humans. We argue that this approach can significantly reduce the barrier to formal verification. The above idea of reading papers to write verifiable code opens new avenues for automating the verification of complex systems, enabling a future where formally verified algorithms from academic research can more seamlessly transition into real-world software systems, thereby improving code reliability and security.","","979-8-3315-2615-3","10.1109/LLM4Code66737.2025.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028265","","Codes;Translation;Large language models;Software algorithms;Transformers;Software systems;Software reliability;Security;Logic;Formal verification","","","","18","IEEE","12 Jun 2025","","","IEEE","IEEE Conferences"
"9 Intent Interface Evolution and the Role of Language Models","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","193","206","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123658.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"7 Closed Loop Automation","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","155","170","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123666.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"Index","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","321","322","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123676.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"Special Session “Untrustworthy AI, Mitigating the Threat of Disinformation with Blockchain”","N. Wasserman",NA,2024 IEEE URUCON,"31 Jan 2025","2024","","","51","52","One of the major risks associated with the explosion of AI capabilities is the generation and propagation of disinformation. Inaccurate and malicious data threatens multiple dimensions of individual and community well-being including public health, democratic governance, climate change, and national security. The risks have been exacerbated by the cognitive drain due to millisecond device interactions and AI-enhanced social media algorithms. The talk will speak to the nature of the threat and potential means for mitigating identified risks. In particular, techniques and lessons from blockchain technologies can be used to enhance the quality of data used by Large Language Models. In addition, such capabilities can be applied to implementing governance rules and processes to contain the risks associated with disinformation.","","979-8-3503-5538-3","10.1109/URUCON63440.2024.10850309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850309","","Blockchains;Fake news;Disasters;Data integrity;Synchronization;Standards;Social networking (online);Satellites;Public healthcare;Privacy","","","","","IEEE","31 Jan 2025","","","IEEE","IEEE Conferences"
"10 State Models","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","207","220","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123657.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"14 Summary and Outlook","C. Vulkán; P. Szilágyi; N. Sprecher",NA; NA; NA,Revolutionizing Network Management: The Journey to AI-native Autonomy,"","2025","","","317","320","End-to-end automation in telecommunications is a challenging yet critical industry objective, representing collective vision for technology evolution in the coming years. Key enablers for achieving this include the expression and the translation of high-level business goals (e.g. intents) into actionable and impactful operations, the deployment and interaction between closed loops, real-time data management, the extraction of meaningful insights from data, ML-driven analytics and intelligence, and seamless orchestration and resource control. Automation processes must be transparent, reliable and robust. This book provides a comprehensive insight into the automation journey. It explores the vision of zero-touch, AI-native network and service autonomy, examining the key trends, catalysts, and technological advancements shaping this paradigm shift. It provides a comprehensive analysis of critical aspects such as data and knowledge management, security and trust, native AI/ML management, intent-based automation, and human-to-machine as well as machine-to-machine interfaces. By delving into advanced concepts such as data-driven, AI-powered analytics, semantic and decision models, network digital twins, and the role of natural language processing and large language models, the book bridges theory and practical application. It highlights how these innovations leverage and maximize intent-driven and closed-loop automation, enabling seamless, intelligent, and autonomous networks. Additionally, it identifies key areas where further research is needed to address existing gaps and unlock the full potential of these technologies. Ultimately, this book provides valuable insights, outlining the transformative potential of zero-touch, AI-native networks and paving the way for seamless, intelligent, and autonomous management of networks and services, driving innovation, efficiency, and new opportunities in the evolving digital landscape.","","9788770228985","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=11123663.pdf&bkn=11123653&pdfType=chapter","","","","","","","","12 Aug 2025","","","River Publishers","River eBook Chapters"
"CyberLlama2 - MEDICALHARM Threat Modeling Assistant","E. Kwarteng; M. Cebe; J. Kwarteng","Computer Science Department, Marquette University, Milwaukee, USA; Computer Science Department, Marquette University, Milwaukee, USA; Institute for Health & Equity, Medical College of Wisconsin, Milwaukee, USA",2024 International Conference on Machine Learning and Applications (ICMLA),"4 Mar 2025","2024","","","930","934","Threat Modeling, a shift-left cybersecurity activity to build security into a software design, has become a challenge for many industries, including Modern Medical Devices. With the advancement of Large Language Models (LLM), more industries are adopting this phenomenon to understand and solve domain-specific challenges. However, thus far, little research has evaluated the effectiveness of LLM in solving threat modeling challenges. To alleviate this problem, we developed a threat modeling assisted LLM to assist MEDICALHARM in identifying cybersecurity, privacy, and safety threats in the Modern Medical Device space. We developed a specialized decoder-only model, CyberLlama2, to assist threat modeling using a large set of -146k- cybersecurity instructions to fine-tune Llama2. The results show an improved performance of our proposed CyberLlama2 model over the baseline and other cybersecurity models.","1946-0759","979-8-3503-7488-9","10.1109/ICMLA61862.2024.00136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903354","LLM;Cybersecurity;Privacy;Safety;Modern Medical Devices;MEDICALHARM Threat Modeling Methodology","Threat modeling;Industries;Training;Performance evaluation;Privacy;Medical devices;Software design;Safety;Reliability;Computer security","","","","26","IEEE","4 Mar 2025","","","IEEE","IEEE Conferences"
"Survey of intelligent collaborative E-learning systems","A. Asselman; A. -E. Nasseh; S. Aammou","Faculty of Sciences, Abdelmalek Essaadi University, Tetwan, MOROCCO; Faculty of Sciences, Abdelmalek Essaadi University, Tetwan, MOROCCO; Faculty of Sciences, Abdelmalek Essaadi University, Tetwan, MOROCCO",2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA),"9 Nov 2017","2017","","","1","7","In recent years, agent technology is used to design solutions that facilitate the implementation of new concepts, especially in the field of E-learning. It is widely used to model the different actors of E-learning systems and manage their interactions, to meet their dynamic and execution needs by their innovative features like intelligence and autonomy. In education field, multi-agent systems makes a great change in the society in the fact that the conventional education system requires the presence of both student and instructor at same place, the same time and at the same interval of time. This kind of system equips with these characters; and provides many advantages like autonomy, reactivity, social ability and proactivity. There are several collaborative E-learning environments using multi-agent technologies that included I-MIND, MASCE, ALLEGRO, etc. Our work aims to define the importance of a collaborative learning system, to define the roles of each actor and the different possible communications between them, to determine the tasks of the agents that can be automated. Furthermore, an analysis of existing multi-agent based collaborative E-learning systems on the basis of their various features covering collaboration features, studenttutor interaction, adaptability measurement and security in order to identify your strengths and weaknesses to determine the most suitable model to our system.","","978-1-5386-3296-3","10.1109/ICETA.2017.8102463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8102463","","Electronic learning;Collaboration;Training;Collaborative work;Multi-agent systems","","4","","18","IEEE","9 Nov 2017","","","IEEE","IEEE Conferences"
"Defensive Configuration for Multi-Agent Perimeter Monitoring","B. Reily; A. Kraft; M. A. Lopez; C. Reardon","DEVCOM Army Research Laboratory; Department of Computer Science, University of Denver; Department of Computer Science, University of Denver; Department of Computer Science, University of Denver",2023 International Symposium on Multi-Robot and Multi-Agent Systems (MRS),"5 Feb 2024","2023","","","135","141","Perimeter monitoring is a required capability of multi-agent systems dealing with security-focused applications. In this paper, we consider the problem of defender configuration for perimeter monitoring, in which a team of defender agents must position and orient themselves to maximize observations of adversarial attackers approaching the perimeter. We envision perimeter monitoring as a crossover of perimeter defense and area coverage. We propose a greedy approximation algorithm that identifies a minimum set of possible fields of view and then selects between them to configure a multi-agent team of defenders by balancing the distance-weighted utility of each observation with the value of covering a diverse set of observations. We evaluate our approach through a variety of realistic multi-agent attack trajectories, showing that our proposed approach achieves near-optimal coverage performance while showing significant gains in time performance.","","979-8-3503-7076-8","10.1109/MRS60187.2023.10416787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416787","","Integer programming;Approximation algorithms;Trajectory;Iterative methods;Monitoring;Multi-agent systems","","","","27","IEEE","5 Feb 2024","","","IEEE","IEEE Conferences"
"Distributed Stochastic Optimisation with Uncertain Coupling Constraints","A. D. Duca; F. Ruiz; R. Scattolini","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy",2024 IEEE 63rd Conference on Decision and Control (CDC),"26 Feb 2025","2024","","","4431","4436","Large-scale multi-agent systems are increasingly relevant in various aspects of society; their operation requires advances in multi-agent distributed optimisation algorithms that can handle uncertain environments. This paper presents a distributed algorithm suitable for solving convex constraint-coupled multi-agent problems with uncertainty directly affecting the coupling constraints. The algorithm exploits the problem structure to solve the large-scale uncertain problem efficiently, leveraging the scenario approach to approximate the coupling chance-constraint. We prove that the number of scenarios required to guarantee a given violation probability level is independent of the agent number, making the solution scalable. We apply the algorithm to a multi-microgrid aggregation problem to provide ancillary services to the Grid, a relevant decarbonisation and energy security topic.","2576-2370","979-8-3503-1633-9","10.1109/CDC56724.2024.10886359","Hitachi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10886359","","Couplings;Uncertainty;Approximation algorithms;Probabilistic logic;Low carbon economy;Security;Forecasting;Distributed algorithms;Optimization;Multi-agent systems","","","","14","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Integrated Sensing and Communication for Effective Multi-Agent Cooperation Systems","Z. Sun; Z. Yu; B. Guo; B. Yang; Y. Zhang; D. W. K. Ng","Northwestern Polytechnical University, China; Northwestern Polytechnical University, China; Northwestern Polytechnical University, China; Northwestern Polytechnical University, China; Northwestern Polytechnical University, China; University of New South Wales, Australia",IEEE Communications Magazine,"3 Sep 2024","2024","62","9","68","73","Multi-agent systems (MASs) have emerged as effective means to accomplish important tasks without human involvement in various real-world environments. In MASs, task completion efficiency is determined by the level of cooperation among agents. Meanwhile, achieving high levels of cooperation relies on accurate and comprehensive environmental perception. To this end, agents exchange their local perceptions to expand the scope of their sensing information. However, it limits the improvement of sensing performance by relying solely on information exchange, particularly for mobile target sensing. To address this, we introduce the integrated sensing and communication (ISAC) technique to MASs. This enables the agents to perform distributed radio sensing, while concurrently exchanging their local perceptions. In this article, we propose an ISAC-based MAS framework, where agents can dynamically determine ISAC strategies and cooperatively perceive the environment through ISAC operations. The features of the proposed framework are elucidated and compared with existing networked ISAC systems and communication-centric MASs. For the proposed framework, we suggest a deep reinforcement learning (DRL)-based system design. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss potential challenges and opportunities for future research.","1558-1896","","10.1109/MCOM.002.2300560","National Natural Science Foundation of China(grant numbers:61960206008,62025205,62102322,62302396); Australian Research Council's Discovery Projects(grant numbers:DP230100603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400392","","Sensors;Receivers;Autonomous aerial vehicles;Feature extraction;Satellite broadcasting;Remote sensing;Multi-agent systems;Integrated sensing and communication","","5","","15","IEEE","15 Jan 2024","","","IEEE","IEEE Magazines"
"Legal Query RAG","R. S. M. Wahidur; S. Kim; H. Choi; D. S. Bhatti; H. -N. Lee","School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; Artificial Intelligence Graduate School, Gwangju Institute of Science and Technology, Gwangju, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea",IEEE Access,"3 Mar 2025","2025","13","","36978","36994","Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence (AI) for various core tasks. However, these technologies remain in their early stages and face challenges such as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding misleading responses, commonly referred to as hallucinations. To address these limitations, this paper introduces Legal Query RAG (LQ-RAG), a novel Retrieval-Augmented Generation framework with a recursive feedback mechanism specifically designed to overcome the critical shortcomings of standard RAG implementations in legal applications. The proposed framework incorporates four key components: a custom evaluation agent, a specialized response generation model, a prompt engineering agent, and a fine-tuned legal embedding LLM. Together, these components effectively minimize hallucinations, improve domain-specific accuracy, and deliver precise, high-quality responses for complex queries. Experimental results demonstrate that the fine-tuned embedding LLM achieves a 13% improvement in Hit Rate and a 15% improvement in Mean Reciprocal Rank (MRR). Comparisons with general LLMs reveal a 24% performance gain when using the Hybrid Fine-Tuned Generative LLM (HFM), the specialized response generation model integrated into the LQ-RAG framework. Furthermore, LQ-RAG achieves a 23% improvement in relevance score over naive configurations and a 14% improvement over RAG with Fine-Tuned LLMs (FTM). These findings underscore the potential of domain-specific fine-tuned LLMs, combined with advanced RAG modules and feedback mechanisms, to significantly enhance the reliability and performance of AI in legal practice. The reliance of this study on a proprietary model as the evaluation agent, combined with the lack of feedback from human experts, highlights the need for improvement. Future efforts should focus on developing a specialized legal evaluation agent and enhancing its performance by incorporating feedback from domain experts.","2169-3536","","10.1109/ACCESS.2025.3542125","Institute of Information & communications Technology Planning & Evaluation (IITP); Korea Government (MSIT) (IITP-2025-RS-2021-II210118, Development of decentralized consensus composition technology for large-scale nodes); IITP (Institute of Information & Communications Technology Planning & Evaluation)-ITRC (Information Technology Research Center); Korea Government [Ministry of Science and Information and Communication Technology (ICT)](grant numbers:IITP-2025-RS-2021-II211835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887211","Retrieval-augmented generation;legal query;LLM agent;information retrieval","Law;Retrieval augmented generation;Accuracy;Tuning;Semantics;Hybrid power systems;Adaptation models;Training;Reliability;Mathematical models","","1","","63","CCBY","14 Feb 2025","","","IEEE","IEEE Journals"
"Metadata: An Integral Component of the Modern Data Strategy","M. Mohammed; J. R. Talburt; H. Syed; Mehjabeen","Information Science University of Arkansas at Little Rock, Little Rock, AR, USA; Information Science University of Arkansas at Little Rock, Little Rock, AR, USA; Information Science UALR, Atlanta, USA; *, Hyderabad, India","2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)","9 Apr 2024","2023","","","1628","1631","This paper explores the pivotal role of metadata within the data management practices of organizations, and its subsequent impact on business decision-making. This paper presents a comprehensive exploration of the application of Generative AI in the realm of data analytics. As the volume and complexity of data continue to grow, effective metadata management becomes pivotal to understanding and utilizing data. With the ability to simulate human inferencing processes and generate contextual insights, generative AI offers promising avenues in improving metadata understanding, governance, integration, and quality. A crucial point of discussion in this paper is how the quality of metadata directly influences the output of Generative AI models. We present a proof-of-concept that illustrates this relationship using the COVID19 dataset and OpenAI text-ada-002 embeddings with GPT3.5 chat completion API. The paper further acknowledges potential concerns related to security, bias, privacy, IP, and the long-term effects on critical thinking abilities.","","979-8-3503-2759-5","10.1109/CSCE60160.2023.00267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487383","Metadata;Artificial Intelligence;Data Analytics;Data Governance;Data Management","Privacy;Data analysis;Generative AI;Decision making;Organizations;Documentation;Metadata","","1","","8","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Index","T. Mariprasath; K. R. Cheepati; M. Rivera",NA; NA; NA,"Practical Guide to Machine Learning, NLP, and Generative AI: Libraries, Algorithms, and Applications","","2025","","","159","160","This is an essential resource for beginners and experienced practitioners in machine learning. This comprehensive guide covers a broad spectrum of machine learning topics, starting with an in-depth exploration of popular machine learning libraries. Readers will gain a thorough understanding of Scikit-learn, TensorFlow, PyTorch, Keras, and other pivotal libraries like XGBoost, LightGBM, and CatBoost, which are integral for efficient model development and deployment. The book delves into various neural network architectures, providing readers with a solid foundation in understanding and applying these models. Beginning with the basics of the Perceptron and its application in digit classification, it progresses to more complex structures such as multilayer perceptrons for financial forecasting, radial basis function networks for air quality prediction, and convolutional neural networks (CNNs) for image classification. Additionally, the book covers recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) and gated recurrent units (GRUs), which are crucial for time-series analysis and sequential data applications. Supervised machine learning algorithms are meticulously explained, with practical examples to illustrate their application. The book covers logistic regression and its use in predicting sports outcomes, decision trees for plant classification, random forests for traffic prediction, and support vector machines for house price prediction. Gradient boosting machines and their applications in genomics, AdaBoost for bioinformatics data classification, and extreme gradient boosting (XGBoost) for churn prediction are also discussed, providing readers with a robust toolkit for various predictive tasks. Unsupervised learning algorithms are another significant focus of the book, introducing readers to techniques for uncovering hidden patterns in data. Hierarchical clustering for gene expression data analysis, principal component analysis (PCA) for climate predictions, and singular value decomposition (SVD) for signal denoising are thoroughly explained. The book also explores applications like robot navigation and network security, demonstrating the versatility of these techniques. Natural language processing (NLP) is comprehensively covered, highlighting its fundamental concepts and various applications. The book discusses the overview of NLP, its fundamental concepts, and its diverse applications such as chatbots, virtual assistants, clinical NLP applications, and social media analytics. Detailed sections on text pre-processing, syntactic analysis, machine translation, text classification, named entity recognition, and sentiment analysis equip readers with the knowledge to build sophisticated NLP models. The final chapters of the book explore generative AI, including generative adversarial networks (GANs) for image generation, variational autoencoders for vibrational encoder training, and autoregressive models for time series forecasting. It also delves into Markov chain models for text generation, Boltzmann machines for pattern recognition, and deep belief networks for financial forecasting. Special attention is given to the application of recurrent neural networks (RNNs) for generation tasks, such as wind power plant predictions and battery range prediction, showcasing the practical implementations of generative AI in various fields.","","9788770046527","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10850575.pdf&bkn=10850512&pdfType=chapter","","","","","","","","22 Jan 2025","","","River Publishers","River eBook Chapters"
"Bibliography","T. Mariprasath; K. R. Cheepati; M. Rivera",NA; NA; NA,"Practical Guide to Machine Learning, NLP, and Generative AI: Libraries, Algorithms, and Applications","","2025","","","157","158","This is an essential resource for beginners and experienced practitioners in machine learning. This comprehensive guide covers a broad spectrum of machine learning topics, starting with an in-depth exploration of popular machine learning libraries. Readers will gain a thorough understanding of Scikit-learn, TensorFlow, PyTorch, Keras, and other pivotal libraries like XGBoost, LightGBM, and CatBoost, which are integral for efficient model development and deployment. The book delves into various neural network architectures, providing readers with a solid foundation in understanding and applying these models. Beginning with the basics of the Perceptron and its application in digit classification, it progresses to more complex structures such as multilayer perceptrons for financial forecasting, radial basis function networks for air quality prediction, and convolutional neural networks (CNNs) for image classification. Additionally, the book covers recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) and gated recurrent units (GRUs), which are crucial for time-series analysis and sequential data applications. Supervised machine learning algorithms are meticulously explained, with practical examples to illustrate their application. The book covers logistic regression and its use in predicting sports outcomes, decision trees for plant classification, random forests for traffic prediction, and support vector machines for house price prediction. Gradient boosting machines and their applications in genomics, AdaBoost for bioinformatics data classification, and extreme gradient boosting (XGBoost) for churn prediction are also discussed, providing readers with a robust toolkit for various predictive tasks. Unsupervised learning algorithms are another significant focus of the book, introducing readers to techniques for uncovering hidden patterns in data. Hierarchical clustering for gene expression data analysis, principal component analysis (PCA) for climate predictions, and singular value decomposition (SVD) for signal denoising are thoroughly explained. The book also explores applications like robot navigation and network security, demonstrating the versatility of these techniques. Natural language processing (NLP) is comprehensively covered, highlighting its fundamental concepts and various applications. The book discusses the overview of NLP, its fundamental concepts, and its diverse applications such as chatbots, virtual assistants, clinical NLP applications, and social media analytics. Detailed sections on text pre-processing, syntactic analysis, machine translation, text classification, named entity recognition, and sentiment analysis equip readers with the knowledge to build sophisticated NLP models. The final chapters of the book explore generative AI, including generative adversarial networks (GANs) for image generation, variational autoencoders for vibrational encoder training, and autoregressive models for time series forecasting. It also delves into Markov chain models for text generation, Boltzmann machines for pattern recognition, and deep belief networks for financial forecasting. Special attention is given to the application of recurrent neural networks (RNNs) for generation tasks, such as wind power plant predictions and battery range prediction, showcasing the practical implementations of generative AI in various fields.","","9788770046527","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10850525.pdf&bkn=10850512&pdfType=chapter","","","","","","","","22 Jan 2025","","","River Publishers","River eBook Chapters"
"Using Transfer Learning for Code-Related Tasks","A. Mastropaolo; N. Cooper; D. N. Palacio; S. Scalabrino; D. Poshyvanyk; R. Oliveto; G. Bavota","SEART Software Institute, Università della Svizzera italiana, Lugano, Switzerland; SEMERU William & Mary, Williamsburg, VA, USA; SEMERU William & Mary, Williamsburg, VA, USA; University of Molise, Campobasso, CB, Italy; SEMERU William & Mary, Williamsburg, VA, USA; University of Molise, Campobasso, CB, Italy; SEART Software Institute, Università della Svizzera italiana, Lugano, Switzerland",IEEE Transactions on Software Engineering,"18 Apr 2023","2023","49","4","1580","1598","Deep learning (DL) techniques have been used to support several code-related tasks such as code summarization and bug-fixing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing (NLP) tasks. The basic idea behind these models is to first pre-train them on a generic dataset using a self-supervised task (e.g., filling masked words in sentences). Then, these models are fine-tuned to support specific tasks of interest (e.g., language translation). A single model can be fine-tuned to support multiple tasks, possibly exploiting the benefits of transfer learning. This means that knowledge acquired to solve a specific task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classification). While the benefits of transfer learning have been widely studied in NLP, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-fixing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task fine-tuning on the model's performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks benefit from a multi-task fine-tuning.","1939-3520","","10.1109/TSE.2022.3183297","European Research Council(grant numbers:851720); National Science Foundation(grant numbers:CCF-1955853,CCF-1815186,CCF-2007246); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797060","Deep learning;empirical software engineering","Task analysis;Codes;Multitasking;Electronic mail;Computer bugs;Natural language processing;Java","","39","","87","IEEE","15 Jun 2022","","","IEEE","IEEE Journals"
"An Adaptive Model for Detection and Prevention of DDoS and Flash Crowd Flooding Attacks","B. A. Khalaf; S. A. Mostafa; A. Mustapha; N. Abdullah","Faculty of Computer Science and Information Technology, Universiti Tun Hussin Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology, Universiti Tun Hussin Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology, Universiti Tun Hussin Onn Malaysia, Johor, Malaysia; Faculty of Computer Science and Information Technology, Universiti Tun Hussin Onn Malaysia, Johor, Malaysia","2018 International Symposium on Agent, Multi-Agent Systems and Robotics (ISAMSR)","22 Nov 2018","2018","","","1","6","The following topics are dealt with: multi-agent systems; software agents; Internet; robot vision; mobile robots; computer network security; Java; feature extraction; mobile ad hoc networks; routing protocols.","","978-1-5386-7856-5","10.1109/ISAMSR.2018.8540546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540546","Software agent;Flash Crowd (FC);Distributed Denial of Service (DDoS);Network Application Layer (NAL)","Computer crime;Adaptation models;IP networks;Internet;Kalman filters;Floods;Information technology","","11","","31","IEEE","22 Nov 2018","","","IEEE","IEEE Conferences"
"Data Injection Attack on Decentralized Optimization","S. Xiaoxiao Wu; H. -T. Wai; A. Scaglione; A. Nedić; A. Leshem","College of Information Engineering, Shenzhen University, Shenzhen, China; School of ECEE, Arizona State Univ., Tempe, AZ, USA; School of ECEE, Arizona State Univ., Tempe, AZ, USA; School of ECEE, Arizona State Univ., Tempe, AZ, USA; Faculty of Engg., Bar-Ilan Univ., Israel","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","3644","3648","This paper studies the security aspect of gossip-based decentralized optimization algorithms for multi agent systems against data injection attacks. Our contributions are two-fold. First, we show that the popular distributed projected gradient method (by Nedić et al.) can be attacked by coordinated insider attacks, in which the attackers are able to steer the final state to a point of their choosing. Second, we propose a metric that can be computed locally by the trustworthy agents processing their own iterates and those of their neighboring agents. This metric can be used by the trustworthy agents to detect and localize the attackers. We conclude the paper by supporting our findings with numerical experiments.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462528","Decentralized optimization;gossip algorithms;data injection attack","Task analysis;Gradient methods;Steady-state;Security;Indexes","","10","","24","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Learning-Based Defense Against Prediction-Based Displacement Attacks on Formation Control","B. Chen; Y. Li; S. Wu; C. Fang; J. He","Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China; Student Innovation Center, Shanghai Jiao Tong University, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai Jiao Tong University, Shanghai, China",2024 China Automation Congress (CAC),"13 Feb 2025","2024","","","2985","2990","Formation control is a widely performed task in multi-agent systems. However, agents under formation control are susceptible to attacks, and how to defend against the attack is important for system security. This paper aims to design a new method to protect the system from prediction-based displacement attack, which predicts the movement of agents, diverts them from their original tracks and drives them to a preset trap. The defense is challenging due to the robustness of the attacker's prediction mechanism. Also, undermining the attack can be costly and harm the system's performance. To address this issue, we first build obstacle-avoidance and formation control models of the holonomic and non-holonomic agents and specify the prediction-based attacks to limit the cases where the defense strategy can be effective. Then, we propose a learning-based defense strategy, which learns the prediction mechanism of the attackers. It modifies the control input based on self-adaptive fuzzy methods to reduce prediction accuracy and guarantee that the agents can reach the goal. Simulation results demonstrate that our method is better than the previous noise-based one at reducing the accuracy of the attacker's prediction.","2688-0938","979-8-3503-6860-4","10.1109/CAC63892.2024.10864667","National Natural Science Foundation of China(grant numbers:62103266,62373247); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10864667","","Accuracy;Tracking;System performance;Simulation;Predictive models;Formation control;Robustness;Trajectory;Security;Multi-agent systems","","","","21","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"Research on Source-Load-Storage Collaborative Control Strategy for Novel Distribution Networks under Non-Ideal Communication Environments","Y. Chen; D. Li; H. Zhao; Y. Zheng; X. Zheng; S. Dong","College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; State Grid Beijing Tongzhou Electric Power Supply Company, Beijing, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China",2024 IEEE 8th Conference on Energy Internet and Energy System Integration (EI2),"15 May 2025","2024","","","1947","1952","In order to address the issues of complex and variable economic and security control, as well as the difficulties of centralized computing in novel distribution networks under non-ideal communication environments, a highly scalable and reliable distributed control strategy is proposed. Initially, a global sampling method is employed to enhance the optimization of the state transition matrix, thereby accelerating the algorithmic convergence. Subsequently, a self-switching control strategy for node communication states is introduced to enhance the flexibility of the control strategy. Then, based on the consensus principle of multi-agent systems, the incremental cost of source-load-storage agent nodes is used as the consensus variable to achieve optimal economic operation of the distribution network. Finally, the effectiveness of the proposed strategy is validated through simulation case studies.","","979-8-3315-2352-7","10.1109/EI264398.2024.10991418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10991418","Novel Distribution Network;Consensus Principle;Non-Ideal Communication;Source-Load-Storage Collaboration","Costs;Collaboration;Distribution networks;System integration;Switches;Sampling methods;Real-time systems;Power markets;Security;Reliability","","","","14","IEEE","15 May 2025","","","IEEE","IEEE Conferences"
"Federated Learning With Client Clustering Selection and Quality-Aware Model Aggregation(2024)","Y. Peng; C. Wang; H. Shi; R. Ma; H. Guan; H. Yang","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; National Key Laboratory of Modeling and Simulation for Complex Systems, Beijing Simulation Center, Beijing, China",IEEE Internet of Things Journal,"","2025","PP","99","1","1","The Industrial Internet of Things (IIoT) is revolutionizing industries through device interconnectivity, enabling real-time data collection and transmission for enhanced monitoring, control, and automation. This has led to improvements in predictive maintenance, production optimization, and supply chain management. The integration of machine learning (ML) and deep learning (DL) within IIoT has been accelerated by increased data availability, computational advancements, and algorithmic innovations, with applications ranging from image processing to autonomous vehicles. Foundation models, such as ChatGPT, are becoming prevalent in IIoT for their capabilities in natural language processing and computer vision. However, the growth of IIoT and foundation models presents challenges, including data volume, real-time processing requirements, computational costs, and security vulnerabilities. Federated learning (FL) addresses these issues by allowing distributed model training without raw data transfer, enhancing privacy and security. FL is particularly beneficial for IIoT’s decentralized architecture and real-time decision-making needs. Despite the advantages, FL faces challenges such as data heterogeneity and communication overhead. To overcome these, we propose a federated learning framework, Fed-CCSQMA, which includes client selection and model aggregation modules to mitigate data heterogeneity’s impact. The client selection module uses Principal Component Analysis (PCA) and clustering to select clients with diverse yet representative data, while the model aggregation module assigns weights based on model accuracy to ensure faster global model convergence. We test our proposed framework on FMNIST and CIFAR-10 datasets, with accuracy surpassing baseline methods and faster convergence, demonstrating an improvement in overall generalizability and learning efficiency.","2327-4662","","10.1109/JIOT.2025.3572901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11011660","Federated Learning;Clustering;Client Selection;Model Aggregation","Federated learning;Training;Industrial Internet of Things;Data models;Computational modeling;Accuracy;Foundation models;Convergence;Servers;Security","","","","","IEEE","23 May 2025","","","IEEE","IEEE Early Access Articles"
"LLM4SecHW: Leveraging Domain-Specific Large Language Model for Hardware Debugging","W. Fu; K. Yang; R. G. Dutta; X. Guo; G. Qu",Kansas State University; Michigan Technological University; Silicon Assurance; Kansas State University; University of Maryland,2023 Asian Hardware Oriented Security and Trust Symposium (AsianHOST),"24 Jan 2024","2023","","","1","6","This paper presents LLM4SECHW, a novel framework for hardware debugging that leverages domain-specific Large Language Model (LLM). Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain-specific data. To address these challenges, we propose a unique approach to compile a dataset of open-source hardware design defects and their remediation steps, utilizing version control data. This dataset provides a substantial foundation for training machine learning models for hardware. LLM4SECHW employs fine-tuning of medium-sized LLMs based on this dataset, enabling the identification and rectification of bugs in hardware designs. This pioneering approach offers a reference workflow for the application of fine-tuning domain-specific LLMs in other research areas. We evaluate the performance of our proposed system on various open-source hardware designs, demonstrating its efficacy in accurately identifying and correcting defects. Our work brings a new perspective on automating the quality control process in hardware design.","","979-8-3503-4099-0","10.1109/AsianHOST59942.2023.10409307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409307","Hardware Debugging;Large Language Model;Domain-Specific Models","Training;Debugging;Quality control;Hardware;Software;Open source hardware;Task analysis","","22","","42","IEEE","24 Jan 2024","","","IEEE","IEEE Conferences"
"PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents","X. Zhang; H. Xu; Z. Ba; Z. Wang; Y. Hong; J. Liu; Z. Qin; K. Ren","State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; University of Connecticut, Stamford, CT, USA; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Blockchain and Data Security, School of Cyber Science and Technology, College of Computer Science and Technology, Zhejiang University, Hangzhou, China",IEEE Transactions on Dependable and Secure Computing,"12 Nov 2024","2024","21","6","5242","5258","Swift advancements in large language model (LLM) technologies lead to widespread research and applications, particularly in integrating LLMs with auxiliary tools, known as tool-using LLM agents. However, amid user interactions, the transmission of private information to both LLMs and tools poses considerable privacy risks to users. In this paper, we delve into current privacy-preserving solutions for LLMs and outline three pivotal challenges for tool-using LLM agents: generalization to both open-source and closed-source LLMs and tools, compliance with privacy requirements, and applicability to unrestricted tasks. To tackle these challenges, we present PrivacyAsst, the first privacy-preserving framework tailored for tool-using LLM agents, encompassing two solutions for different application scenarios. First, we incorporate a homomorphic encryption scheme to ensure computational security guarantees for users as a safeguard against both open-source and closed-source LLMs and tools. Moreover, we propose a shuffling-based solution to broaden the framework's applicability to unrestricted tasks. This solution employs an attribute-based forgery generative model and an attribute shuffling mechanism to craft privacy-preserving requests, effectively concealing individual inputs. In addition, we introduce an innovative privacy concept, $t$t-closeness in image data, for privacy compliance within this solution. Finally, we implement PrivacyAsst, accompanied by two case studies, demonstrating its effectiveness in advancing privacy-preserving artificial intelligence.","1941-0018","","10.1109/TDSC.2024.3372777","National Key R&D Program of China(grant numbers:2023YFB2904000); National Natural Science Foundation of China(grant numbers:62172359,U20A20178,62072395); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LD24F020010); Fundamental Research Funds for the Central Universities(grant numbers:2021FZZX001-27); Hangzhou Leading Innovation and Entrepreneurship Team(grant numbers:TD2020003); National Science Foundation(grant numbers:CNS-2308730); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458329","Large language model (LLM);tool-using LLM agent;privacy;homomorphic encryption;  $t$  -Closeness","Privacy;Task analysis;Image color analysis;Cryptography;Data privacy;Data models;Forgery","","9","","100","IEEE","5 Mar 2024","","","IEEE","IEEE Journals"
"Evaluating Segmentation Accuracy with Diverse Prompt Strategies in Medsam","M. Nouman; G. Khoriba; E. A. Rashed","Graduate School of Information Science, University of Hyogo, Kobe, Japan; School of Information Technology and Computer Science, Nile University, Giza, Egypt; Graduate School of Information Science, University of Hyogo, Kobe, Japan",2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI),"12 May 2025","2025","","","1","5","Segment Anything Model (SAM) shows robustness in prompt-based segmentation of natural images. However, adapting SAM to the medical domain as a foundation model, MedSAM, presents unique challenges arising from clinical settings. This study evaluates MedSAM's performance across five medical imaging modalities (MRI, CT, ultrasound, endoscopy, and fundus IR) using four different prompting strategies: standard bounding boxes (SBB), rotated bounding boxes (RBB), contour points (CP), and simulated human annotations (SHA). MedSAM's performance is assessed under precise and perturbed conditions for each prompt type to evaluate its segmentation capabilities comprehensively. We empirically evaluate MedSAM's abilities and adaptability by experimenting with zero-shot learning and fine-tuning approaches. The results indicate that performance varies markedly with prompt type, accuracy, and imaging modality. The study concludes that MedSAM is sensitive to prompt precision and anatomical complexity. Fine-tuning improves performance, but further refinements are necessary to make it robust in various clinical settings. This work highlights the need for prompt engineering and provides practical insights to enhance segmentation accuracy in SAM-like foundation models.","1945-8452","979-8-3315-2052-6","10.1109/ISBI60581.2025.10980865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10980865","MedSAM;deep learning;medical image;prompt strategies","Image segmentation;Ethics;Accuracy;Ultrasonic imaging;Foundation models;Magnetic resonance imaging;Robustness;Prompt engineering;Standards;Biomedical imaging","","","","13","IEEE","12 May 2025","","","IEEE","IEEE Conferences"
"Integrated Localization and Control for Accurate Multi-Agent Formation","Y. Cai; Y. Shen","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",2018 IEEE International Conference on Communications (ICC),"30 Jul 2018","2018","","","1","6","High-accuracy formation is of great significance for multi- agent systems to perform complex tasks, and the accuracy of the formation is determined jointly by the network localization and formation control procedures. Existing studies commonly treat the two procedures separately and do not exploit an integrated design, leading to suboptimal formation performance. This paper establishes a general framework for high-accuracy multi-agent formation by integrated localization and control. In particular, we first propose a new metric called formation error to characterize the minimum squared distance between a real formation and a target one over arbitrary translation and rotation. Then we develop an integrated localization and control scheme to minimize the formation error. In the case study, we design the minimum mean formation error control algorithm along with a specific link selection strategy. Numerical results validate the performance gain of the integrated scheme over existing methods, and demonstrate effects of system parameters, which can serve as a guideline for practical system design.","1938-1883","978-1-5386-3180-5","10.1109/ICC.2018.8422184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8422184","","Estimation;Distance measurement;Multi-agent systems;Geometry;Trajectory;Task analysis","","3","","12","IEEE","30 Jul 2018","","","IEEE","IEEE Conferences"
"Setfit for Automated Essay Scoring: Extending Longformer to a Sentence Transformer","L. Krug; J. Bundeli; J. Meier; E. Nazarenko","Computer Science and Information Technology, Lucerne University of Applied Sciences and Arts, Luzern, Switzerland; Computer Science and Information Technology, Lucerne University of Applied Sciences and Arts, Luzern, Switzerland; Computer Science and Information Technology, Lucerne University of Applied Sciences and Arts, Luzern, Switzerland; Computer Science and Information Technology, Lucerne University of Applied Sciences and Arts, Luzern, Switzerland",2025 IEEE Swiss Conference on Data Science (SDS),"18 Jul 2025","2025","","","142","145","Automated Essay Scoring (AES) is a challenging task requiring models to evaluate writing quality with human-like consistency while being computationally efficient and scalable. Current approaches often rely on computationally intensive prompt engineering or resource-heavy transformer models, limiting their practical application. In this study, we present a novel prompt-free approach using SetFit for AES that achieves competitive accuracy while significantly reducing computational overhead. Unlike traditional transformer-based models such as DeBERTa, SetFit enables sentence transformer fine-tuning with contrastive learning, making it suitable for essay scoring even in low-data regimes. To handle full-length essays, we extended Longformer into a custom sentence transformer with a 4096-token context window, overcoming the 512-token limitation of standard transformer architectures. Our fine-tuned model, published on Hugging Face, has gained over $\mathbf{6, 0 0 0}$ downloads, indicating strong community interest in efficient, prompt-free approaches. Our results demonstrate that SetFit with an extended Longformer sentence transformer achieves competitive performance, making it a viable alternative to computationally expensive models. Beyond essay scoring, our approach shows promise for other longform text analysis tasks such as legal document review, research paper assessment, and content quality evaluation in educational contexts. This work contributes to the growing exploration of efficient NLP methods for educational assessment, offering a practical alternative to resource-intensive prompt-based solutions.","2835-3420","979-8-3315-9467-1","10.1109/SDS66131.2025.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081510","SetFit;Automated Essay Scoring (AES);Longformer;Sentence Transformers","Text analysis;Limiting;Reviews;Law;Computational modeling;Linguistics;Transformers;Prompt engineering;Standards;Faces","","","","15","IEEE","18 Jul 2025","","","IEEE","IEEE Conferences"
"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis","Y. Sun; D. Wu; Y. Xue; H. Liu; H. Wang; Z. Xu; X. Xie; Y. Liu","Nanyang Technological University Singapore, Singapore; Nanyang Technological University Singapore, Singapore; MetaTrust Labs Singapore, Singapore; East China Normal University, Shanghai, China; Xi'an Jiaotong University, Xi'an, China; Nanyang Technological University Singapore, Singapore; Singapore Management University Singapore, Singapore; Nanyang Technological University Singapore, Singapore",2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),"14 Jun 2024","2024","","","2048","2060","Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities. In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.","1558-1225","979-8-4007-0217-4","10.1145/3597503.3639117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549149","","Codes;Accuracy;Smart contracts;Computer bugs;Static analysis;Transformers;Logic","","23","","66","CCBY","14 Jun 2024","","","IEEE","IEEE Conferences"
"Multi-Agent System for Rogue Drone Interception","N. Souli; P. Kolios; G. Ellinas","Department of Electrical and Computer Engineering and the KIOS Center of Excellence, University of Cyprus, Nicosia, Cyprus; Department of Electrical and Computer Engineering and the KIOS Center of Excellence, University of Cyprus, Nicosia, Cyprus; Department of Electrical and Computer Engineering and the KIOS Center of Excellence, University of Cyprus, Nicosia, Cyprus",IEEE Robotics and Automation Letters,"6 Mar 2023","2023","8","4","2221","2228","A multitude of applications based on unmanned aircraft vehicles (UAVs), in conjunction with the UAVs' diminishing costs and increasing capabilities, present a major threat to public safety as well as critical infrastructure security, leading to the need for versatile and robust counter-drone solutions. This work presents a novel multi-agent jamming system, where a group of cooperative autonomous agents employ various algorithms (detection, tracking, jamming, and self-localization) to counter unauthorized drone operations. The proposed system employs a joint wireless jamming and cooperative positioning framework to best track and intercept the moving rogue drone, utilizing a relative positioning system based on signals of opportunity in conjunction with inertial and vision measurements. Software-defined radio technology is incorporated on the pursuer agent to achieve rogue drone GPS disruption, while at the same time the autonomous agents cooperate to compute the location estimate of the pursuer agent. In the presence of jamming, to improve the wireless communication performance of the autonomous agents, a cellular communication architecture is also used as an additional channel for control and information exchange. For evaluation purposes, a prototype multi-agent system is designed, implemented, and tested in a real-world environment to demonstrate its enhanced localization and jamming performance, when compared to single-agent approaches.","2377-3766","","10.1109/LRA.2023.3245412","European Union's Horizon 2020 research and innovation programme(grant numbers:739551); Deputy Ministry of Research, Innovation and Digital Policy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044959","Aerial systems: perception and autonomy;localization;multi-robot systems;sensor fusion","Drones;Jamming;Location awareness;Global Positioning System;Visualization;Prototypes;Global navigation satellite system","","17","","36","IEEE","15 Feb 2023","","","IEEE","IEEE Journals"
"SCAR: Power Side-Channel Analysis at RTL Level","A. Srivastava; S. Das; N. Choudhury; R. Psiakis; P. H. Silva; D. Pal; K. Basu","Department of Electrical and Computer Engineering, University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Dallas, Richardson, TX, USA; Secure Systems Research Center, Technology Innovation Institute, Abu Dhabi, United Arab Emirates; Secure Systems Research Center, Technology Innovation Institute, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, University of Illinois Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Texas at Dallas, Richardson, TX, USA",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,"22 May 2024","2024","32","6","1110","1123","Power side-channel (PSC) attacks exploit the dynamic power consumption of cryptographic operations to leak sensitive information about encryption hardware. Therefore, it is necessary to conduct a PSC analysis to assess the susceptibility of cryptographic systems and mitigate potential risks. Existing PSC analysis primarily focuses on postsilicon implementations, which are inflexible in addressing design flaws, leading to costly and time-consuming postfabrication design re-spins. Hence, presilicon PSC analysis is required for the early detection of vulnerabilities to improve design robustness. In this article, we introduce SCAR, a novel presilicon PSC analysis framework based on graph neural networks (GNNs). SCAR converts register-transfer level (RTL) designs of encryption hardware into control-data flow graphs (CDFGs) and use that to detect the design modules susceptible to side-channel leakage. Furthermore, we incorporate a deep-learning-based explainer in SCAR to generate quantifiable and human-accessible explanations of our detection and localization decisions. We have also developed a fortification component as a part of SCAR that uses large-language models (LLMs) to automatically generate and insert additional design code at the localized zone to shore up the side-channel leakage. When evaluated on popular encryption algorithms like advanced encryption standard (AES), RSA, and PRESENT, and postquantum cryptography (PQC) algorithms like Saber and CRYSTALS-Kyber, SCAR, achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall. Additionally, through explainability analysis, SCAR reduces features for GNN model training by 57% while maintaining comparable accuracy. We believe that SCAR will transform the security-critical hardware design cycle, resulting in faster design closure at a reduced design cost.","1557-9999","","10.1109/TVLSI.2024.3390601","Technology Innovation Institute (TII), Abu Dhabi, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508974","Graph neural network (GNN);large language model (LLM);power side-channel (PSC) attack;register-transfer level (RTL)","Encryption;Power demand;Graph neural networks;Feature extraction;Large language models;Side-channel attacks;Register-transfer level","","6","","49","IEEE","26 Apr 2024","","","IEEE","IEEE Journals"
"ReposVul: A Repository-Level High-Quality Vulnerability Dataset","X. Wang; R. Hu; C. Gao; X. -C. Wen; Y. Chen; Q. Liao","Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China",2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"20 Jun 2024","2024","","","472","483","Open-Source Software (OSS) vulnerabilities bring great challenges to the software security and pose potential risks to our society. Enormous efforts have been devoted into automated vulnerability detection, among which deep learning (DL)-based approaches have proven to be the most effective. However, the performance of the DL-based approaches generally relies on the quantity and quality of labeled data, and the current labeled data present the following limitations: (1) Tangled Patches: Developers may submit code changes unrelated to vulnerability fixes within patches, leading to tangled patches. (2) Lacking Inter-procedural Vulnerabilities: The existing vulnerability datasets typically contain function-level and file-level vulnerabilities, ignoring the relations between functions, thus rendering the approaches unable to detect the inter-procedural vulnerabilities. (3) Outdated Patches: The existing datasets usually contain outdated patches, which may bias the model during training. To address the above limitations, in this paper, we propose an automated data collection framework and construct the first repository-level high-quality vulnerability dataset named ReposVul. The proposed framework mainly contains three modules: (1) A vulnerability untangling module, aiming at distinguishing vulnerability-fixing related code changes from tangled patches, in which the Large Language Models (LLMs) and static analysis tools are jointly employed. (2) A multi-granularity dependency extraction module, aiming at capturing the inter-procedural call relationships of vulnerabilities, in which we construct multiple-granularity information for each vulnerability patch, including repository-level, file-level, function-level, and line-level. (3) A trace-based filtering module, aiming at filtering the outdated patches, which leverages the file path trace-based filter and commit time trace-based filter to construct an up-to-date dataset. The constructed repository-level ReposVul encompasses 6,134 CVE entries representing 236 CWE types across 1,491 projects and four programming languages. Thorough data analysis and manual checking demonstrate that ReposVul is high in quality and alleviates the problems of tangled and outdated patches in previous vulnerability datasets.","2574-1934","979-8-4007-0502-1","10.1145/3639478.3647634","National Key R&D Program of China(grant numbers:2022YFB3103900); National Natural Science Foundation of China(grant numbers:62002084); Natural Science Foundation of Guangdong Province(grant numbers:2023A1515011959); Shenzhen Basic Research(grant numbers:JCYJ20220531095214031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554864","Open-Source Software;Software Vulnerability Datasets;Data Quality","Computer languages;Data analysis;Codes;Source coding;Manuals;Static analysis;Data collection","","5","","73","","20 Jun 2024","","","IEEE","IEEE Conferences"
"ML-On-Rails: Safeguarding Machine Learning Models in Software Systems – A Case Study","H. Abdelkader; M. Abdelrazek; S. Barnett; J. -G. Schneider; P. Rani; R. Vasa","Deakin University, Australia; Deakin University, Australia; Deakin University, Australia; Monash University, Australia; RMIT University, Australia; Deakin University, Australia",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","178","183","Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in production.CCS CONCEPTS• Computing methodologies → Artificial intelligence; Machine learning; • General and reference → Reliability; Validation; Design.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556360","Robustness;Trustworthy-AI;Protocol","Protocols;Computational modeling;Machine learning;Production;Software systems;Robustness;Software reliability","","2","","49","","18 Jun 2024","","","IEEE","IEEE Conferences"
"ARCS-R: Mission Critical Combined Reliability and Cybersecurity Systems Engineering Analysis","D. L. Van Bossuyt; N. Papakonstantinou; B. Hale; R. Arlitt; S. R. Palatheerdham","Department of Systems Engineering, Naval Postgraduate School; Docent VTT Technical Research Centre of Finland; Department of Computer Science, Naval Postgraduate School; Masters in Cyber Security, University of South Brittany, Lorient; Masters in Cyber Security, University of South Brittany, Lorient",2024 Annual Reliability and Maintainability Symposium (RAMS),"18 Mar 2024","2024","","","1","8","This paper explores how reliability analysis and cyber-security analysis can be combined using Artificial Intelligence and Machine Learning (AI/ML), and Large Language Models (LLM) to produce a continuously updated resilience analysis. This is achieved by modeling both the hardware and software of the system, and employing LLMs and AI/ML to continuously search for new software vulnerabilities and feed that information into continuously updating resilience models. A case study of a drone is presented that demonstrates the promise of the proposed method. It is expected that using the proposed method, named Assessment for Risk in Cybersecurity and Safety - Resilience (ARCS-R), will reduce failure rate of mission-critical cyber-physical systems by reducing the likelihood of a potential initiating event causing a prolonged degradation in system performance that impacts system resilience.","2577-0993","979-8-3503-0769-6","10.1109/RAMS51492.2024.10457626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457626","Large Language Model;Artificial Intelligence;Machine Learning;LLM;AI;ML;Cyber-Physical System;Failure Rate;Resilience","System performance;Mission critical systems;Random access memory;Machine learning;Reliability engineering;Software;Software reliability","","1","","25","USGov","18 Mar 2024","","","IEEE","IEEE Conferences"
"Real-Time Handgun Detection Using Transformers on Nvidia Jetson AGX Xavier","L. A. Bustamante; J. C. Gutiérrez","Escuela profesional de Ciencia de la Computación, Universidad Nacional de San Agustín de Arequipa, Arequipa, Perú; Escuela profesional de Ciencia de la Computación, Universidad Nacional de San Agustín de Arequipa, Arequipa, Perú",2024 L Latin American Computer Conference (CLEI),"8 Oct 2024","2024","","","1","6","This study addresses the pressing need for enhanced security measures in Peru, where recent data from an INEI survey indicates that 25% of residents in major cities have fallen victim to crime. We present an innovative approach to bolstering public safety by employing Transformer neural networks for real-time firearm detection in surveillance cameras. Diverging from conventional methods such as YOLO, our research capitalizes on the capabilities of the Real-Time Detection Transformer (RT-DETR), which utilizes a Hybrid Encoder to facilitate real-time object detection without compromising accuracy. Our model, evaluated on Nvidia Jetson AGX Xavier, achieved a remarkable F1 score of 99.52 % at 38 frames per second, affirming the feasibility of deploying Transformer models on low-power embedded devices by implementing in CUDA. Our findings indicate that Transformer models have the potential to significantly enhance real-time threat detection and fortify urban security infrastructure, presenting a proactive solution to combat the rising challenge of firearm-related crimes. Source code available at https://github.com/labt1/GunDetection-RTDETR.","2771-5752","979-8-3315-4097-5","10.1109/CLEI64178.2024.10700426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10700426","Transformers;Object Detection;Nvidia Jetson;Real-Time;Gun detection;CUDA;Edge Computing","YOLO;Surveys;Weapons;Urban areas;Graphics processing units;Transformers;Real-time systems;Public security;Threat assessment;Security","","1","","29","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Classification and Time-Frequency Localization of Arbitrary LPWAN Signals With Radial Deformable DETR","C. Ho Kong; H. Hu","Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University, Hung Hom, Hong Kong",IEEE Access,"28 Mar 2025","2025","13","","53065","53083","With the increasing adoption of Internet-of-Things (IoT) technologies, numerous devices utilizing protocols such as Sigfox and LoRa are now widely available inexpensively and operate in unlicensed ISM bands. However, challenges such as inventory management, unauthorized usage, and network performance must be addressed. Future adoption of emerging IoT protocols with various modulation schemes, bandwidth, and data rates can further complicate this. Therefore, it is important not only to classify but also to localize the frequency, bandwidth, and time of these LPWAN signals on the air for management, security, or band planning purposes. SOTA algorithms usually look through the whole received signal on the time domain or frequency domain only to perform classification tasks, without finding out the corresponding time-frequency location of the signal. This paper proposes to classify and localize time-frequency locations of LPWAN signals by an enhanced version of Deformable DEtection TRansformer (Deformable DETR). We devise an attention radius suitable for processing Low Power Wide Area Network (LPWAN) Spectrogram traces extracted from Software Defined Radios (SDRs) IQ data with Short-Time Fourier Transform (STFT). Inspired by Large Language Models (LLMs), sequences of STFT vectors from SDR IQs can leverage attention mechanisms, and finding out LPWAN signals in spectrograms is similar to object detection tasks in computer vision. Our method eliminates the need for hand-crafting CNN layers or signal processing pipelines for different LPWAN protocols provided that sufficient training samples are available. Therefore, we build a fully annotated dataset for Lora and Sigfox in multiple frequencies, bandwidths, packet data, and time, as well as data augmentation techniques that serve both training and validation datasets for our modified Deformable DETR model. The experimental results demonstrate an average precision of over 89.5% for LoRa signals and over 79.8% when mixed with ultra-narrow-band signals.","2169-3536","","10.1109/ACCESS.2025.3554080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937698","Attention mechanisms;multiple signal classification;radio spectrum management;reconnaissance","Low-power wide area networks;Protocols;Time-frequency analysis;LoRa;Internet of Things;Bandwidth;Spectrogram;Radio frequency;Location awareness;Transformers","","","","39","CCBYNCND","24 Mar 2025","","","IEEE","IEEE Journals"
"DeRAG: Decentralized Multi-Source RAG System with Optimized Pyth Network","J. Yu; H. Sato","The University of Tokyo, Tokyo, Japan; National Institute of Informatics / The University of Tokyo, Tokyo, Japan",2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA),"20 Feb 2025","2024","","","106","115","Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, they face significant challenges in accessing up-to-date information and providing verifiable responses, particularly in domain-specific contexts. Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution, but they introduce new vulnerabilities related to data integrity, privacy, and centralization. This paper introduces DeRAG (Decentralized Multi-Source Retrieval-Augmented Generation), a novel approach that leverages blockchain technology and advanced cryptographic techniques to address these challenges. DeRAG incorporates a decentralized multi-source data ecosystem, a RAG-Optimized Pyth Consensus protocol for data validation, and a domain-specific validator network structured as a Directed Acyclic Graph. The system also features an innovative RAG Processing Layer with a decentralized index structure based on Distributed Hash Tables and content-addressable storage. We present a comprehensive performance evaluation framework that assesses the system’s scalability, efficiency, output quality, and economic model effectiveness in a decentralized context. Results demonstrate that DeRAG significantly enhances the security, efficiency, and privacy of RAG systems, paving the way for more robust and reliable RAG applications in information retrieval and generation.","2158-9208","979-8-3315-0971-2","10.1109/ISPA63168.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10885343","Large Language Model;Context-Aware Reasoning;Decentralized Oracle;Retrieval-Augmented Generation","Performance evaluation;Privacy;Scalability;Retrieval augmented generation;Ecosystems;Natural language processing;Consensus protocol;Indexes;Reliability;Optimization","","","","21","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Transparent use-case management for AIoT devices","J. C. Will; H. Homann","Faculty of Electrical Engineering, Hannover University of Applied Sciences and Arts, Germany; Faculty of Electrical Engineering, Hannover University of Applied Sciences and Arts, Germany","2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)","18 Oct 2024","2024","","","498","503","This paper presents an understandable approach to managing use-cases for Artificial Intelligence of Things (AIoT) devices in smart home environments, with a focus on balancing user privacy, functionality, and convenience. Recognizing the growing integration of smart home technologies and AI, particularly in aiding elderly individuals, we propose a transparent, user-centric model that allows individual users to control the extent of data transmission and processing. We categorize users into three distinct groups: privacy-oriented users, who prefer local data processing; convenience-oriented users, who favor cloud-based AI services for natural language processing; and technically-informed users, who desire a transparent choice between local and cloud-based processing.(p)(/p)We demonstrate the feasibility of this approach with a smart home assistant prototype, showcasing the flexibility in use-case management and its potential to cater to diverse user preferences. Our model employs a RaspberryPi-based smart home assistant with sensor integration, capable of both local processing as well as interfacing with cloud-based Large Language Models (LLMs) like ChatGPT for complex query handling. The system architecture supports MQTT protocol for IoT device communication and ensures data security through robust encryption protocols. (p)(/p)This work contributes to the discourse on smart home technologies by offering a framework that respects user autonomy in data privacy while harnessing the benefits of AI, thereby enhancing the acceptance and effectiveness of smart home systems in modern living environments.","2576-3555","979-8-3503-7397-4","10.1109/CoDIT62066.2024.10708374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10708374","","Cloud computing;Privacy;Protocols;Systems architecture;Prototypes;Process control;Smart homes;Internet of Things;Artificial intelligence;Older adults","","","","20","IEEE","18 Oct 2024","","","IEEE","IEEE Conferences"
"Continuous GNN-Based Anomaly Detection on Edge Using Efficient Adaptive Knowledge Graph Learning","S. Yun; R. Masukawa; W. Y. Chung; M. Na; N. D. Bastian; M. Imani","University of California, Irvine, CA, USA; University of California, Irvine, CA, USA; University of California, Irvine, CA, USA; Kookmin University, Seoul, Republic of Korea; United States Military Academy, West Point, NY, USA; University of California, Irvine, CA, USA","2025 Design, Automation & Test in Europe Conference (DATE)","21 May 2025","2025","","","1","7","The increasing demand for robust security solutions across various industries has made Video Anomaly Detection (VAD) a critical task in applications such as intelligent surveillance, evidence investigation, and violence detection. Traditional approaches to VAD often rely on finetuning large pre-trained models, which can be computationally expensive and impractical for real-time or resource-constrained environments. To address this, MissionGNN introduced a more efficient method by training a graph neural network (GNN) using a fixed knowledge graph (KG) derived from large language models (LLMs) like GPT-4. While this approach demonstrated significant efficiency in computational power and memory, it faces limitations in dynamic environments where frequent updates to the KG are necessary due to evolving behavior trends and shifting data patterns. These updates typically require cloud-based computation, posing challenges for edge computing applications. In this paper, we propose a novel framework that facilitates continuous KG adaptation directly on edge devices, overcoming the limitations of cloud dependency. Our method dynamically modifies the KG through a three-phase process: pruning, alternating, and creating nodes, enabling real-time adaptation to changing data trends. This continuous learning approach enhances the robustness of anomaly detection models, making them more suitable for deployment in dynamic and resource-constrained environments.","1558-1101","978-3-9826741-0-0","10.23919/DATE64628.2025.10993194","National Science Foundation(grant numbers:2127780,2319198,2321840,2312517,2235472); Semiconductor Research Corporation; Army Research Office(grant numbers:W911NF2410360); U.S. Army Combat Capabilities Development Command; Army Research Laboratory(grant numbers:USMA 21050); Air Force Office of Scientific Research(grant numbers:FA9550-22-1-0253); U.S. Military Academy; U.S. Army; U.S. Department of Defense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10993194","Video Anomaly Detection;Graph Neural Networks;Knowledge Graph;Edge Computing;Continuous Learning;Adaptive Knowledge Graphs","Training;Image edge detection;Knowledge graphs;Streaming media;Market research;Real-time systems;Graph neural networks;Computational efficiency;Anomaly detection;Edge computing","","","","34","","21 May 2025","","","IEEE","IEEE Conferences"
"Automatically Generating Rules of Malicious Software Packages via Large Language Model","X. Zhang; X. Du; H. Chen; Y. He; W. Niu; Q. Li","School of Cyberspace Security, Beijing Jiaotong University, China; School of Cyberspace Security, Beijing Jiaotong University, China; School of Cyberspace Security, Beijing Jiaotong University, China; School of Cyberspace Security, Beijing Jiaotong University, China; School of Cyberspace Security, Beijing Jiaotong University, China; School of Cyberspace Security, Beijing Jiaotong University, China",2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),"11 Jul 2025","2025","","","734","747","Today’s security tools predominantly rely on predefined rules crafted by experts, making them poorly adapted to the emergence of software supply chain attacks. To tackle this limitation, we propose a novel tool, RuleLLM, which leverages large language models (LLMs) to automate rule generation for OSS ecosystems. RuleLLM extracts metadata and code snippets from malware as its input, producing YARA and Semgrep rules that can be directly deployed in software development. Specifically, the rule generation task involves three subtasks: crafting rules, refining rules, and aligning rules. To validate RuleLLM’s effectiveness, we implemented a prototype system and conducted experiments on the dataset of 1,633 malicious packages. The results are promising—RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a precision of 85.2% and a recall of 91.8%, outperforming state-of-the-art (SOTA) tools and scored-based approaches. We further analyzed generated rules and proposed a rule taxonomy: 11 categories and 38 subcategories.","2158-3927","979-8-3315-1201-9","10.1109/DSN64029.2025.00072","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11068806","open source software;malware;rule generation;yara;semgrep","Codes;Large language models;Ecosystems;Taxonomy;Supply chains;Prototypes;Metadata;Malware;Security;Software development management","","","","71","IEEE","11 Jul 2025","","","IEEE","IEEE Conferences"
"Federated Service for Semantic Misalignment in Supply–Demand Matching","S. Wang; R. Qin; J. Li; X. Liang; F. -Y. Wang","Faculty of Innovation Engineering, Macau University of Science and Technology, Macao, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Chinese Economics and Social Security, The University of Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Computational Social Systems,"","2025","PP","99","1","10","As digital transformation accelerates, data has become a core driver of technological innovation and economic growth. However, a key challenge in data utilization is the semantic misalignment between data supply and the demands of business scenarios. This misalignment significantly hinders efficient data flow and collaborative utilization. To address this issue, this article proposes a federated service solution integrating blockchain and decentralized autonomous organizations and operations (DAOs), large language models (LLMs) and scenarios engineering, federated learning and edge computing, as well as encryption technologies and privacy-computing. A five-layer federated service framework is introduced, consisting of the foundation layer, the data-scenario layer, the semantic coordination layer, the incentive-security layer, and the application layer, which is designed to ensure efficient and context-aware data supply–demand matching while preserving privacy and scalability. Moreover, the core mechanisms for semantic coordination are proposed, and a detailed solution process for resolving semantic misalignment with these mechanisms, as well as an illustrative example, is also presented. The proposed federated service framework offers an effective solution to semantic misalignment in supply–demand matching, fostering seamless data collaboration across diverse business scenarios. This work provides an intelligent service paradigm that leverages distributed data co-governance to address semantic challenges in the digital economy.","2329-924X","","10.1109/TCSS.2025.3581336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11081886","Blockchain;data supply–demand matching;decentralized autonomous organizations and operations (DAOs);federated service;semantic misalignment","Semantics;Data models;Collaboration;Data privacy;Technological innovation;Supply and demand;Industries;Uncertainty;Measurement;Hands","","","","","IEEE","16 Jul 2025","","","IEEE","IEEE Early Access Articles"
"Ghosts in the Markup: Techniques to Fight Large Language Model-Powered Web Scrapers","W. Brach; M. Petrik; K. Košt’ál; M. Ries","Faculty of Informatics and Information Technologies, Slovak Technical University, Bratislava, Slovakia; Faculty of Informatics and Information Technologies, Slovak Technical University, Bratislava, Slovakia; Faculty of Informatics and Information Technologies, Slovak Technical University, Bratislava, Slovakia; Faculty of Informatics and Information Technologies, Slovak Technical University, Bratislava, Slovakia",2025 37th Conference of Open Innovations Association (FRUCT),"23 May 2025","2025","","","37","46","The rise of large language models (LLMs) has revolutionized web scraping, creating new challenges for website owners seeking to protect valuable content. Traditional defenses such as CAPTCHAs and rate limiting are insufficient against sophisticated AI-driven scraping systems. Current state-of-the-art approaches primarily rely on server-side implementations requiring significant computational resources, IP-based blocking mechanisms, and complex pattern recognition systems that struggle to distinguish between legitimate users and advanced AI scrapers. This paper presents a two-step client-side defense strategy that combines content obfuscation and defensive prompt injection. Our approach dynamically alters HTML structure by wrapping characters in span elements with randomized content and strategic CSS rules while simultaneously injecting carefully crafted prompts designed to manipulate LLM extraction behavior. Experimental evaluation on three diverse datasets demonstrates the effectiveness of this combined approach, achieving near-zero extraction rates (0.6% Exact Match, 0.030 ROUGE-L) for structured content while maintaining a seamless user experience. Performance analysis shows that our method increases the HTML document file size by approximately 1773%, compared to over 6593% for alternative techniques such as Content Overload, providing a practical balance between security and usability while keeping rendering times comparable to unprotected pages. This lightweight implementation integrates seamlessly with existing Web infrastructure without requiring significant computational resources, addressing key limitations of server-side defenses while providing robust protection against unauthorized LLM-based content extraction. For further information, source code, and associated resources, please refer to the source code repository 1.","2305-7254","978-952-65246-3-4","10.23919/FRUCT65909.2025.11008269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008269","","Source coding;Large language models;Rendering (computer graphics);User experience;HTML;Security;Resource management;Data mining;Protection;Wrapping","","","","0","","23 May 2025","","","IEEE","IEEE Conferences"
"Human-Guided Artificial Intelligence (HGAI), a Framework for Overcoming AI's Blind Spots","Z. Chen; S. B. Kelil; M. Y. M. Naser; J. S. Metcalfe; S. Bhattacharya","Electrical & Computer Engineering Department, Kennesaw State University, Marietta, USA; Computer Science Department, Kennesaw State University, Marietta, USA; Electrical & Computer Engineering Department, Kennesaw State University, Marietta, USA; US DEVCOM Army Research Laboratory Aberdeen Proving Ground, Aberdeen, USA; Engineering Technology Department, Kennesaw State University, Marietta, USA",SoutheastCon 2025,"25 Apr 2025","2025","","","1417","1423","Artificial Intelligence (AI) systems frequently exhibit systematic blind spots, often referred to as hallucinations in Large Language Models (LLMs), posing risks in high-stakes applications such as autonomous systems, security, and military operations. This paper explores how human intuitive responses, along with conscious reasoning processes, can be integrated to mitigate AI blind spots and enhance decision-making effectiveness within Human-AI teams. We introduce Human-Guided Artificial Intelligence (HGAI) as a framework for achieving this goal. Specifically, we examine the role of System-1 intuitive processing, as captured through physiological signals such as electroencephalography (EEG) and electrocardiography (ECG), System-2 reasoning-based decision-making, and Multi-Modal Fusion (MMF) mechanisms while assessing the knowledge required to develop more reliable, context-aware, and ethically aligned intelligent decision-making systems for highly complex environments.","1558-058X","979-8-3315-0484-7","10.1109/SoutheastCon56624.2025.10971607","Kennesaw State University; US Army; DEVCOM; ARL; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10971607","Artificial Intelligence (AI);Human-AI Teams;Physiological Signals;Multi-Modal Fusion (MMF)","Systematics;Large language models;Decision making;Electrocardiography;Physiology;Electroencephalography;Reliability;Security;Artificial intelligence;Research and development","","","","41","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Quantum Computing in Cybersecurity and LLM","F. M. A. Pour; M. H. Arastoo","Faculty of Technology and Engineering, Behbahan Khatam-Alanbia University of Technology, Khuzestan, Iran; Faculty of Technology and Engineering, Azad University of Kashan, Kashan, Iran",2025 11th International Conference on Web Research (ICWR),"22 May 2025","2025","","","323","329","Quantum computing is fundamentally transforming the cybersecurity landscape by simultaneously introducing robust defense mechanisms and sophisticated attack vectors. This paper explores the dual nature of quantum computing in cybersecurity, examining how it enhances artificial intelligence-powered malware detection systems and strengthens encryption protocols, while also enabling the development of adaptive quantum-enhanced threats such as polymorphic malware and quantum key distribution attacks. Additionally, the paper analyzes the convergence of quantum computing with Large Language Models (LLMs) and neural networks, highlighting implications for threat intelligence across social media platforms, cybersecurity operations, and open-source intelligence. Through comprehensive analysis and the proposition of quantum-resilient security frameworks, this research provides cybersecurity professionals with actionable strategies for navigating the emerging quantum computing era.","2837-8296","979-8-3315-0891-3","10.1109/ICWR65219.2025.11006202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006202","Quantum Computing;LLM;Ransomware;QML;NLP;Neural Network","Quantum computing;Protocols;Social networking (online);Navigation;Neural networks;Vectors;Encryption;Quantum key distribution;Ransomware;Computer security","","","","25","IEEE","22 May 2025","","","IEEE","IEEE Conferences"
"Understanding ChatGPT: Impact Analysis and Path Forward for Teaching Computer Science and Engineering","P. Banerjee; A. K. Srivastava; D. A. Adjeroh; R. Reddy; N. Karimian","Lane Department of Computer Science and Electrical Engineering, Benjamin M. Statler College of Engineering and Mineral Resources, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, Benjamin M. Statler College of Engineering and Mineral Resources, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, Benjamin M. Statler College of Engineering and Mineral Resources, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, Benjamin M. Statler College of Engineering and Mineral Resources, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, Benjamin M. Statler College of Engineering and Mineral Resources, West Virginia University, Morgantown, WV, USA",IEEE Access,"27 Jan 2025","2025","13","","11049","11069","Large Language Models (LLMs) like ChatGPT have become the most popular regenerative AI applications, used for obtaining responses for queries in different domains. The responses of ChatGPT are already becoming mainstream and are challenging conventional methods of learning. This article focuses on the application of ChatGPT for academic instructional purposes in the field of computer engineering and related majors. The capability of ChatGPT for instructional purposes is evaluated based on the responses to different questions about these engineering streams. This article explores different opportunities (with use cases), that ChatGPT can provide in augmenting the learning experience. It also provides scenarios of limitations and modifying the evaluation process to prevent the use of ChatGPT, which may lead to an inaccurate dissemination of accepted facts. In this paper, common classroom problems and their respective responses from ChatGPT in the domains of Computer Science, Cyber Security, Data Science, and Electrical Engineering are analyzed to determine the categories of queries for which ChatGPT offers reliable responses and those for which it may be factually incorrect. A student survey is performed to demonstrate that students must be made aware that ChatGPT may not be suitable for certain types of queries and means of upgrading the evaluation process.","2169-3536","","10.1109/ACCESS.2024.3524102","U.S. National Science Foundation and the U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10833612","ChatGPT;education;LLM;computer science and engineering;electrical engineering","Chatbots;Artificial intelligence;Measurement;Education;Computer science;Writing;Object recognition;Electrical engineering;Translation;Robot sensing systems","","","","55","CCBY","8 Jan 2025","","","IEEE","IEEE Journals"
"Standardisation of AML/CFT Regulatory Frameworks using Natural Language Processing","C. Ngwu; Z. Jaroucheh; N. Pitropakis","School of Computing, Engineering & the Built Environment, Edinburgh Napier University, United Kingdom; School of Computing, Engineering & the Built Environment, Edinburgh Napier University, United Kingdom; School of Computing, Engineering & the Built Environment, Edinburgh Napier University, United Kingdom",2024 6th International Conference on Blockchain Computing and Applications (BCCA),"22 Jan 2025","2024","","","332","339","With the G-20 prioritising the development of a solution that will address the challenges of cross-border payment, a lot of focus and research has been exploring ways of using a decentralised ledger technology (DLT) solution to address this challenge. Some of the issues that have been identified with the DLT option are interoperability, scalability, security, and privacy. Central Bank Digital Currencies (CBDCs) implementing DLT have also been acknowledged by experts as having the potential to address some of these challenges. Given the focus on CBDCs as an alternative technology that could address the shortcomings of traditional cross-border payment systems, some of the design challenges that are impacting their widespread adoption have come to the fore. Considering that early CBDC projects were in reaction to emerging digital currencies and therefore were meant to address the domestic needs of each implementing jurisdiction, the lack of a common development framework for developing the technology contributed to the challenge of the interoperability of the various CBDCs across jurisdictions and has impacted it’s fullscale adoption as an alternative to existing cross-border payment systems. With specific focus on regulatory frameworks used by various jurisdictions and also an identified challenge of CBDCs, this paper explores Customer Due Diligence (CDD) regulation as a mechanism for combating anti-money laundering (AML) and counter-terrorist financing (CFT). As part of the efforts to adapt cross-border payment system into a world of distributed ledgers relying on smart contracts, it reviews the impact of divergent regulatory frameworks that limit the interoperability of CBDCs. It discusses some of the existing techniques in place for conducting CDD, such as Know Your Customer (KYC) and how they fit with the new technology. It concludes with a proposal on how the incongruence of legal frameworks and AML/CFT regulations, can be addressed and standardised using new technologies such as Large Language Models (LLMs).","","979-8-3503-5153-8","10.1109/BCCA62388.2024.10844458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10844458","Blockchain;AML/CFT;NLP;AI;KYC","Text analysis;Distributed ledger;Scalability;Semantics;Smart contracts;Natural language processing;Regulation;Security;Zero knowledge proof;Interoperability","","","","30","IEEE","22 Jan 2025","","","IEEE","IEEE Conferences"
"IoT Vulnerability Detection using Featureless LLM CyBert Model","S. Binhulayyil; S. Li; N. Saxena","School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK; School of Computer Science and Informatics, Cardiff University, Cardiff, UK","2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","4 Apr 2025","2024","","","2474","2480","This work aims to leverage large language models (LLMs) and a featureless approach to effectively detect vulnerabilities in Internet of Things (IoT) network traffic. By directly learning from the Ripple20 dataset, a featureless LLM model, CyBERT, was designed that can efficiently distinguish between secure and vulnerable IoT devices without relying on handcrafted features. This LLM-based classifier could be instrumental in identifying IoT networks that pose potential threats to other connected devices by uncovering critical vulnerabilities. The experimental results demonstrate the exceptional capabilities of the featureless CyBERT model, which achieves high accuracy, precision, recall, and F1 score in detecting zero-day vulnerabilities. Moreover, the model significantly outperforms traditional methods in terms of detection speed. These results have profound implications for the future of IoT security, paving the way for real-time threat and attack detection.","2324-9013","979-8-3315-0620-9","10.1109/TrustCom63139.2024.00343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10945147","","Training;Privacy;Accuracy;Telecommunication traffic;Feature extraction;Real-time systems;Threat assessment;Internet of Things;Security;Object recognition","","","","18","IEEE","4 Apr 2025","","","IEEE","IEEE Conferences"
"Adversarial Training for Robustness Enhancement in LLM-Based Code Vulnerability Detection","Y. Zhao; X. Guan","Software Evaluation Center, North China Institute of Computing Technology, Beijing, China; Software Evaluation Center, North China Institute of Computing Technology, Beijing, China","2025 IEEE 7th International Conference on Communications, Information System and Computer Engineering (CISCE)","10 Jul 2025","2025","","","1147","1152","This paper proposes a new approach to code vulnerability detection that uses large language models (LLMs) and incorporates adversarial training techniques. It aims to enhance the robustness of the vulnerability detection model and improve the performance of vulnerability detection. The effect of adversarial training on model performance enhancement is explored through experimental validation. It is proved experimentally that the method proposed we propose can significantly improve the robustness and accuracy and of the vulnerability detection model. It provides effective technical support for security problems in software development.","2833-2423","979-8-3315-0161-7","10.1109/CISCE65916.2025.11065803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11065803","code vulnerability detection;adversarial training;LLM;robustness","Training;Codes;Accuracy;Large language models;Data processing;Robustness;Computer security;Software development management;Information systems","","","","15","IEEE","10 Jul 2025","","","IEEE","IEEE Conferences"
"Artificial Intelligence and Ethics","C. Brooks",Georgetown University,"Inside Cyber: How AI, 5G, IoT, and Quantum Computing Will Transform Privacy and Our Security","","2025","","","119","139","Summary <p>Governments, businesses, civil society organizations, and individuals are among the stakeholders who have to guarantee the development and application of ethical artificial intelligence (AI). The goal of ethical considerations is to keep large language models from participating in improper or unethical acts. This chapter presents the guiding elements of James Conner's AI ethical framework: transparency and explainability, bias mitigation, privacy protection, security, human‐centered design, inclusivity, and regulatory compliance. Two areas where Department of Justice will focus its AI enforcement efforts: election security and national security. The Department of Defense's description of the chief digital and artificial intelligence office (CDAO) is a useful example for government agencies looking to get insight into the significance of CDOs and how they will develop in their functions in light of future technologies like AI: CDAO Mission and CDAO Vision.</p>","","9781394254965","10.1002/9781394310562.ch16","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10951936.pdf&bkn=10950206&pdfType=chapter","","Artificial intelligence;Ethics;Security;Privacy;Law;Intellectual property;Computer vision;Regulation;Data models;Transforms","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Research on Automatic Government Data Cataloging System Based on Data Architecture and Local Large Language Model: A Case Study of Suining City","X. Zheng; F. Miao; F. Yi; J. Zhuo; B. Wen; R. Huang","School of Architecture and Civil Engineering, Suining Data Element Laboratory Chengdu University, Chengdu; The Big Data Research Institute, Suining Data Element Laboratory Chengdu University, Chengdu; The Big Data Research Institute, Suining Data Element Laboratory Chengdu University, Chengdu; The Big Data Research Institute, Suining Data Element Laboratory Chengdu University, Chengdu; The Big Data Research Institute, Suining Data Element Laboratory Chengdu University, Chengdu; The Big Data Research Institute, Chengdu University, Chengdu",2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD),"21 Jul 2025","2025","","","480","485","With the in-depth advancement of city-wide digital transformation, government departments generate massive data that is challenging to inventory and manage effectively. Traditional cataloging systems rely heavily on manual input—a time-consuming, error-prone approach that cannot adapt to dynamic data environments. Additionally, data privacy and security constraints often preclude the use of cloud-based solutions, and there is a dearth of specialized tools to automate the cataloging process. To address these issues, this paper proposes an automatic data cataloging method based on a ""One Body, Two Wings"" Data Architecture (DA) and a local large language model (L3M). In this concept, the ""One Body"" refers to a unified data foundation, while the ""Two Wings"" denote intelligent governance and secure sharing. The proposed system automatically obtains metadata and integrates knowledge graphs with semantic classification to achieve automatic directory generation, label optimization, and dynamic catalog updates. Experimental results on three government systems in Suining City (urban management, citizen card, and cloud imaging) show that the classification accuracy reaches approximately 85%, significantly reducing the manual workload and labor costs. This work demonstrates the practical potential of deploying local large language models to obtain secure, efficient, and intelligent government data governance.","2769-3554","979-8-3315-1936-0","10.1109/ICAIBD64986.2025.11082084","Chengdu University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11082084","Data Architecture;Automatic Cataloging;Local Large Language Model;Semantic Classification;Data Governance","Large language models;Government;Urban areas;Semantics;Manuals;Knowledge graphs;Metadata;Data governance;Security;Optimization","","","","7","IEEE","21 Jul 2025","","","IEEE","IEEE Conferences"
"Contrastive Learning for API Aspect Analysis","G. M. Shahariar; T. Hasan; A. Iqbal; G. Uddin","Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; York University, Canada",2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE),"8 Nov 2023","2023","","","637","648","We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.","2643-1572","979-8-3503-2996-4","10.1109/ASE56229.2023.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298556","API aspects;Contrastive learning;Transformers;API review;Aspect detection;LIME","Training;Semantics;Documentation;Transformers;Linear programming;Performance analysis;Security","","2","","39","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"LLMScenario: Large Language Model Driven Scenario Generation","C. Chang; S. Wang; J. Zhang; J. Ge; L. Li","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","16 Oct 2024","2024","54","11","6581","6594","Scenario engineering plays a vital role in various Industry 5.0 applications. In the field of autonomous driving systems, driving scenario data are important for the training and testing of critical modules. However, the corner scenario cases are usually rare and necessary to be extended. Existing methods cannot handle the interpretation and reasoning of the generation process well, which reduces the reliability and usability of the generated scenarios. With the rapid development of Foundation Models, especially the large language model (LLM), we can conduct scenario generation via more powerful tools. In this article, we propose LLMScenario, a novel LLM-driven scenario generation framework, which is composed of scenario prompt engineering, LLM scenario generation, and evaluation feedback tuning. The minimum scenario description specific to LLM is given by scenario analysis and ablation studies. We also appropriately design the score functions in terms of reality and rarity to evaluate the generated scenarios. The model performance is further enhanced through chain-of-thoughts and experiences. Different LLMs are also compared with our framework. Experimental results on naturalistic datasets demonstrate the effectiveness of LLMScenario, which can provide solid support for scenario engineering in Industry 5.0.","2168-2232","","10.1109/TSMC.2024.3392930","National Key Research and Development Program of China(grant numbers:2021YFB2501200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529537","Large language model (LLM);scenario engineering;scenario generation","Scenario generation;Cognition;Autonomous vehicles;Tuning;Testing;Semantics;Task analysis","","19","","92","IEEE","13 May 2024","","","IEEE","IEEE Journals"
"Nothing Is Harder to Resist Than the Temptation of AI","A. Park; J. Kietzmann; J. Killoran; Y. Cui; P. v. Esch; A. Dabirian","University of Victoria, Victoria, BC, V8P 5C2, Canada; Gustavson School of Business, University of Victoria, Victoria, BC, V8P 5C2, Canada; Queen’s University, Kingston, ON, K7L 3N6, Canada; Auckland University of Technology, Auckland, New Zealand; Kennesaw State University, Kennesaw, GA, 30144, USA; California State University, Fullerton, CA, 92831, USA",IT Professional,"22 Jan 2024","2023","25","6","13","20","The use of generative AI has become increasingly prevalent in the business world. With the ability to create original content and automate certain tasks, businesses have been quick to adopt this technology. However, as with any emerging technology, there are potential pitfalls to be aware of. This article aims to review the current state of generative AI in business and highlight some of the potential risks associated with its use. Specifically, we examine issues such as pausing giant AI experiments, misinformation, data accuracy, process automation, shift of power, control of civilization, organizational security, and the potential for AI-generated content to deceive individuals. By bringing these concerns to the forefront, we hope to encourage a more thoughtful and cautious approach to the use of generative AI in business.","1941-045X","","10.1109/MITP.2023.3340529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411726","","Automation;Generative AI;Process control;Security;Task analysis;Business;Risk management;Performance evaluation;Data integrity;Information integrity","","5","","31","IEEE","22 Jan 2024","","","IEEE","IEEE Magazines"
"Opportunities and Challenges of Software Engineering Bots: A Forward-Looking Analysis","G. Melo","Toronto Metropolitan University, Toronto, Canada",2025 IEEE/ACM International Workshop on Bots in Software Engineering (BotSE),"26 Jun 2025","2025","","","28","32","The landscape of software engineering bots has evolved dramatically with the emergence of sophisticated AI-powered development tools and foundation models. This paper examines software engineering bots’ current state and near-future trajectory, highlighting key opportunities and challenges. Analyzing real-world implementations and surveys, we identify emerging patterns in bot adoption and critical considerations for the software engineering community. Our analysis indicates that while bots offer unprecedented code generation, review, and maintenance capabilities, achieving reliable integration and maintaining code quality present significant technical and organizational challenges.","","979-8-3315-2708-2","10.1109/BotSE67031.2025.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050025","bots;chatbots;artificial intelligence;human-machine collaboration;software engineering","Surveys;Codes;Reviews;Foundation models;Human-machine systems;Chatbots;Trajectory;Software reliability;Maintenance;Software engineering","","","","26","IEEE","26 Jun 2025","","","IEEE","IEEE Conferences"
